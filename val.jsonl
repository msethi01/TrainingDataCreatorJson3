{"instruction": "How do I note?", "response": "Note: While applying the clock-gating techniques, the tool considers generated clocks similar to defined clocks. The clock-gating cell selectively prevents clock edges, thus preventing the gated-clock signal from clocking the gated register. Figure 46 shows a latch-based clock-gating cell and the waveforms of the signals are shown with respect to the clock signal, CLK. Figure 46 Latch-Based Clock Gating Flip Flop Control Logic Register Bank D Q G Latch CLK ENCLK ENL EN CLK DATA IN DATA OUT CLK CLK EN ENL ENCLK The clock input to the register bank, ENCLK, is gated on or off by the AND gate. ENL is the enabling signal that controls the gating; it derives from the EN signal on the multiplexer shown in Figure 45. The register bank is triggered by the rising edge of the ENCLK signal. The latch prevents glitches on the EN signal from propagating to the register\u2019s clock pin. When the CLK input of the 2-input AND gate is at logic state 1, any glitching of the EN signal could, without the latch, propagate and corrupt the register clock signal. The latch eliminates this possibility because it blocks signal changes when the clock is at logic 1."}
{"instruction": "How do I 2.gsr -nets {vdd vss} ## enable power integrity flow set_app_options -name opt.common.power_integrity -value true performing voltage drop analysis note?", "response": "2.gsr -nets {VDD VSS} ## Enable power integrity flow set_app_options -name opt.common.power_integrity -value true Performing Voltage Drop Analysis Note: Before running the analyze_rail command to calculate voltage drops and current violations, ensure you have provided the input files required by each analysis mode. For more information about preparing input files, see Preparing Design and Input Data for Rail Analysis. Voltage drop analysis is supported in both RedHawk Fusion and RedHawk-SC Fusion analysis flows. To invoke the RedHawk/RedHawk-SC tool for calculating voltage drops of the specified power and ground networks, use the analyze_rail command with the following options: - Use the -voltage_drop option to specify the analysis mode. \u25e6 static: Performs static voltage drop analysis. \u25e6 dynamic: Performs dynamic voltage drop analysis. \u25e6 dynamic_vectorless: Performs dynamic voltage drop analysis without Verilog Change Dump (VCD) inputs. \u25e6 dynamic_vcd: Performs dynamic voltage drop analysis with VCD inputs. When enabled, use the -switching_activity option to specify a switching activity file. - Use the -nets option to specify the power and ground supply nets to analyze. The tool considers all the switched or internal power nets of the specified power nets in the analysis. You do not need to explicitly specify the internal power nets. - (Optional) Use the -switching_activity option to specify a switching activity file. The supported file formats are: VCD, SAIF, and ICC_ACTIVITY. When not specified, by default the tool uses the toggle rate for rail analysis."}
{"instruction": "How do I - to write errors to an ascii file, use the following script?", "response": "- To write errors to an ASCII file, use the following script: set fileName \"errors.txt\" set fd [open $fileName w] set dm [open_drc_error_data rail_pg_em.VDD.err] set all_errs [get_drc_errors -error_data $dm *] foreach_in_collection err $all_errs { set info [ get_attribute $err error_info]; puts $fd $info } close $fd"}
{"instruction": "How do I - to specify a name for the routing guide, use the -name option. - to specify the routing layers affected by the routing guide, use the -layers option. specify the routing layers by using the layer names from the technology file. - to create the routing guide in a different physical cell, use the -cell option. when you use this option, the tool creates the routing guide in the cell's reference block using the coordinate system of the cell's top-level block. see also - deriving routing guides - querying routing guides - removing routing guides using routing guides to control the routing direction you can use a routing guide to control the routing direction within the routing guide boundary, either by requiring routes to be in the preferred direction within the routing guide boundary or by switching the preferred direction within the routing guide boundary. - to force the router to route all nets in the preferred direction within the routing guide boundary, use the -preferred_direction_only option with the create_routing_guide command. you can use this type of routing guide to prevent wrong-way jog wires on specific layers. by default, this routing guide applies to all layers within the routing guide boundary. to require preferred direction routing only for specific layers, use the -layers option to specify the affected layers.?", "response": "- To specify a name for the routing guide, use the -name option. - To specify the routing layers affected by the routing guide, use the -layers option. Specify the routing layers by using the layer names from the technology file. - To create the routing guide in a different physical cell, use the -cell option. When you use this option, the tool creates the routing guide in the cell's reference block using the coordinate system of the cell's top-level block. See Also - Deriving Routing Guides - Querying Routing Guides - Removing Routing Guides Using Routing Guides to Control the Routing Direction You can use a routing guide to control the routing direction within the routing guide boundary, either by requiring routes to be in the preferred direction within the routing guide boundary or by switching the preferred direction within the routing guide boundary. - To force the router to route all nets in the preferred direction within the routing guide boundary, use the -preferred_direction_only option with the create_routing_guide command. You can use this type of routing guide to prevent wrong-way jog wires on specific layers. By default, this routing guide applies to all layers within the routing guide boundary. To require preferred direction routing only for specific layers, use the -layers option to specify the affected layers."}
{"instruction": "How do I note?", "response": "Note: Information about inverted registers is written to the verification guidance (.svf) file. You must include the .svf file in the Formality tool when verifying blocks with sequential output inversion. To control sequential output inversion for - The entire block, use the compile.seqmap.enable_output_inversion application option. The default is true. - Specific hierarchical or leaf cells, use the set_register_output_inversion command. An instance-specific sequential output inversion setting specified with the set_register_output_inversion command overrides a block-specific setting specified with the compile.seqmap.enable_output_inversion application option. The compile_fusion command allows the mapping of sequential elements in the design to sequential library cells whose output phase is inverted. In certain cases, the tool might infer a register that has one type of asynchronous control, but your library has the opposite type of pin. In such cases, the tool maps to the opposite type of register and inverts all the data inputs and outputs. Preventing Connections to Register QN Pins During sequential mapping, the tool can map registers to library cells that have both Q and QN pins, if doing so improves the QoR. However, you can prevent the tool from using the QN pin of such registers by setting the compile.seqmap.disable_qn_usage application option to true. Mapping of Synchronous Enable Logic to a MUX When the RTL contains synchronous enable logic, the compile_fusion command maps the enable logic to library cells with a synchronous enable pin. When the library cells with a synchronous enable pin are not available, the compile_fusion command maps the pushed-out synchronous logic to a MUX, as shown in this figure."}
{"instruction": "How do I 1. specify the programmable spare cell that a standard cell can map to by setting the same psc_type_id attribute setting on the corresponding library cell for the standard cell and the programmable spare cell. the following example specifies that \u25e6 the standard cells named buf1 and inv1 can map to the programmable spare cell named fill1x by setting the psc_type_id attribute to 1 for the corresponding library cells. \u25e6 the standard cells named nand2 and nor2 can map to the programmable spare cell named fill2x by setting the psc_type_id attribute to 2 for the corresponding library cells. fc_shell> set_attribute [get_lib_cells fill_lib/fill1x] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells stdcell_lib/buf1] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells stdcell_lib/inv1] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells fill_lib/fill2x] \\ psc_type_id 2 fc_shell> set_attribute [get_lib_cells stdcell_lib/nand2] \\ psc_type_id 2 fc_shell> set_attribute [get_lib_cells stdcell_lib/nor2] \\ psc_type_id 2 2. insert the programmable spare cells by using the create_stdcell_fillers command. the following example inserts programmable spare cells named fill1x and fill2x?", "response": "1. Specify the programmable spare cell that a standard cell can map to by setting the same psc_type_id attribute setting on the corresponding library cell for the standard cell and the programmable spare cell. The following example specifies that \u25e6 The standard cells named BUF1 and INV1 can map to the programmable spare cell named fill1x by setting the psc_type_id attribute to 1 for the corresponding library cells. \u25e6 The standard cells named NAND2 and NOR2 can map to the programmable spare cell named fill2x by setting the psc_type_id attribute to 2 for the corresponding library cells. fc_shell> set_attribute [get_lib_cells fill_lib/fill1x] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells stdcell_lib/BUF1] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells stdcell_lib/INV1] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells fill_lib/fill2x] \\ psc_type_id 2 fc_shell> set_attribute [get_lib_cells stdcell_lib/NAND2] \\ psc_type_id 2 fc_shell> set_attribute [get_lib_cells stdcell_lib/NOR2] \\ psc_type_id 2 2. Insert the programmable spare cells by using the create_stdcell_fillers command. The following example inserts programmable spare cells named fill1x and fill2x: fc_shell> create_stdcell_fillers \\ -lib_cells {fill_lib/fill1x fill_lib/fill2x} During the ECO flow, the tool swaps an ECO cell with a programmable spare cell based on the psc_type_id attribute setting, cell width, and voltage area. If the tool removes an ECO cell from the design, it can reuse the corresponding programmable spare cell."}
{"instruction": "How do I - understanding library cell requirements for multiplexer optimization - implementing multiplexing logic from cell library understanding library cell requirements for multiplexer optimization the multiplexer optimization requires the presence of at least a 2?", "response": "- Understanding Library Cell Requirements for Multiplexer Optimization - Implementing Multiplexing Logic From Cell Library Understanding Library Cell Requirements for Multiplexer Optimization The multiplexer optimization requires the presence of at least a 2:1 multiplexer cell in the logic library. The inputs or outputs of this cell can be inverted. If a 2:1 multiplexer primitive cell does not exist in the library, you see the following warning message: Warning: Target library does not contain any 2-1 multiplexer. (OPT-853) An implementation of the MUX_OP cell from the target library is created, but it might not be the best implementation possible. All multiplexer cells in the target library can be used to construct the implementation of the MUX_OP cell except - Enabled multiplexer cells - Bused output multiplexer - Multiplexers larger than 32 : 1 For Fusion Compiler to make the best use of the multiplexer cells available in your logic library, recompile the library or obtain a library compiled with version V3.4a or later from your ASIC vendor. Implementing Multiplexing Logic From Cell Library During logic synthesis, the tool implements multiplexing logic from the cell library using one of the following methods: - AND, OR, and INV gates - MUX gates The tool evaluates the area effect of both methods and selects the solution with the smallest area."}
{"instruction": "How do I - enabling advanced legalization algorithms - setting up for variant-aware legalization - checking if library cells are legally placeable minimizing large displacements during legalization to minimize large displacements of cells during legalization, enable - orientation optimization by setting the place.legalize.optimize_orientations application option to true. when you do so, the tool considers flipping the orientations of cells to reduce the displacements during legalization. - stream placement by setting the place.legalize.stream_place application option to true. when you do so, the tool moves many cells a small distance, to avoid moving a single cell a large distance during legalization. you can further control stream placement by using the place.legalize.stream_effort and place.legalize.stream_effort_limit application options. optimizing pin access during legalization for advanced technology nodes, to improve the routability of areas with high pin densities by redistributing the cells, enable pin optimization during legalization by setting the place.legalize.optimize_pin_access_using_cell_spacing application option to true. enabling advanced pg net checks for advanced technology nodes, such as 12 nanometer, 7 nanometer, or smaller technology nodes, you can enable advanced physical drc checks between cells and prerouted pg nets by setting the place.legalize.enable_advanced_prerouted_net_check application option to true. when you enable this feature, you can further control the accessibility checks performed for standard cell pins by using the place.legalize.advanced_layer_access_check and place.legalize.advanced_libpin_access_check application options. however, the default behavior of this feature is suitable for most designs.?", "response": "- Enabling Advanced Legalization Algorithms - Setting Up for Variant-Aware Legalization - Checking if Library Cells Are Legally Placeable Minimizing Large Displacements During Legalization To minimize large displacements of cells during legalization, enable - Orientation optimization by setting the place.legalize.optimize_orientations application option to true. When you do so, the tool considers flipping the orientations of cells to reduce the displacements during legalization. - Stream placement by setting the place.legalize.stream_place application option to true. When you do so, the tool moves many cells a small distance, to avoid moving a single cell a large distance during legalization. You can further control stream placement by using the place.legalize.stream_effort and place.legalize.stream_effort_limit application options. Optimizing Pin Access During Legalization For advanced technology nodes, to improve the routability of areas with high pin densities by redistributing the cells, enable pin optimization during legalization by setting the place.legalize.optimize_pin_access_using_cell_spacing application option to true. Enabling Advanced PG Net Checks For advanced technology nodes, such as 12 nanometer, 7 nanometer, or smaller technology nodes, you can enable advanced physical DRC checks between cells and prerouted PG nets by setting the place.legalize.enable_advanced_prerouted_net_check application option to true. When you enable this feature, you can further control the accessibility checks performed for standard cell pins by using the place.legalize.advanced_layer_access_check and place.legalize.advanced_libpin_access_check application options. However, the default behavior of this feature is suitable for most designs."}
{"instruction": "How do I 0.2 microns specifies the minimum spacing between the macros specified in the mx_excluded_cells parameter and fill shapes. mx_excluded_cells {} specifies the macros that use the spacing defined in the mx_excluded_cells_oversize_value parameter. vx_iso_via_distance 3.0 microns specifies the maximum distance within which a neighboring via must exist. vx_min_spacing minspacing 9 specifies the minimum spacing between vias. be careful when changing the value of these parameters. a value smaller than the default can cause drc violations, while a value larger than the default negatively impacts the via density. vx_per_metal 1 specifies the maximum number of via fill shapes contained in a metal fill shape. you can use this parameter to increase the via density. however, changing the default can cause many metal fill shapes to be connected to each other, which can increase the overall capacitance for the nearby signal nets. exclude layer parameters?", "response": "0.2 microns Specifies the minimum spacing between the macros specified in the mx_EXCLUDED_CELLS parameter and fill shapes. mx_EXCLUDED_CELLS {} Specifies the macros that use the spacing defined in the mx_EXCLUDED_CELLS_OVERSIZE_VALUE parameter. vx_iso_via_distance 3.0 microns Specifies the maximum distance within which a neighboring via must exist. vx_min_spacing minSpacing 9 Specifies the minimum spacing between vias. Be careful when changing the value of these parameters. A value smaller than the default can cause DRC violations, while a value larger than the default negatively impacts the via density. vx_per_metal 1 Specifies the maximum number of via fill shapes contained in a metal fill shape. You can use this parameter to increase the via density. However, changing the default can cause many metal fill shapes to be connected to each other, which can increase the overall capacitance for the nearby signal nets. Exclude layer parameters"}
{"instruction": "How do I 189. the hierarchical synthesis feasibility flow uses the following block-level information generated post compile_fusion?", "response": "189. The hierarchical synthesis feasibility flow uses the following block-level information generated post compile_fusion: - Floorplanning information to generate the ETM in step 2 - Power intent and test model (CTL) information for top-level synthesis in step 4"}
{"instruction": "How do I 1. read in the full chip design and apply constraints as shown in the following example?", "response": "1. Read in the full chip design and apply constraints as shown in the following example: fc_shell> set REF_LIBS \"stdcell.ndm macro.ndm\" fc_shell> create_lib TOP -technology techfile \\ -ref_libs $REF_LIBS fc_shell> analyze -format verilog $rtl_files fc_shell> elaborate TOP fc_shell> set_top_module TOP fc_shell> load_upf fullchip.upf fc_shell> read_sdc fullchip.sdc 2. For the design partitions, split the constraints as shown in the following example: fc_shell> set_budget_options -add_blocks {BLOCK1 BLOCK2} fc_shell> split_constraints 3. Create the subblock design libraries with design information and enable block-specific reference library setup as shown in the following example: fc_shell> copy_lib -to_lib BLOCK1.nlib -no_design fc_shell> copy_lib -to_lib BLOCK2.nlib -no_design fc_shell> set_attribute -objects BLOCK1.nlib \\"}
{"instruction": "How do I 2.3} {v1 v2 3.2}} specifying nondefault vias by default, when routing nets with nondefault routing rules, zroute selects vias based on the design rules. to specify the vias to use when routing nets with nondefault routing rules, use the create_routing_rule command. you can define the nondefault vias by using either the -cuts option or the -vias option. - when you use the -cuts option, the tool determines the suitable via definitions from the technology file based on the specification in the -cuts option and the rules defined in the technology file. - when you use the -vias option, you explicitly specify the nondefault via definitions, including the allowed cut numbers and rotation for each via definition. specifying nondefault vias using the -cuts option the syntax for specifying nondefault vias using the -cuts option is create_routing_rule rule_name -cuts { {cut_layer1 {cut_name1, ncuts} {cut_name2, ncuts} \u2026} {cut_layer2 {cut_name1, ncuts} {cut_name2, ncuts} \u2026} ... {cut_layern {cut_name1, ncuts} {cut_name2, ncuts} \u2026} } the cut_layer arguments refer to the via layer names in the technology file and the cut_name arguments refer to the cut names defined in the cutnametbl attribute in the associated layer section in the technology file. you can specify multiple cut names per layer. for each cut name, you must specify the minimum number of cuts, which must be an integer between 1 and 255.?", "response": "2.3} {V1 V2 3.2}} Specifying Nondefault Vias By default, when routing nets with nondefault routing rules, Zroute selects vias based on the design rules. To specify the vias to use when routing nets with nondefault routing rules, use the create_routing_rule command. You can define the nondefault vias by using either the -cuts option or the -vias option. - When you use the -cuts option, the tool determines the suitable via definitions from the technology file based on the specification in the -cuts option and the rules defined in the technology file. - When you use the -vias option, you explicitly specify the nondefault via definitions, including the allowed cut numbers and rotation for each via definition. Specifying Nondefault Vias Using the -cuts Option The syntax for specifying nondefault vias using the -cuts option is create_routing_rule rule_name -cuts { {cut_layer1 {cut_name1, ncuts} {cut_name2, ncuts} \u2026} {cut_layer2 {cut_name1, ncuts} {cut_name2, ncuts} \u2026} ... {cut_layern {cut_name1, ncuts} {cut_name2, ncuts} \u2026} } The cut_layer arguments refer to the via layer names in the technology file and the cut_name arguments refer to the cut names defined in the cutNameTbl attribute in the associated Layer section in the technology file. You can specify multiple cut names per layer. For each cut name, you must specify the minimum number of cuts, which must be an integer between 1 and 255."}
{"instruction": "How do I 157. all receivers (r) have a matched length constraint from the balanced driver (d). example 34 wire matching constraints for all receivers to driver d fc_shell> create_wire_matching -for [get_nets -physical_context $net] \\ -relative -tolerance 0.01 -driver [get_pins -physical_context d/o1] fc_shell> create_wire_matching -for [get_pins \\ -physical_context {r1/a r2/a r3/a}] -tolerance 0.01 -relative in example 35, the commands create wire matching constraints for the example in figure 158. example 35 wire matching constraints for receivers connected to driver d' fc_shell> create_wire_matching -for [get_nets -physical_context $net] \\ -tolerance 0.01 -relative -driver $driver_full_name fc_shell> create_wire_matching \\ -for [get_pins -physical_context -of_objects $net] \\ -exclude $driver_full_name -tolerance 0.01 -relative in example 36, the command creates a constraint group named abc for the nets and bundles that match the pattern pr*. an absolute tolerance of 10 um is set to match the length of each object in the constraint group. see figure 159. example 36 wire matching constraints for pr* nets fc_shell> create_wire_matching abc \\ -for [get_nets -physical_context pr*] \\ -tolerance 10 {abc} in example 37, the command creates a constraint group named def for the nets and bundles that match the pattern pr*. a tolerance of 10 percent is set to match the length of each object in the constraint group.?", "response": "157. All receivers (R) have a matched length constraint from the balanced driver (D). Example 34 Wire Matching Constraints for All Receivers to Driver D fc_shell> create_wire_matching -for [get_nets -physical_context $net] \\ -relative -tolerance 0.01 -driver [get_pins -physical_context D/o1] fc_shell> create_wire_matching -for [get_pins \\ -physical_context {R1/a R2/a R3/a}] -tolerance 0.01 -relative In Example 35, the commands create wire matching constraints for the example in Figure 158. Example 35 Wire Matching Constraints for Receivers Connected to Driver D' fc_shell> create_wire_matching -for [get_nets -physical_context $net] \\ -tolerance 0.01 -relative -driver $driver_full_name fc_shell> create_wire_matching \\ -for [get_pins -physical_context -of_objects $net] \\ -exclude $driver_full_name -tolerance 0.01 -relative In Example 36, the command creates a constraint group named abc for the nets and bundles that match the pattern pr*. An absolute tolerance of 10 um is set to match the length of each object in the constraint group. See Figure 159. Example 36 Wire Matching Constraints for pr* Nets fc_shell> create_wire_matching abc \\ -for [get_nets -physical_context pr*] \\ -tolerance 10 {abc} In Example 37, the command creates a constraint group named def for the nets and bundles that match the pattern pr*. A tolerance of 10 percent is set to match the length of each object in the constraint group."}
{"instruction": "How do I note?", "response": "Note: For a list of all the predefined lexical attributes, see the man page for the set_lib_cell_naming_convention command. After you specify the naming convention, perform lexical classification, by using the classify_lib_cell_attributes command as shown in the following example: fc_shell> classify_lib_cell_attributes -libset myLibset To report the lexical classification results, use the report_lib_cell_classification_attributes command. To report the library cells which are not lexical classified under the given naming convention, use the report_lexical_unclassified_lib_cells command. If there are lexically unclassified cells, specifies addition naming conventions and rerun lexical classification. 4. (Optional) Exclude specific cells from library analysis by using the set_use_for_library_analysis command. By default, the tool excludes physical-only cells during library analysis. However, you can exclude additional cells, such as library cells with a dont_use setting, as shown in the following example: fc_shell> set exclude_cells [filter_collection [get_lib_cells $libs/*] \\ \"dont_use == true\"] fc_shell> set_use_for_library_analysis $exclude_cells false To report the cells that are excluded, use the report_lexical_ignored_lib_cells command. 5. Identify all library cell families by using the identify_lib_cell_families command."}
{"instruction": "How do I - to enable crosstalk-driven mode, set the route.global.crosstalk_driven, route.track.crosstalk_driven, and time.si_enable_analysis application options to true. - to enable the fixing of soft drc violations after the final detail routing iteration, set the route.common.post_detail_route_fix_soft_violations application option to true. fc_shell> set_app_options \\ -name route.common.post_detail_route_fix_soft_violations \\ -value true shielding nets zroute shields routed nets net shieldingshielding nets by generating shielding wires that are based on the shielding widths and spacing defined in the shielding rules. in addition to shielding nets on the same layer, you also have the option to shield one layer above and one layer below. shielding above or below the layer is called coaxial shielding. figure 116 shows an example of coaxial shielding. coaxial shielding provides even better signal isolation than same-layer shielding, but it uses more routing resources. figure 116 coaxial shielding you can perform shielding either before or after signal routing. shielding before signal routing, which is referred to as preroute shielding, provides better shielding coverage but can result in congestion issues during signal routing. preroute shielding is typically used to shield critical clock nets. shielding after signal routing, which is referred to as postroute shielding, has a very minimal impact on routability, but provides less protection to the shielded nets.?", "response": "- To enable crosstalk-driven mode, set the route.global.crosstalk_driven, route.track.crosstalk_driven, and time.si_enable_analysis application options to true. - To enable the fixing of soft DRC violations after the final detail routing iteration, set the route.common.post_detail_route_fix_soft_violations application option to true. fc_shell> set_app_options \\ -name route.common.post_detail_route_fix_soft_violations \\ -value true Shielding Nets Zroute shields routed nets net shieldingshielding nets by generating shielding wires that are based on the shielding widths and spacing defined in the shielding rules. In addition to shielding nets on the same layer, you also have the option to shield one layer above and one layer below. Shielding above or below the layer is called coaxial shielding. Figure 116 shows an example of coaxial shielding. Coaxial shielding provides even better signal isolation than same-layer shielding, but it uses more routing resources. Figure 116 Coaxial Shielding You can perform shielding either before or after signal routing. Shielding before signal routing, which is referred to as preroute shielding, provides better shielding coverage but can result in congestion issues during signal routing. Preroute shielding is typically used to shield critical clock nets. Shielding after signal routing, which is referred to as postroute shielding, has a very minimal impact on routability, but provides less protection to the shielded nets."}
{"instruction": "How do I - detail route the clock mesh net. - have a circuit-level model for each of the gates in your clock tree and a transistor model for each of the transistors in the circuit-level models. - have access to a spice simulator such, as nanosim, finesim, or hspice.?", "response": "- Detail route the clock mesh net. - Have a circuit-level model for each of the gates in your clock tree and a transistor model for each of the transistors in the circuit-level models. - Have access to a SPICE simulator such, as NanoSim, FineSim, or HSPICE."}
{"instruction": "How do I - assigns tracks in horizontal partitions. - assigns tracks in vertical partitions. - reroutes overlapping wires. after track assignment finishes, all nets are routed but not very carefully. there are many violations, particularly where the routing connects to pins. detail routing works to correct those violations. note?", "response": "- Assigns tracks in horizontal partitions. - Assigns tracks in vertical partitions. - Reroutes overlapping wires. After track assignment finishes, all nets are routed but not very carefully. There are many violations, particularly where the routing connects to pins. Detail routing works to correct those violations. Note: Because track assignment replaces the global routes with actual metal shapes, the block no longer contains global routes after track assignment completes. By default, the route_track command is not timing-driven or crosstalk-driven. - To enable timing-driven mode, set the route.track.timing_driven application option. - To enable crosstalk-driven mode, set the route.track.crosstalk_driven and time.si_enable_analysis application options to true. At the end of track assignment, Zroute reports a summary of the wire length and via count. Example 20 shows a track assignment report. Example 20 Track Assignment Report Wire length and via report: --------------------------- Number of M1 wires: 215327 : 0 Number of M2 wires: 1124740 VIA12SQ_C: 1067462 ... Total number of wires: 2508734 vias: 2769482 Total M1 wire length: 924480.9 Total M2 wire length: 4147032.0 ... Total wire length: 21281278.0 Longest M1 wire length: 1541.7 Longest M2 wire length: 926.0 ..."}
{"instruction": "How do I note?", "response": "Note: These violations are also reported after each detail route iteration. The check_routes command saves the error data to a file named zroute.err. You cannot control the naming of the error data file generated by the check_routes command, but you can rename the error data file after running the command. To rename the error data file, use the following commands: fc_shell> write_drc_error_data -error_data zroute.err \\ -file_name export.err fc_shell> open_drc_error_data -file_name export.err fc_shell> attach_drc_error_data [get_drc_error_data export.err] \\ -name newname.err After you run the check_routes command, you can use the DRC query commands to get more information about the violations or use the error browser to examine the violations in the GUI. For information about analyzing the DRC violations, see Using the DRC Query Commands. For information about using the error browser, see the Fusion Compiler Graphical User Interface User Guide. After running check_routes, you can use the following command to run incremental detail routing that uses the check_routes results as input: fc_shell> route_detail -incremental true \\ -initial_drc_from_input true Note: Incremental detail routing does not fix open nets. To fix open nets, you must run ECO routing. For information about ECO routing, see Performing ECO Routing. Performing Signoff Design Rule Checking Signoff design rule checking runs the IC Validator tool within the Fusion Compiler tool to check the routing design rules defined in the foundry runset. To perform signoff design rule checking, run the signoff_check_drc command, as described in Performing Signoff Design Rule Checking. In addition, you can use the signoff_fix_drc command to automatically fix the DRC violations detected by the signoff_check_drc command. For more information, see Automatically Fixing Signoff DRC Violations. Note: An IC Validator license is required to run the signoff_check_drc and signoff_fix_drc commands."}
{"instruction": "How do I 2.0 fc_shell> set_app_options \\ -name place.coarse.ir_drop_default_target_low_percentage \\ -value 6.0 instead of using a percentage of the total number of cells to define the cell categories, you can use the cell voltage drop as a percentage of supply voltage by setting the place.coarse.ir_drop_default_target_high_population and place.coarse.ir_drop_default_target_low_population application options to false. they are true by default. the following example puts cells with a voltage drop larger than 8 percent of the supply voltage into the upper category and cells with a voltage drop between 4 to 8 percent of the supply voltage into the middle category?", "response": "2.0 fc_shell> set_app_options \\ -name place.coarse.ir_drop_default_target_low_percentage \\ -value 6.0 Instead of using a percentage of the total number of cells to define the cell categories, you can use the cell voltage drop as a percentage of supply voltage by setting the place.coarse.ir_drop_default_target_high_population and place.coarse.ir_drop_default_target_low_population application options to false. They are true by default. The following example puts cells with a voltage drop larger than 8 percent of the supply voltage into the upper category and cells with a voltage drop between 4 to 8 percent of the supply voltage into the middle category: fc_shell> set_app_options \\ -name place.coarse.ir_drop_default_target_high_population \\ -value false fc_shell> set_app_options \\ -name place.coarse.ir_drop_default_target_high_percentage \\ -value 8.0 fc_shell> set_app_options \\ -name place.coarse.ir_drop_default_target_low_population \\ -value false fc_shell> set_app_options \\ -name place.coarse.ir_drop_default_target_low_percentage \\ -value 4.0 You can specify a percentage of the total number of cells to define the upper category and cell voltage drop as a percentage of supply voltage to define the middle category, or vice versa. The following example puts the top 2 percent of the total cells in the upper category, and the next cells that have a voltage drop larger than 4 percent of supply voltage in the middle category: fc_shell> set_app_options \\ -name place.coarse.ir_drop_default_target_high_population \\ -value true fc_shell> set_app_options \\ -name place.coarse.ir_drop_default_target_high_percentage \\"}
{"instruction": "How do I - honors nondefault width rules for the wires on all layers of the via ladder to use the default width for wires on lower layers of the via ladder and honor the nondefault width only for the top layer, use the -ndr_on_top_layer_only true option. - does not require the via inserted above the pin layer to be contained within the pin shape to prevent changes to the pin shape boundary, you can require that the via inserted above the pin layer be fully contained within the pin shape boundary, which includes the pin shape and its extensions. to specify this requirement, use one or both of the following options?", "response": "- Honors nondefault width rules for the wires on all layers of the via ladder To use the default width for wires on lower layers of the via ladder and honor the nondefault width only for the top layer, use the -ndr_on_top_layer_only true option. - Does not require the via inserted above the pin layer to be contained within the pin shape To prevent changes to the pin shape boundary, you can require that the via inserted above the pin layer be fully contained within the pin shape boundary, which includes the pin shape and its extensions. To specify this requirement, use one or both of the following options: \u25e6 -connect_within_metal true This option controls via enclosures for all types of via ladders: performance, electromigration, and pattern-must-join. \u25e6 -connect_within_metal_for_via_ladder true This option controls the via enclosures only for performance and electromigration via ladders. By default, the tool uses the global setting specified by the -connect_within_metal option. To explicitly specify the behavior for performance and electromigration via ladders, set this option to true or false. - Does not extend the via enclosures to meet the minimum length rule for the layer To enable patching of the via enclosures to meet the minimum length rule and allow additional stacking of the via ladder, use the -allow_patching true option. - Uses staggering only when it is defined for the via ladder rule Via ladder staggering extends the row metal of a via ladder level in the preferred direction of the upper layer for the level to avoid obstructions above or near the pin, which can increase the success rate of via ladder insertion. In many cases, the via ladder rule specifies the maximum number of stagger tracks. To allow staggering if the maximum number of stagger tracks is not defined for the via ladder rule, use the -auto_stagger true option. - Does not report on the insertion status To report the via ladder insertion status for each target pin, use the -verbose true option."}
{"instruction": "How do I - all unplaced cells, use the -unplaced_cells option. - specific cells, use the -cells option. - only the cells that have changed due to eco operations, use the -eco_changed_cells option. a cell is considered changed if the eco_change_status attribute of the cell has one of the following values?", "response": "- All unplaced cells, use the -unplaced_cells option. - Specific cells, use the -cells option. - Only the cells that have changed due to ECO operations, use the -eco_changed_cells option. A cell is considered changed if the eco_change_status attribute of the cell has one of the following values: \u25e6 create_cell \u25e6 change_link \u25e6 add_buffer \u25e6 size_cell When you run the eco_netlist command, the tool automatically sets the eco_change_status attribute on the changed cells. If you update the design without using the eco_netlist command, you can manually set the eco_change_status attribute by using the set_attribute command. Controlling Placement When Using the place_eco_cells Command The place_eco_cells command places and legalizes each ECO cell based on the connectivity and the delays associated with the cell. You can control the placement of the ECO cells as follows: - To use the channels areas between macro cells for ECO placement, use the -channel_aware option. By default, the command avoids channel areas during ECO placement."}
{"instruction": "How do I - introduction to physical datapath with relative placement - relative placement flow - creating relative placement groups - adding objects to a group - specifying options for relative placement groups - changing the structures of relative placement groups - generating relative placement groups for clock sinks - performing placement and legalization of relative placement groups - analyzing relative placement groups - saving relative placement information - summary of relative placement commands introduction to physical datapath with relative placement relative placement is usually applied to datapaths and registers, but you can apply it to any cell in your design, controlling the exact relative placement topology of gate-level logic groups and defining the circuit layout. you can use relative placement to explore qor benefits, such as shorter wire lengths, reduced congestion, better timing, skew control, fewer vias, better yield, and lower dynamic and leakage power.?", "response": "- Introduction to Physical Datapath With Relative Placement - Relative Placement Flow - Creating Relative Placement Groups - Adding Objects to a Group - Specifying Options for Relative Placement Groups - Changing the Structures of Relative Placement Groups - Generating Relative Placement Groups for Clock Sinks - Performing Placement and Legalization of Relative Placement Groups - Analyzing Relative Placement Groups - Saving Relative Placement Information - Summary of Relative Placement Commands Introduction to Physical Datapath With Relative Placement Relative placement is usually applied to datapaths and registers, but you can apply it to any cell in your design, controlling the exact relative placement topology of gate-level logic groups and defining the circuit layout. You can use relative placement to explore QoR benefits, such as shorter wire lengths, reduced congestion, better timing, skew control, fewer vias, better yield, and lower dynamic and leakage power."}
{"instruction": "How do I 1. write the script to run on the blocks in the design. the following example performs the steps necessary to compile each subblock in the design. you can add additional commands to the script as needed to perform other operations. open_lib $block_libfilename open_block $block_refname.design commit_upf set_editability -from_level 1 -value false compile_fusion -from initial_map -to initial_map create_abstract -read_only create_frame save_block $block_refname -as $block_refname/initial_map compile_fusion -from logic_opto -to logic_opto create_abstract -read_only create_frame save_block $block_refname -as $block_refname/logical_opt compile_fusion -from initial_place -to initial_place create_abstract -read_only create_frame save_block $block_refname -as $block_refname/initial_place save_block save_lib close_block -force -purge close_lib 2. run the run_block_script command to process each block in parallel by using the script created in step 1. the following example specifies the settings for distributed processing with the set_host_options command and runs the block_synthesis.tcl synthesis script on the subblocks in parallel by using the load sharing facility (lsf). the command writes data and log files to the my_work_dir directory. fc_shell> set block_list [get_attribute \\ [get_cells -hierarchical -filter \"is_soft_macro==true\" \\ ref_full_name] fc_shell> set block_list [lsort -unique $block_list] fc_shell> set_host_options -name myhost -submit_command \"sh\" fc_shell> run_block_script \\ -host_options myhost \\?", "response": "1. Write the script to run on the blocks in the design. The following example performs the steps necessary to compile each subblock in the design. You can add additional commands to the script as needed to perform other operations. open_lib $block_libfilename open_block $block_refname.design commit_upf set_editability -from_level 1 -value false compile_fusion -from initial_map -to initial_map create_abstract -read_only create_frame save_block $block_refname -as $block_refname/initial_map compile_fusion -from logic_opto -to logic_opto create_abstract -read_only create_frame save_block $block_refname -as $block_refname/logical_opt compile_fusion -from initial_place -to initial_place create_abstract -read_only create_frame save_block $block_refname -as $block_refname/initial_place save_block save_lib close_block -force -purge close_lib 2. Run the run_block_script command to process each block in parallel by using the script created in step 1. The following example specifies the settings for distributed processing with the set_host_options command and runs the block_synthesis.tcl synthesis script on the subblocks in parallel by using the Load Sharing Facility (LSF). The command writes data and log files to the my_work_dir directory. fc_shell> set block_list [get_attribute \\ [get_cells -hierarchical -filter \"is_soft_macro==true\" \\ ref_full_name] fc_shell> set block_list [lsort -unique $block_list] fc_shell> set_host_options -name myhost -submit_command \"sh\" fc_shell> run_block_script \\ -host_options myhost \\"}
{"instruction": "How do I 1.nib set_ref_libs -add ../blk2/blk2.nib to report the reference libraries for the current design, use the report_ref_libs command. when you link a top-level design, if an abstract view for a lower-level block is available, the tool links to that abstract view by default. the default precedence of the different views used when linking lower-level blocks is as follows?", "response": "1.nib set_ref_libs -add ../BLK2/BLK2.nib To report the reference libraries for the current design, use the report_ref_libs command. When you link a top-level design, if an abstract view for a lower-level block is available, the tool links to that abstract view by default. The default precedence of the different views used when linking lower-level blocks is as follows: 1. Abstract view 2. Design view 3. Frame view 4. Outline view Changing the Block From Abstract to Design View To change a block from its abstract to its design view, use the change_abstract command and specify the view you are changing to by using the -view option. When you change a block from its abstract view to its design view, remove the existing constraints and apply full-chip timing constraints. The following example changes the BLK1 block from its abstract view to its design view and applies full-chip timing constraints. 1. Remove any existing timing constraints: fc_shell> remove_scenarios -all fc_shell> remove_modes -all fc_shell> remove_corners -all"}
{"instruction": "How do I 4. (optional) ignore specific scenarios during concurrent clock and data optimization as described in excluding specific scenarios. 5. (optional) ignore specific sinks during concurrent clock and data optimization as described in excluding specific sinks. 6. (optional) control the effort of timing optimization performed during concurrent clock and data optimization as described in controlling timing optimization effort. 7. (optional) control the effort of hold timing optimization performed during concurrent clock and data optimization as described in controlling hold time optimization effort. 8. (optional) control the adjustment of i/o clock latencies performed during concurrent clock and data optimization as described in controlling the adjustment of i/o clock latencies. 9. (optional) analyze the effects of concurrent clock and data optimization as described in reporting concurrent clock and data timing. controlling multibit optimization performed during the clock_opt command the following topics describe how you can control the multibit optimization performed during the clock_opt command?", "response": "4. (Optional) Ignore specific scenarios during concurrent clock and data optimization as described in Excluding Specific Scenarios. 5. (Optional) Ignore specific sinks during concurrent clock and data optimization as described in Excluding Specific Sinks. 6. (Optional) Control the effort of timing optimization performed during concurrent clock and data optimization as described in Controlling Timing Optimization Effort. 7. (Optional) Control the effort of hold timing optimization performed during concurrent clock and data optimization as described in Controlling Hold Time Optimization Effort. 8. (Optional) Control the adjustment of I/O clock latencies performed during concurrent clock and data optimization as described in Controlling the Adjustment of I/O Clock Latencies. 9. (Optional) Analyze the effects of concurrent clock and data optimization as described in Reporting Concurrent Clock and Data Timing. Controlling Multibit Optimization Performed During the clock_opt Command The following topics describe how you can control the multibit optimization performed during the clock_opt command: - Enabling the Rewiring of Mixed-Drive-Strength Multibit Cells - Enabling Post-Clock-Tree-Synthesis Multibit Debanking Enabling the Rewiring of Mixed-Drive-Strength Multibit Cells Some logic libraries have mixed-drive-strength multibit cells where some bits have a higher drive strength than others. For designs that use such cells, if a violating path goes through a lower-drive-strength bit, the tool can rewire the mixed-drive-strength multibit cell such that the violating path goes through a higher-drive-strength bit. To enable this feature for the clock_opt command, set the clock_opt.flow.enable_multibit_rewiring application option to true. Enabling Post-Clock-Tree-Synthesis Multibit Debanking To improve the timing QoR, the clock_opt command can debank multibit registers during the final_opt stage, if doing so does not introduce hold timing violations. To enable this feature, set the clock_opt.flow.enable_multibit_debanking application option to true before you run the final_opt stage of the clock_opt command. Performing Power or Area Recovery on the Clock Network If you enable concurrent clock and data optimization for the clock_opt command, the tool performs clock power recovery on clock cells and registers during the final_opt stage."}
{"instruction": "How do I - certain endpoints by specifying a file containing the endpoints by using the ccd.targeted_ccd_end_points_file application option, as shown in the following example?", "response": "- Certain endpoints by specifying a file containing the endpoints by using the ccd.targeted_ccd_end_points_file application option, as shown in the following example: fc_shell> set_app_options -name ccd.targeted_ccd_end_points_file \\ -value endpoint_targets.tcl If you specify both path groups and endpoints, the tool optimizes only the specified endpoints that are in the specified paths groups. - The worst 300 timing paths by setting the ccd.enable_top_wns_optimization application option to true Specifying Optimization Targets at the Postroute Stage At the final stages of your design flow, you can use the route_opt command to perform concurrent clock and data optimization on specific critical path groups or endpoints by using the following steps: 1. Enable targeted concurrent clock and data optimization for the route_opt command by setting the route_opt.flow.enable_targeted_ccd_wns_optimization application option to true. 2. Specify the optimization target by using one or both of the following settings: \u25e6 Specify the path groups to optimize by using the ccd.targeted_ccd_path_groups application option Note: If a path group you specify with this application option is also specified as a path group to skip by using the ccd.skip_path_groups application option, the path group is skipped during targeted concurrent clock and data optimization. \u25e6 Specify the endpoints to optimize by using the ccd.targeted_ccd_end_points_file application option You must specify at least one of these settings. If you specify both, the tool optimizes the specified endpoints of the specified paths groups. 3. (Optional) Specify the type of optimization to perform by using the ccd.targeted_ccd_select_optimization_moves application option. The valid values are"}
{"instruction": "How do I - shielding checks these checks detect possible issues with shielding by checking for the following conditions?", "response": "- Shielding checks These checks detect possible issues with shielding by checking for the following conditions: \u25e6 Signal net shapes with a shape_use attribute of shield_route. \u25e6 PG net shapes, which might be a PG strap or rail, but have a shape_use attribute of detail_route. \u25e6 Signal, clock, or PG nets that have a shielding nondefault rule but no associated shield shapes, which might be caused by inappropriate shape_use attributes. To enable these checks, use the -check_shield true option. - Via ladder checks These checks detect possible issues with via ladder insertion by checking for the following conditions: \u25e6 Library cell pins whose top-layer terminals have multiple pin shapes but do not have a pattern_must_join attribute This check applies only to signal and secondary PG pins. \u25e6 Missing performance or electromigration via ladder on a pin that has the is_em_via_ladder_required attribute \u25e6 Via ladder application options that do not have the required settings To enable these checks, use the -via_ladder true option. The check_routability command supports the following additional pin connection controls that apply to all pin access checks: - Pin access edges During frame view extraction, the tool annotates the frame views with information about the pin access edges. By default, the check_routability command ignores the pin access edges and allows pin connections at any point. You can restrict pin connections to the defined access edges by setting the -obey_access_edges option to true. You can further restrict pin connections to the access edge mark, which can be a narrow rectangle, a short line, or even a single point, by setting the -access_edge_whole_side option to true. In addition, the command can report unconnected pins that do not have an access edge defined. To enable this check, use the -report_no_access_edge true option. - Via rotation By default, pin connections can use rotated vias. To disallow pin connections that use rotated vias, set the -allow_via_rotation option to false."}
{"instruction": "How do I - place.legalize.enable_multi_cell_access_check?", "response": "- place.legalize.enable_multi_cell_access_check: considers impact on a cell's pin access due to presence of abutting neighbor cells and any prerouted net PG net shapes in the vicinity. Default is false. - place.legalize.enable_multi_cell_track_capacity_check: analyzes the track capacity of the cell's pins and abutting neighbor cells. Track capacity estimates the sufficiency of routing tracks in the area to access all the specified pins. To enable this application option, set the place.legalize.enable_multi_cell_access_check application option to true. Default is false."}
{"instruction": "How do I 2. route the block. during routing, zroute inserts the redundant vias and fixes hard design rule violations. in general, redundant via insertion has the same priority as other hard design rules; however, if design rule checking does not converge during detail routing, zroute automatically relaxes the redundant via constraints to improve drc convergence. preserving timing during redundant via insertion when you insert redundant vias, it changes the timing of your block. short nets tend to slow down due to increased capacitance, whereas long nets tend to speed up due to decreased resistance. zroute redundant via insertion has a timing-preservation mode that allows you to perform redundant via insertion without affecting the block timing by preventing insertion of redundant vias on critical nets. to enable timing-preservation mode for redundant via insertion, define the timing preservation constraints by using the following options of the add_redundant_vias command?", "response": "2. Route the block. During routing, Zroute inserts the redundant vias and fixes hard design rule violations. In general, redundant via insertion has the same priority as other hard design rules; however, if design rule checking does not converge during detail routing, Zroute automatically relaxes the redundant via constraints to improve DRC convergence. Preserving Timing During Redundant Via Insertion When you insert redundant vias, it changes the timing of your block. Short nets tend to slow down due to increased capacitance, whereas long nets tend to speed up due to decreased resistance. Zroute redundant via insertion has a timing-preservation mode that allows you to perform redundant via insertion without affecting the block timing by preventing insertion of redundant vias on critical nets. To enable timing-preservation mode for redundant via insertion, define the timing preservation constraints by using the following options of the add_redundant_vias command: - -timing_preserve_setup_slack_threshold - -timing_preserve_hold_slack_threshold - -timing_preserve_nets You should timing-preservation mode only at the end of the flow, after using the normal redundant via insertion flows, which converge both the redundant via rate and the timing QoR. Timing-preservation mode can slightly increase the redundant via rate while maintaining timing. However, if you use timing-preservation mode earlier in the flow, before timing is met, it might severely reduce the redundant via rate due to critical nets. Reporting Redundant Via Rates After redundant via insertion, whether concurrent or postroute, Zroute generates a redundant via report that provides the following information: - The via conversion rate for nondefault vias The via conversion rate for nondefault vias is listed at the top of the report as the total optimized via conversion rate."}
{"instruction": "How do I - setting constraints of variant cell - setting application option - grouping variant cell - running and placing group repeaters - troubleshooting setting constraints of variant cell to set the constraints of variant cell?", "response": "- Setting Constraints of Variant Cell - Setting Application Option - Grouping Variant Cell - Running and Placing Group Repeaters - Troubleshooting Setting Constraints of Variant Cell To set the constraints of variant cell: 1. Use the set_repeater_group_constraints -type variant_cell_swapping command to set the variant for cell swapping. 2. Use the report_repeater_group_constraints command to view the report of the repeater group constraint settings, after setting the variant for cell swapping. The report provides the \u25e6 Track pattern of the variant cell group \u25e6 Variant site ids \u25e6 Honor cell group not sharing timing \u25e6 First track alignment 3. Use the remove_repeater_group_constraints command to remove the constraints and reset the settings based on the report. Setting Application Option To set application option before using the add group repeaters and place_group_repeaters commands, use the set_app_option -name eco.placement.variant_cell_swapping -value true command. Based on the requirement, you can plan the variant cell flow. The rules of variant reference swapping differ in various variant cell flows. By default, the variant cell support in the eco_change_legal_reference command detects the variant mode and settings automatically. The eco_change_legal_reference command checks whether the current reference of the cell is legal or illegal for the placed location. If it is illegal, the command attempts to find a legal reference from its variant cell group defined to swap with the illegal one."}
{"instruction": "How do I - compile.auto_floorplan.keep_placement_blockages (default?", "response": "- compile.auto_floorplan.keep_placement_blockages (default: auto) \u25e6 auto: Removes existing placement blockages if either of the following two application options is set to all. compile.auto_floorplan.place_hard_macros, compile.auto_floorplan.shape_voltage_areas \u25e6 true: Keeps existing placement blockages. \u25e6 false: Removes existing placement blockages. Automatic Floorplanning Example The following example specifies constraints for automatic floorplanning: # Application options with default settings # set_app_options -name compile.auto_floorplan.initialize -value auto # set_app_options -name compile.auto_floorplan.place_pins -value unplaced # set_app_options -name compile.auto_floorplan.place_ios -value unplaced # set_app_options -name compile.auto_floorplan.place_hard_macros \\ # -value unplaced # set_app_options -name compile.auto_floorplan.shape_voltage_areas \\ # -value unshaped # Constraint settings for automatic floorplanning set_auto_floorplan_constraints -core_utilization 0.75 -side_ratio {1 2} set_macro_constraints -preferred_location {0 0} RAM0 set_block_pin_constraints -self -sides {1 3} set_individual_pin_constraints -ports [get_ports reset] -side 1 set_shaping_options -guard_band_size 2 Setting Up Multivoltage Designs The following topics describe the tasks you need to perform when setting up multivoltage designs: - Applying the Multivoltage Power Intent - Preparing the Power Network"}
{"instruction": "How do I 1) or dense mode (0) for the specified metal layer. note?", "response": "1) or dense mode (0) for the specified metal layer. Note: If you use the -fill_all_tracks true option, the default changes to 0. mx_fill_staggering 0.075 microns Controls staggering for fill patterns. mx_ignore_route_guide 0 Controls whether the IC Validator tool honors (0) or ignores (1) routing guides on the specified layer during metal fill insertion. mx_ignore_system_blockage 0 Controls whether the IC Validator tool honors (0) or ignores (1) system blockages on the specified layer during metal fill insertion. mx_via_enclosure Derived from ContactCode section Specifies the minimum metal enclosure values for vias. Fill size and spacing parameters mx_fill_width defaultWidth9 Specifies the width of the metal fill shapes. mx_min_fill_length Derived from minArea9 Specifies the minimum length of the metal fill shapes. mx_max_fill_length 5 microns Specifies the maximum length of the metal fill shapes. Decreasing this value can increase the via density at the expense of decreasing the metal density. mx_fill2fill_side_ spacing Specifies the minimum spacing between fill shapes. mx_fill2route_side_ spacing Specifies the minimum spacing between metal fill and net shapes. 9. Attribute setting from the Layer section of the technology file"}
{"instruction": "How do I - an xml file that provides information about the via ladder rules the xml file has the following syntax?", "response": "- An XML file that provides information about the via ladder rules The XML file has the following syntax: <EmRule> <!-- template definition --> <Template name=\"rule_name\" for_electro_migration=\"true\" for_high_performance=\"true\"> <Layer name=\"layer_name\" row_number=\"cuts_per_row\" ... more layers </Template> \u2026 more templates </EmRule> Each Template section specifies the template for a via ladder rule. Within a template section, the via ladder structure is specified by specifying the number of cuts per row for each metal layer involved in the via ladder. By default, the generated XML file is named auto_perf_via_ladder_rule.xml. To specify a different name, use the -xml_file option. - A script file that defines the via ladder rules The script file contains a create_via_rule command for each template defined in the XML file, as well as set_attribute commands to set the appropriate via rule attributes. By default, the generated script file is named auto_perf_via_ladder_rule.tcl. To specify a different file name, use the -rule_file option. Via Ladder Association File The command generates a script file that defines the association between the library cell pins and the via ladder rules. This script file contains set_via_ladder_candidate commands for the library cell pins. By default, the generated script file is named auto_perf_via_ladder_association.tcl. To specify a different file name, use the -association_file option."}
{"instruction": "How do I - defining the search path - setting up libraries - using pin access checker utility - analyzing libraries - reading the design - mitigating design mismatches - importing the floorplan information - setting up multivoltage designs - specifying timing constraints and settings - specifying logical design rule constraints - specifying clock gating constraints - specifying physical constraints for placement and legalization - specifying placement settings - specifying legalization settings - controlling the optimization of cells, nets, pins, and ports - specifying settings for preroute optimization - setting up for power-related features - specifying the routing resources?", "response": "- Defining the Search Path - Setting Up Libraries - Using Pin Access Checker Utility - Analyzing Libraries - Reading the Design - Mitigating Design Mismatches - Importing the Floorplan Information - Setting Up Multivoltage Designs - Specifying Timing Constraints and Settings - Specifying Logical Design Rule Constraints - Specifying Clock Gating Constraints - Specifying Physical Constraints for Placement and Legalization - Specifying Placement Settings - Specifying Legalization Settings - Controlling the Optimization of Cells, Nets, Pins, and Ports - Specifying Settings for Preroute Optimization - Setting Up for Power-Related Features - Specifying the Routing Resources"}
{"instruction": "How do I - check_legality, when advanced legalization is enabled by setting the place.legalize.enable_advanced_legalizer application option to true. - insert_via_ladders - route_auto, route_global, route_track, route_detail, and route_eco note?", "response": "- check_legality, when advanced legalization is enabled by setting the place.legalize.enable_advanced_legalizer application option to true. - insert_via_ladders - route_auto, route_global, route_track, route_detail, and route_eco Note: When you run routing with a single thread, the result is deterministic; if you start with the same block, you always get the same result. However, if you use multiple threads, the routing results are not deterministic; the final routing is slightly different between runs due to the varying division of tasks between threads. Global routing supports a deterministic mode for multicore routing. To enable this mode, set the route.global.deterministic application option to on. - route_opt - signoff_check_drc - signoff_fix_drc - signoff_create_metal_fill - signoff_fix_isolated_via - write_def By default, all commands use a single thread. To enable multithreading for those commands that support it, set the -max_cores option of the set_host_options command to a value greater than one and less than or equal to the number of cores available on your machine, which is the number of CPUs in the machine times the number of cores per CPU. The number of cores specified by the -max_cores option applies to all commands that support multithreading. When you enable multithreading, multithreaded commands create and use the specified number of threads, even if the number is more than the number of available cores. You must set an appropriate number of threads, so that the command does not try to use more resources than it has. Overthreading can reduce performance because the extra threads compete for resources. For best performance, do not run more than one thread per available core. For example, if your machine has two CPUs and each CPU has three cores, specify six as the maximum number of threads: fc_shell> set_host_options -max_cores 6"}
{"instruction": "How do I - specify the configuration for the specific hosts as described in running ic validator on specific hosts. - specify the lsf configuration as described in running ic validator using a job scheduler, but use the -add_hosts option in addition to the other command options. this configuration represents both optional resources and maximum resources \u2013 the resources can be given to the ic validator tool at the beginning, middle, or end of the run, or not given at all. for example, to use four cores on the current host and potentially acquire up to four more hosts from lsf, use the following commands?", "response": "- Specify the configuration for the specific hosts as described in Running IC Validator on Specific Hosts. - Specify the LSF configuration as described in Running IC Validator Using a Job Scheduler, but use the -add_hosts option in addition to the other command options. This configuration represents both optional resources and maximum resources \u2013 the resources can be given to the IC Validator tool at the beginning, middle, or end of the run, or not given at all. For example, to use four cores on the current host and potentially acquire up to four more hosts from LSF, use the following commands: fc_shell> set_host_options -target ICV -max_cores 4 fc_shell> set_host_options -target ICV -add_hosts \\ -submit_protocol lsf -num_processes 4 Defining the Layer Mapping for IC Validator In-Design Commands A layer mapping file maps the technology layers to the layers used in the runset file. You specify the location of the layer mapping file by setting the signoff.physical.layer_map_file application option. Each line in the layer mapping file specifies a layer in the technology file and the corresponding layer in the runset file using the following syntax: tf_layer[:tf_purpose][:use_type][:mask_type] runset_layer[:runset_data_type] Each line can contain the following items: - tf_layer \u2013 Layer number in the technology file, a required item - tf_purpose \u2013 Purpose number in the technology file - use_type \u2013 Fusion Compiler usage of the geometries in the design"}
{"instruction": "How do I 1. use the redhawk fusion feature to analyze the pg network by using the analyze_rail -min_path_resistance command. 2. load the redhawk fusion results by using the open_rail_result command. 3. enable voltage-drop-aware clock tree synthesis by setting the clock_opt.flow.enable_voltage_drop_aware application option to true. 4. run the clock_opt command. using nondefault routing rules for critical nets during optimization to improve timing qor, the tool can use nondefault routing rules on timing critical nets during preroute optimization. in addition, the tool can guide the router to honor these nondefault rule assignments as soft constraints. to enable this capability for the clock_opt command, set the clock_opt.flow.optimize_ndr application option to true. fc_shell> set_app_options -name clock_opt.flow.optimize_ndr \\ -value true performing concurrent clock and data optimization during the clock_opt command applying useful skew techniques during datapath optimization to improve the timing qor by taking advantage of the positive slack and adjusting the clock arrival times of registers is referred to as concurrent clock and data (ccd) optimization. by default, the tool performs concurrent clock and data optimization during the clock_opt command. to disable concurrent clock and data optimization for the clock_opt command, set the clock_opt.flow.enable_ccd application option to false. you can change the default behavior of concurrent clock and data optimization during the clock_opt command by performing any of the following optional steps?", "response": "1. Use the RedHawk Fusion feature to analyze the PG network by using the analyze_rail -min_path_resistance command. 2. Load the RedHawk Fusion results by using the open_rail_result command. 3. Enable voltage-drop-aware clock tree synthesis by setting the clock_opt.flow.enable_voltage_drop_aware application option to true. 4. Run the clock_opt command. Using Nondefault Routing Rules for Critical Nets During Optimization To improve timing QoR, the tool can use nondefault routing rules on timing critical nets during preroute optimization. In addition, the tool can guide the router to honor these nondefault rule assignments as soft constraints. To enable this capability for the clock_opt command, set the clock_opt.flow.optimize_ndr application option to true. fc_shell> set_app_options -name clock_opt.flow.optimize_ndr \\ -value true Performing Concurrent Clock and Data Optimization During the clock_opt Command Applying useful skew techniques during datapath optimization to improve the timing QoR by taking advantage of the positive slack and adjusting the clock arrival times of registers is referred to as concurrent clock and data (CCD) optimization. By default, the tool performs concurrent clock and data optimization during the clock_opt command. To disable concurrent clock and data optimization for the clock_opt command, set the clock_opt.flow.enable_ccd application option to false. You can change the default behavior of concurrent clock and data optimization during the clock_opt command by performing any of the following optional steps: 1. (Optional) Limit the latency adjustment values for concurrent clock and data optimization as described in Limiting the Latency Adjustment Values. 2. (Optional) Control the latency adjustment of boundary registers for concurrent clock and data optimization as described in Excluding Boundary Paths. 3. (Optional) Ignore specific path groups during concurrent clock and data optimization as described in Excluding Specific Path Groups."}
{"instruction": "How do I - *+ scll ../libs/scll.ndm /remote/project/libs/scll.ndm *+ schh ../libs/schh.ndm /remote/project/libs/schh.ndm * macros ../blocks/macros /remote/project/blocks/macros \"*\" = library currently open \"+\" = library has technology information - set_ref_libs -rebind when you make a change that invalidates the reference library list, such as moving a reference library to a new location, you need to rebind the reference libraries. to do so, use the -rebind option, which rebinds each reference library path specified by the search_path variable to libraries that are currently loaded in memory. for example, fc_shell> current_lib {lib_a} fc_shell> set_app_var search_path {. ../reflibs ../clibs} . ../libs ../blocks fc_shell> set_ref_libs -rebind ../reflibs/lib_c ../reflibs/lib_d ../clibs/stdhvt.ndm} rebinding a library does not affect the bindings of blocks already existing in the design library. to rebind these blocks using an updated reference library list, use the -rebind option with the link_block command. library configuration library configuration allows you to specify which vendor libraries to use as reference libraries for the current design. you specify the technology file, physical libraries, and logic libraries by using the search_path and link_library variables, and then you use the create_lib or set_ref_libs command to assemble the cell libraries.?", "response": "- *+ SCLL ../libs/SCLL.ndm /remote/project/libs/SCLL.ndm *+ SCHH ../libs/SCHH.ndm /remote/project/libs/SCHH.ndm * MACROS ../BLOCKS/MACROS /remote/project/BLOCKS/MACROS \"*\" = Library currently open \"+\" = Library has technology information - set_ref_libs -rebind When you make a change that invalidates the reference library list, such as moving a reference library to a new location, you need to rebind the reference libraries. To do so, use the -rebind option, which rebinds each reference library path specified by the search_path variable to libraries that are currently loaded in memory. For example, fc_shell> current_lib {lib_A} fc_shell> set_app_var search_path {. ../REFLIBS ../CLIBS} . ../LIBS ../BLOCKs fc_shell> set_ref_libs -rebind ../REFLIBS/lib_C ../REFLIBS/lib_D ../CLIBS/stdhvt.ndm} Rebinding a library does not affect the bindings of blocks already existing in the design library. To rebind these blocks using an updated reference library list, use the -rebind option with the link_block command. Library Configuration Library configuration allows you to specify which vendor libraries to use as reference libraries for the current design. You specify the technology file, physical libraries, and logic libraries by using the search_path and link_library variables, and then you use the create_lib or set_ref_libs command to assemble the cell libraries."}
{"instruction": "How do I 5. (optional) create a multibit register bank from specific single-bit registers as described in creating a multibit register bank from specific single-bit registers. 6. (optional) control the naming style used for multibit banking as described in specifying naming styles for multibit registers. 7. perform optimization by using the compile_fusion command. 8. (optional) report multibit information. to report the multibit register banks inferred by the tool, use the report_transformed_registers -multibit command, as described in reporting multibit registers. to report multibit statistics such as the total number of single-bit and multibit registers, the multibit banking ratio, and so on, use the report_multibit command. for more information about how to determine why multibit register banking is not performed for specific cells, see identifying why multibit banking is not performed. 9. (optional) analyze the multibit banking components in the gui as described in viewing multibit components in the gui. creating a multibit register bank from specific single-bit registers before you run the compile_fusion command on an rtl design, you can create a multibit bus consisting of specific unmapped single-bit registers by using the create_multibit command. then, when you run the compile_fusion command,?", "response": "5. (Optional) Create a multibit register bank from specific single-bit registers as described in Creating a Multibit Register Bank From Specific Single-Bit Registers. 6. (Optional) Control the naming style used for multibit banking as described in Specifying Naming Styles for Multibit Registers. 7. Perform optimization by using the compile_fusion command. 8. (Optional) Report multibit information. To report the multibit register banks inferred by the tool, use the report_transformed_registers -multibit command, as described in Reporting Multibit Registers. To report multibit statistics such as the total number of single-bit and multibit registers, the multibit banking ratio, and so on, use the report_multibit command. For more information about how to determine why multibit register banking is not performed for specific cells, see Identifying Why Multibit Banking Is Not Performed. 9. (Optional) Analyze the multibit banking components in the GUI as described in Viewing Multibit Components in the GUI. Creating a Multibit Register Bank From Specific Single-Bit Registers Before you run the compile_fusion command on an RTL design, you can create a multibit bus consisting of specific unmapped single-bit registers by using the create_multibit command. Then, when you run the compile_fusion command,"}
{"instruction": "How do I note?", "response": "Note: An IC Validator license is required to run the signoff_create_metal_fill command."}
{"instruction": "How do I - input port in3 with a buffer - output port out1 with a buffer - output port out3 with the libcell_buff library cell - output port out4 with a pair of inverters fc_shell> set_isolate_ports {in3 out1} fc_shell> set_isolate_ports -driver libcell_buff out3 -force fc_shell> set_isolate_ports -type inverter out4 the following figures show the design without and with the isolation logic inserted by the set_isolate_ports commands, including buffers and an inverter pair?", "response": "- Input port in3 with a buffer - Output port out1 with a buffer - Output port out3 with the LIBCELL_BUFF library cell - Output port out4 with a pair of inverters fc_shell> set_isolate_ports {in3 out1} fc_shell> set_isolate_ports -driver LIBCELL_BUFF out3 -force fc_shell> set_isolate_ports -type inverter out4 The following figures show the design without and with the isolation logic inserted by the set_isolate_ports commands, including buffers and an inverter pair: Q D Q D Q D Without port isolation Q D Q D Q D After running the port isolation commands in1 clk out3 rst in3 in2 out4 out2 out1 out1 out2 out3 out4 in1 clk rst in2 in3 LIBCELL_BUFF Fixing Multiple-Port Nets Multiple-port nets include - Feedthrough nets, where an input port feeds into an output port - Nets connected to multiple output ports, logically equivalent outputs"}
{"instruction": "How do I 0.3 for rising delays and 0.2 for falling delays?", "response": "0.3 for rising delays and 0.2 for falling delays: fc_shell> set_input_transition -rise 0.3 [get_ports CLK1] fc_shell> set_input_transition -fall 0.2 [get_ports CLK1] Identifying the Clock Endpoints When deriving the clock trees, the tool identifies two types of clock endpoints: - Sink pins clock tre synthesistop pindefinedexceptions for clock tre synthesistop pinsclock tre exceptionstop pinclock tre sinksdefiningSink pins are the clock endpoints that are used for delay balancing. The tool assigns an insertion delay of zero to all sink pins and uses this delay during delay balancing. During clock tree synthesis, the tool uses sink pins in calculations and optimizations for both design rule constraints and clock tree timing (skew and insertion delay). Sink pins are also referred to as balancing pins. - Ignore pins clock tre synthesisexclude pindefinedexceptions for clock tre synthesisexclude pinsclock tre exceptionsexclude pinIgnore pins are clock endpoints that are excluded from clock tree timing calculations and optimizations. The tool uses ignore pins only in calculations and optimizations for design rule constraints. During clock tree synthesis, the tool isolates ignore pins from the clock tree by inserting a guide buffer before the pin. Beyond the ignore pin, the tool never performs skew or insertion delay optimization, but does perform design rule fixing. The tool identifies the following clock endpoints as sink pins: - A clock pin on a sequential cell (a latch or flip-flip), unless that cell drives a generated clock - A clock pin on a macro cell"}
{"instruction": "How do I - internal sink pins of modules that have a combinational clock path if a module contains both internal sink pins and combinational clock paths, it is impossible to balance the internal sink pins. to resolve the balancing conflict, the tool defines the internal sink pins as ignore pins. for example, assume that you have a module that contains a combinational path in the clock network, as shown in figure (a) in figure 76, which is represented by the extracted timing model (etm) shown in figure (b). figure 76 module with combinational clock path q clk_out d clk (a) netlist (b) etm with check pin clk d q clk_out clk_check_pin_1 combinational delay in this case, the internal check pin inside the etm has delays coming from the etm timing library; these delays are considered by clock tree synthesis as sinks and are balanced with other sinks at the top level. if the path to the check pin is the shortest path, top-level clock tree synthesis cannot insert buffers on the shortest path and therefore leaves a large skew at the top level. setting the internal check pin as an ignore pin resolves the skew issue coming from the etm. - sink pins that cannot be balanced simultaneously if a block contains two clocks that drive common sinks, one of which is a clock pin that is the parent of another sink, due to a generated clock, it is impossible to simultaneously balance the parent and child clock pins with another common sink. to resolve the balancing conflict, the tool defines the parent clock pin as an ignore pin. for example, in figure 77, ff_gen/clk and ff0_1-10/clk are sinks of clkb, ff1_1-10/clk and ff0_1-10/clk are sinks of clka, and ff_gen/clk is the parent of?", "response": "- Internal sink pins of modules that have a combinational clock path If a module contains both internal sink pins and combinational clock paths, it is impossible to balance the internal sink pins. To resolve the balancing conflict, the tool defines the internal sink pins as ignore pins. For example, assume that you have a module that contains a combinational path in the clock network, as shown in figure (a) in Figure 76, which is represented by the extracted timing model (ETM) shown in figure (b). Figure 76 Module With Combinational Clock Path Q Clk_out D Clk (a) Netlist (b) ETM with check pin Clk D Q Clk_out Clk_check_pin_1 Combinational delay In this case, the internal check pin inside the ETM has delays coming from the ETM timing library; these delays are considered by clock tree synthesis as sinks and are balanced with other sinks at the top level. If the path to the check pin is the shortest path, top-level clock tree synthesis cannot insert buffers on the shortest path and therefore leaves a large skew at the top level. Setting the internal check pin as an ignore pin resolves the skew issue coming from the ETM. - Sink pins that cannot be balanced simultaneously If a block contains two clocks that drive common sinks, one of which is a clock pin that is the parent of another sink, due to a generated clock, it is impossible to simultaneously balance the parent and child clock pins with another common sink. To resolve the balancing conflict, the tool defines the parent clock pin as an ignore pin. For example, in Figure 77, FF_gen/CLK and FF0_1-10/CLK are sinks of clkb, FF1_1-10/CLK and FF0_1-10/CLK are sinks of clka, and FF_gen/CLK is the parent of"}
{"instruction": "How do I * d4 3 lexical_well_substrate_bias_arch itecure ws ws 4 lexical_device_threshold_voltage ulvt | svt | lvt | mvt ulvt you can specify the naming convention for this library by using the set_lib_cell_naming_convention command, as shown in the following example?", "response": "* D4 3 lexical_well_substrate_bias_arch itecure WS WS 4 lexical_device_threshold_voltage ULVT | SVT | LVT | MVT ULVT You can specify the naming convention for this library by using the set_lib_cell_naming_convention command, as shown in the following example: fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 1 -pattern {[a-zA-Z0-9]*} -attribute \"lexical_cell_prefix_name\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 2 -pattern {D[0-9]*} -attribute \"lexical_drive1_name\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 3 -pattern {WS} -attribute \"lexical_well_substrate_bias_architecure\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 4 -pattern {ULVT|SVT|LVT|MVT} -attribute \"lexical_device_threshold_voltage\" Note: For a list of all the predefined lexical attributes, see the man page for the set_lib_cell_naming_convention command. After you specify the naming convention, perform lexical classification by using the classify_lib_cell_attributes command, as shown in the following example: fc_shell> classify_lib_cell_attributes -libset myLibset To report the lexical classification results, use the report_lib_cell_classification_attributes command. To report the library cells which are not lexical classified under the given naming convention, use the report_lexical_unclassified_lib_cells command. If there are lexically unclassified cells, specify additional naming conventions and rerun lexical classification. 4. (Optional) Exclude specific cells from library analysis by using the set_use_for_library_analysis command."}
{"instruction": "How do I 3. (optional) create multisource sink groups for tap assignment by using the create_multisource_clock_sink_group command. skew groups defined for the clock can be distributed among different taps. to retain the objects of a skew group under the same tap, create a corresponding multisource sink group. use the following commands to remove, report, or manipulate multisource sink groups?", "response": "3. (Optional) Create multisource sink groups for tap assignment by using the create_multisource_clock_sink_group command. Skew groups defined for the clock can be distributed among different taps. To retain the objects of a skew group under the same tap, create a corresponding multisource sink group. Use the following commands to remove, report, or manipulate multisource sink groups: \u25e6 remove_multisource_clock_sink_groups \u25e6 report_multisource_clock_sink_groups \u25e6 get_multisource_clock_sink_groups \u25e6 add_to_multisource_clock_sink_group \u25e6 remove_from_multisource_clock_sink_group 4. (Optional) Specify name prefixes and suffixes to use when merging and splitting clock cells during tap assignment by using the following application options: \u25e6 cts.multisource.subtree_merge_cell_name_prefix \u25e6 cts.multisource.subtree_merge_cell_name_suffix \u25e6 cts.multisource.subtree_split_cell_name_prefix \u25e6 cts.multisource.subtree_split_cell_name_suffix \u25e6 cts.multisource.subtree_split_net_name_prefix \u25e6 cts.multisource.subtree_split_net_name_suffix"}
{"instruction": "How do I - considering the dynamic power qor during placement - performing ir-drop-aware placement - spreading repeater cells during placement performing placement with inaccurate constraints at early stages during the early stages of a design cycle, placement constraints can be inaccurate causing the tool to exit during coarse placement. to continue with placement when a region is overutilized due to it being too small or being covered by blockages, set the place.coarse.handle_early_data application option to true. when you do so, the tool issues warning messages as shown in the following example output?", "response": "- Considering the Dynamic Power QoR During Placement - Performing IR-Drop-Aware Placement - Spreading Repeater Cells During Placement Performing Placement With Inaccurate Constraints at Early Stages During the early stages of a design cycle, placement constraints can be inaccurate causing the tool to exit during coarse placement. To continue with placement when a region is overutilized due to it being too small or being covered by blockages, set the place.coarse.handle_early_data application option to true. When you do so, the tool issues warning messages as shown in the following example output: Warning: Utilization of move_bound_0 is 109%. (PLACE-089) Warning: Placement continues with over utilized regions in the design. (PLACE-083) Warning: Overutilized regions for move_bound_0 are modified by removal of partial and full blockages. (PLACE-084) When you enable this feature, the tool prints a warning message if the utilization of a region is more that 90 percent. To change this threshold, use the place.coarse.utilization_warning_threshold application option. Generating Automatic Group Bounds for Clock Gating Cells The tool can automatically generate group bounds for integrated clock gating cells and the sequential cells they drive. To do so, use the following application option setting: fc_shell> set_app_options -name place.coarse.icg_auto_bound \\ -value true When you enable this feature, the tool creates the automatic group bounds at the beginning of placement and removes them at the end of placement. The tool does not include cells that already belong to another group bound in the automatic group bounds. To limit the maximum number of fanouts that can be included in an automatic bound, use the icg_auto_bound_fanout_limit application option setting, as shown in the following example: fc_shell> set_app_options \\ -name place.coarse.icg_auto_bound_fanout_limit -value 30 The default fanout limit is 40."}
{"instruction": "How do I - the logic_opto and initial_drc stages, which are earlier stages of the compile_fusion command, by using the set_fix_multiple_port_nets command this command sets the fix_multiple_port_nets attribute on the specified objects. - the initial_opto and final_opto stages, which are later stages of the compile_fusion command, by setting the opt.port.eliminate_verilog_assign application option to true this application option is false by default. if you want to fix multiple-port nets, use the set_fix_multiple_port_nets command and fix these issues during the early stages of the compile_fusion command, when the tool can perform more optimization techniques. if you do not explicitly specify a multiple-port-net setting using the set_fix_multiple_port_nets command, but you set the opt.port.eliminate_verilog_assign application option to true, the tool issues the following message and uses the set_fix_multiple_port_nets -all -buffer_constants command setting to fix multiple-port nets during the logic_opto and initial_drc stages of the compile_fusion command?", "response": "- The logic_opto and initial_drc stages, which are earlier stages of the compile_fusion command, by using the set_fix_multiple_port_nets command This command sets the fix_multiple_port_nets attribute on the specified objects. - The initial_opto and final_opto stages, which are later stages of the compile_fusion command, by setting the opt.port.eliminate_verilog_assign application option to true This application option is false by default. If you want to fix multiple-port nets, use the set_fix_multiple_port_nets command and fix these issues during the early stages of the compile_fusion command, when the tool can perform more optimization techniques. If you do not explicitly specify a multiple-port-net setting using the set_fix_multiple_port_nets command, but you set the opt.port.eliminate_verilog_assign application option to true, the tool issues the following message and uses the set_fix_multiple_port_nets -all -buffer_constants command setting to fix multiple-port nets during the logic_opto and initial_drc stages of the compile_fusion command: Information: The opt.port.eliminate_verilog_assign application option has been set to true. The tool will run set_fix_multiple_port_nets -all -buffer_constant by internally enabling MPN fixing due to the absence of explicit set_fix_multiple_port_nets constraint. (MPN-0004) However, if you explicitly specify a multiple-port-net setting using the set_fix_multiple_port_nets command, the tool honors it. For example, the following settings enable multiple-port-net fixing for feedthrough nets during the logic_opto and"}
{"instruction": "How do I 137. figure 136 violation type?", "response": "137. Figure 136 Violation Type"}
{"instruction": "How do I 1. the build_clock stage, during which the tool synthesizes and optimizes the clock trees for all clocks in all modes of all active scenarios. after clock tree synthesis, the tool sets the synthesized clocks as propagated. 2. the route_clock stage, during which the tool detail routes the synthesized clock nets. 3. the final_opto stage, during which the tool performs further optimization, timing- driven placement, and legalization. it then global routes the block and performs extensive global-route-based optimization, which includes incremental legalization and route patching. you can limit the clock_opt command to one or more contiguous stages by using the -from option to specify the stage from which you want to begin and the -to option to specify the stage after which you want to end. if you do not specify the -from option, the tool begins from the build_clock stage. similarly, if you do not specify the -to option, the tool continues until the final_opto stage is completed. for example, to synthesize and optimize the clock trees and detail route the clock nets only, limit the execution to the build_clock and route_clock stages by use the following command?", "response": "1. The build_clock stage, during which the tool synthesizes and optimizes the clock trees for all clocks in all modes of all active scenarios. After clock tree synthesis, the tool sets the synthesized clocks as propagated. 2. The route_clock stage, during which the tool detail routes the synthesized clock nets. 3. The final_opto stage, during which the tool performs further optimization, timing- driven placement, and legalization. It then global routes the block and performs extensive global-route-based optimization, which includes incremental legalization and route patching. You can limit the clock_opt command to one or more contiguous stages by using the -from option to specify the stage from which you want to begin and the -to option to specify the stage after which you want to end. If you do not specify the -from option, the tool begins from the build_clock stage. Similarly, if you do not specify the -to option, the tool continues until the final_opto stage is completed. For example, to synthesize and optimize the clock trees and detail route the clock nets only, limit the execution to the build_clock and route_clock stages by use the following command: fc_shell> clock_opt -to route_clock Because the tool performs global routing during the final_opto stage, you should specify all router related settings, such as routing rules, route guides, application option settings required for the technology node, and so on, before you run the clock_opt command. You should run the final_opto only one time for each block. After this stage is completed, all the signal routes in the block are global routed. Therefore, when subsequently run any routing command, such as the route_auto,route_global, or route_group command, the tool skips global routing. To avoid errors during subsequent track assignment and detail routing, do not change any global route shapes after you run the clock_opt command. Considering Voltage Drop Information During Clock Tree Synthesis By default, the clock_opt command does not consider voltage drop information when inserting clock buffers. Therefore, the tool might place clock buffers in locations with high PG resistance, which increases the voltage drop. To prevent this, you can use the"}
{"instruction": "How do I - u1 as an etm in the pd_top design - top-level power intent create_power_domain pd_top -include_scope create_supply_net vdd_ao ... load_upf block.upf -scope u1 ... connect_supply_net vdd_ao -port u1/vdd_ao - block-level power intent?", "response": "- U1 as an ETM in the PD_Top design - Top-Level power intent create_power_domain PD_TOP -include_scope create_supply_net VDD_AO ... load_upf block.upf -scope U1 ... connect_supply_net VDD_AO -port U1/VDD_AO - Block-level power intent: block.upf create_power_domain PD_BLK -include_scope create_power_switch sw1 -domain PD_BLK \\ -input_supply_port {in VDD_AO} \\ -output_supply_port {out VDD_SW} ... ... add_port_state sw1/out -state {ON 1.08} -state {OFF off} ... create_pst pst -supplies {VDD_SW ...} Overview of Abstract Views In an abstract view, the gate-level netlist for the block is modeled by a partial gate- level netlist that contains only the required interface logic of the block. All other logic is removed. Figure 191 shows a block and its abstract view, where the logic is preserved between - The input port and the first register of each timing path - The last register of each timing path and the output port"}
{"instruction": "How do I - error?", "response": "- Error: When you apply this policy, the tool does not mitigate the violation, but issues an error message. - Tolerate: When you apply this policy, the tool makes predictable and explainable assumptions to continue with the flow. No changes are made to the design or setup. - Repair: When you apply this policy, the tool mitigates the violation using one or more repair strategies and records it. Repair can include changes in design and setup. - Strict: When you apply this policy, depending on the policy supported by the check, the tool applies the policies in this order \u2013 error, tolerate, repair. - Lenient: When you apply this policy, depending on the policy supported by the check, the tool applies the policies in this order \u2013 repair, tolerate, error. Note: - Quality of results of the top-level flow might get impacted due to the toleration and repair of errors. Error toleration and repair is mainly provided for managing data violations during the early design exploration phases. In the implementation flow, you should use the strict policy for all checks. Prerequisites for Handling Early Design Data The tool performs the repairs and saves the records in the subblocks within the check_hier_design command itself. For the repairs, which need frame recreation, you need to run the save_lib command to save the frame on disk. - To save the repair and its record to the subblock successfully, the following setup is required: 1. Enable the reference libraries, containing the subblocks, for edit open_lib top.nlib -ref_libs_for_edit"}
{"instruction": "How do I 39. figure 39 metrics dialog box filtering runs to control which runs are displayed in the table, click the runs button and select or deselect the exploration runs accordingly from the visible runs (summary) column. for example, to display only the first four runs in a table, select the runs as shown in figure 40. figure 40 choose baseline and visible runs dialog box?", "response": "39. Figure 39 Metrics Dialog Box Filtering Runs To control which runs are displayed in the table, click the Runs button and select or deselect the exploration runs accordingly from the Visible Runs (Summary) column. For example, to display only the first four runs in a table, select the runs as shown in Figure 40. Figure 40 Choose Baseline and Visible Runs Dialog Box"}
{"instruction": "How do I 14. fc_shell> create_voltage_area -power_domains {pd1} \\ -region { {{0 0} {10 10}} {{10 0} {30 10}} {{15 5} {35 15}} } {pd1} fc_shell> get_voltage_area_shapes -of_objects pd1 {voltage_area_shape_1 voltage_area_shape_2 voltage_area_shape_3} after you use the -merge_regions option to merge these shapes, the voltage area consists of a single rectilinear shape, as shown on the right side of figure 14. the merged voltage area shape is named voltage_area_shape_3, which was the last voltage area shape defined when the voltage area was created. fc_shell> set_voltage_area pd1 -merge_regions information?", "response": "14. fc_shell> create_voltage_area -power_domains {PD1} \\ -region { {{0 0} {10 10}} {{10 0} {30 10}} {{15 5} {35 15}} } {PD1} fc_shell> get_voltage_area_shapes -of_objects PD1 {VOLTAGE_AREA_SHAPE_1 VOLTAGE_AREA_SHAPE_2 VOLTAGE_AREA_SHAPE_3} After you use the -merge_regions option to merge these shapes, the voltage area consists of a single rectilinear shape, as shown on the right side of Figure 14. The merged voltage area shape is named VOLTAGE_AREA_SHAPE_3, which was the last voltage area shape defined when the voltage area was created. fc_shell> set_voltage_area PD1 -merge_regions Information: Merging abutted and overlapping shapes in voltage_area 'PD1'. (NDMUI-154) 1 fc_shell> get_voltage_area_shapes -of_objects PD1 {VOLTAGE_AREA_SHAPE_3} Figure 14 Merging Voltage Area Shapes To report the stacking order of the voltage area shapes, use the report_voltage_areas -verbose command. To modify the stacking order of the voltage area shapes, use the set_voltage_area_shape command, as described in Modifying the Stacking Order. Resolving Overlapping Voltage Areas If voltage area shapes from two or more voltage areas overlap, either completely or partially, the tool uses the stacking order of the shapes to determine the effective shapes of the voltage areas. By default, the stacking order is the order in which you define the shapes, with the last shape defined on top. The tool assigns the overlapped region to the voltage area associated with the top shape. Unlike merging shapes within a voltage area, when the tool resolves overlapping shapes from different voltage areas, it does not remove any shapes; only the interpretation of the shapes changes."}
{"instruction": "How do I - for the entire block when you disable this feature by setting the ccd.adjust_io_clock_latency to false or when you disable concurrent clock and data optimization for boundary paths by setting the ccd.optimize_boundary_timing application option to false - for specific i/o paths when you exclude the corresponding path groups from concurrent clock and data optimization by using the ccd.skip_path_groups application option - for specific i/o clocks when you disable latency adjustment those i/o clocks by using the set_latency_adjustment_options -exclude_clocks command?", "response": "- For the entire block when you disable this feature by setting the ccd.adjust_io_clock_latency to false or when you disable concurrent clock and data optimization for boundary paths by setting the ccd.optimize_boundary_timing application option to false - For specific I/O paths when you exclude the corresponding path groups from concurrent clock and data optimization by using the ccd.skip_path_groups application option - For specific I/O clocks when you disable latency adjustment those I/O clocks by using the set_latency_adjustment_options -exclude_clocks command"}
{"instruction": "How do I 3. clock-specific clock routing rule set by the set_clock_routing_rules -clocks command 4. global clock routing rule set by the set_clock_routing_rules command assigning nondefault routing rules to signal nets to assign a nondefault routing rule to a net, use the -rule option with the set_routing_rule command. the specified rule must be a rule that you previously defined with the create_routing_rule command. you must specify the nets to which to assign the nondefault routing rule. you can assign multiple nondefault routing rules to a net. to change the routing rule assignment for one or more nets, - use the -default_rule option to reset the nets to the default routing rule. to assign different nondefault routing rules to the nets, use the -rule option after resetting the nets to the default routing rule. - use the -no_rule option to remove all routing rules from the nets and allow the tool to automatically assign a routing rule to them. - use the -clear option to remove all routing rules and net-specific layer constraints from the nets. for example, to assign a nondefault routing rule called widemetal to the clk net, use the following command?", "response": "3. Clock-specific clock routing rule set by the set_clock_routing_rules -clocks command 4. Global clock routing rule set by the set_clock_routing_rules command Assigning Nondefault Routing Rules to Signal Nets To assign a nondefault routing rule to a net, use the -rule option with the set_routing_rule command. The specified rule must be a rule that you previously defined with the create_routing_rule command. You must specify the nets to which to assign the nondefault routing rule. You can assign multiple nondefault routing rules to a net. To change the routing rule assignment for one or more nets, - Use the -default_rule option to reset the nets to the default routing rule. To assign different nondefault routing rules to the nets, use the -rule option after resetting the nets to the default routing rule. - Use the -no_rule option to remove all routing rules from the nets and allow the tool to automatically assign a routing rule to them. - Use the -clear option to remove all routing rules and net-specific layer constraints from the nets. For example, to assign a nondefault routing rule called WideMetal to the CLK net, use the following command: fc_shell> set_routing_rule -rule WideMetal [get_nets CLK] To reset the routing rule for the CLK net to the default routing rule, use the following command: fc_shell> set_routing_rule -default_rule [get_nets CLK] Reporting Nondefault Routing Rule Assignments The Fusion Compiler tool provides commands to report the nondefault routing rules assigned by the set_routing_rule command and the clock routing rules assigned by the set_clock_routing_rules command. - To report the nondefault routing rules assigned by the set_routing_rule command, use the -of_objects option with the report_routing_rules command. fc_shell> report_routing_rules -of_objects [get_nets *] - To report the clock routing rule assignments, use the report_clock_routing_rules command. This command reports only the routing rules assigned by the set_clock_routing_rules command."}
{"instruction": "How do I - generic eco flow for timing or functional changes - freeze silicon eco flow - signoff eco flow - incremental signoff eco flow - eco fusion flow - eco fusion power integrity flow - manually instantiating spare cells - automatically adding spare cells - adding programmable spare cells - making eco changes using the eco_netlist command - making eco changes using netlist editing commands - resizing cells - adding buffers on nets - adding buffers on routed nets - optimizing the fanout of a net - reporting available sites for placing eco cells - identifying and reverting nonoptimal eco changes - placing eco cells?", "response": "- Generic ECO Flow for Timing or Functional Changes - Freeze Silicon ECO Flow - Signoff ECO Flow - Incremental Signoff ECO Flow - ECO Fusion Flow - ECO Fusion Power Integrity Flow - Manually Instantiating Spare Cells - Automatically Adding Spare Cells - Adding Programmable Spare Cells - Making ECO Changes Using the eco_netlist Command - Making ECO Changes Using Netlist Editing Commands - Resizing Cells - Adding Buffers on Nets - Adding Buffers on Routed Nets - Optimizing the Fanout of a Net - Reporting Available Sites for Placing ECO Cells - Identifying and Reverting Nonoptimal ECO Changes - Placing ECO Cells"}
{"instruction": "How do I 2. (optional) enable flexible h-tree synthesis by setting the cts.multisource.flexible_htree_synthesis application option to true. when you enable this feature, the tool identifies the best count, configuration, and placement of tap drivers for minimum clock tree insertion delay. to specify that the tool derives a symmetric configuration of tap drivers, set the cts.multisource.tap_selection application option to symmetric. the default is user, and by default the tool uses the configuration specified by the -tap_boxes option of the set_regular_multisource_clock_tree_options command for the tap drivers.?", "response": "2. (Optional) Enable flexible H-tree synthesis by setting the cts.multisource.flexible_htree_synthesis application option to true. When you enable this feature, the tool identifies the best count, configuration, and placement of tap drivers for minimum clock tree insertion delay. To specify that the tool derives a symmetric configuration of tap drivers, set the cts.multisource.tap_selection application option to symmetric. The default is user, and by default the tool uses the configuration specified by the -tap_boxes option of the set_regular_multisource_clock_tree_options command for the tap drivers."}
{"instruction": "How do I 8. (optional) specify name prefixes and suffixes to use when merging and splitting clock cells during local subtree synthesis by using the following application options?", "response": "8. (Optional) Specify name prefixes and suffixes to use when merging and splitting clock cells during local subtree synthesis by using the following application options: \u25e6 cts.multisource.subtree_merge_cell_name_prefix \u25e6 cts.multisource.subtree_merge_cell_name_suffix \u25e6 cts.multisource.subtree_split_cell_name_prefix \u25e6 cts.multisource.subtree_split_cell_name_suffix \u25e6 cts.multisource.subtree_split_net_name_prefix \u25e6 cts.multisource.subtree_split_net_name_suffix When you run the synthesize_multisource_clock_subtrees command the tool performs the following steps: 1. Merges equivalent clock cells to remove any artificial boundaries between clusters of sinks. 2. Optimizes the subtree by splitting, sizing, and relocating the clock cells, and snaps the first level of clock cells to the clock straps. 3. Routes the clock nets. By default, it global routes the nets. However, if you enable fishbone routing by using the cts.multisource.subtree_routing_mode application option, the tool creates a mix of detail routed fishbone trunks and fingers, and global comb routes to connect the net loads to the fishbone fingers, as shown in Figure 97. You can control the fishbone routing by using the following application options: \u25e6 cts.routing.fishbone_max_tie_distance \u25e6 cts.routing.fishbone_max_sub_tie_distance \u25e6 cts.routing.fishbone_bias_threshold \u25e6 cts.routing.fishbone_bias_window \u25e6 cts.routing.fishbone_horizontal_bias_spacing \u25e6 cts.routing.fishbone_vertical_bias_spacing 4. Refines the clock trees by sizing and relocating cells based on the routing."}
{"instruction": "How do I note?", "response": "Note: If the tool does not find any suitable power cell that matches the targeted voltage drop and RON value, it skips cell resizing. To perform voltage driven cell sizing, 1. Perform a voltage drop analysis using RH Fusion to get the maximum voltage drop of the cells driven by the power switch in question. Refer Performing Voltage Drop Analysis for more information. 2. Set the RON value limit to be considered for cell switching using the set_power_switch_resistance command. Provide the following values as input, \u25e6 Lib cell name - specifies the name of the library cell. \u25e6 On resistance value - specifies the RON value limit to be considered for cell sizing. You can get this information from the switch model files. For example, fc_shell> set_power_switch_resistance saed32hvt_pg_ss0p_v125c/HEAD2X2_HVT 0.015 3. Run the size_power_switches command to perform cell size switching. Provide the value for-max_irdrop option which specifies the allowed maximum effective voltage drop on supply nets for standard cells driven by switches. This is a mandatory input. For example, fc_shell> size_power_switches -max_irdop 0.05 For the other non-mandatory inputs to further refine the command action, see the man page for the size_power_switches command. The tool also provides the following commands to further help with power switch resistance: - get_power_switch_resistance - This command fetches the RON value of a power switch cell. - reset_power_switch_resistance - This command resets the RON value specified for a power switch cell. - report_power_switch_resistance - This command reports all power switch library cells with a set RON. Use the -verbose option with the command to display the group of cells with the same footprint, the Ron value and the number of cells instantiated in the design. When using the -verbose option, you can also use -same_vt option to report power switch cells and their RON value from current library with same VT but different driving strength cells."}
{"instruction": "How do I 1. specify the message ids to query by using the global scope application option, shell.common.check_error_list. fc_shell> set_app_options -name shell.common.check_error_list \\ -value {des-001 cmd-005} shell.common.check_error_list {des-001 cmd-005} 2. use the link command as an example for the previous command step. fc_shell> link error?", "response": "1. Specify the message IDs to query by using the global scope application option, shell.common.check_error_list. fc_shell> set_app_options -name shell.common.check_error_list \\ -value {DES-001 CMD-005} shell.common.check_error_list {DES-001 CMD-005} 2. Use the link command as an example for the previous command step. fc_shell> link Error: Current block is not defined. (DES-001) 3. Query the specified message IDs that were issued during the link command step by using the check_error command. fc_shell> check_error -verbose {DES-001} 1 The result shows that the DES-001 message ID was issued during the link command step. If none of the specified message IDs was issued, the check_error command returns 0."}
{"instruction": "How do I note?", "response": "Note: After you modify the clock-gate latency using the set_clock_gate_latency command, if you compile your design using the compile_fusion command, it is not necessary to use the apply_clock_gate_latency command to apply the latency values. The tool annotates the specified value during compilation. To remove clock latency information previously set on clock-gating cells with the set_clock_gate_latency or apply_clock_gate_latency commands, use the reset_clock_gate_latency command. This command removes the clock latency values on the specified clocks. If you do not specify the clock, the clock latency values on all the clock-gating cells are removed. The set_clock_latency Command Use the set_clock_latency command to specify clock network latency for specific clock- gating cells. In Figure 55, lat_cgtoreg is the estimated delay from the clock pin of the clock-gating cell to the clock pin of the gated register and lat_reg is the estimated clock-network latency to the clock pins of the registers without clock gating. Figure 55 Clock Latency With Clock-Gating Design Clock- Gating Cell Register Clock tree delay Clock tree delay lat_reg lat_cgtoreg For all clock pins of registers (gated or ungated) in the design that are driven by a specific clock, use the lat_reg value for the set_clock_latency command. For clock pins of all the clock-gating cells, use the difference between the lat_reg and lat_cgtoreg"}
{"instruction": "How do I - specific via ladder constraints; if the constraints conflict, the instance-specific constraints override the global constraints. - to define global via ladder constraints, which are referred to as via ladder rules, use the set_via_ladder_rules command, as described in defining global via ladder constraints. - to define instance-specific via ladder constraints, use the set_via_ladder_constraints command, as described in defining instance-specific via ladder constraints. in addition, you can specify via ladder candidates for specific pins by using the set_via_ladder_candidate command. this command is used by preroute optimization to insert via ladders on timing-critical paths, as described in specifying automatic via ladder insertion settings for preroute optimization. to report the via ladder constraints, use the report_via_ladder_constraints command. see also - inserting via ladders defining global via ladder constraints to define global via ladder constraints, which are referred to as via ladder rules, use the set_via_ladder_rules command to set via ladder constraints on library cell pins. you must specify the mapping between library cell pins and via ladder templates, as well as the pins to which to apply the mapping. use the following options to specify the mapping?", "response": "- specific via ladder constraints; if the constraints conflict, the instance-specific constraints override the global constraints. - To define global via ladder constraints, which are referred to as via ladder rules, use the set_via_ladder_rules command, as described in Defining Global Via Ladder Constraints. - To define instance-specific via ladder constraints, use the set_via_ladder_constraints command, as described in Defining Instance-Specific Via Ladder Constraints. In addition, you can specify via ladder candidates for specific pins by using the set_via_ladder_candidate command. This command is used by preroute optimization to insert via ladders on timing-critical paths, as described in Specifying Automatic Via Ladder Insertion Settings for Preroute Optimization. To report the via ladder constraints, use the report_via_ladder_constraints command. See Also - Inserting Via Ladders Defining Global Via Ladder Constraints To define global via ladder constraints, which are referred to as via ladder rules, use the set_via_ladder_rules command to set via ladder constraints on library cell pins. You must specify the mapping between library cell pins and via ladder templates, as well as the pins to which to apply the mapping. Use the following options to specify the mapping: - -master_pin_map or -master_pin_map_file These options specify the mappings for specific library pins using the following format: { {lib_cell/pin {via_ladder_list}} ...}"}
{"instruction": "How do I 2.0 ns for rising-edge shortest-path calculations for the u2/clk pin for the clk clock in the current corner, use the following command?", "response": "2.0 ns for rising-edge shortest-path calculations for the U2/CLK pin for the CLK clock in the current corner, use the following command: fc_shell> set_clock_balance_points -clock [get_clocks CLK] \\ -rise -early -delay 2.0 -balance_points [get_pins U2/CLK] To report the user-defined insertion delay for sink pins, use the remove_clock_tre_exceptions comand-stop_pinscomandsremove_clock_tre_exceptions-stop_pins report_clock_balance_points command. This command reports the user-defined sink pins, the derived sink pins with user-specified insertion delay, and the user-defined ignore pins. Defining Ignore Pins To define one or more pins as ignore pins, use the following syntax: set_clock_balance_points [-clock clock] -consider_for_balancing false -balance_points pins The -clock option is not required. If you do not use it, all user-defined ignore pins apply to all clocks."}
{"instruction": "How do I 2. run the analyze_rail command with the following options?", "response": "2. Run the analyze_rail command with the following options: \u25e6 Use the -voltage_drop and -electromigration options to enable electromigration analysis. \u25e6 Use the -nets option to specify the power and ground supply nets to analyze. The tool considers all the switched or internal power nets of the specified power nets in the analysis. You do not need to explicitly specify the internal power nets. \u25e6 (Optional) By default, the analyze_rail command automatically generates a GSR file for running RedHawk rail analysis. To specify an external GSR file to append to the GSR file generated in the current run, use the -extra_gsr_option_file option. \u25e6 (Optional) To specify an existing RedHawk script file that was generated in an earlier analyze_rail run, use the -redhawk_script_file option. When using an existing script file for rail analysis, the tool honors the settings in this script file and ignores all other currently enabled options. For example, if you run the analyze_rail -voltage_drop static -redhawk_script_file myscript.tcl command, the tool ignores the -voltage_drop option. When the -redhawk_script_file option is not specified, you must specify the nets to analyze by using the -nets option. 3. Check the analysis results, as described in Viewing PG Electromigration Analysis Results. Viewing PG Electromigration Analysis Results When PG electromigration analysis is complete, you can check for current violations by displaying a PG electromigration map or checking the generated errors in the error browser. - Displaying the PG Electromigration Map - Checking PG Electromigration Violations Displaying the PG Electromigration Map A PG electromigration map is a visual display of the color-coded electromigration values overlaid on the physical supply nets. For static analysis, the map displays average electromigration values. For dynamic analysis, the map displays average electromigration values, peak electromigration values, or root mean square electromigration values. Figure 204 shows an example of the electromigration map in which problem areas are highlighted in different colors. Use the options to investigate the problem areas by"}
{"instruction": "How do I 9. report the library cell gaps, redundancies, outliers, and equivalence identified during library analysis by using the report_library_analysis command, as shown in the following example?", "response": "9. Report the library cell gaps, redundancies, outliers, and equivalence identified during library analysis by using the report_library_analysis command, as shown in the following example: fc_shell> report_library_analysis -analysis myLibset_run1 \\ -analysis_type redundancy -detail fc_shell> report_library_analysis -analysis myLibset_run1 \\ -analysis_type outlier -detail fc_shell> report_library_analysis -analysis myLibset_run1 \\ -analysis_type equivalent -detail fc_shell> report_library_analysis -analysis myLibset_run1 \\ -analysis_type gap -detail 10. Generate plots by using the gui_plot_lib_cells_attributes command, as shown in the following example: fc_shell> gui_plot_lib_cells_attributes -libset myLibset \\ -lib_cell_type all -x_attrib_name leakage -y_attrib_name avgsl_fall_delay \\ -chart_type scatter Comparing Libraries To compare libraries, perform the following steps: 1. Create a collection of the libraries you want to analyze, which is known as a libset, by using the create_libset command, as shown in the following example: fc_shell> create_libset -name myLibset -libs {abcd1.db abcd2.db} When you perform library analysis, you can specify the libset name, instead of specifying a list of libraries. In addition, the tool uses the libset name, instead of using a list of the library names, when it generates reports, plots, and so on during analysis. You can include a cell library in more then one libset. 2. Set up the library analysis environment by using the create_environment command. The create_environment command associates a predefined libset with a process label, process number, voltage, and temperature, which is then used for analyzing the libraries. You can specify the process label, process number, voltage, and temperature of the analysis environment by using one of the following methods: \u25e6 Specify the process label, process number, voltage, and temperature values by using the -ppvt option, as shown in the following example: fc_shell> create_environment -name env1 -libset myLibset \\ -ppvt {{SS1p, 1.0, 0.6, 125}}"}
{"instruction": "How do I 2. create the category blockage by using the create_placement_blockage command with the -type category, -blocked_percentage, and -category options. the argument to the -category option is the name of the user attribute. the following example defines a blockage that prevents a certain category of cells from being placed within it?", "response": "2. Create the category blockage by using the create_placement_blockage command with the -type category, -blocked_percentage, and -category options. The argument to the -category option is the name of the user attribute. The following example defines a blockage that prevents a certain category of cells from being placed within it: define_user_attribute -type boolean \\ -classes lib_cell dr_cells set_attribute [get_lib_cells mylib/dr*] dr_cells true create_placement_blockage -boundary {{4300 2400} {4500 2700}} \\ -type category -blocked_percentage 45 -category dr_cell The following example defines a blockage that allows only a certain category of cells to be placed within it: define_user_attribute -type boolean \\ -classes lib_cell non_iso set_attribute [get_lib_cells mylib/*] non_iso true set_attribute [get_lib_cells mylib/iso*] non_iso false create_placement_blockage -boundary {{4300 2400}{4500 2700}} \\ -type category -blocked_percentage 30 -category non_iso Note: Category blockages are honored during placement, but not during legalization, optimization, or clock tree synthesis. Defining Blockages That Exclude Registers To define partial blockages that exclude registers, specify the boundary, type (-type register option), blockage percentage (-blocked_percentage option), and optionally the name for the placement blockage. For example, to define a partial blockage that excludes registers, but allows a cell density of 50 percent for other cells, use the following commands: create_placement_blockage -boundary {{10 20} {100 200}} \\ -type register -blocked_percentage 50 Note: Register blockages are honored during placement, but not during legalization, optimization, or clock tree synthesis."}
{"instruction": "How do I - enabling area recovery in regions of high utilization - enabling advanced logic restructuring specifying parasitic estimation settings for the preroute optimization the following topics describe how you can control parasitic estimation during the preroute stage of the design flow?", "response": "- Enabling Area Recovery in Regions of High Utilization - Enabling Advanced Logic Restructuring Specifying Parasitic Estimation Settings for the Preroute Optimization The following topics describe how you can control parasitic estimation during the preroute stage of the design flow: - Enabling Global-Route-Layer-Based (GRLB) Preroute Optimization - Enabling Route-Driven Estimation (RDE) for Preroute Optimization Enabling Global-Route-Layer-Based (GRLB) Preroute Optimization To improve correlation with the postroute stage of the design flow, you can enable global-route-layer-based RC estimation during the compile_fusion and clock_opt commands. With this feature, the tool uses global routes for all nets and identifies the layers with the most appropriate per-unit resistance values. The nets are then constrained to minimum and maximum layers for preroute RC estimation. To enable this feature, use the opt.common.use_route_aware_estimation application option. If you set it to - auto, the feature is enabled only when there is a variation in the per-unit resistance value of the different routing layers - true, the feature is enabled irrespective of the variation in the per-unit resistance value of the different routing layers If you enable this feature, use the remove_route_aware_estimation command to remove all global route based estimation before you perform routing. Enabling Route-Driven Estimation (RDE) for Preroute Optimization To improve correlation with the postroute stage of the design flow, the tool can perform global routing, perform extraction based on the global routes, and use this parasitic information when performing optimization. If the tool detects that the technology used by the design is an advanced technology node that is less than 16 nm, the tool enables this feature by default. To enable it for any technology, set the opt.common.enable_rde application option to true. When enabled, the tool performs route-driven parasitic estimation during the final_opt stage of the compile_fusion and clock_opt commands. The parasitic information is"}
{"instruction": "How do I 3. (optional) specify a naming convention for library cells and perform lexical classification by using lexical attributes. a library cell naming convention can help the tool identify cell families in the subsequent steps of the flow. for example, assume a cell in your library is named and2d4wsulvt. this name is the concatenation of the and2, d4, ws, and ulvt strings, which represent cell characteristics that are associated to a lexical attribute as shown in the following table?", "response": "3. (Optional) Specify a naming convention for library cells and perform lexical classification by using lexical attributes. A library cell naming convention can help the tool identify cell families in the subsequent steps of the flow. For example, assume a cell in your library is named AND2D4WSULVT. This name is the concatenation of the AND2, D4, WS, and ULVT strings, which represent cell characteristics that are associated to a lexical attribute as shown in the following table: Table 3 Lexical Attributes for the Naming Convention of an Example Library Column Lexical Attribute Allowed Pattern Values for Example Cell 1 lexical_cell_prefix_name [a-zA-Z0-9]* AND2 2 lexical_drive1_name D[0-9]* D4 3 lexical_well_substrate_bias_arch itecure WS WS 4 lexical_device_threshold_voltage ULVT | SVT | LVT | MVT ULVT"}
{"instruction": "How do I 1. specify the supply net type in the upf file by using the supply_net_pg_type attribute, as shown in the following upf file example?", "response": "1. Specify the supply net type in the UPF file by using the supply_net_pg_type attribute, as shown in the following UPF file example: \u2026 create_supply_net VDD \u2026 \u2026 set_design_attributes -elements VDD \\ -attribute supply_net_pg_type power \u2026 2. Enable floating supply nets by setting the mv.pg.create_floating_pg application option to true before you create supply nets with the connect_pg_net command, as shown in the following script example: \u2026 \u2026 # Apply the UPF load_upf $upf_input_file_name commit_upf \u2026 \u2026 # create logical supply nets set_app_options -as_user_default \\ -name mv.pg.create_floating_pg -value true connect_pg_net -automatic \u2026 check_mv_design The tool creates the logical supply nets in the topmost hierarchy of the current UPF scope for domain-independent supply nets. For domain-dependent supply nets, the tool creates the logical supply nets in the topmost hierarchy of the domain. The tool does not create a logical supply net in the following situations: - There is a conflict between the connection and the supply net type specified with the supply_net_pg_type attribute. - The supply net is connected to a PG pin of an instance. - The connect_pg_net command is not specified in the current or parent physical hierarchy."}
{"instruction": "How do I 4.81004e-06 fi6/vss 4.80959e-06 fi3/vss 4.79702e-06 fi4/vss 4.79684e-06 fi1/vss 4.79594e-06 ... the following example writes the calculated effective voltage drop values for the vdd and vss nets to an output file called inst_effvd.rpt. fc_shell> open_rail_result fc_shell> report_rail_result -type effective_voltage_drop \\ -supply_nets { vdd vss } inst_effvd.rpt fc_shell> sh cat inst_effvd.rpt #cell_instance_pg_pin_name mapped_pg_pin_name supply_net_name #average_effective_voltage_drop_in_tw #max_effective_voltage_drop_in_tw min_effective_voltage_drop_in_tw #max_effective_voltage_drop s32_reg_14_/vdd vss vdd -7.750034332e-03 -1.017999649e-02 -5.850076675e-03 -1.029992104e-02 s32_reg_15_/vdd vss vdd -7.709980011e-03 -1.013994217e-02 -5.810022354e-03 -1.026010513e-02 s46_reg_8_/vdd vss vdd -7.179975510e-03 -9.490013123e-03 -5.330085754e-03 -1.021003723e-02 ... the following example writes the minimum path resistance values of cell instances to an output file called inst_minres.rpt. fc_shell> open_rail_result fc_shell> report_rail_result -type instance_minimum_path_resistance \\ -supply_nets { vdd vss } inst_minres.rpt fc_shell> sh cat inst_minres.rpt rmax(ohm) = 162.738?", "response": "4.81004e-06 FI6/vss 4.80959e-06 FI3/vss 4.79702e-06 FI4/vss 4.79684e-06 FI1/vss 4.79594e-06 ... The following example writes the calculated effective voltage drop values for the VDD and VSS nets to an output file called inst_effvd.rpt. fc_shell> open_rail_result fc_shell> report_rail_result -type effective_voltage_drop \\ -supply_nets { VDD VSS } inst_effvd.rpt fc_shell> sh cat inst_effvd.rpt #cell_instance_pg_pin_name mapped_pg_pin_name supply_net_name #average_effective_voltage_drop_in_tw #max_effective_voltage_drop_in_tw min_effective_voltage_drop_in_tw #max_effective_voltage_drop S32_reg_14_/VDD VSS VDD -7.750034332e-03 -1.017999649e-02 -5.850076675e-03 -1.029992104e-02 S32_reg_15_/VDD VSS VDD -7.709980011e-03 -1.013994217e-02 -5.810022354e-03 -1.026010513e-02 S46_reg_8_/VDD VSS VDD -7.179975510e-03 -9.490013123e-03 -5.330085754e-03 -1.021003723e-02 ... The following example writes the minimum path resistance values of cell instances to an output file called inst_minres.rpt. fc_shell> open_rail_result fc_shell> report_rail_result -type instance_minimum_path_resistance \\ -supply_nets { VDD VSS } inst_minres.rpt fc_shell> sh cat inst_minres.rpt Rmax(ohm) = 162.738"}
{"instruction": "How do I - leaf cells, as described in adding leaf cells - leaf cells, as described in adding hard macro cells - relative placement groups, as described in adding relative placement groups - blockages, as described in adding blockages when you add an object to a relative placement group, - the relative placement group to which you are adding the object must exist. - the object must be added to an empty location in the relative placement group. - only one object can be added in one location of relative placement group. to remove objects from a relative placement group, use the remove_from_rp_group command. you can remove leaf cells (-cells), relative placement groups (-rp_group), and blockages (-blockage). when you remove objects from a group, the space previously occupied by the removed objects is left unoccupied. adding leaf cells to add leaf cells, including physical only cells, to a relative placement group, use the -cells option with the add_to_rp_group command. specify the column and row position within the relative placement group at which to add the cell by using the -column and -row options. in a relative placement group, a leaf cell can occupy multiple column positions or multiple row positions, which is known as leaf cell straddling. to specify the number of columns and rows the cells occupies, use the -num_columns and -num_rows options respectively. if you do not set these options, the default is 1 for both the number of columns and rows. for example, to add a leaf cell that occupies two columns and one row at position (0,0), use fc_shell> add_to_rp_group rp1 -cells u23 \\ -column 0 -row 0 -num_columns 2 -num_rows 1 straddling is for leaf cells only, and not for hierarchical groups or blockages.?", "response": "- Leaf cells, as described in Adding Leaf Cells - Leaf cells, as described in Adding Hard Macro Cells - Relative placement groups, as described in Adding Relative Placement Groups - Blockages, as described in Adding Blockages When you add an object to a relative placement group, - The relative placement group to which you are adding the object must exist. - The object must be added to an empty location in the relative placement group. - Only one object can be added in one location of relative placement group. To remove objects from a relative placement group, use the remove_from_rp_group command. You can remove leaf cells (-cells), relative placement groups (-rp_group), and blockages (-blockage). When you remove objects from a group, the space previously occupied by the removed objects is left unoccupied. Adding Leaf Cells To add leaf cells, including physical only cells, to a relative placement group, use the -cells option with the add_to_rp_group command. Specify the column and row position within the relative placement group at which to add the cell by using the -column and -row options. In a relative placement group, a leaf cell can occupy multiple column positions or multiple row positions, which is known as leaf cell straddling. To specify the number of columns and rows the cells occupies, use the -num_columns and -num_rows options respectively. If you do not set these options, the default is 1 for both the number of columns and rows. For example, to add a leaf cell that occupies two columns and one row at position (0,0), use fc_shell> add_to_rp_group rp1 -cells U23 \\ -column 0 -row 0 -num_columns 2 -num_rows 1 Straddling is for leaf cells only, and not for hierarchical groups or blockages."}
{"instruction": "How do I - bot signifies bottom level design. - mid signifies intermediate level design, which instantiates bot. - top signifies the top-level design, which instantiates mid. setting the policy for early data checks to set or modify the policy for all the checks, or modify the policy settings for data checks, use the set_early_data_check_policy command. use the following options to configure the policy?", "response": "- BOT signifies bottom level design. - MID signifies intermediate level design, which instantiates BOT. - TOP signifies the top-level design, which instantiates MID. Setting the Policy for Early Data Checks To set or modify the policy for all the checks, or modify the policy settings for data checks, use the set_early_data_check_policy command. Use the following options to configure the policy: - -checks: Specifies a predefined list of data checks. It supports an asterisk wildcard character (*) as the name of a check. - -policy: Specifies the type of policy depending on what is supported by the check. This can be \u25e6 error: The tool issues an error message if the check does not support error as a policy. \u25e6 tolerate: The tool issues an error message if the check does not support tolerate as a policy. \u25e6 repair: The tool issues an error message if the check does not support repair as a policy. \u25e6 strict: The tool sets the first matching policy in the following order: 1. error 2. tolerate 3. repair For example, when a check supports the error and tolerate policies, and you select strict as the policy type, the error policy type is set."}
{"instruction": "How do I 16.gds gds_map_file ./redhawk.gdslayermap output_directory sramlp2rw128x16_output use_lef_pins_for_tracing 1 vdd_nets { vdd } gnd_nets { vss } top_cell sramlp2rw128x16 examples the following example generates redhawk macro models for the sramlp2rw128x16, sramlp2rw32x4, sramlp2rw64x32 and sramlp2rw64x8 memories?", "response": "16.gds GDS_MAP_FILE ./redhawk.gdslayermap OUTPUT_DIRECTORY SRAMLP2RW128x16_output USE_LEF_PINS_FOR_TRACING 1 VDD_NETS { VDD } GND_NETS { VSS } TOP_CELL SRAMLP2RW128x16 Examples The following example generates RedHawk macro models for the SRAMLP2RW128x16, SRAMLP2RW32x4, SRAMLP2RW64x32 and SRAMLP2RW64x8 memories: ## Specify the path to the RedHawk executable ## set_app_options -name rail.redhawk_path -value $env(REDHAWK_IMAGE)/bin ## Specify one configuration file per memory ## exec_gds2rh -config_file SRAMLP2RW128x16.config exec_gds2rh -config_file SRAMLP2RW32x4.config exec_gds2rh -config_file SRAMLP2RW64x32.config exec_gds2rh -config_file SRAMLP2RW64x8.config The following example runs top-level rail analysis with the macro models that are generated in the previous example: ## Run rail analysis at top level ## open_lib open_block top link_block ## Specify rail analysis setting ## create_taps set_app_options -name rail.tech_file -value test.tech set_app_options -name rail.lib_files -value test.lib \u2026 ## Set macro model file ## set_app_options -name rail.macro_models \\"}
{"instruction": "How do I note?", "response": "Note: If a cell has a placement status of fixed, it is treated like a don\u2019t touch cell during clock tree synthesis. Setting Size-Only Settings To set a size-only setting on a cell, use the set_size_only command. The following command sets a size-only setting on a cell: fc_shell> set_size_only [get_cells cell_name] true To report all the size-only settings set on the current block, use the report_size_only command. fc_shell> report_size_only -all Copying Clock Tree Exceptions Across Modes You can specify that the tool copies clock tree exceptions from one mode to one or more equivalent modes by using the set_clock_tree_options command with the -copy_exceptions_across_modes, -from_mode, and -to_mode options. The following example specifies that during clock tree synthesis the clock tree exceptions in the mode name M1 should be copied to the modes name M2 and M3: fc_shell> set_clock_tree_options -copy_exceptions_across_modes \\ -from_mode M1 -to_mode {M2 M3}"}
{"instruction": "How do I - specify the checkpoint report name with the -enable option. - to enable the report to run before one or more checkpoints, specify the checkpoint names with the -before option. - to enable the report to run after one or more checkpoints, specify the checkpoint names with the -after option. when associating a report with multiple checkpoints, you can list the checkpoint names or specify the asterisk wildcard character. assume you have defined three checkpoints in your script named remove_buffers, place_opt_to_initial_opto, and place_opt_to_final_opto using the eval_checkpoint command, and two checkpoint reports named timing and app_options using the create_checkpoint_report command. to enable your timing report to run after the place_opt_to_initial_opto and place_opt_to_final_opto checkpoints, use the following command?", "response": "- Specify the checkpoint report name with the -enable option. - To enable the report to run before one or more checkpoints, specify the checkpoint names with the -before option. - To enable the report to run after one or more checkpoints, specify the checkpoint names with the -after option. When associating a report with multiple checkpoints, you can list the checkpoint names or specify the asterisk wildcard character. Assume you have defined three checkpoints in your script named remove_buffers, place_opt_to_initial_opto, and place_opt_to_final_opto using the eval_checkpoint command, and two checkpoint reports named timing and app_options using the create_checkpoint_report command. To enable your timing report to run after the place_opt_to_initial_opto and place_opt_to_final_opto checkpoints, use the following command: associate_checkpoint_report -enable timing \\ -after { place_opt_to_initial_opto place_opt_to_final_opto } To enable your app_options report to run before all the checkpoints in your script, use the following command: associate_checkpoint_report -enable app_options -before * You can also add the command like this, which enables the report to run before the remove_buffers checkpoint and any checkpoints beginning with the phrase place_opt: associate_checkpoint_report -enable app_options -before remove_buffers place_opt* The following is a portion of your checkpointed script showing what happens when the script runs. Note that the highlighted code is not actually in your script, but shows the checkpoint behaviors that are executed when the tool encounters the checkpoints in the run. - The code that is commented out identifies the original script body - The blue code identifies the checkpointed Tcl commands in your golden flow - The green code identifies the app_options report, configured to run before each checkpoint - The purple code identifies the timing report, configured to run after the place_opt_to_final_opto checkpoint"}
{"instruction": "How do I - placement area - port locations - cell locations - placement blockages?", "response": "- Placement Area - Port Locations - Cell Locations - Placement Blockages"}
{"instruction": "How do I - and y-increments of the array. for example, fc_shell> create_mim_capacitor_array \\ -lib_cell my_lib/mim_ref_cell -x_increment 20 -y_increment 20 by default, the create_mim_capacitor_array command - inserts mim capacitors for the entire block to restrict the insertion to a specific region, use the -boundary option to specify the rectangular or rectilinear region. you can specify only a single region. the command does not consider standard cells, macros, placement blockages, or voltage areas when inserting the mim capacitors. - inserts the mim capacitors in the r0 orientation to change the orientation, use the -orientation option. when you change the orientation, it affects all mim capacitor cells in the inserted array. - uses the following naming convention for the inserted mim capacitor cells?", "response": "- and y-increments of the array. For example, fc_shell> create_mim_capacitor_array \\ -lib_cell my_lib/mim_ref_cell -x_increment 20 -y_increment 20 By default, the create_mim_capacitor_array command - Inserts MiM capacitors for the entire block To restrict the insertion to a specific region, use the -boundary option to specify the rectangular or rectilinear region. You can specify only a single region. The command does not consider standard cells, macros, placement blockages, or voltage areas when inserting the MiM capacitors. - Inserts the MiM capacitors in the R0 orientation To change the orientation, use the -orientation option. When you change the orientation, it affects all MiM capacitor cells in the inserted array. - Uses the following naming convention for the inserted MiM capacitor cells: mimcap!library_cell_name!number To identify the MiM capacitor cells inserted in a specific run, use the -prefix option to specify a prefix string. When you use this option, the tool uses the following naming convention: mimcap!prefix!library_cell_name!number Inserting Filler Cells To ensure that all power nets are connected, you can fill empty space in the standard-cell rows with filler cells. The Fusion Compiler tool supports filler cells with and without metal"}
{"instruction": "How do I - insert additional metal and via fill to specific layers or regions, as described in adding to existing metal and via fill. - replacing existing metal and via fill in specific locations, as described in replacing existing metal and via fill. - replacing existing metal and via fill in modified regions, as described in performing metal fill insertion only in modified regions. adding to existing metal and via fill to insert metal and via fill without first removing the existing metal fill, use the -mode add option with the signoff_create_metal_fill command. when you use this mode, you can control where the metal fill insertion occurs by using one or more of the following options?", "response": "- Insert additional metal and via fill to specific layers or regions, as described in Adding to Existing Metal and Via Fill. - Replacing existing metal and via fill in specific locations, as described in Replacing Existing Metal and Via Fill. - Replacing existing metal and via fill in modified regions, as described in Performing Metal Fill Insertion Only in Modified Regions. Adding to Existing Metal and Via Fill To insert metal and via fill without first removing the existing metal fill, use the -mode add option with the signoff_create_metal_fill command. When you use this mode, you can control where the metal fill insertion occurs by using one or more of the following options: - -select_layers, which restricts the layers on which to perform metal fill insertion, as described in Specifying the Layers for Metal Fill Insertion - -coordinates or -excluded_coordinates, which restrict the regions on which to perform metal fill insertion, as described in Specifying the Regions for Metal Fill Insertion. When you use these options with the -mode add option, the tool does not fix density errors regardless of the setting of the signoff.create_metal_fill.fix_density_errors application option. See Also - Pattern-Based Metal Fill Insertion - Track-Based Metal Fill Insertion Replacing Existing Metal and Via Fill To remove and insert metal and via fill only for the specified locations, use the -mode replace option with the signoff_create_metal_fill command. You would typically"}
{"instruction": "How do I - create_lib -ref_libs you can specify a relationship between a new design library and its lower-level reference libraries by using the create_lib command. for example, fc_shell> create_lib lib_b \\ -ref_libs {../libs/lib_c ../stnd/stdhvt.ndm} ... {lib_b} - set_ref_libs -ref_libs for an existing design library, open the library and then use the set_ref_libs command to specify the reference libraries. for example, fc_shell> current_lib {lib_b} fc_shell> set_ref_libs \\ -ref_libs {../libs/lib_c ../stnd/stdhvt.ndm} ../libs/lib_c ../stnd/stdhvt.ndm - report_ref_libs to report the reference libraries of a design library, use the report_ref_libs command.?", "response": "- create_lib -ref_libs You can specify a relationship between a new design library and its lower-level reference libraries by using the create_lib command. For example, fc_shell> create_lib lib_B \\ -ref_libs {../LIBS/lib_c ../STND/stdhvt.ndm} ... {lib_B} - set_ref_libs -ref_libs For an existing design library, open the library and then use the set_ref_libs command to specify the reference libraries. For example, fc_shell> current_lib {lib_B} fc_shell> set_ref_libs \\ -ref_libs {../LIBS/lib_C ../STND/stdhvt.ndm} ../LIBS/lib_C ../STND/stdhvt.ndm - report_ref_libs To report the reference libraries of a design library, use the report_ref_libs command."}
{"instruction": "How do I - by default, zroute reuses existing filler cell locations for diode insertion. to prevent zroute from reusing these locations, set the route.detail.reuse_filler_locations_for_diodes application option to false. zroute considers voltage areas when inserting diode cells and also observes the logic hierarchy assignments for diode cells. - if a pin has an antenna violation, the diode cells are inserted at the same level of logic hierarchy as the violating pin. - if a top-level port has an antenna violation, by default, the diode cells are inserted at the top level. however, if the port belongs to a voltage area, you can insert the diode cells in the logic hierarchy associated with the voltage area by setting the route.detail.use_lower_hierarchy_for_port_diodes application option to true. to remove the redundant diodes during routing, set the route.detail.delete_redundant_diodes_during_routing application option to true. inserting diodes after detail routing you can fix antenna violations after detail routing by explicitly specifying which violations to fix and providing constraints for fixing them. based on the specified constraints and the setting of the route.detail.diode_insertion_mode application option, zroute either inserts new diodes or reuses existing spare diodes to fix the specified violations. to insert diodes after detail routing, use the create_diodes command. when you use this command, you must use the -options option to specify the location of each antenna violation to fix by specifying the port and cell instance, the reference cell for the diode, the number of diodes to insert, the highest allowed routing layer used for connecting the diode, and the maximum distance from the specified pin that the diode can be inserted. if a diode cannot be inserted or reused within the specified distance, the tool does not insert a diode for that violation. use the following format to specify these values?", "response": "- By default, Zroute reuses existing filler cell locations for diode insertion. To prevent Zroute from reusing these locations, set the route.detail.reuse_filler_locations_for_diodes application option to false. Zroute considers voltage areas when inserting diode cells and also observes the logic hierarchy assignments for diode cells. - If a pin has an antenna violation, the diode cells are inserted at the same level of logic hierarchy as the violating pin. - If a top-level port has an antenna violation, by default, the diode cells are inserted at the top level. However, if the port belongs to a voltage area, you can insert the diode cells in the logic hierarchy associated with the voltage area by setting the route.detail.use_lower_hierarchy_for_port_diodes application option to true. To remove the redundant diodes during routing, set the route.detail.delete_redundant_diodes_during_routing application option to true. Inserting Diodes After Detail Routing You can fix antenna violations after detail routing by explicitly specifying which violations to fix and providing constraints for fixing them. Based on the specified constraints and the setting of the route.detail.diode_insertion_mode application option, Zroute either inserts new diodes or reuses existing spare diodes to fix the specified violations. To insert diodes after detail routing, use the create_diodes command. When you use this command, you must use the -options option to specify the location of each antenna violation to fix by specifying the port and cell instance, the reference cell for the diode, the number of diodes to insert, the highest allowed routing layer used for connecting the diode, and the maximum distance from the specified pin that the diode can be inserted. If a diode cannot be inserted or reused within the specified distance, the tool does not insert a diode for that violation. Use the following format to specify these values: {port_name instance_name diode_reference number_of_diodes max_routing_layer max_routing_distance} Note: You can use this command to insert diodes for top-level ports by specifying the name of the top-level block for the cell instance."}
{"instruction": "How do I 44. to set the application options, use the set_app_options command. to see the current settings, use the report_app_options command. table 44 application options for signoff drc fixing application option default description signoff.fix_drc. advanced_guidance_for_rules all controls whether signoff drc fixing uses advanced routing guidance. by default, the signoff_fix_drc command uses advanced routing guidance (all). to disable this feature, set the application option to off. signoff.fix_drc.check_drc local controls whether signoff drc checking is performed only on those portions of the block affected by signoff drc fixing (local, which is the default) or on the entire block (global). signoff.fix_drc. custom_guidance (none) enables a signoff drc fixing run that targets double-patterning odd-cycle violations when it is set to dpt. for information about fixing double-patterning odd-cycle violations, see automatically fixing double-patterning odd-cycle violations.?", "response": "44. To set the application options, use the set_app_options command. To see the current settings, use the report_app_options command. Table 44 Application Options for Signoff DRC Fixing Application option Default Description signoff.fix_drc. advanced_guidance_for_rules all Controls whether signoff DRC fixing uses advanced routing guidance. By default, the signoff_fix_drc command uses advanced routing guidance (all). To disable this feature, set the application option to off. signoff.fix_drc.check_drc local Controls whether signoff DRC checking is performed only on those portions of the block affected by signoff DRC fixing (local, which is the default) or on the entire block (global). signoff.fix_drc. custom_guidance (none) Enables a signoff DRC fixing run that targets double-patterning odd-cycle violations when it is set to dpt. For information about fixing double-patterning odd-cycle violations, see Automatically Fixing Double-Patterning Odd-Cycle Violations."}
{"instruction": "How do I - the naming convention used to identify the inserted tap cell instances by default, the tool uses the following naming convention for inserted tap cells?", "response": "- The naming convention used to identify the inserted tap cell instances By default, the tool uses the following naming convention for inserted tap cells: tapfiller!library_cell_name!number Use the -separator option to change the separator character from its default of \u201c!.\u201d To identify the tap cells inserted in a specific run, use the -prefix option to specify a prefix string. When you use this option, the tool uses the following naming convention: tapfiller!prefix!library_cell_name!number Using the create_cell_array Command ad_tap_cel_aray comandcomandsad_tap_cel_arayTo add a tap cell array with specific placement, use the create_cell_array command. You must specify the name of the library cell to use for tap cell insertion (-lib_cell option), and the x- and y-pitches. For example, fc_shell> create_cell_array -lib_cell myreflib/mytapcell \\ -x_pitch 10 -y_pitch 10 By default, the create_cell_array command inserts a tap cell in each location in the specified grid for the entire block. The tool starts inserting the tap cells at the bottom-left of the core area and uses the specified x- and y-pitches to determine the location of the subsequent tap cells. The inserted tap cells are snapped to the site rows and placed in the default orientation for the site row. If a location is occupied by a fixed cell, hard macro, soft macro, or power-switch cell, the command does not insert a tap cell at that location. You can modify the following aspects of the default behavior: - The region in which to insert tap cells Use the -voltage area option to restrict the tap cell insertion to the specified voltage areas. Use the -boundary option to restrict the tap cell insertion to the specified region. If you use both options, the command inserts tap cells only in the overlapping region. - The offset from the insertion region boundary Use the -x_offset option to shift the pattern startpoint to the right by the specified distance in microns. Use the -y_offset option to shift the pattern startpoint up by the specified distance in microns."}
{"instruction": "How do I - specify the maximum allowed manhattan distance between the sinks in one relative placement group by using the -distance option. if the manhattan distance between sinks is more than the specified distance, they are put into separate relative placement groups. the default distance is 100 microns. - specify a maximum number of rows by using the -max_rp_rows option. the default is 32. - allow the tool to decide the number of rows and columns based on the distribution of the cells by using the -auto_shape option. before you run the create_clock_rp_groups command, the block must be placed. after you create the clock relative placement groups, reset the placement of the block by using the reset_placement command, and rerun placement and optimization by using the place_opt command. performing placement and legalization of relative placement groups the following topics provide information related to the placement and legalization of relative placement groups?", "response": "- Specify the maximum allowed Manhattan distance between the sinks in one relative placement group by using the -distance option. If the Manhattan distance between sinks is more than the specified distance, they are put into separate relative placement groups. The default distance is 100 microns. - Specify a maximum number of rows by using the -max_rp_rows option. The default is 32. - Allow the tool to decide the number of rows and columns based on the distribution of the cells by using the -auto_shape option. Before you run the create_clock_rp_groups command, the block must be placed. After you create the clock relative placement groups, reset the placement of the block by using the reset_placement command, and rerun placement and optimization by using the place_opt command. Performing Placement and Legalization of Relative Placement Groups The following topics provide information related to the placement and legalization of relative placement groups: - Relative Placement in a Design Containing Obstructions - Legalizing Relative Placement Groups in a Placed Design - Creating New Relative Placement Groups in a Placed Design Relative Placement in a Design Containing Obstructions During placement, relative placement groups avoid placement blockages (obstructions) that are defined in the DEF file or created by the create_placement_blockage command. A relative placement group can be broken into pieces that straddle obstructions, yet maintain the relative placement structure. If the height of the obstruction is below a certain threshold, the relative placement cells are shifted vertically; otherwise, the relative placement column is shifted horizontally. Figure 185 shows the placement of relative placement cells in a design containing obstructions that are either defined in the DEF file or created by create_placement_blockage. The obstruction in columns one and two is below the"}
{"instruction": "How do I 1. apply the signal electromigration constraints by using the read_signal_em_constraints command. for more information, see loading the signal electromigration constraints. 2. apply switching activity by either reading in a saif file with the read_saif command or annotating the switching activity information on the nets with the set_switching_activity command. for more information, see annotating the switching activity. 3. report the signal electromigration information by using the report_signal_em command. for more information, see analyzing signal electromigration. 4. fix any signal electromigration violations by using the fix_signal_em command. for more information, see fixing signal electromigration violations. the following example script shows the signal electromigration flow. # open the block and apply the signal electromigration constraints open_block block1_routed read_signal_em_constraints em.itf # load switching information read_saif block1.saif # perform signal electromigration analysis report_signal_em -violated -verbose > block1.signal_em.rpt # fix signal electromigration violations fix_signal_em?", "response": "1. Apply the signal electromigration constraints by using the read_signal_em_constraints command. For more information, see Loading the Signal Electromigration Constraints. 2. Apply switching activity by either reading in a SAIF file with the read_saif command or annotating the switching activity information on the nets with the set_switching_activity command. For more information, see Annotating the Switching Activity. 3. Report the signal electromigration information by using the report_signal_em command. For more information, see Analyzing Signal Electromigration. 4. Fix any signal electromigration violations by using the fix_signal_em command. For more information, see Fixing Signal Electromigration Violations. The following example script shows the signal electromigration flow. # Open the block and apply the signal electromigration constraints open_block block1_routed read_signal_em_constraints em.itf # Load switching information read_saif block1.saif # Perform signal electromigration analysis report_signal_em -violated -verbose > block1.signal_em.rpt # Fix signal electromigration violations fix_signal_em"}
{"instruction": "How do I - in this situation, you should ignore the lexical_device_threshold_voltage attribute when comparing the two libraries. to do so, use the following application option settings before you run the compare_libraries command?", "response": "- In this situation, you should ignore the lexical_device_threshold_voltage attribute when comparing the two libraries. To do so, use the following application option settings before you run the compare_libraries command: fc_shell> set_app_options -name libra.complib.match_full_cell_name -value false fc_shell> set_app_options -name libra.complib.match_lexical_device_threshold_voltage -value false fc_shell> compare_libraries -ref_lib $rvt_lib -ref_lib_analysis sa \\ -target_lib $lvt_lib -target_lib_analysis sa -verbose -plot Note: For a complete list of application options you can use to control name matching during library comparison, see the man page for the compare_libraries command. \u25e6 The reference and target libraries can have a similar naming convention, but the lexical attributes values might not identical. For example, assume a cell named ND2D1SVT in the reference library matches a cell named nand2_svth_x1 in the target library. In this case, the reference and target libraries have the same lexical attributes, which have similar but not the same values, as shown in the following table. Table 5 Reference and Target Library Lexical Attribute Differences Lexical Attribute Reference Library Target Library lexical_cell_prefix_name ND2 nand2 lexical_drive1_name D1 x1 lexical_device_threshold_voltage SVT svth In this situation, you must create a lexical attribute mapping file that contains the following information: lexical_cell_prefix_name ND2 nand2 lexical_drive1_name D1 x1 lexical_device_threshold_voltage SVT svth"}
{"instruction": "How do I - signoff design rule checking (the signoff_check_drc command) - fixing the violations detected during signoff design rule checking (the signoff_fix_drc command) - augmenting the power grid based on voltage drop information (the signoff_create_pg_augmentation command) - inserting metal fill (the signoff_create_metal_fill command) - fixing isolated vias (the signoff_fix_isolated_via command) note?", "response": "- Signoff design rule checking (the signoff_check_drc command) - Fixing the violations detected during signoff design rule checking (the signoff_fix_drc command) - Augmenting the power grid based on voltage drop information (the signoff_create_pg_augmentation command) - Inserting metal fill (the signoff_create_metal_fill command) - Fixing isolated vias (the signoff_fix_isolated_via command) Note: An IC Validator license is required to run the IC Validator In-Design functions. To learn about using these IC Validator In-Design functions, see the following topics: - Preparing to Run IC Validator In-Design Commands - Performing Signoff Design Rule Checking - Automatically Fixing Signoff DRC Violations - Improving Instance Voltage Drop by Augmenting the Power Grid - Inserting Metal Fill With IC Validator In-Design - Automatically Fixing Isolated Vias"}
{"instruction": "How do I 1. read in the full chip design and apply constraints as shown in the following example?", "response": "1. Read in the full chip design and apply constraints as shown in the following example: fc_shell> set REF_LIBS \"stdcell.ndm macro.ndm\" fc_shell> create_lib TOP -technology techfile \\ -ref_libs $REF_LIBS fc_shell> analyze -format verilog $rtl_files fc_shell> elaborate TOP fc_shell> set_top_module TOP fc_shell> load_upf fullchip.upf fc_shell> read_sdc fullchip.sdc 2. Identify the design partitions and split the constraints as shown in the following example: fc_shell> set_budget_options -add_blocks {BLOCK1 BLOCK2} fc_shell> split_constraints 3. Create the subblock design libraries with design information and enable block-specific reference library setup as shown in the following example: fc_shell> copy_lib -to_lib BLOCK1.nlib -no_design fc_shell> copy_lib -to_lib BLOCK2.nlib -no_design"}
{"instruction": "How do I - creating etms in the primetime tool - creating etm cell libraries in the library manager tool?", "response": "- Creating ETMs in the PrimeTime Tool - Creating ETM Cell Libraries in the Library Manager Tool"}
