{"instruction": "How do I - specifying redhawk and redhawk-sc working directories - writing analysis and checking reports performing distributed redhawk fusion rail analysis note?", "response": "- Specifying RedHawk and RedHawk-SC Working Directories - Writing Analysis and Checking Reports Performing Distributed RedHawk Fusion Rail Analysis Note: The distributed rail analysis feature is supported only in the RedHawk Fusion analysis flow. By default, the analyze_rail command uses a single process to perform RedHawk rail analysis. To reduce memory usage and total runtime, use the distributed processing method to run RedHawk Fusion on a machine other than the one on which the Fusion Compiler tool is run. You can submit multiple RedHawk jobs to the target farm machines and run the jobs in parallel in the same Fusion Compiler session. This allows you to run multiple RedHawk analyses with only one Fusion Compiler license. Each of the RedHawk jobs requires one SNPS_INDESIGN_RH_RAIL license key. To examine the generated analysis result, first run the open_rail_result command to load the result that is saved in the directory you specify. You can then investigate the problematic areas in the map and check for violations in a text file or in an error view. You can open one result context at a time. To load the rail context generated in another analysis run, first close the current rail result using the close_rail_result command, and then open the desired rail result (see Displaying Maps in the GUI). License Requirement By default, the number of required licenses is the number of script files or configurations specified with the -multiple_script_files option during distributed processing mode. If the number of available licenses is less than the number of script files at the time when the analyze_rail command is run, the command stops with an error message. To check for available SNPS_INDESIGN_RH_RAIL licenses continuously during the analyze_rail run, set the rail.license_checkout_mode application option to wait. If the tool detects that a license becomes available, the tool reuses or checks out the license and submits a script to a farm machine immediately. By default, the rail.license_checkout_mode application option is set to nowait. When the analysis is finished, run the remove_licenses command to release the SNPS_INDESIGN_RH_RAIL licenses. fc_shell> remove_licenses SNPS_INDESIGN_RH_RAIL"}
{"instruction": "How do I note?", "response": "Note: The remove_clock_trees command does not support clock mesh nets. By default, this command removes the buffers and inverters from all clock trees in all modes. To remove the buffers and inverters from specific clock trees, use the -clocks option to specify the clock trees. By default, when you use the -clocks option, the clocks are selected from the current mode. To select clocks from a specific mode, use the get_clocks -mode command to select the clocks. For example, to remove only the clock tree in the current mode named my_clk, use the following command: fc_shell> remove_clock_trees -clocks [get_clocks my_clk] Table 28 shows how clock tree removal is affected by the structure of the clock tree. Table 28 Clock Tree Removal Behavior Object Impact on clock tree removal Boundary cell Removed. Cells on don\u2019t touch net Preserved. Don\u2019t touch cell Preserved. Fixed cell Preserved. Generated clock Preserved, if generated clock is defined on buffer/inverter pin. Traversal and clock tree removal continue past the generated clock. Guide buffer Removed. Integrated clock-gating (ICG) cell Preserved. Traversal (and clock tree removal) continues past the integrated clock-gating cell. Block abstraction model Preserved. Traversal (and clock tree removal) continues past the block abstraction model. Inverter Removed in pairs only. If a clock tree contains a single inverter, it is not removed. Isolation cell Preserved. Level shifter Preserved."}
{"instruction": "How do I - remove_objects - set_attribute - set_via_def - add_to_edit_group if an object belongs to a via ladder, these commands issue a warning message and do not process the object. the following example sets the design.enable_via_ladder_protection application option to true and attempts to remove a collection of shapes?", "response": "- remove_objects - set_attribute - set_via_def - add_to_edit_group If an object belongs to a via ladder, these commands issue a warning message and do not process the object. The following example sets the design.enable_via_ladder_protection application option to true and attempts to remove a collection of shapes: fc_shell> set_app_options -name design.enable_via_ladder_protection \\ -value true ... fc_shell> sizeof_collection [get_shapes -of net1] 77 fc_shell> remove_shapes [get_shapes -of net1] -verbose Warning: Cannot edit PATH_32_7403 because it is part of via_ladder. (NDM-160) 76 Verifying Via Ladders To verify that the via ladders in the block match the via ladder constraints and are properly connected to pins, use the verify_via_ladders command. By default, this command checks all via ladders. To restrict the checking to specific nets, use the -nets option. Note: If you use the -shift_vias_on_transition_layers true option when you insert the via ladders, you must also use this option when you verify the via ladders; otherwise, the command reports false violations. A via ladder matches the via ladder constraints if the pattern of connected vias and wires for the via ladder structure satisfies any of the via ladder templates assigned to the connected pin. The command flags a violation in the following situations: - A via ladder is connected to a pin that does not have a via ladder constraint - A via ladder is connected to a pin with a via ladder constraint, but none of the templates correctly describe the current via ladder structure - A pin has a via ladder constraint, but does not have a via ladder connected to it"}
{"instruction": "How do I - you can create rail scenarios and associate them with a design scenario in the current design. this allows you to identify possible power integrity issues by running optimization and rail analysis on one or more rail scenarios. for details, see creating and specifying rail scenarios for rail analysis. specifying a design scenario for rail analysis by default, redhawk or redhawk-sc fusion analyzes only the current design scenario for a multicorner-multimode design. to specify a different design scenario for rail analysis, use the rail.scenario_name application option. the tool honors the specified design scenario for the ir-driven features in power integrity flow, such as ir-driven placement, ir- driven concurrent clock and data optimization, and ir-driven optimization.\u200b here is a script example?", "response": "- You can create rail scenarios and associate them with a design scenario in the current design. This allows you to identify possible power integrity issues by running optimization and rail analysis on one or more rail scenarios. For details, see Creating and Specifying Rail Scenarios for Rail Analysis. Specifying a Design Scenario for Rail Analysis By default, RedHawk or RedHawk-SC Fusion analyzes only the current design scenario for a multicorner-multimode design. To specify a different design scenario for rail analysis, use the rail.scenario_name application option. The tool honors the specified design scenario for the IR-driven features in power integrity flow, such as IR-driven placement, IR- driven concurrent clock and data optimization, and IR-driven optimization.\u200b Here is a script example: open_lib open_block set_app_option -name rail.technology -value 7 set_app_options -name rail.tech_file -value ./Apache.tech set_app_options -name rail.lib_files -value {\\ A.lib \\ B.lib } set_app_options -name rail.switch_model_files -value {./switch_cells.txt } source ./test_taps.tcl current_scenario func_max set_app_options -name rail.scenario_name -value func_typ analyze_rail -voltage_drop Creating and Specifying Rail Scenarios for Rail Analysis You can create multiple rail scenarios and associate them with a design scenario in the current design. The created rail scenarios are saved in the design library. To run rail analysis on multiple rail scenarios: 1. Specify the following application options to enable multi-rail-scenario rail analysis: fc_shell> set_app_options -list { rail.enable_new_rail_scenario true rail.enable_parallel_run_rail_scenario true }"}
{"instruction": "How do I 106. this technique increases the flexibility of drc convergence without adversely affecting crosstalk. figure 106 ignoring nondefault spacing rule violations to define the length thresholds within which to ignore the nondefault spacing violations, use the -spacing_length_thresholds option when you create the nondefault routing rule with the create_routing_rule command. the length threshold values are in microns and must have a one-to-one correspondence with the spacing entries specified in the -spacings option. for example, fc_shell> create_routing_rule new_rule \\ -spacings { m1 {0.09 0.15 0.2} m2 {0.09 0.15 0.2} m3 {0.09 0.15 0.2} } \\ -spacing_weight_levels { m1 {hard medium low} m2 {hard medium low} m3 {hard medium low} } \\ -spacing_length_thresholds { m1 {0.01 0 0} m2 {0.01 0 0} m3 {0.01 0 0} } specifying the routing effort for soft spacing violations by default, zroute does not fix soft spacing violations. to fix soft spacing violations, you must assign a routing effort to each of the soft weight levels. these assignments apply to all soft spacing rules.?", "response": "106. This technique increases the flexibility of DRC convergence without adversely affecting crosstalk. Figure 106 Ignoring Nondefault Spacing Rule Violations To define the length thresholds within which to ignore the nondefault spacing violations, use the -spacing_length_thresholds option when you create the nondefault routing rule with the create_routing_rule command. The length threshold values are in microns and must have a one-to-one correspondence with the spacing entries specified in the -spacings option. For example, fc_shell> create_routing_rule new_rule \\ -spacings { M1 {0.09 0.15 0.2} M2 {0.09 0.15 0.2} M3 {0.09 0.15 0.2} } \\ -spacing_weight_levels { M1 {hard medium low} M2 {hard medium low} M3 {hard medium low} } \\ -spacing_length_thresholds { M1 {0.01 0 0} M2 {0.01 0 0} M3 {0.01 0 0} } Specifying the Routing Effort for Soft Spacing Violations By default, Zroute does not fix soft spacing violations. To fix soft spacing violations, you must assign a routing effort to each of the soft weight levels. These assignments apply to all soft spacing rules."}
{"instruction": "How do I - htree_sections [ list \\ [list -section_name \u201csectiona_name\u201d -tap_boundary {{315.8800 1275.8000} {1750.0400 2187.1000}} -prefix \u201csectiona\u201d -tap_boxes {4 4}] \\ [list -section_name \u201csectionb_name\u201d -tap_locations {{700472} {1350 472}} -prefix \u201csection\u201d]] figure 87 mscts multiple h-tree implementing power aware flexible h-tree the tool derives tap driver configurations by adding power awareness in the flexible h-tree synthesis flow, which is optimal in terms of total clock power and overall clock latency.?", "response": "- htree_sections [ list \\ [list -section_name \u201csectionA_name\u201d -tap_boundary {{315.8800 1275.8000} {1750.0400 2187.1000}} -prefix \u201csectionA\u201d -tap_boxes {4 4}] \\ [list -section_name \u201csectionB_name\u201d -tap_locations {{700472} {1350 472}} -prefix \u201csection\u201d]] Figure 87 MSCTS Multiple H-Tree Implementing Power Aware Flexible H-Tree The tool derives tap driver configurations by adding power awareness in the flexible H-Tree synthesis flow, which is optimal in terms of total clock power and overall clock latency."}
{"instruction": "How do I - dynamic analysis with custom macro models - dynamic analysis with package models - analysis with rtl-level vcd files to perform redhawk signoff analysis, 1. modify your gsr file or redhawk run script to include the related configuration settings. 2. set the following application option to enable the redhawk signoff features?", "response": "- Dynamic analysis with custom macro models - Dynamic analysis with package models - Analysis with RTL-level VCD files To perform RedHawk signoff analysis, 1. Modify your GSR file or RedHawk run script to include the related configuration settings. 2. Set the following application option to enable the RedHawk signoff features: fc_shell> set_app_options \\ -name rail.allow_redhawk_license_checkout -value true Setting the rail.allow_redhawk_license_checkout application option to true allows the RedHawk tool to retrieve the related RedHawk licenses for performing signoff features inside the Fusion Compiler tool. The default is off. 3. Run the analyze_rail command with the -extra_gsr_option_file or -redhawk_script_file option to perform the analysis defined in the GSR or RedHawk script file. Specify other settings as necessary. Note: You cannot display the RedHawk signoff analysis results in the Fusion Compiler GUI. 4. Proceed to other steps in the analysis flow. See Also - An Overview for RedHawk Fusion and RedHawk-SC Fusion Writing Analysis and Checking Reports When the checking or analysis is complete, run the report_rail_result command to write the analysis or checking results to a text file. The power unit in the report file is watts, the current unit is amperes, and the voltage unit is volts. To display block-level rail results from top-level RAIL_DATABASE for debugging purposes, use the-top_design and -block_instance options. For more information, see Displaying Block-Level Rail Results. To write rail results for the cells or geometries in the hotspot area only, use the get_instance_result and get_geometry_result commands. For more information, see Generating Instance-Based Analysis Reports and Generating Geometry-Based Analysis Reports."}
{"instruction": "How do I 1. read in the saif file by using the read_saif command. 2. enable scaling by setting the power.use_generated_clock_scaling_factor application option to on. 3. scale the switching activity by using the set_power_clock_scaling command. when you use this command, you must specify the following?", "response": "1. Read in the SAIF file by using the read_saif command. 2. Enable scaling by setting the power.use_generated_clock_scaling_factor application option to on. 3. Scale the switching activity by using the set_power_clock_scaling command. When you use this command, you must specify the following: \u25e6 The clock objects associated with the switching activity you want to scale \u25e6 The clock period used in the SAIF file by using the -period option or the ratio between the clock period used in the SAIF file and the clock period used in the Fusion Compiler tool by using the -ratio option In addition, you can specify the scenario for which to apply the scaling by using the -scenario option."}
{"instruction": "How do I - custom.route.bus_split_ignore_width - custom.route.bus_split_even_bits the custom.route.bus_split_ignore_width application option specifies the wire width threshold of any obstructing power grid or route that triggers the bus trunk to be split into more than one section. the custom.route.bus_split_even_bits application option requires that you set the custom.route.bus_split_ignore_width application option. if the custom.route.bus_split_ignore_width requirement is met, you can use the custom.route.bus_split_even_bits application option to control whether automatic bus splitting routes an even number of nets between pre-routes (power stripes). in the following example, the bus trunk consists of four bits and the custom.route.bus_split_ignore_width application option is set. the spacing between the power and ground rails can accommodate three bits only. figure 163a shows the two possible results when the custom.route.bus_split_even_bits application option is turned off. figure 163b shows the result when the custom.route.bus_split_even_bits application option is turned on. figure 163 splitting bus trunk to route through power and ground rails?", "response": "- custom.route.bus_split_ignore_width - custom.route.bus_split_even_bits The custom.route.bus_split_ignore_width application option specifies the wire width threshold of any obstructing power grid or route that triggers the bus trunk to be split into more than one section. The custom.route.bus_split_even_bits application option requires that you set the custom.route.bus_split_ignore_width application option. If the custom.route.bus_split_ignore_width requirement is met, you can use the custom.route.bus_split_even_bits application option to control whether automatic bus splitting routes an even number of nets between pre-routes (power stripes). In the following example, the bus trunk consists of four bits and the custom.route.bus_split_ignore_width application option is set. The spacing between the power and ground rails can accommodate three bits only. Figure 163a shows the two possible results when the custom.route.bus_split_even_bits application option is turned off. Figure 163b shows the result when the custom.route.bus_split_even_bits application option is turned on. Figure 163 Splitting Bus Trunk to Route Through Power and Ground Rails"}
{"instruction": "How do I 1616.475 1967.000 } { 1617.525 1969.800 } } metal5 0.00448 { { 1608.810 1946.000 } { 1611.190 1974.000 } } metal7 0.00446 { { 1608.000 1946.000 } { 1612.000 1974.000 } } metal9 0.00446 see also - voltage hotspot analysis displaying maps in the gui when analysis is complete, the tool saves the design data and analysis results (*.result) in the in-design.redhawk/design_name.result directory under the redhawk working directory. you must run the open_rail_result command to load the analysis results before displaying a map in the fusion compiler gui. table 70 lists the map types that are supported in the redhawk fusion analysis flow. table 70 analysis maps supported in redhawk fusion flow map type description rail voltage drop map displays the voltage drop map, which is a color-coded display of voltage drop values overlaid on the physical supply nets. for static analysis, the map displays average voltage drop values. for dynamic analysis, the map displays peak voltage drop values, average voltage drop values, or peak voltage rise values. note?", "response": "1616.475 1967.000 } { 1617.525 1969.800 } } metal5 0.00448 { { 1608.810 1946.000 } { 1611.190 1974.000 } } metal7 0.00446 { { 1608.000 1946.000 } { 1612.000 1974.000 } } metal9 0.00446 See Also - Voltage Hotspot Analysis Displaying Maps in the GUI When analysis is complete, the tool saves the design data and analysis results (*.result) in the in-design.redhawk/design_name.result directory under the RedHawk working directory. You must run the open_rail_result command to load the analysis results before displaying a map in the Fusion Compiler GUI. Table 70 lists the map types that are supported in the RedHawk Fusion analysis flow. Table 70 Analysis Maps Supported in RedHawk Fusion Flow Map Type Description Rail voltage drop map Displays the voltage drop map, which is a color-coded display of voltage drop values overlaid on the physical supply nets. For static analysis, the map displays average voltage drop values. For dynamic analysis, the map displays peak voltage drop values, average voltage drop values, or peak voltage rise values. Note: The tool shows all the layers in the LEF or DEF file on the voltage drop map even if the layers are absent in the Fusion Compiler library file. Rail parasitics map Shows parasitic resistance of a given supply net in the block when voltage drop analysis is complete. It shows the resistance for any given shape of the net. Rail power map Displays instance-based power map or power density map for the block when voltage drop analysis is complete, based on the RedHawk power calculation results. Instance-based power map: Shows power values of the cell instances in the block Power density map: Shows power density values for area in the block Rail current map Displays current distribution for the block when voltage drop analysis is complete. Rail minimal path resistance map Shows minimum path resistance values for a selected net when minimal path resistance analysis is complete. Rail electromigration map Shows current violations for the block when static or dynamic electromigration analysis is complete."}
{"instruction": "How do I 11. perform tap assignment by using the synthesize_multisource_clock_taps command, which \u25e6 merges equivalent clock cells to remove any artificial boundaries between clusters of sinks \u25e6 assigns endpoints to the closest tap driver and split cells along the path honoring any sink groups defined \u25e6 copies the upf and sdc constraints and the user-specified attributes onto the newly created cells across the active scenarios 12. synthesize the entire clock tree, from the clock root, by using the following command?", "response": "11. Perform tap assignment by using the synthesize_multisource_clock_taps command, which \u25e6 Merges equivalent clock cells to remove any artificial boundaries between clusters of sinks \u25e6 Assigns endpoints to the closest tap driver and split cells along the path honoring any sink groups defined \u25e6 Copies the UPF and SDC constraints and the user-specified attributes onto the newly created cells across the active scenarios 12. Synthesize the entire clock tree, from the clock root, by using the following command: fc_shell> clock_opt -from build_clock -to route_clock During this step, the tool builds the local subtrees that are driven by the tap drivers. It also fixes DRC violations beyond ignore exceptions of the multisource clock tree, if applicable. You can also synthesize other clocks in the design that are not synthesized. For more information about the clock_opt command, see Synthesizing, Optimizing, and Routing Clock Trees With the clock_opt Command. 13. Analyze the clock tree results as described in Analyzing the Clock Tree Results. Implementing a Regular Multisource Clock Tree Using Integrated Tap Assignment A regular multisource clock tree has tap drivers that are driven by a clock mesh and drive clock-gating cells and sinks. If you insert these tap drivers before you run the compile_fusion command, the tool can assign sinks to these drivers such that it improves the overall timing QoR of the block. It also allows the tool to improve the timing of clock-gating paths. To perform regular multisource clock tree synthesis integrated with placement and optimization, use the following steps: 1. Specify the placement and optimization constraints and settings. 2. Specify the clock tree constraints and settings."}
{"instruction": "How do I - to create a comb routing topology, use the -topology comb option. in a comb topology, each driver and load pin is directly routed to the nearest stripe. however, if the manhattan distance from a pin to the nearest stripe is more than the comb distance, the tool routes the pin to the nearest net shape using a steiner topology, as shown in figure 94. the default comb distance is two global routing cells. to change the comb distance, use the route.common.comb_distance application option.?", "response": "- To create a comb routing topology, use the -topology comb option. In a comb topology, each driver and load pin is directly routed to the nearest stripe. However, if the Manhattan distance from a pin to the nearest stripe is more than the comb distance, the tool routes the pin to the nearest net shape using a Steiner topology, as shown in Figure 94. The default comb distance is two global routing cells. To change the comb distance, use the route.common.comb_distance application option."}
{"instruction": "How do I - derive cut mask constraints for the specified nets or vias. fc_shell> derive_mask_constraint -derive_cut_mask \\ [get_vias -within {{600 600} {700 700}}] - derive mask constraints from overlapping or touching pins or ports. fc_shell> derive_mask_constraint -follow_pin_mask \\ [get_shapes -of_objects [get_nets net1]] - derive the mask constraint from the wider overlapping or touching object rather than the wire track. fc_shell> derive_mask_constraint -follow_wider_object_mask \\ [get_shapes -of_objects [get_nets net1]] see also - multiple-patterning concepts inserting and removing cut metal shapes cut metal shapes are used in double-patterning designs to reduce the line-end minimum spacing. a cut metal layer and the dimensions of the cut metal shapes inserted on that layer are defined in a layer section in the technology file. the width of the cut metal shapes is defined by the cutmetalwidth attribute. the height of the cut metal shapes is defined by either the cutmetalheight or cutmetalextension attribute. a cut metal layer corresponds to the metal layer with the same mask number. for example, cutmetal1 corresponds to metal1. for information about the technology file, see the synopsys technology file and routing rules reference manual. after the routing is finalized, you add cut metal shapes by using the create_cut_metals command. this command inserts a cut metal shape between metal shapes when all of the following conditions are met?", "response": "- Derive cut mask constraints for the specified nets or vias. fc_shell> derive_mask_constraint -derive_cut_mask \\ [get_vias -within {{600 600} {700 700}}] - Derive mask constraints from overlapping or touching pins or ports. fc_shell> derive_mask_constraint -follow_pin_mask \\ [get_shapes -of_objects [get_nets net1]] - Derive the mask constraint from the wider overlapping or touching object rather than the wire track. fc_shell> derive_mask_constraint -follow_wider_object_mask \\ [get_shapes -of_objects [get_nets net1]] See Also - Multiple-Patterning Concepts Inserting and Removing Cut Metal Shapes Cut metal shapes are used in double-patterning designs to reduce the line-end minimum spacing. A cut metal layer and the dimensions of the cut metal shapes inserted on that layer are defined in a Layer section in the technology file. The width of the cut metal shapes is defined by the cutMetalWidth attribute. The height of the cut metal shapes is defined by either the cutMetalHeight or cutMetalExtension attribute. A cut metal layer corresponds to the metal layer with the same mask number. For example, cutMetal1 corresponds to metal1. For information about the technology file, see the Synopsys Technology File and Routing Rules Reference Manual. After the routing is finalized, you add cut metal shapes by using the create_cut_metals command. This command inserts a cut metal shape between metal shapes when all of the following conditions are met: - The spacing between the metal shapes in the preferred direction is cutMetalWidth. - There is no existing cut metal shape in the location. - The metal shapes are on a metal layer that corresponds to a cut metal layer defined in the technology file. The command inserts cut metal shapes between net shapes, a net shape and a PG shape, a PG shape and an obstruction, and metal shapes within a child cell. After inserting"}
{"instruction": "How do I - self gating ungated register summary --------------------------------------------------------------------------- ungated reason | count | bitwidth --------------------------------------------------------------------------- register is clock gated, and ineraction | | is set to none | 2 | 2 the tool as not able to find enough registers | | that are compatible to this register | | in order to be gated. | 2 | 2 --------------------------------------------------------------------------- clock-gate levels and stages the tool uses the following definitions for clock-gate levels and stages in clock-gating commands and options?", "response": "- Self Gating Ungated Register Summary --------------------------------------------------------------------------- Ungated Reason | Count | Bitwidth --------------------------------------------------------------------------- Register is clock gated, and ineraction | | is set to none | 2 | 2 The tool as not able to find enough registers | | that are compatible to this register | | in order to be gated. | 2 | 2 --------------------------------------------------------------------------- Clock-Gate Levels and Stages The tool uses the following definitions for clock-gate levels and stages in clock-gating commands and options: - The clock-gate level is related to the fanin of a clock gate. \u25e6 Level 0 refers to a clock source. \u25e6 Level 1 refers to the first clock gate downstream from the clock source. \u25e6 Level 2 refers to a clock gate driven by a clock gate of level 1. \u25e6 Level N refers to a clock gate driven by a clock gate of level N-1. - The clock-gate stage is related to the fanout of a clock gate. \u25e6 Stage 0 refers to a register driven by a clock gate. \u25e6 Stage 1 refers to a clock gate that only drives registers directly or across buffers, inverters, or hierarchical transitions. In addition, stage 1 is assigned to clock gates whose stage cannot be determined, such as unloaded clock gates. \u25e6 Stage N refers to a clock gate that drives at least one clock gate of stage N-1, either directly or across buffers, inverters, or hierarchical transitions. The tool keeps a separate count of levels and stages for all clock gates and for tool- inserted clock gates. Figure 47 shows a simple circuit with two preexisting clock gates (P1 and P2) and three tool-inserted clock gates (T1, T2, and T3). Table 23 lists the levels and stages for each of the clock gates."}
{"instruction": "How do I - introduction to redundancies, outliers, equivalences, and gaps in library cell families - identifying redundancies, outliers, equivalences, and gaps in library cell families - comparing libraries introduction to redundancies, outliers, equivalences, and gaps in library cell families when a family of cells with the same function is ordered in terms of increasing drive strength, specific cell property values should be monotonically increasing or decreasing. for example, when a family is ordered in terms of increasing drive strength, the area of cells should be monotonically increasing, power should be increasing, and delay (for the same output load and input slew) should be decreasing.?", "response": "- Introduction to Redundancies, Outliers, Equivalences, and Gaps in Library Cell Families - Identifying Redundancies, Outliers, Equivalences, and Gaps in Library Cell Families - Comparing Libraries Introduction to Redundancies, Outliers, Equivalences, and Gaps in Library Cell Families When a family of cells with the same function is ordered in terms of increasing drive strength, specific cell property values should be monotonically increasing or decreasing. For example, when a family is ordered in terms of increasing drive strength, the area of cells should be monotonically increasing, power should be increasing, and delay (for the same output load and input slew) should be decreasing."}
{"instruction": "How do I - inserting metal shapes in the preferred direction - inserting routing guides along the nonpreferred-direction edges - inserting routing blockages along the boundary edges - removing perimeter constraint objects if you change the block boundary after creating the constraint objects, you must regenerate the objects. you cannot manually modify the constraint objects. to check the perimeter constraint objects, use the -check_only option with the derive_perimeter_constraint_objects command. by default, the checking results are stored in an error data file named block_name.err. to specify a different name for the error data file, use the -error_view option see also - removing perimeter constraint objects inserting metal shapes in the preferred direction to insert rectangular metal shapes along the preferred-direction edges of the current block, use the derive_perimeter_constraint_objects command with the -perimeter_objects metal_preferred option. when you specify this object type, the command creates one or more rectangular metal shapes parallel to each preferred-direction boundary edge. the metal shapes are floating; they are not connected to any logical nets. - if your technology does not specify a maximum area for these floating shapes, you can insert continuous metal shapes along the preferred-direction boundary edges, as described in inserting continuous metal shapes parallel to preferred-direction edges. - if your technology specifies a maximum area for these floating shapes or you need more flexibility in placing the metal shapes, insert multiple metal shapes, called metal stubs, along the preferred-direction boundary edges, as described in inserting metal stubs parallel to preferred-direction edges. in addition to inserting metal shapes along the preferred-direction edges, you can insert short metal shapes perpendicular to the nonpreferred-direction edges, as described in inserting short metal shapes perpendicular to nonpreferred-direction edges.?", "response": "- Inserting Metal Shapes in the Preferred Direction - Inserting Routing Guides Along the Nonpreferred-Direction Edges - Inserting Routing Blockages Along the Boundary Edges - Removing Perimeter Constraint Objects If you change the block boundary after creating the constraint objects, you must regenerate the objects. You cannot manually modify the constraint objects. To check the perimeter constraint objects, use the -check_only option with the derive_perimeter_constraint_objects command. By default, the checking results are stored in an error data file named block_name.err. To specify a different name for the error data file, use the -error_view option See Also - Removing Perimeter Constraint Objects Inserting Metal Shapes in the Preferred Direction To insert rectangular metal shapes along the preferred-direction edges of the current block, use the derive_perimeter_constraint_objects command with the -perimeter_objects metal_preferred option. When you specify this object type, the command creates one or more rectangular metal shapes parallel to each preferred-direction boundary edge. The metal shapes are floating; they are not connected to any logical nets. - If your technology does not specify a maximum area for these floating shapes, you can insert continuous metal shapes along the preferred-direction boundary edges, as described in Inserting Continuous Metal Shapes Parallel to Preferred-Direction Edges. - If your technology specifies a maximum area for these floating shapes or you need more flexibility in placing the metal shapes, insert multiple metal shapes, called metal stubs, along the preferred-direction boundary edges, as described in Inserting Metal Stubs Parallel to Preferred-Direction Edges. In addition to inserting metal shapes along the preferred-direction edges, you can insert short metal shapes perpendicular to the nonpreferred-direction edges, as described in Inserting Short Metal Shapes Perpendicular to Nonpreferred-Direction Edges."}
{"instruction": "How do I 1. (optional) configure the qor data, which is captured and displayed in the subsequent steps, by using the set_qor_data_options command. \u25e6 to specify the most critical power scenarios in your design, use the -leakage_scenario and -dynamic_scenario options. the tool uses the power scenarios you specify to generate the high-level summary of the power qor in the qorsum report. if you do not specify these options, it uses the active power scenario with the highest total power for both the leakage and dynamic scenario for the power qor summary. these settings are only used for the power qor summary. the tool uses the power information of all active power scenarios to capture and report the detailed power information in the qorsum report. \u25e6 to specify the most critical clock name and clock scenario, use the -clock_name and -clock_scenario options. the tool uses the clock name and scenario you specify to generate the high-level summary of the clock qor in the qorsum report. if you do not specify these options, the tool identifies the most critical clock and uses it for the clock qor summary. these settings are only used for the clock qor summary. the tool uses all clocks to generate the detailed clock qor information in the qorsum report. \u25e6 to specify a name to identify the run in the qorsum report, use the -run_name option. by default, the tool names each run with a number, such as run1, run2, and so on. you can use this option to give a more meaningful name to each run. you can also specify the run name by using the -run_names option when you generate the qorsum report by using the compare_qor_data command in step 3. if you do so, the tool ignores the run name specify by the set_qor_data_options -run_name command.?", "response": "1. (Optional) Configure the QoR data, which is captured and displayed in the subsequent steps, by using the set_qor_data_options command. \u25e6 To specify the most critical power scenarios in your design, use the -leakage_scenario and -dynamic_scenario options. The tool uses the power scenarios you specify to generate the high-level summary of the power QoR in the QORsum report. If you do not specify these options, it uses the active power scenario with the highest total power for both the leakage and dynamic scenario for the power QoR summary. These settings are only used for the power QoR summary. The tool uses the power information of all active power scenarios to capture and report the detailed power information in the QORsum report. \u25e6 To specify the most critical clock name and clock scenario, use the -clock_name and -clock_scenario options. The tool uses the clock name and scenario you specify to generate the high-level summary of the clock QoR in the QORsum report. If you do not specify these options, the tool identifies the most critical clock and uses it for the clock QoR summary. These settings are only used for the clock QoR summary. The tool uses all clocks to generate the detailed clock QoR information in the QORsum report. \u25e6 To specify a name to identify the run in the QORsum report, use the -run_name option. By default, the tool names each run with a number, such as Run1, Run2, and so on. You can use this option to give a more meaningful name to each run. You can also specify the run name by using the -run_names option when you generate the QORsum report by using the compare_qor_data command in step 3. If you do so, the tool ignores the run name specify by the set_qor_data_options -run_name command."}
{"instruction": "How do I 2. you can use this option with the -clocks option to further restrict the assignment. this option is not valid with the -nets option.?", "response": "2. You can use this option with the -clocks option to further restrict the assignment. This option is not valid with the -nets option."}
{"instruction": "How do I - defining a nondefault routing rule for secondary power and ground pin routing for example, to define a nondefault routing rule named widesvdd for wide m1 and m2 and set the nondefault routing rule on the vdd2 net, to which the secondary power and ground pins are connected, use the following commands?", "response": "- Defining a nondefault routing rule for secondary power and ground pin routing For example, to define a nondefault routing rule named wideSVDD for wide M1 and M2 and set the nondefault routing rule on the VDD2 net, to which the secondary power and ground pins are connected, use the following commands: fc_shell> create_routing_rule wideSVDD -widths { M1 0.3 M2 0.3 } fc_shell> set_routing_rule -rule wideSVDD {VDD2} For more information about using nondefault routing rules, see Using Nondefault Routing Rules. By default, the constraints apply to both the secondary power and ground connections and the tie-off connections. To separate these connections so that you can set constraints only for the secondary power and ground connections, set the route.common.separate_tie_off_from_secondary_pg application option to true. Routing the Secondary Power and Ground Pins If the secondary power and ground pins have the appropriate attributes in the frame view, you can use Zroute to route the secondary power and ground pins. By default, Zroute performs four topology ECO iterations to fix secondary PG cluster fanout violations.The topology ECO iterations can be time consuming, so you can control the effort level, and therefore the runtime, by setting the route.detail.topology_eco_effort_level application option to one of the following values: - off, which disables topology ECO; this is the default - low, which peforms one topology ECO iteration - medium, which peforms four topology ECO iterations - high, which peforms eight topology ECO iterations For example, to connect the secondary power pins to the VDD1 net, run the following command: fc_shell> route_group -nets VDD1 The following example add new cells, which have secondary power and ground pins, to a block; logically connects the power and ground pins; and then connects the secondary power pins to the VDD2 net. fc_shell> add_buffer {TOP/U1001/Z} {libA/BUFHVT} \\ -new_cell_names mynewHVT fc_shell> add_buffer {TOP/U1002/Z} {libA/BUFHVT} \\ -new_cell_names mynewHVT fc_shell> legalize_placement"}
{"instruction": "How do I - inferring select_ops inferring select_ops hdl compiler uses select_op components to implement conditional operations implied by if and case statements. an example of a select_op cell implementation for an 8-bit data signal is shown in figure 31.?", "response": "- Inferring SELECT_OPs Inferring SELECT_OPs HDL Compiler uses SELECT_OP components to implement conditional operations implied by if and case statements. An example of a SELECT_OP cell implementation for an 8-bit data signal is shown in Figure 31."}
{"instruction": "How do I 5. report the associations made for the multivoltage cells by using the report_mv_path command. if the tool failed to associate any multivoltage cells, the command reports the causes for these failures. you must successfully commit the power intent before you continue with the design flow. note?", "response": "5. Report the associations made for the multivoltage cells by using the report_mv_path command. If the tool failed to associate any multivoltage cells, the command reports the causes for these failures. You must successfully commit the power intent before you continue with the design flow. Note: After successfully running the commit_upf command, the tool issues an error message if you try to use additional UPF commands, except for the set_related_supply_net, connect_supply_net, set_design_attributes, set_port_attributes, find_objects, and set_scope commands. To modify the power intent after running the commit_upf command, you must remove the existing UPF specification by using the reset_upf command and then reload the power intent. Using Incomplete UPF Information The Early Data Check Manager allows you to check designs for power and multivoltage issues early in the design cycle. You can configure different error conditions in different ways. The tool provides comprehensive reports about the data checks. The general flow is as follows: 1. Use the set_early_data_check_policy command to define the global violation handling policy for data checks. 2. Proceed with your tool flow. The tool detects and responds to violations throughout the flow. 3. Use the report_early_data_checks command to obtain a report about all violations or specific data checks. 4. Use the get_early_data_check_records command to get a Tcl collection of violations that can be used in other commands. 5. Use the write_early_data_check_config command to save the settings in a file for future use. 6. Use the remove_early_data_check_records command to clear the data in preparation for another iteration. For more information about the Early Data Check Manager, see the Fusion Compiler Data Model User Guide. For a list of power and multivoltage data checks, see the Fusion Compiler Multivoltage User Guide."}
{"instruction": "How do I note?", "response": "Note: You cannot mix these methods of specifying coaxial shielding. If the generated coaxial shielding wires violate minimum area or minimum length rules, Zroute automatically patches the wires to satisfy these design rules. - Connects the shielding wires to the standard-cell power or ground pins and the standard-cell rails To prevent connections to the standard-cell power and ground pins, set the -ignore_shielding_net_pins option to true. To prevent connections to the standard-cell rails, set the -ignore_shielding_net_rails option to true. - Creates the shielding wires such that they surround the shielded routing shape To trim the shielding wires so that they align with the shielded routing shape ends, set the -align_to_shape_end option to true. To force Zroute to create shielding wires only in the preferred direction, set the -preferred_direction_only option to true. Note that the -preferred_direction_only option does not honor route guides to change the preferred routing direction. When you set either of these options to true, extra effort is required to connect the shielding wires to the power and ground network and any shielding wires that are not connected to the power and ground network are deleted. For example, to perform coaxial shielding below the shielded net segment layer on the clock nets, prevent signal routing below the shielded net segment layer, and tie the shielding wires to VSS, use the following command: fc_shell> create_shields -nets $clock_nets \\ -coaxial_below true -coaxial_below_skip_tracks 0 \\ -with_ground VSS When you run the create_shields command, it reports both the net-based and length- based average shielding ratios, as shown in the following example: Shielded 82% side-wall of (reset) Shielded 96% side-wall of (clk) Shielded 2 nets with average ratio as follows. 1) 89.00% (total shield ratio/number of shielded nets) 2) 87.92% (total shield length/total shielded net length)"}
{"instruction": "How do I - to remove the existing shielding only, set the route.common.reshield_modified_nets option to unshield. - to remove the existing shielding and reshield the modified nets, set the route.common.reshield_modified_nets option to reshield. note?", "response": "- To remove the existing shielding only, set the route.common.reshield_modified_nets option to unshield. - To remove the existing shielding and reshield the modified nets, set the route.common.reshield_modified_nets option to reshield. Note: Zroute automatically detects the nets modified within the Fusion Compiler tool; however, nets modified externally and input by reading a DEF file are not supported by incremental shielding. Reporting Shielding Information After you run the create_shields command, you can - Query the shield shapes associated with a shielded net by using the get_shapes -shield_only command, as described in Querying Shield Shapes - Report the shielding statistics by using the report_shields command, as described in Reporting Shielding Statistics Querying Shield Shapes To query the shield shapes associated with a shielded net, use the get_shapes command with the -shield_only option, as shown in the following example: fc_shell> get_shapes -shield_only -of_objects [get_nets myclk] {PATH_19_4317 PATH_19_4318 PATH_19_4319 PATH_19_4320 PATH_19_4322"}
{"instruction": "How do I - they do not have drc violations - they have don\u2019t-touch, size-only, or fixed-placement attribute settings - it is necessary to punch ports on the boundaries of power domains or blocks that have been identified with the set_freeze_ports command after splitting a cell, the tool - names the new cells using the <original_cell_name>_split_<integer> naming convention - copies all the settings and constraints from the original cell to the newly created cells?", "response": "- They do not have DRC violations - They have don\u2019t-touch, size-only, or fixed-placement attribute settings - It is necessary to punch ports on the boundaries of power domains or blocks that have been identified with the set_freeze_ports command After splitting a cell, the tool - Names the new cells using the <original_cell_name>_split_<integer> naming convention - Copies all the settings and constraints from the original cell to the newly created cells"}
{"instruction": "How do I - set_auto_floorplan_constraints the command sets constraints, such as utilization of the floorplan to be created during compile. for example, fc_shell> set_auto_floorplan_constraints -core_utilization 0.8 - report_auto_floorplan_constraints the command reports the constraints set by the set_auto_floorplan_constraints command. to specify macro placement, pin placement, and voltage area shaping constraints, use the following commands?", "response": "- set_auto_floorplan_constraints The command sets constraints, such as utilization of the floorplan to be created during compile. For example, fc_shell> set_auto_floorplan_constraints -core_utilization 0.8 - report_auto_floorplan_constraints The command reports the constraints set by the set_auto_floorplan_constraints command. To specify macro placement, pin placement, and voltage area shaping constraints, use the following commands: - Macro placement constraints set_macro_constraints, create_macro_array, create_keepout_margin - Pin placement constraints set_block_pin_constraints, set_individual_pin_constraints - Voltage area shaping constraints set_shaping_options Application Options Use these application options to set up auto floorplanning. - compile.auto_floorplan.initialize (default: auto) \u25e6 auto: Creates missing floorplan information. The tool exits with an error when inconsistent information is provided. \u25e6 true: Always creates the core and boundary. \u25e6 false: Never creates the core and boundary; uses only existing information. The tool exits with an error when encountering missing or inconsistent information."}
{"instruction": "How do I 3. writes a summary report of the results for information about the summary report, see summary report for automatic design rule fixing. fixing drc violations in each repair loop, the signoff_fix_drc command tries to fix the drc violations detected in the previous signoff drc run. - for the first loop, the tool uses the ic validator data in the directory specified by the signoff.fix_drc.init_drc_error_db application option (or the signoff_drc_run_init directory if you do not use this option). - for successive loops, the tool uses the ic validator drc data from the previous loop, which is stored in the directory specified by the signoff.fix_drc.run_dir application option (or the signoff_fix_drc_run directory if you do not use this option). when fixing drc violations, the signoff_fix_drc command has the following default behavior?", "response": "3. Writes a summary report of the results For information about the summary report, see Summary Report for Automatic Design Rule Fixing. Fixing DRC Violations In each repair loop, the signoff_fix_drc command tries to fix the DRC violations detected in the previous signoff DRC run. - For the first loop, the tool uses the IC Validator data in the directory specified by the signoff.fix_drc.init_drc_error_db application option (or the signoff_drc_run_init directory if you do not use this option). - For successive loops, the tool uses the IC Validator DRC data from the previous loop, which is stored in the directory specified by the signoff.fix_drc.run_dir application option (or the signoff_fix_drc_run directory if you do not use this option). When fixing DRC violations, the signoff_fix_drc command has the following default behavior: - Tries to fix all design rule violations detected in the previous signoff DRC run except those design rules that have more than 1000 violations - Performs DRC fixing on data nets for the whole block To modify the regions for design rule fixing, use one or both of the following options: \u25e6 -coordinates This option restricts design rule fixing to the specified regions. \u25e6 -excluded_coordinates This option prevents design rule fixing in the specified regions. If you specify this option with the -coordinates option, the command does not fix design rules in the overlapping regions. To specify the coordinates, use the following format: {{x1 y1} {x2 y2} ... {xn yn}} Note that there must be a space between each set of coordinates."}
{"instruction": "How do I 0.00 before_reports {} after_reports {app_options timing} before_actions {gropto} after_actions {} replace_actions {} - to return a list of all the checkpoint behaviors defined in your configuration, specify the -list_reports or -list_actions option?", "response": "0.00 before_reports {} after_reports {app_options timing} before_actions {gropto} after_actions {} replace_actions {} - To return a list of all the checkpoint behaviors defined in your configuration, specify the -list_reports or -list_actions option: fc_shell> get_checkpoint_data -list_reports timing app_options fc_shell> get_checkpoint_data -list_actions gropto placer_high_effort_congestion - To return the contents and associations of a checkpoint report or action, specify the report or action with the -report or -action option: fc_shell> get_checkpoint_data -report app_options contents { set name [get_current_checkpoint -name] set pos [get_current_checkpoint -position] report_app_options -non_default \\ > ./checkpoint/$name.$pos.app_options.rpt } before_patterns {*} after_patterns {} See Also - Viewing Your Checkpoint History Viewing Your Checkpoint History To view your checkpoint history, navigate to the checkpoint directory. This directory contains a checkpoint_history.rpt file that captures the history of the checkpoints the tool encounters during each run, along with the runtime and memory usage for each checkpoint. The checkpoint directory also contains any checkpoint reports you have written to it. In some cases, you might want to clear the full or partial contents of your checkpoint history. A typical example is when a checkpointed run fails and you want to clear some or all of the data associated with that run, depending on whether you plan to rerun a portion of the flow or the entire flow. - To clear your full checkpoint history, use the reset_checkpoints command. fc_shell> reset_checkpoints The reset_checkpoints command clears the full contents of the checkpoint directory, including any reports you have written to it. However, before clearing the checkpoint directory, the reset_checkpoints command saves a timestamped copy of the directory in your run directory."}
{"instruction": "How do I guidelines?", "response": "guidelines: - To reduce runtime and memory usage, run the update_timing command before running the commands in parallel; otherwise, each command that requires updated timing runs the update_timing command independently. - To pass variables from a child process to the parent process, you must write the contents of the variables to a file during the child process, and then read that file in the parent process. See Also - Running Commands in the Background - Running Commands in Parallel Running Commands in the Background To improve runtime, you can run checking and reporting commands in the background while you run other commands in the foreground. This is useful in interactive sessions when you want to continue your work while the tool generates a report. To run commands in the background, use the redirect command with the -bg option. When you use this command, fc_shell returns immediately to execute the next command. If you issue an exit command in the parent process, the tool waits for all redirect -bg commands to complete before exiting. To list the commands supported by the redirect -bg command, use the list_commands -bg command. You can run either a single command or source a Tcl script that contains"}
{"instruction": "How do I - by default, this command shows all user-defined via mappings. - to show the via mappings for specific source vias, use the -from option. - to show the via mappings for specific replacement vias, use the -to option. to remove mappings from the via mapping table, use the remove_via_mappings command. - to remove all via mappings, use the -all option. - to remove the via mappings for specific source vias, use the -from option. - to remove the via mappings for specific replacement vias, use the -to option. using a subset of the via mapping table for redundant via insertion to use a subset of the via mapping table for redundant via insertion, specify which weight groups from the via mapping table to use by setting one or both of the route.common.redundant_via_include_weight_group_by_layer_name and route.common.redundant_via_exclude_weight_group_by_layer_name application options. - if you set only the route.common.redundant_via_include_weight_group_by_layer_name application option, zroute selects redundant vias only from the specified weight groups. if you do not specify an entry for a layer, redundant via insertion is not performed on that layer. - if you set only the route.common.redundant_via_exclude_weight_group_by_layer_name application option, zroute selects redundant vias from all weight groups in the original via mapping table, except the specified weight groups. if you do not specify an entry for a layer, all weight groups are used for that layer. - if you set both application options, zroute selects redundant vias from the weight groups specified in the route.common.redundant_via_include_weight_group_by_layer_name application, excluding the weight groups specified in the route.common.redundant_via_exclude_weight_group_by_layer_name application option. postroute redundant via insertion to perform postroute redundant via insertion, use the add_redundant_vias command. this command can replace single-cut vias with multiple-cut via arrays, single-cut vias with other single-cut vias that have a different contact code, and multiple-cut via arrays?", "response": "- By default, this command shows all user-defined via mappings. - To show the via mappings for specific source vias, use the -from option. - To show the via mappings for specific replacement vias, use the -to option. To remove mappings from the via mapping table, use the remove_via_mappings command. - To remove all via mappings, use the -all option. - To remove the via mappings for specific source vias, use the -from option. - To remove the via mappings for specific replacement vias, use the -to option. Using a Subset of the Via Mapping Table for Redundant Via Insertion To use a subset of the via mapping table for redundant via insertion, specify which weight groups from the via mapping table to use by setting one or both of the route.common.redundant_via_include_weight_group_by_layer_name and route.common.redundant_via_exclude_weight_group_by_layer_name application options. - If you set only the route.common.redundant_via_include_weight_group_by_layer_name application option, Zroute selects redundant vias only from the specified weight groups. If you do not specify an entry for a layer, redundant via insertion is not performed on that layer. - If you set only the route.common.redundant_via_exclude_weight_group_by_layer_name application option, Zroute selects redundant vias from all weight groups in the original via mapping table, except the specified weight groups. If you do not specify an entry for a layer, all weight groups are used for that layer. - If you set both application options, Zroute selects redundant vias from the weight groups specified in the route.common.redundant_via_include_weight_group_by_layer_name application, excluding the weight groups specified in the route.common.redundant_via_exclude_weight_group_by_layer_name application option. Postroute Redundant Via Insertion To perform postroute redundant via insertion, use the add_redundant_vias command. This command can replace single-cut vias with multiple-cut via arrays, single-cut vias with other single-cut vias that have a different contact code, and multiple-cut via arrays"}
{"instruction": "How do I - compile.auto_floorplan.place_pins (default?", "response": "- compile.auto_floorplan.place_pins (default: unplaced) compile.auto_floorplan.place_ios (default: unplaced) compile.auto_floorplan.place_hard_macros (default: unplaced) compile.auto_floorplan.shape_voltage_areas (default: unshaped) \u25e6 all: Always places and shapes objects even if they are fixed. Unplaced and unshaped objects will always be placed and shaped. \u25e6 unfixed: Places and shapes objects that are not fixed. Use the fixed information from DEF files or a Tcl floorplan, and use the set_fixed_objects command to modify. Unplaced and unshaped objects will always be placed and shaped. \u25e6 unplaced, unshaped: Never places or shapes objects when they are already placed. Unplaced and unshaped objects will always be placed and shaped. \u25e6 none: Never places or shapes objects even if they are not already placed. This table summarizes how the tool handles fixed, placed, and unplaced objects for each setting of these four application options during auto floorplanning. Application option setting Fixed objects Placed objects Unplaced objects all Placed and shaped Placed and shaped Placed and shaped unfixed Kept Placed and shaped Placed and shaped unplaced, unshaped Kept Kept Placed and shaped none Kept Kept Kept - compile.auto_floorplan.keep_bounds (default: auto)"}
{"instruction": "How do I 190. in contrast to the feasibility flow, the hierarchical synthesis flow uses the following block-level information generated post place and route?", "response": "190. In contrast to the feasibility flow, the hierarchical synthesis flow uses the following block-level information generated post place and route: - Floorplanning information to generate the ETM in step 3 - Power intent and test model (CTL) information for top-level synthesis in step 5"}
{"instruction": "How do I 1. specify your clock tree constraints and settings. 2. for multivoltage designs, \u25e6 enable multivoltage support by using the following application option setting?", "response": "1. Specify your clock tree constraints and settings. 2. For multivoltage designs, \u25e6 Enable multivoltage support by using the following application option setting: fc_shell> set_app_options \\ -name cts.multisource.enable_full_mv_support -value true \u25e6 Enable the addition of physical feedthrough cells, which belong to a voltage area physically but not logically, by using the following application option setting: fc_shell> set_app_options \\ -name opt.common.allow_physical_feedthrough -value true \u25e6 Run the check_mv_design command and fix all multivoltage issues 3. Enable cell electromigration fixing by setting the cts.multisource.cell_em_aware application option to true. 4. Create the clock mesh by using the create_clock_straps command, as described in Creating Clock Straps. 5. Insert the mesh drivers by using the create_clock_drivers command, as described in Inserting Clock Drivers."}
{"instruction": "How do I - specify all clock tree synthesis settings, including clock routing rules - specify that the inserted clock drivers should be aligned with the clock straps of the mesh structure by setting the cts.multisource.enable_clock_driver_snapping application option to true during multisource clock tree synthesis, you can use the create_clock_drivers command for the following?", "response": "- Specify all clock tree synthesis settings, including clock routing rules - Specify that the inserted clock drivers should be aligned with the clock straps of the mesh structure by setting the cts.multisource.enable_clock_driver_snapping application option to true During multisource clock tree synthesis, you can use the create_clock_drivers command for the following: - Insert mesh drivers for both regular or structural multisource clock trees. - Tap drivers for regular multisource clock trees. When you insert mesh or tap drivers, - Specify the loads to drive by using the -loads option. The load can be a net or a set of pins or ports connected to the same net. - Specify where to add the clock drivers as exact locations by using the -location option. Alternatively, you can add the clock drivers in an X by Y grid pattern over the entire core area by using the -boxes option. You can limit the X by Y grid pattern to a specific area by using the -boundary option with the -boxes option. - Specify a list of cells that can be used as clock drivers by using the -lib_cells option. You can specify both single-rail and dual-rail library cells. Alternatively, you can select the existing driver of the load net as a template for the clock drivers by using the -template option. If the template cell you specify is a buffer or inverter, you can also specify different versions of this buffer or inverter as clock drivers by using the -lib_cells option with the -template option. When you insert mesh drivers, to specify that the outputs of all the clock drivers must be connected together to drive the mesh net, use the -short_outputs option. The following example creates a grid of eight by eight mesh drivers, places them in a regular pattern that covers the full core area, shorts the outputs of the clock drivers and connects to the net named clk_mesh, and transfers preexisting routing shapes and vias from the net named clk to this net: fc_shell> create_clock_drivers -loads [get_nets clk1_mesh] \\ -boxes {8 8} -lib_cells [get_lib_cells my_lib/CKBUF8X] \\ -short_outputs -output_net_name clk_mesh \\ -transfer_wires_from [get_nets clk]"}
{"instruction": "How do I 2. example 54 > set_app_options -name rail.license_checkout_mode \\ -value wait > set_host_options -submit_protocol sge \\ -submit_command {qsub} > analyze_rail -submit_to_other_machines \\ -multiple_script_files {{ result1 rh1.tcl} {result2 rh2.tcl} \\ {result3 rh3.tcl}} the above example submits three redhawk scripts to the sge farm machines and waits for an available license during the analyze_rail run. voltage driven power switch cell sizing the fusion compiler tool can resize power switch cells to reduce total power and to mitigate the voltage drop in the design. power switch cells have different voltage thresholds (vt) and are classified as high, standard, low, and ultra-low. for a power cell with high threshold voltage, the power consumption and leakage current is low, but the timing is also slow with a smaller drive strength. for a power cell with a low threshold voltage, the timing is faster but the power consumption and leakage current is high. with power switch cell sizing, the tool supports different drive strength and voltage threshold to reduce ir (voltage) drop in the design. note?", "response": "2. Example 54 > set_app_options -name rail.license_checkout_mode \\ -value wait > set_host_options -submit_protocol sge \\ -submit_command {qsub} > analyze_rail -submit_to_other_machines \\ -multiple_script_files {{ result1 RH1.tcl} {result2 RH2.tcl} \\ {result3 RH3.tcl}} The above example submits three RedHawk scripts to the SGE farm machines and waits for an available license during the analyze_rail run. Voltage Driven Power Switch Cell Sizing The Fusion Compiler tool can resize power switch cells to reduce total power and to mitigate the voltage drop in the design. Power switch cells have different voltage thresholds (VT) and are classified as high, standard, low, and ultra-low. For a power cell with high threshold voltage, the power consumption and leakage current is low, but the timing is also slow with a smaller drive strength. For a power cell with a low threshold voltage, the timing is faster but the power consumption and leakage current is high. With power switch cell sizing, the tool supports different drive strength and voltage threshold to reduce IR (voltage) drop in the design. Note: When the tool switches power cells (swapping), it can only swap cells with the same footprint but with different threshold voltages. This means, the cell cannot be swapped if the cell size and pin shapes are different. To perform power switch cell swapping, you can use the size_power_switches command to set the maximum IR drop target. To meet this target when swapping power cells, the tool tries to find another cell with higher VT, but lower RON value, so that it can reduce the IR drop. You can set the ON resistance (RON) value to be considered by the tool for swapping using the set_power_switch_resistance command. Based on the IR drop target and the set RON value of the power cell, the tool tries to replace power cells to meet the targeted voltage drop."}
{"instruction": "How do I instructions in the changing the block from abstract to design view section to switch the blocks you want to optimize to design views. perform the following steps to set up tho?", "response": "instructions in the Changing the Block From Abstract to Design View section to switch the blocks you want to optimize to design views. Perform the following steps to set up THO: 1. To perform THO after block-level compile_fusion, you must propagate the CCD useful skew balancing offsets and latency offsets for integrated clock gating cells to the top-level timing context using the following commands: fc_shell> promote_clock_data -auto_clock all -latency_offset fc_shell> promote_clock_data -auto_clock all -balance_points 2. To enable and perform THO on the blocks, ensure that the blocks you want to optimize with THO are enabled for editing: fc_shell> set_editability -blocks {list of blocks} -value true 3. To verify block editability, use the following command: fc_shell> report_editability -blocks [get_blocks -hierarchical] 4. During THO, operations such as congestion updates and route-driven estimation (RDE) calculations inside subblocks as well as ECO-routing can be run using distributed processing. Use the following commands to perform the setup. You must specify your own queue parameters such as the qsub parameters as shown in the following example: Note: If host options are not specified, the tool runs the operations for the subblocks serially in the main process. fc_shell> set_host_options -name for_tho -num_processes 4 -max_cores 16 -submit_command [list qsub -P queue -pe mt 16 -l mem_free=100G -cwd] fc_shell> set_hierarchy_options -host_option for_tho -blocks {list of blocks} 5. To enable THO, use the init_hier_optimization command. Depending on the stage, you must use different options as shown in the following sections. Performing THO During compile_fusion final_place Stage During the final_place stage of compile_fusion, placement can be performed transparently across the top and blocks. Placement occurs within the boundaries of the"}
{"instruction": "How do I - all common enable signals are identified and used for multilevel clock-gate expansion. - the user-specified maximum number of clock-gate levels is reached. the tool names a newly-inserted clock gate as follows, based on the status of the set of clock gates whose enable functions are modified by the new clock gate?", "response": "- All common enable signals are identified and used for multilevel clock-gate expansion. - The user-specified maximum number of clock-gate levels is reached. The tool names a newly-inserted clock gate as follows, based on the status of the set of clock gates whose enable functions are modified by the new clock gate: - If at least one of the clock gates is a tool-inserted clock gate, the base name is clock_gate_ml. The name is appended with the register base name, similar to the naming convention for other tool-inserted clock gates. - If all of the clock gates are preexisting clock gates, the tool uses the name of the preexisting clock gate with the largest fanout and appends _ml_pe. The compile.clockgate.max_number_of_levels application option specifies the maximum number of allowed clock-gate levels. The default is 0, which allows an infinite number of levels. However, to use the clock-gate level expansion capability, you must set this application option to a nonzero value (in addition to setting the compile.clockgate.enable_level_expansion application option to true). If you specify clock-gate level expansion, but the maximum number of allowed levels is low, the expansion operation cannot take full advantage of the tool's ability to analyze common enable signals. To enable or disable clock-gate level expansion for specific clock gates, use the set_clock_gate_transformations -expand_levels command."}
{"instruction": "How do I 4. (optional) specify additional settings for ir-drop-aware placement, as described in controlling ir-drop-aware placement. 5. run the final optimization stage of the clock_opt command by using the clock_opt -from final_opto command. controlling concurrent clock and data optimization applying useful skew techniques during datapath optimization to improve the timing qor by taking advantage of the positive slack and adjusting the clock arrival times of registers is referred to as concurrent clock and data (ccd) optimization. concurrent clock and data optimization is enabled by default for the compile_fusion and clock_opt commands. to enabled it for the route_opt command, set the route_opt.flow.enable_ccd application option to true. you can change the default behavior of concurrent clock and data optimization by - limiting the latency adjustment values - excluding boundary paths - excluding specific path groups - excluding specific scenarios - excluding specific sinks - controlling timing optimization effort - controlling hold time optimization effort - controlling the adjustment of i/o clock latencies - performing dynamic-voltage-drop-driven concurrent clock and data optimization during the route_opt command - specifying optimization targets at the preroute stage - specifying optimization targets at the postroute stage - enabling buffer removal at the postroute stage - reporting concurrent clock and data timing - scaling ccd offsets to new scenarios for reporting timing - skewing latch and discrete clock gates?", "response": "4. (Optional) Specify additional settings for IR-drop-aware placement, as described in Controlling IR-Drop-Aware Placement. 5. Run the final optimization stage of the clock_opt command by using the clock_opt -from final_opto command. Controlling Concurrent Clock and Data Optimization Applying useful skew techniques during datapath optimization to improve the timing QoR by taking advantage of the positive slack and adjusting the clock arrival times of registers is referred to as concurrent clock and data (CCD) optimization. Concurrent clock and data optimization is enabled by default for the compile_fusion and clock_opt commands. To enabled it for the route_opt command, set the route_opt.flow.enable_ccd application option to true. You can change the default behavior of concurrent clock and data optimization by - Limiting the Latency Adjustment Values - Excluding Boundary Paths - Excluding Specific Path Groups - Excluding Specific Scenarios - Excluding Specific Sinks - Controlling Timing Optimization Effort - Controlling Hold Time Optimization Effort - Controlling the Adjustment of I/O Clock Latencies - Performing Dynamic-Voltage-Drop-Driven Concurrent Clock and Data Optimization During the route_opt Command - Specifying Optimization Targets at the Preroute Stage - Specifying Optimization Targets at the Postroute Stage - Enabling Buffer Removal at the Postroute Stage - Reporting Concurrent Clock and Data Timing - Scaling CCD Offsets to New Scenarios for Reporting Timing - Skewing Latch and Discrete Clock Gates"}
{"instruction": "How do I - when you use the create_voltage_area -merge_regions command, the merges the shapes specified with the -region option. - when you use the create_voltage_area_shape -merge_regions command, the merges the shape specified with the -region option and the existing shapes of the specified voltage area. - when you use the set_voltage_area -merge_regions command, the tool merges all existing shapes of the specified voltage area. the tool merges the voltage area shapes based on their stacking order. by default, the stacking order is the order in which you define the shapes, with the last shape defined on top. the merged shape replaces the top shape of a set of abutting or overlapping shapes; the other shapes in the set are removed and are no longer associated with the voltage area.?", "response": "- When you use the create_voltage_area -merge_regions command, the merges the shapes specified with the -region option. - When you use the create_voltage_area_shape -merge_regions command, the merges the shape specified with the -region option and the existing shapes of the specified voltage area. - When you use the set_voltage_area -merge_regions command, the tool merges all existing shapes of the specified voltage area. The tool merges the voltage area shapes based on their stacking order. By default, the stacking order is the order in which you define the shapes, with the last shape defined on top. The merged shape replaces the top shape of a set of abutting or overlapping shapes; the other shapes in the set are removed and are no longer associated with the voltage area."}
{"instruction": "How do I 0. all registers in bank r0 share the same enable condition en0 and all registers in both register banks are clocked by clock clk. if in one clock cycle the registers in bank r0 are disabled (in other words, enable signal en0 is 0), those registers do not change their values. therefore, in the next clock cycle, the registers in bank r1 do not change their values and consequently they are candidates for clock gating. figure 62 before clock gating figure 63 shows the clock-gate implementation. clock gate cg1 is first inserted for register bank r0. the tool creates a new register rd1 to generate a delayed version of enable signal en0. the delayed signal drives sequential clock gate seqcg1 for register bank r1. figure 63 after clock gating the fusion compiler tool implements several different clock-gating strategies as needed, based on analyzing the circuit. for example, if the fanin registers do not all have the same enable signals, the tool looks for common signals upstream from the enable signals and uses those signals as inputs to the inserted delay register. sequential clock gates are inserted by the compile_fusion command. during the initial_map stage, the tool analyzes the local hierarchy of the registers and inserts clock gates as needed. during the logic_opto stage, the tool performs analysis across hierarchical boundaries and inserts additional clock gates as needed. the name of a clock gate inserted as a result of fanin-based sequential clock gating begins with the prefix seq_clock_gate_fi_. the rest of the name follows standard clock-?", "response": "0. All registers in bank R0 share the same enable condition EN0 and all registers in both register banks are clocked by clock CLK. If in one clock cycle the registers in bank R0 are disabled (in other words, enable signal EN0 is 0), those registers do not change their values. Therefore, in the next clock cycle, the registers in bank R1 do not change their values and consequently they are candidates for clock gating. Figure 62 Before Clock Gating Figure 63 shows the clock-gate implementation. Clock gate CG1 is first inserted for register bank R0. The tool creates a new register RD1 to generate a delayed version of enable signal EN0. The delayed signal drives sequential clock gate SEQCG1 for register bank R1. Figure 63 After Clock Gating The Fusion Compiler tool implements several different clock-gating strategies as needed, based on analyzing the circuit. For example, if the fanin registers do not all have the same enable signals, the tool looks for common signals upstream from the enable signals and uses those signals as inputs to the inserted delay register. Sequential clock gates are inserted by the compile_fusion command. During the initial_map stage, the tool analyzes the local hierarchy of the registers and inserts clock gates as needed. During the logic_opto stage, the tool performs analysis across hierarchical boundaries and inserts additional clock gates as needed. The name of a clock gate inserted as a result of fanin-based sequential clock gating begins with the prefix seq_clock_gate_fi_. The rest of the name follows standard clock-"}
{"instruction": "How do I 1. partition the design at the top-level and generate the design libraries for the lower-level blocks as described in partitioning and planning the full chip design. 2. synthesize each lower-level block and generate the block-level information required for top-level synthesis as described in synthesizing a subblock. 3. synthesize the top-level as described in synthesizing the top-level. see also - overview of abstract views - creating abstract views - creating a frame view partitioning and planning the full chip design the hierarchical synthesis flow using abstracts includes the following two cases?", "response": "1. Partition the design at the top-level and generate the design libraries for the lower-level blocks as described in Partitioning and Planning the Full Chip Design. 2. Synthesize each lower-level block and generate the block-level information required for top-level synthesis as described in Synthesizing a Subblock. 3. Synthesize the top-level as described in Synthesizing the Top-Level. See Also - Overview of Abstract Views - Creating Abstract Views - Creating a Frame View Partitioning and Planning the Full Chip Design The hierarchical synthesis flow using abstracts includes the following two cases: - Floorplans of subblock and top-level design are not available - Floorplans of subblock and top-level design are available"}
{"instruction": "How do I 6. save the block to disk. when you run the signoff_create_pg_augmentation command, the ic validator tool uses the on-disk information for the block, not the information in memory. to ensure accurate information, use the save_block command to save the current state of the block before running the signoff_create_pg_augmentation command. 7. perform timing-driven power grid augmentation by using the signoff_create_pg_augmentation command with one or both of the following options?", "response": "6. Save the block to disk. When you run the signoff_create_pg_augmentation command, the IC Validator tool uses the on-disk information for the block, not the information in memory. To ensure accurate information, use the save_block command to save the current state of the block before running the signoff_create_pg_augmentation command. 7. Perform timing-driven power grid augmentation by using the signoff_create_pg_augmentation command with one or both of the following options: \u25e6 -nets This option explicitly specifies the critical nets. \u25e6 -timing_preserve_setup_slack_threshold This option enables the tool to automatically determine the critical nets based on a setup slack threshold. The unit for the slack threshold setting is the library time unit. You must also specify the technology node for which to perform the augmentation by using the -node option. If you must perform incremental PG augmentation because your design contains more than one power net, use the -mode add command on subsequent runs to append the PG augmentation shapes to the existing shapes. 8. Run voltage drop analysis using RedHawk Fusion, as described in Performing Voltage Drop Analysis. This second voltage drop analysis run measures the voltage drop after power grid augmentation. During timing-driven PG augmentation, the signoff_create_pg_augmentation command 1. Performs timing analysis, including multicorner-multimode analysis, to minimize timing impact 2. Identifies timing-critical nets based on the options you specify 3. Removes existing PG augmentation shapes By default, the command removes the PG augmentation shapes from the entire block. To keep the existing PG augmentation shapes, use the -mode add option. 4. Inserts PG augmentation shapes for the specified regions, except in the areas around the critical nets"}
{"instruction": "How do I note that there must be a space between each set of coordinates. - performs design rule checking for all rules specified in the foundry runset \u25e6 to check only specific rules, use the -select_rules option. specify the rules by specifying a matching pattern for the rule names. the rule names are specified in the comment section in the runset file. \u25e6 to prevent checking of specific rules, use the -unselect_rules option. specify the rules by specifying a matching pattern for the rule names. the rule names are specified in the comment section in the runset file. note?", "response": "Note that there must be a space between each set of coordinates. - Performs design rule checking for all rules specified in the foundry runset \u25e6 To check only specific rules, use the -select_rules option. Specify the rules by specifying a matching pattern for the rule names. The rule names are specified in the COMMENT section in the runset file. \u25e6 To prevent checking of specific rules, use the -unselect_rules option. Specify the rules by specifying a matching pattern for the rule names. The rule names are specified in the COMMENT section in the runset file. Note: You can use this option with the -select_rules option to customize the set of rules checked by the signoff_check_drc command. For example, to restrict the signoff_check_drc command to route validation, select the metal layer rules and exclude the metal density rules."}
{"instruction": "How do I - specifying redhawk and redhawk-sc working directories specifying ideal voltage sources as taps taps are used to model the external voltage source environment in which the device under analysis operates; they are not part of the design itself, but can be thought of as virtual models of voltage sources. the locations of the ideal voltage sources and the ideal power supplies in the design are required to achieve accurate rail analysis results. to create a tap, use the create_taps command. you can create multiple taps by running multiple create_taps commands. to specify a name for the created tap, use the -name option. if the specified name is already present in the design, the tool issues a warning message, and replaces the original tap with the new one. when you create multiple taps by a single invocation of the create_taps command, all of the created taps are named by using the tap_name_nnn naming convention, where nnn is a unique integer identifier and tap_name is the string argument to the -name option.?", "response": "- Specifying RedHawk and RedHawk-SC Working Directories Specifying Ideal Voltage Sources as Taps Taps are used to model the external voltage source environment in which the device under analysis operates; they are not part of the design itself, but can be thought of as virtual models of voltage sources. The locations of the ideal voltage sources and the ideal power supplies in the design are required to achieve accurate rail analysis results. To create a tap, use the create_taps command. You can create multiple taps by running multiple create_taps commands. To specify a name for the created tap, use the -name option. If the specified name is already present in the design, the tool issues a warning message, and replaces the original tap with the new one. When you create multiple taps by a single invocation of the create_taps command, all of the created taps are named by using the tap_name_NNN naming convention, where NNN is a unique integer identifier and tap_name is the string argument to the -name option."}
{"instruction": "How do I - load_upf top_1.upf on wed oct 26 17?", "response": "- load_upf top_1.upf on Wed Oct 26 17:03:18 2016 \u2026 \u2026 ## End - load_upf ## Start - load_upf top_2.upf on Wed Oct 26 17:03:49 2016 \u2026 \u2026 ## End - load_upf ## Start - load_upf top_3.upf on Wed Oct 26 17:04:22 2016 \u2026 \u2026 ## End - load_upf Preparing the Power Network To learn about preparing the power network for physical implementation, see - Creating Logical Power and Ground Connections - Creating Floating Logical Supply Nets Creating Logical Power and Ground Connections After you read in the design, you must ensure that there are logical connections between the power and ground nets and the power, ground, and tie-off pins on the cells in your design. If your design does not already have these connections, use the connect_pg_net command to create them. This command creates the logical power and ground connections for leaf cells, hierarchical cells, and physical-only cells in both single- voltage and multivoltage designs. Before creating the logical power and ground connections, you must resolve any PG conflicts among the netlist, floorplan, and UPF specification. - For multivoltage designs, the conflicts are resolved when you commit the power intent, as described in Loading and Applying UPF Information. - For single-voltage designs, you must run the resolve_pg_netx command to resolve the conflicts. Note that the UPF specification for a single-voltage design is the default power domain generated by the read_verilog command. If your design contains unmapped instances, the tool issues an information message to indicate that only mapped instances are connected."}
{"instruction": "How do I 0.7 0.0 150 800}, the layermaxratio value for the m1 layer is 400, and the following diodes are connected to a single m1 net?", "response": "0.7 0.0 150 800}, the layerMaxRatio value for the M1 layer is 400, and the following diodes are connected to a single M1 net: diode A with a diode-protection value of 0.5, diode B with a diode-protection value of 1.0, and diode C with a diode-protection value of 1.5. In this example, the v4 value is not specified so a value of 0 is used. Use the following commands to define the antenna rules for this example: fc_shell> define_antenna_rule -mode 1 -diode_mode diode_mode \\ -metal_ratio 400 -cut_ratio 20 fc_shell> define_antenna_layer_rule -mode 1 -layer \"M1\" \\ -ratio 400 -diode_ratio {0.7 0.0 150 800} As shown in Table 37, for diode modes 7 and 8, the maximum antenna ratio is computed as (metal_area - equi_gate_area) / gate_area where the equivalent gate area, equi_gate_area, is computed by using the diode ratio vector: (diode_protection + v1) * (v2 + v3). - For diode mode 7, the equivalent metal area is computed using the maximum diode protection, which is 1.5, so the maximum antenna ratio for the net is (metal_area - ((1.5 + 0) * (150 + 800))) / gate_area = (metal_area - 1025) / gate_area - For diode mode 8, the equivalent metal area is computed using the total diode protection, which is 0.5+1.0+1.5=3.0, so the maximum antenna ratio for the net is (metal_area - ((3.0 + 0) * 150 + 800)) / gate_area = (metal_area - 1250) / gate_area Example for Diode Mode 14 With Multiple Gate Oxide Thicknesses Assume that the antenna area is calculated using surface area in single-layer mode (antenna mode 1), the design contains cells with two different gate oxide thicknesses, the diode ratio vector for the first oxide thickness (gate class 0) for the M1 layer is {1 0 1e9 1e9 0 1 2 285 1 285}, the diode ratio vector for the second oxide thickness (gate class 1) for the M1 layer is {1 0 1e9 1e9 1 0 2 165 1 28}, and the layerMaxRatio value for the M1 layer is 285. For diode mode 14, the diode ratio vector contains 10 values for each gate class. Because the design uses only two gate oxide thicknesses, only the vectors for gate class 0 and gate class 1 are used. However, you must specify all 40 values when defining the antenna"}
{"instruction": "How do I - the maximum vertical length to specify the maximum vertical length, use the -max_constraint_length option. - the cells considered for the check these cells are called constrained cells. to specify the constrained filler cells, use the -constraint_fillers option. by default, all standard cells are constrained. to exclude specific standard cells from consideration, use the -exception_cells option. to ignore the length violation when a constrained cell abuts an exception cell, use the -no_violation_along_exception_cells option. - the filler cells that can be used to replace the violating filler cells these filler cells are called unconstrained cells; they are not considered when measuring the stack height. \u25e6 to specify the filler cells that can be used to break a continuous edge of constrained cells, use the -non_constraint_fillers option. \u25e6 to specify the filler cells whose left side can be used to break a continuous edge of constrained cells, use the -non_constraint_left_fillers option. \u25e6 to specify the filler cells whose right side can be used to break a continuous edge of constrained cells, use the -non_constraint_right_fillers option. note?", "response": "- The maximum vertical length To specify the maximum vertical length, use the -max_constraint_length option. - The cells considered for the check These cells are called constrained cells. To specify the constrained filler cells, use the -constraint_fillers option. By default, all standard cells are constrained. To exclude specific standard cells from consideration, use the -exception_cells option. To ignore the length violation when a constrained cell abuts an exception cell, use the -no_violation_along_exception_cells option. - The filler cells that can be used to replace the violating filler cells These filler cells are called unconstrained cells; they are not considered when measuring the stack height. \u25e6 To specify the filler cells that can be used to break a continuous edge of constrained cells, use the -non_constraint_fillers option. \u25e6 To specify the filler cells whose left side can be used to break a continuous edge of constrained cells, use the -non_constraint_left_fillers option. \u25e6 To specify the filler cells whose right side can be used to break a continuous edge of constrained cells, use the -non_constraint_right_fillers option. Note: Any filler cells that are not identified as constrained, unconstrained, or exception cells are considered constrained cells. For example, the following command identifies and fixes maximum vertical edge length violations where the vertical length exceeds 180 um: fc_shell> replace_fillers_by_rules \\ -replacement_rule max_vertical_constraint -max_constraint_length 180 \\ -constraint_fillers {DECAP8 DECAP4} -non_constraint_fillers {FILL1} \\ -exception_cells {BUF1 TAP1} Figure 133 shows the original stack, which violates this rule, and the resulting stack, which uses unconstrained filler cells to fix the violation. The constrained cells are shown in red, while the unconstrained cells are shown in green."}
{"instruction": "How do I - to report the clock tree design rule constraints, target skew, and target latency, use -type configurations. - to report the clock tree references, use -type references. - to report the clock cell spacing rules, use -type spacing_rules. - to report the nondefault routing rule used for the clock nets, use -type routing_rules. implementing clock trees and performing post-cts optimization before you perform clock tree synthesis, you should save the block. this allows you to refine the clock tree synthesis goals and rerun clock tree synthesis with the same starting point, if necessary. the following topics describe the different methods available for implementing clock trees?", "response": "- To report the clock tree design rule constraints, target skew, and target latency, use -type configurations. - To report the clock tree references, use -type references. - To report the clock cell spacing rules, use -type spacing_rules. - To report the nondefault routing rule used for the clock nets, use -type routing_rules. Implementing Clock Trees and Performing Post-CTS Optimization Before you perform clock tree synthesis, you should save the block. This allows you to refine the clock tree synthesis goals and rerun clock tree synthesis with the same starting point, if necessary. The following topics describe the different methods available for implementing clock trees: - Performing Standalone Clock Trees Synthesis - Synthesizing, Optimizing, and Routing Clock Trees With the clock_opt Command - Controlling Concurrent Clock and Data Optimization"}
{"instruction": "How do I - the density rule for the layer the tool determines the density rule for each layer by using the following settings, in order of priority?", "response": "- The density rule for the layer The tool determines the density rule for each layer by using the following settings, in order of priority: 1. The setting of the mx_min_density parameter in the IC Validator parameter file specified by the signoff.report_metal_density.user_defined_options application option 2. The setting of the signoff.report_metal_density.min_density application option 3. The setting of the minDensity attribute in the DensityRule section of the technology file 4. 10 percent - The grid sizes for the layer The tool determines the grid sizes for each layer by using the following settings, in order of priority: 1. The setting of the mx_min_density_window or mx_density_gradient_window parameter in the IC Validator parameter file specified by the signoff.report_metal_density.user_defined_options application option 2. The setting of the signoff.report_metal_density.density_window or signoff.report_metal_density.gradient_window_size application option 3. The setting of the windowSize attribute in the DensityRule section of the technology file 4. 50 microns For information about defining the metal density and density gradient rules in the technology file, including the windowSize attribute, see the Synopsys Technology File and Routing Rules Reference Manual. By default, the signoff_report_metal_density command - Uses the fill data for the metal density calculation \u25e6 To use the design view if its timestamp is newer than the fill data, set the signoff.report_metal_density.fill_view_data application option to read_if_uptodate. \u25e6 To use the design view regardless of its timestamp, set the signoff.report_metal_density.fill_view_data application option to discard."}
{"instruction": "How do I - for diode mode 2, the maximum antenna ratio for the net is the largest of the maximum antenna ratio values for the diodes, 2300. - for diode mode 3, the maximum antenna ratio for the net is the sum of the maximum antenna ratios for the diodes, 400 + 2200 + 2300 = 4900. - for diode mode 4, the maximum antenna ratio for the net is computed by using the formula from table 37 using the sum of the diode-protection values of the diodes, (0.5+1.0+1.5) * 200 + 2000 = 2600. example for diode modes 5 and 6 assume that the antenna area is calculated using surface area in single-layer mode (antenna mode 1), the diode ratio vector for the m1 layer is {0 0 1 0} (the default diode ratio vector), the layermaxratio value for the m1 layer is 400, and the following diodes are connected to a single m1 net?", "response": "- For diode mode 2, the maximum antenna ratio for the net is the largest of the maximum antenna ratio values for the diodes, 2300. - For diode mode 3, the maximum antenna ratio for the net is the sum of the maximum antenna ratios for the diodes, 400 + 2200 + 2300 = 4900. - For diode mode 4, the maximum antenna ratio for the net is computed by using the formula from Table 37 using the sum of the diode-protection values of the diodes, (0.5+1.0+1.5) * 200 + 2000 = 2600. Example for Diode Modes 5 and 6 Assume that the antenna area is calculated using surface area in single-layer mode (antenna mode 1), the diode ratio vector for the M1 layer is {0 0 1 0} (the default diode ratio vector), the layerMaxRatio value for the M1 layer is 400, and the following diodes are connected to a single M1 net: diode A with a diode-protection value of 0.5, diode B with a diode-protection value of 1.0, and diode C with a diode-protection value of 1.5. In this example, the v4 value is not specified so a value of 0 is used. Use the following command to define the antenna rules for this example: fc_shell> define_antenna_rule -mode 1 -diode_mode diode_mode \\ -metal_ratio 400 -cut_ratio 20 Because this example uses the global layerMaxRatio value and the default diode ratio vector, you do not need to define a layer-specific antenna rule. As shown in Table 37, for diode modes 5 and 6, the maximum antenna ratio is computed as metal_area / (gate_area + equi_gate_area) where the equivalent gate area, equi_gate_area, is computed by using the diode ratio vector: (diode_protection + v1) * (v2 + v3). - For diode mode 5, the equivalent gate area is computed using the maximum diode protection, which is 1.5, so the maximum antenna ratio for the net is metal_area / (gate_area + ((1.5 + 0) * (1 + 0))) = metal_area/(gate_area + 1.5) - For diode mode 6, the equivalent gate area is computed using the total diode protection, which is 0.5+1.0+1.5=3.0, so the maximum antenna ratio for the net is metal_area / (gate_area + ((3.0 + 0) * (1 + 0))) = metal_area/(gate_area + 3.0)"}
{"instruction": "How do I - dependent leakage power is used to calculate leakage power. to change the leakage power calculation mode, set the power.leakage_mode application option. for example, to set the application option to average do the following?", "response": "- dependent leakage power is used to calculate leakage power. To change the leakage power calculation mode, set the power.leakage_mode application option. For example, to set the application option to average do the following: fc_shell> set_app_options -name power.leakage_mode -value average To report threshold voltage usage, use the report_threshold_voltage_groups command. Classification is based on the threshold_voltage_group attribute from the reference library cells or the default_threshold_voltage_group attribute on the library. You can also set the attributes on the objects by using the set_attribute command. You can generate several types of reports with the report_threshold_voltage_groups command. For example, the summary report (using the -summary option) lists the number, area, and percentage of cells in each threshold voltage group. The default report (without using any options) shows the cell count and area based on the following groups: repeater, combinational, register, sequential, clock network, and physical-only cells. For the repeater, register, and clock network cell groups, you can obtain additional information by using the -detailed option. The -hierarchy option of the report_threshold_voltage_groups command generates a report based on the design hierarchy. The report shows the cell count, area, and area ratio for a top-level block and its subblocks. The -hierarchy option is mutually exclusive with the other report types, but can be used with the -datapath_cells_only and -standard_cells_only options. You can restrict the hierarchy-based report to specific threshold voltage groups by using the -hier_vt_groups option. You can also specify to report only the top-level cells that contain more than a specified number of constituent cells by using the -hier_threshold option. These options must be used with the -hierarchy option. If you specify a cell list, a separate report is created for each top-level cell in the list."}
{"instruction": "How do I 2.?", "response": "2."}
{"instruction": "How do I step 3, you need to run the route_custom command on each matching box. 5. report the list of mismatching nets. % grep \"mismatch\" log_file 6. reduce the number of the mismatched nets (if any) by using one of the following methods?", "response": "step 3, you need to run the route_custom command on each matching box. 5. Report the list of mismatching nets. % grep \"Mismatch\" log_file 6. Reduce the number of the mismatched nets (if any) by using one of the following methods: \u25e6 Rebalance the net routing of one or more groups in the same channel among the available routing layers by a. Resetting the routing layers. For example, - Set the routing layers for route group A to the M3 and M4 layers, and for group B to the M5 and M6 layers, or - Set the routing layers for route groups A and B to the M3 and M4 layers, and route all mismatched nets on the M5 and M6 layers. b. Rerunning initial routing with the rebalanced routing layer settings. \u25e6 Set a larger value for the custom.route.single_loop_match_max_spacing application option. \u25e6 Allow different layers for the single loop. fc_shell> set_app_options \\ -name custom.route.single_loop_match_offset_layer -value true 7. Insert buffer cells on the routed DDR net. In the following example, ddr_BUF is used as the prefix for the names of the added ECO nets and buffers for easy identification. The buffer cells are added at an interval that is 20 percent of the total net length. fc_shell> set ddr_buf \"BUF001\" fc_shell> add_buffer_on_route -net_prefix ddr_BUF \\ -cell_prefix ddr_BUF \\ -repeater_distance_length_ratio 0.2 \\ -respect_blockages [get_nets $ddr_nets] $ddr_BUF 8. Perform legalization and verify that the placement is legal. fc_shell> legalize_placement -cells $ddr_BUF fc_shell> check_legality -cells $ddr_BUF"}
{"instruction": "How do I 1. before you run custom router to perform routing on a ddr design, \u25e6 group the nets as a bundle for length matching. fc_shell> create_bundle -name grp1 [get_nets $nets] \u25e6 create and apply the routing rules for the ddr nets. fc_shell> create_routing_rule ddr -widths {layer value \u2026} \\ -spacings {layer value \u2026 } fc_shell> set_routing_rule -rule ddr -min_routing_layer m4 \\ -max_routing_layer m5 $all_ddr_nets?", "response": "1. Before you run Custom Router to perform routing on a DDR design, \u25e6 Group the nets as a bundle for length matching. fc_shell> create_bundle -name GRP1 [get_nets $nets] \u25e6 Create and apply the routing rules for the DDR nets. fc_shell> create_routing_rule DDR -widths {layer value \u2026} \\ -spacings {layer value \u2026 } fc_shell> set_routing_rule -rule DDR -min_routing_layer M4 \\ -max_routing_layer M5 $all_ddr_nets"}
{"instruction": "How do I note?", "response": "Note: Relative placement group blockages are honored during placement, but not during legalization, optimization, or clock tree synthesis. Defining Blockages That Allow Relative Placement Cells Only To define partial blockages that allow relative placement cells only, specify the boundary, type (-type allow_rp_only option), blockage percentage (-blocked_percentage option), and optionally the name for the placement blockage. For example, to defines a partial blockage that allows relative placement cells only, with a maximum allowed cell density of 80 percent (a blocked percentage of 20), and enclosed by the rectangle with corners at (10, 20) and (100,200), use the following command: create_placement_blockage -name rp_only -type allow_rp_only \\ -boundary {10 20 100 200} -blocked_percentage 20 These blockages allows the tool to place relative placement groups, which are usually large structures, without disturbing the placement of other cells. After the tool places and legalizes the relative placement groups, you can remove these blockages and allow the tool to place other cells in them. Note: Relative-placement-cells-only blockages are honored during placement, but not during legalization, optimization, or clock tree synthesis. Defining Blockages That Allow Buffers Only To define a partial blockage that allows buffers only, specify the boundary, type (-type allow_buffer_only option), blockage percentage (-blocked_percentage option), and optionally the name for the placement blockage. For example, to define a partial blockage that allows only the placement of buffers and inverters, with a cell density of 30 percent, use the following commands: create_placement_blockage -boundary {{10 20} {100 200}} \\ -type allow_buffer_only -blocked_percentage 70"}
{"instruction": "How do I - to remove all metal fill from a block, specific layers of a block, or specific regions of a block, use the signoff_create_metal_fill command with the -mode remove option. this command uses the ic validator tool to remove metal fill from the current block, as described in removing metal fill with the ic validator tool. - to remove specific fill shapes, use the remove_shapes or remove_fill_cells commands. you would typically use this method when manually fixing drc violations related to the metal fill. you might also be able to fix the drc violation by modifying the metal fill shapes, as described in modifying metal fill.?", "response": "- To remove all metal fill from a block, specific layers of a block, or specific regions of a block, use the signoff_create_metal_fill command with the -mode remove option. This command uses the IC Validator tool to remove metal fill from the current block, as described in Removing Metal Fill With the IC Validator Tool. - To remove specific fill shapes, use the remove_shapes or remove_fill_cells commands. You would typically use this method when manually fixing DRC violations related to the metal fill. You might also be able to fix the DRC violation by modifying the metal fill shapes, as described in Modifying Metal Fill."}
{"instruction": "How do I 11. route the connections between the clock mesh and the local subtrees by using the route_clock_straps command, as described in routing to clock straps. 12. analyze the clock mesh by using the analyze_subcircuit command, as described in analyzing the clock mesh. 13. enable integrated structural multisource clock tree synthesis by setting the clock_opt.flow.enable_multisource_clock_trees application option to true, and synthesize the entire clock tree from the clock root by using the clock_opt -from build_clock -to route_clock command. fc_shell> set_app_options \\ -name clock_opt.flow.enable_multisource_clock_trees -value true fc_shell> clock_opt -from build_clock -to route_clock during this step, the tool incrementally optimizes the existing subtrees. 14. analyze the clock tree results as described in analyzing the clock tree results. improving the correlation between the smscts and ccd to improve the correlation between the structural multisource clock tree synthesis (smscts) and concurrent clock and data (ccd), the fusion compiler tool supports the enhanced ccd engine with more smscts specific offsets. this improves the ccd balance point implementation and timing after the smscts step in the clock_opt command. to enable the improved correlation between the smscts and ccd in the compile_fusion command and build_clock stage of the clock_opt command, set the cts.multisource.enable_subtree_synthesis_aware_ccd application option to true. the improved correlation between the smscts and ccd?", "response": "11. Route the connections between the clock mesh and the local subtrees by using the route_clock_straps command, as described in Routing to Clock Straps. 12. Analyze the clock mesh by using the analyze_subcircuit command, as described in Analyzing the Clock Mesh. 13. Enable integrated structural multisource clock tree synthesis by setting the clock_opt.flow.enable_multisource_clock_trees application option to true, and synthesize the entire clock tree from the clock root by using the clock_opt -from build_clock -to route_clock command. fc_shell> set_app_options \\ -name clock_opt.flow.enable_multisource_clock_trees -value true fc_shell> clock_opt -from build_clock -to route_clock During this step, the tool incrementally optimizes the existing subtrees. 14. Analyze the clock tree results as described in Analyzing the Clock Tree Results. Improving the Correlation Between the SMSCTS and CCD To improve the correlation between the structural multisource clock tree synthesis (SMSCTS) and concurrent clock and data (CCD), the Fusion Compiler tool supports the enhanced CCD engine with more SMSCTS specific offsets. This improves the CCD balance point implementation and timing after the SMSCTS step in the clock_opt command. To enable the improved correlation between the SMSCTS and CCD in the compile_fusion command and build_clock stage of the clock_opt command, set the cts.multisource.enable_subtree_synthesis_aware_ccd application option to true. The improved correlation between the SMSCTS and CCD: - Improves the overall latency implementation after SMSCTS in the clock_opt command. - Reduces long and short path violations. - Improves timing after the clock_opt command with CCD offsets."}
{"instruction": "How do I 1. calculates the net delays of the block if the design library contains global route information, the tool uses the global route information to calculate the net delays; otherwise, it uses virtual routing to calculate the net delays. the global routing results can vary depending on whether the initial net delays were calculated by using global route information or virtual routing. to remove existing global route information from the signal nets in the block, use the -global_route and -net_types signal options with the remove_routes command, as shown in the following example?", "response": "1. Calculates the net delays of the block If the design library contains global route information, the tool uses the global route information to calculate the net delays; otherwise, it uses virtual routing to calculate the net delays. The global routing results can vary depending on whether the initial net delays were calculated by using global route information or virtual routing. To remove existing global route information from the signal nets in the block, use the -global_route and -net_types signal options with the remove_routes command, as shown in the following example: fc_shell> remove_routes -global_route -net_types signal 2. Performs an initial routing phase (phase 0), in which the tool routes the unconnected nets and calculates the overflow for each global routing cell"}
{"instruction": "How do I note?", "response": "Note: When you enable concurrent clock and data optimization, including power or area recovery that uses this technique, the tool does not optimize the clock latency or skew during postroute clock tree optimization; it only fixes the logical DRC violations on the clock network."}
{"instruction": "How do I note?", "response": "Note: The clock gates cloned to gate wrapper cells alone, inherit their names from parent clock gates and have dft_wrapper_ appended to their names. This is not applicable to the names of self-gates that drive wrapper cells only. Table 25 shows how different types of clock gates are renamed. Table 25 How different clock gate types are renamed Clock Gate Type Name New Name tool-inserted clock_gate_reg dft_wrapper_clock_gate_reg pre-existing icg_0 dft_wrapper_icg_0 Use the -gating_wrapper_cells_only option with the get_clock_gates command to return all clock gates (except self-gates) that drive wrapper cells only. This option can be used in conjunction with the already existing options. Use the compile.clockgate.substring_list_for_dft_wrapper_dont_rename_icgs application option to restrict the renaming of pre-existing clock gates (driving wrapper cells only). This option accepts a list of strings as values. The clock gates matching any of the substrings provided in the application option are not renamed. Note: The compile.clockgate.substring_list_for_dft_wrapper_dont_rename_icgs application option does not have any effect on the -gating_wrapper_cells_only querying option. Consider the following example: set_app_options -name compile.clockgate.substring_list_for_dft_wrapper_dont_rename_icgs -value {clk_gate_ctl clk_gate_snps} This prevents the renaming of any pre-existing clock gates that have the clk_gate_ctl or clk_gate_snps pattern in their name but can still be queried using the get_clock_gates -gating_wrapper_cells_only command."}
{"instruction": "How do I - resolving multiple references with the uniquify process - preserving cells and nets during optimization - restricting optimization to cell sizing only - preserving networks during optimization - marking the clock networks - disabling design rule checking (drc) - preserving pin names during sizing - preserving ports of existing hierarchies - isolating input and output ports - fixing multiple-port nets - controlling the addition of new cells to modules, hierarchical cells, and voltage areas - specifying a cell name prefix for optimization resolving multiple references with the uniquify process the uniquify command resolves multiple references, except those with the dont_touch attribute, throughout the hierarchy in the current design. the uniquify process copies and renames any multiply referenced design so that each instance references a unique design. the process removes the original design from memory after it creates the new and unique designs. the original design and any collections that contain it or its objects are no longer accessible. after this process finishes, the tool optimizes each design copy based on the unique context of its cell instance. to control the names of the reference copies, use the design.uniquify_naming_style application option. the default is %s_%d, where %s denotes the name of the existing reference and %d denotes the smallest integer value that forms a unique design name. this example creates unique reference copies of the specified cells a1, a2, and a3. fc_shell> uniquify {a1 a2 a3} 3?", "response": "- Resolving Multiple References With the Uniquify Process - Preserving Cells and Nets During Optimization - Restricting Optimization to Cell Sizing Only - Preserving Networks During Optimization - Marking the Clock Networks - Disabling Design Rule Checking (DRC) - Preserving Pin Names During Sizing - Preserving Ports of Existing Hierarchies - Isolating Input and Output Ports - Fixing Multiple-Port Nets - Controlling the Addition of New Cells to Modules, Hierarchical Cells, and Voltage Areas - Specifying a Cell Name Prefix for Optimization Resolving Multiple References With the Uniquify Process The uniquify command resolves multiple references, except those with the dont_touch attribute, throughout the hierarchy in the current design. The uniquify process copies and renames any multiply referenced design so that each instance references a unique design. The process removes the original design from memory after it creates the new and unique designs. The original design and any collections that contain it or its objects are no longer accessible. After this process finishes, the tool optimizes each design copy based on the unique context of its cell instance. To control the names of the reference copies, use the design.uniquify_naming_style application option. The default is %s_%d, where %s denotes the name of the existing reference and %d denotes the smallest integer value that forms a unique design name. This example creates unique reference copies of the specified cells A1, A2, and A3. fc_shell> uniquify {A1 A2 A3} 3"}
{"instruction": "How do I 1. apply a cts_fixed_balance_pin attribute on the sink pin by using the set_attribute command as shown in the following example?", "response": "1. Apply a cts_fixed_balance_pin attribute on the sink pin by using the set_attribute command as shown in the following example: fc_shell> set_attribute -objects reg21/CK \\ -name cts_fixed_balance_pin -value true 2. Set the ccd.respect_cts_fixed_balance_pins application option to true. These setting prevents the tool from adjusting the latencies of the sinks that have a cts_fixed_balance_pin attribute set to true. However, changes in the clock tree due to concurrent clock and data optimization performed on other sinks can change the latencies of the sinks that have a cts_fixed_balance_pin attribute set to true. To prevent the tool from making any changes to the clock paths between the sinks that have a cts_fixed_balance_pin attribute set to true and the clock root, set the"}
{"instruction": "How do I - clock and clock effects, see the \u201cdefining clocks\u201d topic in the fusion compiler timing analysis user guide. - exceptions for timing paths and constraints for boundary paths, see the \u201cconstraining timing paths\u201d topic in the fusion compiler timing analysis user guide. - modes, corners, and scenarios, see the \u201cdefining modes, corners, and scenarios\u201d topic in the fusion compiler timing analysis user guide. - operating conditions and on-chip variation (ocv) related settings, see the \u201cspecifying operating conditions\u201d topic in the fusion compiler timing analysis user guide. - parasitic information for rc estimation and extraction, see the \u201cperforming parasitic extraction\u201d topic in the fusion compiler timing analysis user guide. - routing layers, see specifying the routing resources. specifying logical design rule constraints minimum capacitance, maximum capacitance, and maximum transition are logical design rule constraints that your design must meet to function as intended. they are technology- specific restrictions that are specified in the logic libraries. however, you can specify more restrictive design rule constraints by using the constraint commands given in table 7. during optimization, the fusion compiler tries to meet the design rule constraints, even if it means violating optimization constraints such as timing, power, and area goals; these design rule constraints have a higher priority. after optimization, you can use the reporting commands given in table 7 to identify design rule constraint violations in a block. table 7 design rule commands to do this use this command specify the minimum allowed capacitance for input ports, library cell pins, leaf cell pins, clocks, or blocks set_min_capacitance specify the maximum allowed capacitance for input ports, library cell pins, leaf cell pins, clocks, or blocks set_max_capacitance specify the maximum allowed signal transition time for input ports, library cell pins, leaf cell pins, clocks, or blocks set_max_transition remove a user-specified minimum capacitance constraint remove_min_capacitance?", "response": "- Clock and clock effects, see the \u201cDefining Clocks\u201d topic in the Fusion Compiler Timing Analysis User Guide. - Exceptions for timing paths and constraints for boundary paths, see the \u201cConstraining Timing Paths\u201d topic in the Fusion Compiler Timing Analysis User Guide. - Modes, corners, and scenarios, see the \u201cDefining Modes, Corners, and Scenarios\u201d topic in the Fusion Compiler Timing Analysis User Guide. - Operating conditions and on-chip variation (OCV) related settings, see the \u201cSpecifying Operating Conditions\u201d topic in the Fusion Compiler Timing Analysis User Guide. - Parasitic information for RC estimation and extraction, see the \u201cPerforming Parasitic Extraction\u201d topic in the Fusion Compiler Timing Analysis User Guide. - Routing layers, see Specifying the Routing Resources. Specifying Logical Design Rule Constraints Minimum capacitance, maximum capacitance, and maximum transition are logical design rule constraints that your design must meet to function as intended. They are technology- specific restrictions that are specified in the logic libraries. However, you can specify more restrictive design rule constraints by using the constraint commands given in Table 7. During optimization, the Fusion Compiler tries to meet the design rule constraints, even if it means violating optimization constraints such as timing, power, and area goals; these design rule constraints have a higher priority. After optimization, you can use the reporting commands given in Table 7 to identify design rule constraint violations in a block. Table 7 Design Rule Commands To do this Use this command Specify the minimum allowed capacitance for input ports, library cell pins, leaf cell pins, clocks, or blocks set_min_capacitance Specify the maximum allowed capacitance for input ports, library cell pins, leaf cell pins, clocks, or blocks set_max_capacitance Specify the maximum allowed signal transition time for input ports, library cell pins, leaf cell pins, clocks, or blocks set_max_transition Remove a user-specified minimum capacitance constraint remove_min_capacitance"}
{"instruction": "How do I - left alignment (default) - right alignment - pin alignment controlling the cell alignment can improve the timing and routability of your design. aligning by the left edges by default, the fusion compiler tool aligns the leaf cells by aligning the left edges. to explicitly specify this alignment method, use the -alignment left option of the set_rp_group_options command. figure 175 shows cells that are left aligned. figure 175 bottom-left-aligned relative placement group u4 u3 u2 u1 row 3 row 2 row 1 row 0 col 0 aligning by the right edges to align a group by aligning the right edges, use the -alignment right option of the set_rp_group_options command. note?", "response": "- Left alignment (default) - Right alignment - Pin alignment Controlling the cell alignment can improve the timing and routability of your design. Aligning by the Left Edges By default, the Fusion Compiler tool aligns the leaf cells by aligning the left edges. To explicitly specify this alignment method, use the -alignment left option of the set_rp_group_options command. Figure 175 shows cells that are left aligned. Figure 175 Bottom-Left-Aligned Relative Placement Group U4 U3 U2 U1 row 3 row 2 row 1 row 0 col 0 Aligning by the Right Edges To align a group by aligning the right edges, use the -alignment right option of the set_rp_group_options command. Note: For hierarchical relative placement groups, the bottom-right alignment does not propagate through the hierarchy. Figure 175 shows cells that are right aligned."}
{"instruction": "How do I 2. enable the use of the target library subset by setting the opt.common.enable_target_library_subset_opt application option to 1. when you set target library subsets, remember the following points?", "response": "2. Enable the use of the target library subset by setting the opt.common.enable_target_library_subset_opt application option to 1. When you set target library subsets, remember the following points: - The subset restriction applies to hierarchical cells but not to leaf cells. - The command enforces the subset restriction on the specified blocks and their subdesigns in the hierarchy, except those subdesigns where a different subset restriction is set. - A subset specified at a lower level supersedes any subset specified at a higher level. Figure 7 Logic Hierarchy of Design"}
{"instruction": "How do I 3. performs two rerouting phases, in which the tool tries to reduce congestion by ripping up and rerouting nets around global routing cells with overflows 4. updates the design timing based on the first three global routing phases 5. identifies newly critical nets based on the timing update 6. performs three rerouting phases, in which the tool rips up and reroutes the newly critical nets to increase the number of rerouting phases after the timing update, set the global routing effort level to high or ultra by using the -effort_level option with the route_global command. to reduce runtime, at a possible cost to the timing qor, set the route.global.advance_node_timing_driven_effort application option. - when you set this application option to medium, the tool rips up and reroutes the newly critical nets only if doing so does not increase congestion. in addition, the number of rerouting phases is reduced for medium-, high-, and ultra-effort global routing. - when you set this application option to low, the tool does not perform the timing update and the number of rerouting phases is reduced for medium, high-, and ultra- effort global routing. the following table summarizes the timing-driven global routing behavior based on the effort level settings. the shaded cell shows the default behavior. table 32 number of rerouting phases based on the effort level settings route.global.advance_node_timing_driven_effort setting route_global -effort_level setting low medium high (default) low 2 2 2 medium (default) 3 33,4 53,5 high 4 43,4 63,4 ultra 6 63,4 83,4 to control the tradeoff between timing qor and drc convergence, set the route.global.timing_driven_effort_level application option. by default, this option has a setting of high, which favors timing qor over drc convergence. 3. timing update performed after the second rerouting phase. 4. newly critical nets are ripped up and rerouted only if it does not increase congestion. 5. newly critical nets are always ripped up and rerouted.?", "response": "3. Performs two rerouting phases, in which the tool tries to reduce congestion by ripping up and rerouting nets around global routing cells with overflows 4. Updates the design timing based on the first three global routing phases 5. Identifies newly critical nets based on the timing update 6. Performs three rerouting phases, in which the tool rips up and reroutes the newly critical nets To increase the number of rerouting phases after the timing update, set the global routing effort level to high or ultra by using the -effort_level option with the route_global command. To reduce runtime, at a possible cost to the timing QoR, set the route.global.advance_node_timing_driven_effort application option. - When you set this application option to medium, the tool rips up and reroutes the newly critical nets only if doing so does not increase congestion. In addition, the number of rerouting phases is reduced for medium-, high-, and ultra-effort global routing. - When you set this application option to low, the tool does not perform the timing update and the number of rerouting phases is reduced for medium, high-, and ultra- effort global routing. The following table summarizes the timing-driven global routing behavior based on the effort level settings. The shaded cell shows the default behavior. Table 32 Number of Rerouting Phases Based on the Effort Level Settings route.global.advance_node_timing_driven_effort setting route_global -effort_level setting low medium high (default) low 2 2 2 medium (default) 3 33,4 53,5 high 4 43,4 63,4 ultra 6 63,4 83,4 To control the tradeoff between timing QoR and DRC convergence, set the route.global.timing_driven_effort_level application option. By default, this option has a setting of high, which favors timing QoR over DRC convergence. 3. Timing update performed after the second rerouting phase. 4. Newly critical nets are ripped up and rerouted only if it does not increase congestion. 5. Newly critical nets are always ripped up and rerouted."}
{"instruction": "How do I 62.74% flip-flop cells banking ratio ((c)/(a+c))?", "response": "62.74% Flip-flop cells banking ratio ((C)/(A+C)): 62.96% BitsPerflop: 2.13 Reasons for sequential cells not mapping to multibit during RTL Banking: Explanations: r12: Cell is single bit because its parent multibit cell was debanked due to improve timing (Number of cells: 91) r31: Cell cannot be banked to multibit because it is assigned to use single-bit lib cell (Number of cells: 63) Reasons for multibit sequential cells not debanking to single bit cells during RTL Debanking: Explanations:: r45: Multibit cell cannot be debanked because it is not critical enough (Number of cells: 478) To report all the cells that are ignored during banking and debanking, use the -ignored_cells option with the report_multibit command. To report the compatible multibit and single-bit library cells that can be used for banking and debanking, use the check_multibit_library command. The following example generates a compatible library cell report for RTL banking and debanking: fc_shell> check_multibit_library -stage RTL \\ -banking -debanking **************************************** Report : check_multibit_library Flow : RTL BANKING **************************************** ---------------------------------------------------------------- Single bit Lib cell Compatibility Multi bit Lib cell ---------------------------------------------------------------- SB_reg1 PIN ORDER MISMATCH MB_reg1 SB_reg2 COMPATIBLE MB_reg2 ---------------------------------------------------------------- Singlebit with NO Multibit Equivalents ---------------------------------------------------------------- SB_reg3"}
{"instruction": "How do I - minimum density, which specifies the minimum percentage of metal allowed in the density checking window this rule is checked when you enable density error fixing by setting the signoff.create_metal_fill.fix_density_errors application option to true. by default, the ic validator tool uses the setting defined for the mindensity attribute in the densityrule sections of the technology file. if this attribute is not defined for a layer, the ic validator tool uses a default of 10 percent. to override the default, define the mx_min_density parameter in the ic validator parameter file. - density gradient, which specifies the maximum percentage difference between the fill density of adjacent density checking windows this rule is checked if it is defined in the technology file and you enable density error fixing by setting the signoff.create_metal_fill.fix_density_errors application option to true. the ic validator tool uses the setting defined for the maxgradientdensity attribute in the densityrule sections of the technology file. if this attribute is not defined for a layer, the ic validator tool does not check this rule. - maximum open area, which specifies the maximum size of a square area that contains no polygons and does not interact with any polygons this rule is checked if it is defined. the maximum open area rule is defined in an ic validator parameter file. use the following syntax to define the maximum open area rule for each metal layer?", "response": "- Minimum density, which specifies the minimum percentage of metal allowed in the density checking window This rule is checked when you enable density error fixing by setting the signoff.create_metal_fill.fix_density_errors application option to true. By default, the IC Validator tool uses the setting defined for the minDensity attribute in the DensityRule sections of the technology file. If this attribute is not defined for a layer, the IC Validator tool uses a default of 10 percent. To override the default, define the mx_min_density parameter in the IC Validator parameter file. - Density gradient, which specifies the maximum percentage difference between the fill density of adjacent density checking windows This rule is checked if it is defined in the technology file and you enable density error fixing by setting the signoff.create_metal_fill.fix_density_errors application option to true. The IC Validator tool uses the setting defined for the maxGradientDensity attribute in the DensityRule sections of the technology file. If this attribute is not defined for a layer, the IC Validator tool does not check this rule. - Maximum open area, which specifies the maximum size of a square area that contains no polygons and does not interact with any polygons This rule is checked if it is defined. The maximum open area rule is defined in an IC Validator parameter file. Use the following syntax to define the maximum open area rule for each metal layer: mn_max_open_area_rule = value; where n is the metal layer number and value is the side length of the square in microns. For more information about the IC Validator parameter file, see Using an IC Validator Parameter File. For more information about the technology file, see the Synopsys Technology File and Routing Rules Reference Manual."}
{"instruction": "How do I - the disallowed spacing range the spacing distance is specified as the number of sites. to specify the disallowed spacing range, use the -tap_distance_range option. - the tap cells considered for the check these cells are called target cells. to specify the target tap cells, use the -tap_cells option. - the tap cells used to replace the violating tap cells these cells are called replacement cells. \u25e6 to specify the replacement tap cell for left-side violations, use the -left_violation_tap option. \u25e6 to specify the replacement tap cell for right-side violations, use the -right_violation_tap option. \u25e6 to specify the replacement tap cell for both-side violations, use the -both_violation_tap option. note?", "response": "- The disallowed spacing range The spacing distance is specified as the number of sites. To specify the disallowed spacing range, use the -tap_distance_range option. - The tap cells considered for the check These cells are called target cells. To specify the target tap cells, use the -tap_cells option. - The tap cells used to replace the violating tap cells These cells are called replacement cells. \u25e6 To specify the replacement tap cell for left-side violations, use the -left_violation_tap option. \u25e6 To specify the replacement tap cell for right-side violations, use the -right_violation_tap option. \u25e6 To specify the replacement tap cell for both-side violations, use the -both_violation_tap option. Note: The target and replacement tap cells must all have the same size."}
{"instruction": "How do I 3. -max_fanout specifies the maximum number of cells gated by the same clock-gating cell. the default is infinite. -objects specifies the list of objects to which the clock-gating options are applied. the design objects can be hierarchical instances, power domains, and designs. enabling or disabling clock gating on design objects you can enable or disable clock gating on certain design objects by overriding all necessary conditions set by the clock-gating style. the set_clock_gating_objects command specifies the design objects on which clock gating should be enabled or disabled during the compile_fusion command. the following example excludes all registers in the subdesign adder, except the out1_reg bank. the out1_reg bank is clock gated according to the specified clock-gating style?", "response": "3. -max_fanout Specifies the maximum number of cells gated by the same clock-gating cell. The default is infinite. -objects Specifies the list of objects to which the clock-gating options are applied. The design objects can be hierarchical instances, power domains, and designs. Enabling or Disabling Clock Gating on Design Objects You can enable or disable clock gating on certain design objects by overriding all necessary conditions set by the clock-gating style. The set_clock_gating_objects command specifies the design objects on which clock gating should be enabled or disabled during the compile_fusion command. The following example excludes all registers in the subdesign ADDER, except the out1_reg bank. The out1_reg bank is clock gated according to the specified clock-gating style: fc_shell> set_clock_gating_objects -exclude ADDER \\ -include ADDER/out1_reg[*]"}
{"instruction": "How do I - analyzing the routing results - saving route information - deriving mask colors - inserting and removing cut metal shapes introduction to zroute zroute has five routing engines?", "response": "- Analyzing the Routing Results - Saving Route Information - Deriving Mask Colors - Inserting and Removing Cut Metal Shapes Introduction to Zroute Zroute has five routing engines: global routing, track assignment, detail routing, ECO routing, and routing verification. You can invoke global routing, track assignment, and detail routing by using task-specific commands or by using an automatic routing command. You invoke ECO routing and route verification by using task-specific commands. Zroute includes the following main features: - Multithreading on multicore hardware for all routing steps, including global routing, track assignment, and detail routing - A realistic connectivity model where Zroute recognizes electrical connectivity if the rectangles touch; it does not require the center lines of wires to connect - A dynamic maze grid that permits Zroute to go off-grid to connect pins, while retaining the speed advantages of gridded routers - A polygon manager, which allows Zroute to recognize polygons and to understand that design rule checks (DRCs) are aimed at polygons - Concurrent optimization of design rules, antenna rules, wire optimization, and via optimization during detail routing - Concurrent redundant via insertion during detail routing - Support for soft rules built into global routing, track assignment, and detail routing - Timing- and crosstalk-driven global routing, track assignment, detail routing, and ECO routing - Intelligent design rule handling, including merging of redundant design rule violations and intelligent convergence - Net group routing with layer constraints and nondefault routing rules - Clock routing - Route verification"}
{"instruction": "How do I note?", "response": "Note: When back-annotating the analysis results from the previous run after moving nets in the design, the instance-based map reflects the updated instance location change, but the parasitic map does not. When opening rail results, the tool does not detect if the result is generated with or without the -bg option. The command might not work correctly if you run the open_rail_result -back_annotate command to open the rail result that is generated without the -bg option, or vice versa. Reporting and Checking the Status of the Background Process When the RedHawk or RedHawk-SC Fusion process is finished, the tool issues the following message: Info: Running REDHAWK_BINARY in background with log file: LOG_FILE The LOG_FILE file is saved in the PWD directory. To check if the background analyze_rail process is active, use the report_background_jobs command. Setting Up the Executables To run RedHawk Fusion or RedHawk-SC Fusion features, enable the features and specify the location of the RedHawk or RedHawk-SC executable by setting the rail.product and rail.redhawk_path application options. For example, fc_shell> set_app_options -name rail.product -value redhawk|redhawk_sc fc_shell> set_app_options -name rail.redhawk_path \\ -value /tools/RedHawk_Linux64e5_V19.0.2p2/bin You must ensure that the specified executable is compatible with the Fusion Compiler version that you are using. See Also - Specifying RedHawk and RedHawk-SC Working Directories Specifying RedHawk and RedHawk-SC Working Directories During rail analysis, RedHawk/RedHawk-SC Fusion creates a working directory to store the generated files, including analysis logs, scripts, and various output data. By default,"}
{"instruction": "How do I 2. use the machine learning data to improve correlation by performing the following steps in the subsequent iteration, iteration n+1?", "response": "2. Use the machine learning data to improve correlation by performing the following steps in the subsequent iteration, iteration N+1: a. Perform clock tree synthesis and clock routing by completing the build_clock and route_clock stages of the clock_opt command. b. Specify the output directory for the machine learning data by using the new est_delay.ml_delay_opto_dir application option. fc_shell> set_application_option \\ -name est_delay.ml_delay_opto_dir -value ./ML c. Enable the machine learning model, which was derived in the previous iteration, by setting the new est_delay.ml_delay_gre_mode application option to enable. fc_shell> set_application_option \\ -name est_delay.ml_delay_gre_mode -value enable d. Perform global-route-based optimization with the machine learning data by using the clock_opt -from final_opto command. e. Perform detail routing. f. Disable the machine learning model after you complete all the detail routing steps by using the estimate_delay -disable_model command, as shown in the following example: fc_shell> estimate_delay -disable_model -output_dir ./ML g. Continue with postroute optimization and the rest of the steps in the implementation flow. When using this feature, ensure that - The same scenarios are active at each step and each iteration of the flow - The machine learning model is re-created if there are changes to the design, its environment, or the flow"}
{"instruction": "How do I - sorting the data - filtering metrics - filtering runs - example analysis see also - exploring the detailed comparison data?", "response": "- Sorting the Data - Filtering Metrics - Filtering Runs - Example Analysis See Also - Exploring the Detailed Comparison Data"}
{"instruction": "How do I - total optimized via conversion rate = 98.96% (2902130 / 2932706 vias) layer via1 = 97.81% (1091973/ 1116367 vias) weight 1 = 97.81% (1091973 vias) un-optimized = 2.19% (24394 vias) \u2026 total double via conversion rate = 98.96% (2902130 / 2932706 vias) layer via1 = 97.81% (1091973/ 1116367 vias) layer via2 = 99.97% (1071650/ 1071978 vias) \u2026 the optimized via conversion rate based on total routed via count = 98.96% (2902130 / 2932706 vias) layer via1 = 97.81% (1091973/ 1116367 vias) weight 1 = 97.81% (1091973 vias) un-optimized = 2.19% (24394 vias) \u2026 total number of nets = 255165 0 open nets, of which 0 are frozen total number of excluded ports = 0 ports of 0 unplaced cells connected to 0 nets 0 ports without pins of 0 cells connected to 0 nets 0 ports of 0 cover cells connected to 0 non-pg nets total number of drcs = 7398 total number of antenna violations = antenna checking not active information?", "response": "- Total optimized via conversion rate = 98.96% (2902130 / 2932706 vias) Layer VIA1 = 97.81% (1091973/ 1116367 vias) Weight 1 = 97.81% (1091973 vias) Un-optimized = 2.19% (24394 vias) \u2026 Total double via conversion rate = 98.96% (2902130 / 2932706 vias) Layer VIA1 = 97.81% (1091973/ 1116367 vias) Layer VIA2 = 99.97% (1071650/ 1071978 vias) \u2026 The optimized via conversion rate based on total routed via count = 98.96% (2902130 / 2932706 vias) Layer VIA1 = 97.81% (1091973/ 1116367 vias) Weight 1 = 97.81% (1091973 vias) Un-optimized = 2.19% (24394 vias) \u2026 Total number of nets = 255165 0 open nets, of which 0 are frozen Total number of excluded ports = 0 ports of 0 unplaced cells connected to 0 nets 0 ports without pins of 0 cells connected to 0 nets 0 ports of 0 cover cells connected to 0 non-pg nets Total number of DRCs = 7398 Total number of antenna violations = antenna checking not active Information: Routes in non-preferred voltage areas = 8170 (ZRT-559) Topology ECO iteration 1 ended with 0 qualifying violations. Routing Signal Nets by Using Automatic Routing Before you run automatic routing, - Set the common routing application options For information about the common routing application options, see the route.common_options man page. - Set the global routing application options For information about the detail routing application options, see the route.global_options man page."}
{"instruction": "How do I * layer | overflow | # grcs has name | total | max | overflow (%) | max overflow --------------------------------------------------------------- both dirs | 39 | 8 | 14 ( 0.26%) | 1 h routing | 6 | 2 | 4 ( 0.07%) | 2 v routing | 33 | 8 | 10 ( 0.18%) | 1 in the default congestion report, - \u201ch routing\u201d refers to results for horizontal routes only and \u201cv routing\u201d refers to results for vertical routes only. - the total overflow value is the total number of wires in the block that do not have a corresponding track available. the max overflow value is the highest number of overutilized wires in a single global routing cell. - the grcs overflow value is the total number of overcongested global routing cells in the design. the grcs max overflow value is the number of global routing cells that have the maximum overflow. note?", "response": "* Layer | overflow | # GRCs has Name | total | max | overflow (%) | max overflow --------------------------------------------------------------- Both Dirs | 39 | 8 | 14 ( 0.26%) | 1 H routing | 6 | 2 | 4 ( 0.07%) | 2 V routing | 33 | 8 | 10 ( 0.18%) | 1 In the default congestion report, - \u201cH routing\u201d refers to results for horizontal routes only and \u201cV routing\u201d refers to results for vertical routes only. - The total overflow value is the total number of wires in the block that do not have a corresponding track available. The max overflow value is the highest number of overutilized wires in a single global routing cell. - The GRCs overflow value is the total number of overcongested global routing cells in the design. The GRCs max overflow value is the number of global routing cells that have the maximum overflow. Note: The overflow and global routing cell numbers reported by the report_congestion command might look slightly more optimistic than those reported by the route_global command because the tool rounds down the congestion information before saving it with the design. Use the following options to modify the default behavior: - -rerun_global_router Use this option to rerun the global routing even if the block already has a congestion map. - -boundary coordinates Use this option to restrict the reporting to a specific region of the block. - -layers layers Use this option to restrict the reporting to specific layers. - -mode global_route_cell_edge_based Use this option to report overflow information for each global routing cell. - -include_soft_congestion_maps"}
{"instruction": "How do I - switching activity applied with the set_switching_activity command - switching activity within block abstracts the scaled switching activity is persistent in the design. you can write it out by using the write_saif command and use it in the subsequent steps of the design flow. specifying switching probability for supply nets the leakage power of a block is scaled based on the switching probability of its supply nets, which represents the fraction of time a supply net is in the on state. you can specify the switching probability for one or more supply nets by using the set_supply_net_probability -static_probability command. by default, the switching probability is applied to the current scenario and the corresponding mode and corner. to specify modes, corners, or scenarios in which to apply the setting, use the -modes, -corners, or -scenarios option. to get the switching probability of a supply net, use the get_supply_net_probability command, and to reset the value, use the reset_supply_net_probability command. by default, the tool propagates supply net activity through power switches and determines the static probability of the switched supply net based on the upf power switch constraint. for example, consider the following upf power switch constraint?", "response": "- Switching activity applied with the set_switching_activity command - Switching activity within block abstracts The scaled switching activity is persistent in the design. You can write it out by using the write_saif command and use it in the subsequent steps of the design flow. Specifying Switching Probability for Supply Nets The leakage power of a block is scaled based on the switching probability of its supply nets, which represents the fraction of time a supply net is in the on state. You can specify the switching probability for one or more supply nets by using the set_supply_net_probability -static_probability command. By default, the switching probability is applied to the current scenario and the corresponding mode and corner. To specify modes, corners, or scenarios in which to apply the setting, use the -modes, -corners, or -scenarios option. To get the switching probability of a supply net, use the get_supply_net_probability command, and to reset the value, use the reset_supply_net_probability command. By default, the tool propagates supply net activity through power switches and determines the static probability of the switched supply net based on the UPF power switch constraint. For example, consider the following UPF power switch constraint: create_power_switch my_switch \\ -output_supply_port {vout VDDS} \\ -input_supply_port {vin VDD} \\ -control_port {ms_sel ctrl1} \\ -control_port {ms_ctrl ctrl2} \\ -on_state {on vin {ms_ctrl && !ms_sel}}"}
{"instruction": "How do I - certain path groups by specifying them by using the ccd.targeted_ccd_path_groups application option, as shown in the following example?", "response": "- Certain path groups by specifying them by using the ccd.targeted_ccd_path_groups application option, as shown in the following example: fc_shell> set_app_options -name ccd.targeted_ccd_path_groups \\ -value {PG1 PG2} If a path group you specify with this application option is also specified as a path group to skip with the ccd.skip_path_groups application option, the path group is skipped during concurrent clock and data optimization."}
{"instruction": "How do I 1. annotate switching activity on the design, as described in annotating the switching activity. 2. ensure that at least one scenario is enabled for dynamic-power optimization by using the set_scenario_status -dynamic_power true command. 3. enable power-driven placement for the create_placement, refine_placement, compile_fusion, or clock_opt command by using one of the following methods?", "response": "1. Annotate switching activity on the design, as described in Annotating the Switching Activity. 2. Ensure that at least one scenario is enabled for dynamic-power optimization by using the set_scenario_status -dynamic_power true command. 3. Enable power-driven placement for the create_placement, refine_placement, compile_fusion, or clock_opt command by using one of the following methods: \u25e6 Enable dynamic-power-driven placement by setting the place.coarse.enhanced_low_power_effort application option to none, low, medium, or high. The default is low. During dynamic-power-driven placement, the tool tries to improve both the timing and power of timing-critical nets and the power of the other nets. This improves the power QoR without affecting the timing QoR. Performing IR-Drop-Aware Placement During placement, the tool can use the voltage (IR) drop values of cells to identify areas of high power density and spread the cells with high voltage drop values, which reduces the power density of such areas. To perform IR-drop-aware placement, use the following steps: 1. Place and legalize the block. 2. Set up for RedHawk Fusion and perform static or dynamic voltage drop analysis by using the analyze_rail -voltage_drop command as shown in the following example: fc_shell> source redhawk_setup.tcl fc_shell> analyze_rail -voltage_drop static -nets {VDD VSS} For more information, see Performing Voltage Drop Analysis. 3. Enable IR-drop-aware placement by setting the place.coarse.ir_drop_aware application option to true."}
{"instruction": "How do I - the is_clock_is_used_as_clock attribute of the u2/a, u2/y, and ff3/d pins to false and considers the network beyond the u2/a pin as part of the data network - the u2/a pin as an ignore pin and excludes the network beyond this from clock tree synthesis during clock tree synthesis, the tool adds a guide buffer and isolates the u2/a pin from the rest of the clock network. during subsequent data path optimization, the tool can fix any existing drc violations beyond the guide buffer. assume you want clock tree synthesis to fix drc violations up to the ff3/d pin using clock tree constraints. to do so, you must 1. specify the ff3/d pin as an explicit ignore pin as follows?", "response": "- The is_clock_is_used_as_clock attribute of the U2/A, U2/Y, and FF3/D pins to false and considers the network beyond the U2/A pin as part of the data network - The U2/A pin as an ignore pin and excludes the network beyond this from clock tree synthesis During clock tree synthesis, the tool adds a guide buffer and isolates the U2/A pin from the rest of the clock network. During subsequent data path optimization, the tool can fix any existing DRC violations beyond the guide buffer. Assume you want clock tree synthesis to fix DRC violations up to the FF3/D pin using clock tree constraints. To do so, you must 1. Specify the FF3/D pin as an explicit ignore pin as follows: fc_shell> set_clock_balance_points -clock [get_clocks CLK] \\ -consider_for_balancing false -balance_points [get_pins FF3/D]"}
{"instruction": "How do I - the tool disables propagation of equal-opposite logic and phase inversion. - constants and unloaded ports continue to propagate across hierarchies. to disable constant propagation and unloaded propagation globally, set the compile.flow.constant_and_unloaded_propagation_with_no_boundary_opt application option to false. the default is true. the setting does not affect object-level specifications. controlling specific types of boundary optimization to control the level of boundary optimization for hierarchical cell pins, hierarchical cells, or blocks, use the set_boundary_optimization command. the tool uses the following order of precedence to determine the setting to use during optimization?", "response": "- The tool disables propagation of equal-opposite logic and phase inversion. - Constants and unloaded ports continue to propagate across hierarchies. To disable constant propagation and unloaded propagation globally, set the compile.flow.constant_and_unloaded_propagation_with_no_boundary_opt application option to false. The default is true. The setting does not affect object-level specifications. Controlling Specific Types of Boundary Optimization To control the level of boundary optimization for hierarchical cell pins, hierarchical cells, or blocks, use the set_boundary_optimization command. The tool uses the following order of precedence to determine the setting to use during optimization: 1. Setting applied on pins of hierarchical cell instances 2. Setting applied on hierarchical cell instances, which applies to its pins as well 3. Setting applied on modules, which applies to all its cells and pins as well"}
{"instruction": "How do I 141. figure 140 bins value set to 5?", "response": "141. Figure 140 Bins Value Set to 5"}
{"instruction": "How do I 1. see also - routing nets in the gui routing nets in the gui to route nets interactively in the active layout view, draw the routes with the layout editing tolscreate route tolcreate route tool. to activate the create route tool, click the button on the edit toolbar or choose create > route. to draw route segments, you click points in the layout view. the tool displays flylines and target port and pin locations to guide you in drawing the route segments. it can also check for routing design rule violations as you draw the route segments. you can set options to adjust the routing, control the tool operation, and enable or disable routing aids. by default, the create route tool - ignores routing blockages to force the tool to honor these blockages, change the setting in the mouse tool options panel. note?", "response": "1. See Also - Routing Nets in the GUI Routing Nets in the GUI To route nets interactively in the active layout view, draw the routes with the layout editing tolsCreate Route tolCreate Route tool. To activate the Create Route tool, click the button on the Edit toolbar or choose Create > Route. To draw route segments, you click points in the layout view. The tool displays flylines and target port and pin locations to guide you in drawing the route segments. It can also check for routing design rule violations as you draw the route segments. You can set options to adjust the routing, control the tool operation, and enable or disable routing aids. By default, the Create Route tool - Ignores routing blockages To force the tool to honor these blockages, change the setting in the Mouse Tool Options panel. Note: The Create Route tool does not honor routing guides defined by the create_routing_guide command. - Uses the metal width and metal spacing requirements defined in the technology file and ignores nondefault routing rules To force the tool to honor nondefault routing rules, change the setting in the Mouse Tool Options panel. When you enable this feature, the Create Route tool honors the metal width and metal spacing requirements from the nondefault routing rule and uses these settings to determine the width and pitch of the routes. Note: The shielding width and shield spacing defined in the nondefault routing rule are not used by the Create Route tool."}
{"instruction": "How do I 76.20% (1567822/ 2057614 vias) weight 10 = 43.51% (895270 vias) weight 5 = 28.62% (588805 vias) weight 1 = 4.07% (83747 vias) un-optimized = 23.80% (489792 vias) layer v03 = 81.87% (687115 / 839297 vias) weight 10 = 64.50% (541369 vias) weight 5 = 10.75% (90224 vias) weight 1 = 6.62% (55522 vias) un-optimized = 18.13% (152182 vias) layer v04 = 81.60% (226833 / 277977 vias) weight 10 = 81.45% (226418 vias) weight 1 = 0.15% (415 vias) un-optimized = 18.40% (51144 vias) ... layer v09 = 85.47% (1329 / 1555 vias) weight 10 = 85.47% (1329 vias) un-optimized = 14.53% (226 vias) total double via conversion rate = 46.69% (2158006 / 4622189 vias) optimizing wire length and via count during detail routing, zroute optimizes wire length and via count in the areas where drc violations occur; however, it does not optimize the layout in areas where no drc violations occur. to improve the manufacturing yield, use the optimize_routes command to perform standalone optimization of wire length and via count after performing detail routing and redundant via insertion. by default, zroute selects the nets to reroute based on the overall cost. for each selected net, zroute determines whether to reroute all the shapes in the net or just a portion of them. to select the nets to reroute, use the -nets option to specify the nets. when you specify the nets to optimize, you can also use the -reroute_all_shapes_in_nets option to control whether zroute must reroute all the associated net shapes. by default, zroute performs a maximum of 40 detail routing iterations to fix drc violations that exist after the optimization. you can use the -max_detail_route_iterations option to control the maximum number of detail routing iterations. reducing critical areas a critical area is a region of the block where, if the center of a random particle defect falls there, the defect causes circuit failure, thereby reducing yield. a conductive defect causes a short fault, and a nonconductive defect causes an open fault.?", "response": "76.20% (1567822/ 2057614 vias) Weight 10 = 43.51% (895270 vias) Weight 5 = 28.62% (588805 vias) Weight 1 = 4.07% (83747 vias) Un-optimized = 23.80% (489792 vias) Layer V03 = 81.87% (687115 / 839297 vias) Weight 10 = 64.50% (541369 vias) Weight 5 = 10.75% (90224 vias) Weight 1 = 6.62% (55522 vias) Un-optimized = 18.13% (152182 vias) Layer V04 = 81.60% (226833 / 277977 vias) Weight 10 = 81.45% (226418 vias) Weight 1 = 0.15% (415 vias) Un-optimized = 18.40% (51144 vias) ... Layer V09 = 85.47% (1329 / 1555 vias) Weight 10 = 85.47% (1329 vias) Un-optimized = 14.53% (226 vias) Total double via conversion rate = 46.69% (2158006 / 4622189 vias) Optimizing Wire Length and Via Count During detail routing, Zroute optimizes wire length and via count in the areas where DRC violations occur; however, it does not optimize the layout in areas where no DRC violations occur. To improve the manufacturing yield, use the optimize_routes command to perform standalone optimization of wire length and via count after performing detail routing and redundant via insertion. By default, Zroute selects the nets to reroute based on the overall cost. For each selected net, Zroute determines whether to reroute all the shapes in the net or just a portion of them. To select the nets to reroute, use the -nets option to specify the nets. When you specify the nets to optimize, you can also use the -reroute_all_shapes_in_nets option to control whether Zroute must reroute all the associated net shapes. By default, Zroute performs a maximum of 40 detail routing iterations to fix DRC violations that exist after the optimization. You can use the -max_detail_route_iterations option to control the maximum number of detail routing iterations. Reducing Critical Areas A critical area is a region of the block where, if the center of a random particle defect falls there, the defect causes circuit failure, thereby reducing yield. A conductive defect causes a short fault, and a nonconductive defect causes an open fault."}
{"instruction": "How do I - the name of the constraint group if you do not specify a name, the command names the constraint group differential_pair_n or differential_group_n, where n is a unique integer. by default, the command returns an error if the assigned name already exists. to delete the existing constraint group, use the -force option. - the nets in the differential group to specify the nets included in the created group, use the -for option. if you specify two nets, the nets form a differential pair. if you specify three nets, the nets form a differential group. - the constraints for the differential group to define the constraints that are applied per layer to the trunks and override any settings on the individual bits, use the options listed in table 54. table 54 differential group constraints option constraint -twist_style specifies the twist style. valid values are \u25e6diagonal?", "response": "- The name of the constraint group If you do not specify a name, the command names the constraint group differential_pair_n or differential_group_n, where n is a unique integer. By default, the command returns an error if the assigned name already exists. To delete the existing constraint group, use the -force option. - The nets in the differential group To specify the nets included in the created group, use the -for option. If you specify two nets, the nets form a differential pair. If you specify three nets, the nets form a differential group. - The constraints for the differential group To define the constraints that are applied per layer to the trunks and override any settings on the individual bits, use the options listed in Table 54. Table 54 Differential Group Constraints Option Constraint -twist_style Specifies the twist style. Valid values are \u25e6diagonal: Applies 45-degree twists to the wires. \u25e6none (default): Does not twist the wires. \u25e6orthogonal: Applies 90-degree twists to the wires. -twist_interval Specifies the distance between each twist. -twist_offset Specifies the distance between the first twist and the connected pins. The default is 0. -valid_layers Specifies the layers on which to route the differential group. -layer_spacings Specifies the minimum spacing for each routing layer. -layer_widths Specifies the minimum width for each routing layer."}
{"instruction": "How do I - generating hotspots - reporting hotspots - voltage hotspot analysis examples voltage hotspot analysis examples this topic provides examples about how to use the hotspot analysis capability to identify the root cause of voltage violations. cells with overlapping timing windows might switch at the same time, leading to high voltage drops. figure 215 shows how to identify if the cell has overlapping timing windows.?", "response": "- Generating Hotspots - Reporting Hotspots - Voltage Hotspot Analysis Examples Voltage Hotspot Analysis Examples This topic provides examples about how to use the hotspot analysis capability to identify the root cause of voltage violations. Cells with overlapping timing windows might switch at the same time, leading to high voltage drops. Figure 215 shows how to identify if the cell has overlapping timing windows."}
{"instruction": "How do I - the submit protocol \u25e6 to use lsf, use the -submit_protocol lsf option. the lsf bsub command must be in your linux path. \u25e6 to use the univa grid engine (uge) job scheduler, which was previously known as sge (sun grid engine) or grd (global resource directory), use the -submit_protocol grid option. the grid qsub command must be in your linux path. if you do not specify a host name or submit protocol, the ic validator tool runs on the host on which the fusion compiler tool is currently running. - the number of hosts to use use the -num_processes option to specify the number of hosts. - the number of cores to use on each host use the -max_cores option to specify the number of cores per host. - (optional) extra arguments needed for the submit command use the -submit_command option to specify the submit command with the extra arguments. for example, to enable 4 hosts to be acquired by lsf, each of which can use 8 cores (for a total of 32 cores), use the following command?", "response": "- The submit protocol \u25e6 To use LSF, use the -submit_protocol lsf option. The LSF bsub command must be in your Linux path. \u25e6 To use the Univa Grid Engine (UGE) job scheduler, which was previously known as SGE (Sun Grid Engine) or GRD (Global Resource Directory), use the -submit_protocol grid option. The Grid qsub command must be in your Linux path. If you do not specify a host name or submit protocol, the IC Validator tool runs on the host on which the Fusion Compiler tool is currently running. - The number of hosts to use Use the -num_processes option to specify the number of hosts. - The number of cores to use on each host Use the -max_cores option to specify the number of cores per host. - (Optional) Extra arguments needed for the submit command Use the -submit_command option to specify the submit command with the extra arguments. For example, to enable 4 hosts to be acquired by LSF, each of which can use 8 cores (for a total of 32 cores), use the following command: fc_shell> set_host_options -submit_protocol lsf -target ICV \\ -num_processes 4 -max_cores 8 To set the memory requirement of the jobs in the previous example to 1000M, use the following command: fc_shell> set_host_options -submit_protocol lsf -target ICV \\ -num_processes 4 -max_cores 8 \\ -submit_command {bsub -R rusage[mem=1000]} To enable 4 hosts to be acquired by the Grid Engine, each of which can use 8 cores (for a total of 32 cores), use the following command: fc_shell> set_host_options -submit_protocol grid -target ICV \\ -num_processes 4 -max_cores 8"}
{"instruction": "How do I - invalid real metal via cut blockages this check verifies that all real metal via cut blockages match a legal cut size, as defined by the cutwidthtbl, cutheighttbl, and minwidth attributes in the technology file. to disable this check, use the -check_via_cut_blockage false option. - minimum width settings this check verifies that the nondefault minimum width and shield width settings are no larger than the maximum width defined in the technology file. you cannot disable these checks. the check_routability command also supports the following optional checks?", "response": "- Invalid real metal via cut blockages This check verifies that all real metal via cut blockages match a legal cut size, as defined by the cutWidthTbl, cutHeightTbl, and minWidth attributes in the technology file. To disable this check, use the -check_via_cut_blockage false option. - Minimum width settings This check verifies that the nondefault minimum width and shield width settings are no larger than the maximum width defined in the technology file. You cannot disable these checks. The check_routability command also supports the following optional checks: - Blocked power or ground ports To enable this check, use the -check_pg_blocked_ports true option. - Redundant power or ground shapes To enable this check, use the -check_redundant_pg_shapes true option. - Staggered power and ground vias that block the routing tracks To enable this check, use the -check_routing_track_space true option. - Blocked ports on frozen nets To enable this check, use the -check_frozen_net_blocked_ports true option. - Blocked unconnected pins To enable this check, use the -check_no_net_pins true option. - Real metal blockages that overlap library cell pins To enable this check, use the -check_real_metal_blockage_overlap_pin true option. - Invalid real metal via cut blockages in the library cells To enable this check, use the -check_lib_via_cut_blockage true option."}
{"instruction": "How do I - when the design is single-rail buffer only, use the -va_aware option fc_shell> add_group_repeaters -lib_cell <lib_cell> -va_aware - when the design is single-rail buffer based on voltage area, use the -voltage_area_specific_lib_cell option?", "response": "- When the design is single-rail buffer only, use the -va_aware option fc_shell> add_group_repeaters -lib_cell <lib_cell> -va_aware - When the design is single-rail buffer based on voltage area, use the -voltage_area_specific_lib_cell option"}
