{"instruction": "How do I Defining Global Via Ladder Constraints", "input": "", "output": "The via ladder constraints define the pins on which to insert via ladders and the via ladder templates that can be used for those pins.\n You can define both global and instance- specific via ladder constraints; if the constraints conflict, the instance-specific constraints override the global constraints.\n  To define global via ladder constraints, which are referred to as via ladder rules, use the set_via_ladder_rules command, as described in Defining Global Via Ladder Constraints  To define instance-specific via ladder constraints, use the set_via_ladder_constraints command, as described in Defining Instance-Specific Via Ladder Constraints In addition, you can specify via ladder candidates for specific pins by using the set_via_ladder_candidate command.\n This command is used by preroute optimization to insert via ladders on timing-critical paths, as described in Specifying Automatic Via Ladder Insertion Settings for Preroute Optimization To report the via ladder constraints, use the report_via_ladder_constraints command.\n See Also  Inserting Via Ladders"}
{"instruction": "How do I Reading RTL Files in SystemVerilog, Verilog, or VHDL Format", "input": "", "output": "Before you read a design, you must create or open the design library associated with the design, as described in Setting Up Libraries The tool can read RTL designs in SystemVerilog, Verilog, or VHDL and gate-level netlists in Verilog format.\n Use the following methods to read designs:  Reading RTL Files in SystemVerilog, Verilog, or VHDL Format  Reading Designs Using the -autoread Option  Reading Designs Using the VCS Command-line Options  Reading Verilog Gate-Level Netlist Files  Reading Mixed Gate-Level Netlists and RTL Files       By default, when the tool reads the RTL or gate-level netlist files, it creates a block in the current design library and increments its open count.\n The tool determines the top-level module of the block by identifying the module that is not instantiated by any other modules in the specified files and uses the top-level module name as the block name.\n Use the following commands to work with blocks:  create_block : Creates a block  open_block : Opens an existing block  current_block : Sets or reports the current block  save_block : Saves a block  close_blocks : Closes a block See Also  Blocks  Working With Design Libraries"}
{"instruction": "How do I Interrupting or Terminating Command Processing", "input": "", "output": "You interact with the Fusion Compiler tool by using fc_shell commands, which are based on the Tool Command Language (Tcl) and include certain command extensions needed to implement specific Fusion Compiler functionality.\n The Fusion Compiler command language provides capabilities similar to Linux command shells, including variables, conditional execution of commands, and control flow commands.\n You can  Enter individual commands interactively at the fc_shell> prompt in fc_shell  Enter individual commands interactively on the console command line in the GUI  Run one or more Tcl scripts, which are text files that contain fc_shell commands (see Using Tcl Scripts When entering a command, an option, or a file name, you can minimize your typing by pressing the Tab key when you have typed enough characters to specify a unique name; the Fusion Compiler tool completes the remaining characters.\n If the characters you typed could be used for more than one name, the Fusion Compiler tool lists the qualifying names, from which you can select by using the arrow keys and the Enter key.\n If you need to reuse a command from the output for a command-line interface, you can copy and paste the portion by selecting it, moving the pointer to the fc_shell command line, and clicking with the middle mouse button.\n When you run a command, the Fusion Compiler tool echoes the command output (including processing messages and any warnings or error messages) in fc_shell and, if the GUI is open, in the console log view.\n By default, the tool does not use page mode, so the output might scroll.\n To enable page mode, set the sh_enable_page_mode variable to true"}
{"instruction": "How do I Reserving Space for Top-Level Routing", "input": "", "output": "A routing blockage defines a region where routing is not allowed on specific layers.\n Zroute considers routing blockages to be hard constraints.\n To define a routing blockage, use the create_routing_blockage command.\n At a minimum, you must define the boundary of the routing blockage and the affected layers.\n  To create a rectangular routing blockage, use the -boundary option to specify the lower-left and upper-right corners of the rectangle using the following syntax: { { llx lly } { urx ury } }  To create a rectilinear routing blockage, use the -boundary option to specify the coordinates of the polygon using the following syntax: { { x1 y1 } { x2 y2 }...\n } You can also create a rectilinear routing blockage by specifying the polygon as the combined area of a heterogeneous collection of objects with physical geometry, such as poly_rects, geo_masks, shapes, layers, and other physical objects.\n If you specify a layer, the resulting area includes the area of every shape on the layer.\n For all other objects, the resulting area includes the area of each object.\n You can create multiple routing blockages by specifying the boundary polygon as a geo_mask or a collection of physical objects.\n If the resulting area resolves to multiple, noncontiguous polygons, the command creates multiple routing blockages, one corresponding to each polygon.\n  To specify the affected layers, use the -layers option.\n Specify the routing layers by using the layer names from the technology file.\n The layers can be metal, via, or poly layers.\n By default, the tool creates a routing blockage named RB_ objId in the current block that prevents routing of all nets within the routing blockage boundary.\n The routing blockage is considered as real metal for RC extraction and DRC checking and the router must meet the minimum spacing requirements between the routing blockage boundary and the net shapes.\n Use the following options to change the default behavior:  -name blockage_name Specifies a name for the routing blockage.\n  -cell cell Creates the routing blockage in a different physical cell.\n When you use this option, the tool creates the routing blockage in the cell's reference block using the coordinate system of the cell's top-level block.\n  -zero_spacing       Disables the minimum spacing rule between the routing blockage boundary and the net shapes (zero minimum spacing).\n This option also prevents the routing blockage from being treated as real metal during extraction and DRC checking.\n When you use this option, the net shapes can touch, but not overlap, the routing blockage boundary.\n Note: When you create a routing blockage to prevent via insertion, you must use the -zero_spacing option; otherwise, frame view extraction does not use the route guide to trim the via region.\n  -net_types list_of_types Applies the routing blockage only to the specified net types.\n Specify one or more of the following net types: analog_ground analog_power analog_signal clock deep_nwell deep_pwell ground nwell power pwell reset scan signal tie_high, and tie_low.\n To remove the net type settings and prevent routing of all nets in the routing blockage, use the -net_types unset setting.\n To prevent signal routing on the M1 layer within the rectangle with its lower-left corner at (0, 0) and its upper-right corner at (100, 100), use the following command: fc_shell> create_routing_blockage \\ -boundary { {0.0 0.0} {100.0 100.0} } \\ -net_types signal -layers M1 To prevent vias on the V1 layer within the rectangle with its lower-left corner at (0, 0) and its upper-right corner at (100, 100), use the following command: fc_shell> create_routing_blockage \\ -boundary { {0.0 0.0} {100.0 100.0} } \\ -net_types signal -layers V1 -zero_spacing To prevent PG routing on the M2 layer within the rectangle with its lower-left corner at (0, 0) and its upper-right corner at (100, 100), use the following command: fc_shell> create_routing_blockage \\ -boundary { {0.0 0.0} {100.0 100.0} } \\ -net_types {power ground} -layers M2"}
{"instruction": "How do I Querying Metal Fill", "input": "", "output": "The signoff_create_metal_fill command stores the output files in the run directory specified by the signoff.create_metal_fill.run_dir application option (or the signoff_fill_run directory if you do not use this option).\n For each fill run, the command creates a subdirectory named icv_run_# that contains the files generated for that run.\n When you perform metal fill insertion, the tool writes the following files to the output directory:  block.LAYOUT_ERRORS, which contains details about the detected errors  block.RESULTS, which contains a summary of the run results  signoff_create_metal_fill.log, which contains a summary of the run environment  icv_config_out, which contains the paths to the top-level block and the cell libraries  layer.map, which contains the generated layer mapping file  metal_fill_params.rh, which contains the metal fill parameters and options that you specified  metal_fill_compress_params.rh, which contains the storage commands and indicates whether the compression mode was flat (NONE) or hierarchical (AUTO) ./run_details directory, which contains all the data generated by the IC Validator tool for the metal fill insertion run"}
{"instruction": "How do I Preserving Pin Names During Sizing", "input": "", "output": "You can disable design rule checking (DRC) on clock, constant, scan enable, and scan clock nets by using the set_auto_disable_drc_nets command.\n To control design rule checking on specific nets in the current design, use the options with the set_auto_disable_drc_nets command as shown in Table 14 Table 14 Options for the set_auto_disable_drc_nets Command Use this option To do this -none  -all   -constant true | false true false  -on_clock_network true | false true false  -scan true | false true false  For example, to enable design rule checking for all clock, constant, scan enable, and scan clock nets, specify the -none option: fc_shell> set_auto_disable_drc_nets -none To disable design rule checking on nets connecting to clocks and constants, set the -on_clock_network and -constant options to true, as shown in the following example and figure: fc_shell> set_auto_disable_drc_nets \\ -on_clock_network true -constant true        DRC Disabled Clock and Constant Nets Highlighted clkA clkB clkC clksel clkD in logic 1 const_sel clkE DRC disabled clock net DRC disabled constant net"}
{"instruction": "How do I Ungrouping or Preserving Hierarchies During Optimization", "input": "", "output": "The following topics describe how you can customize and control the mapping and optimization that is performed during the initial_map logic_opto, and initial_drc stages of the compile_fusion command:  Ungrouping or Preserving Hierarchies During Optimization  Controlling Boundary Optimization  Controlling Datapath Optimization  Controlling MUX Optimization  Controlling Sequential Mapping  Controlling Register Replication  Controlling Register Merging        Selectively Removing or Preserving Constant and Unloaded Registers  Reporting Cross-Probing Information for Optimized Registers  Controlling High-Fanout-Net Synthesis"}
{"instruction": "How do I Controlling Clock-Gate Latencies", "input": "", "output": "If you want to replace all the technology-independent clock gates in your RTL with integrated clock-gating cells from the libraries, use the replace_clock_gates command before executing the compile_fusion command.\n The replace_clock_gates command identifies combinational elements in the clock network acting as clock gates and replaces them.\n When you use the replace_clock_gates command, the tool honors the constraints specified by the set_clock_gate_style set_lib_cell_purpose, and set_target_ligrary_subset commands when it selects a library cell.\n When the tool identifies combinational elements, these elements must have only two input pins (the clock pin and the enable signal pin) and only one output pin.\n If the cell does not meet these criteria, the cell is not a candidate for replacement.\n The following table lists the WVGTECH instances and their replacement cells.\n Combinational Cell in RTL Integrated Clock Gate Replacement           The new clock-gating cell is inferred with the following name: rep_clock_gate_+<instance_name_of_combinational_cell> The tool reports replaced clock gates as tool-inserted clock gates."}
{"instruction": "How do I Standard Filler Cell Insertion", "input": "", "output": "To ensure that all power nets are connected, you can fill empty space in the standard-cell rows with filler cells.\n The Fusion Compiler tool supports filler cells with and without metal       and supports both single-height and multiheight filler cells.\n Filler cell insertion is often used to add decoupling capacitors to improve the stability of the power supply.\n Note: Before inserting filler cells, ensure that the block is legalized by using the check_legality command.\n The Fusion Compiler tool can select the filler cells to use based on  An ordered list of filler cell references In the standard filler cell insertion flow, you specify an ordered list of filler library cells and the tool inserts the first cell that fits in each gap.\n If your technology requires specific filler cells abutting the standard cells, you can insert these required filler cells first, and then use standard filler cell insertion to fill the remaining space.\n For details about this method, see Standard Filler Cell Insertion  Threshold-voltage rules In the threshold-voltage-based flow, the tool selects the filler cells by using user-defined insertion rules, which are based the threshold-voltage types of the cells that border the gap.\n This flow is typically used only for established foundry nodes.\n For details about this method, see Threshold-Voltage-Based Filler Cell Insertion"}
{"instruction": "How do I Defining Vias", "input": "", "output": "Before you can run Zroute, you must ensure that the block and physical library meet the following requirements:  Library requirements Zroute gets all of the design rule information from the technology file; therefore, you must ensure that all design rules are defined in the technology file before you start routing.\n For more information about the technology file and defining routing design rules, see the Synopsys Technology File and Routing Rules Reference Manual  Block requirements Before you perform routing, your block must meet the following conditions:  Power and ground nets have been routed after design planning and before placement.\n For more information, see the Fusion Compiler Design Planning User Guide  Clock tree synthesis and optimization have been performed.\n For more information, see Clock Tree Synthesis  Estimated congestion is acceptable.\n  Estimated timing is acceptable (about 0 ns of slack).\n  Estimated maximum capacitance and transition have no violations.\n To verify that your block meets the last three prerequisites, you can check the routability of its placement as explained in Checking Routability"}
{"instruction": "How do I Defining the Bus Routing Style", "input": "", "output": "The Fusion Compiler tool provides a set of application options to control custom routing results.\n The application options are applied globally.\n See Using Custom Routing Application Options You can also define net-specific routing constraints that override the routing application options.\n Routing constraints provide guidance during routing.\n In the Fusion Compiler environment, Custom Router honors both the Custom Router constraints and the Fusion Compiler nondefault routing rules.\n Note: You must apply net-specific routing constraints to physical nets.\n When specifying nets that will be constrained, use the get_nets -physical_context command to ensure the physical net is returned.\n For example: fc_shell> create_net_shielding \\ -for [get_nets -physical_context pr*]\\ -disabled_layers {M5 M6} -sharing true \\ -layer_gaps {M2 3 M3 5} {shielding_4} Table 52 lists the commands to define the Custom Router constraints by creating constraint groups.\n For information about the supported Fusion Compiler nondefault routing rules, see Using Nondefault Routing Rules To check or remove the created constraint groups, use the report_constraint_groups or remove_constraint_groups command.\n For more information about checking or removing constraint groups, see Managing Constraint Groups Table 52 Commands to Define Custom Router Routing Constraints Command Description create_bus_routing_style    create_differential_group         Table 52 Commands to Define Custom Router Routing Constraints (Continued) Command Description create_wire_matching    create_length_limit    create_net_shielding   create_net_priority"}
{"instruction": "How do I Specifying Optimization Targets at the Preroute Stage", "input": "", "output": "When running the compile_fusion command, you can reduce dynamic voltage drop across a block by enabling dynamic power shaping optimization by setting the compile.flow.enable_dps application option to true.\n The tool performs dynamic power shaping optimization during the final_opto stage of the compile_fusion command.\n When you enable this feature, the tool performs power analysis and uses useful skew techniques to reduce dynamic voltage drop.\n Optionally, you can customize the power analysis performed during dynamic power shaping optimization by using the ccd.dps.use_case application option."}
{"instruction": "How do I Track Adherence Options", "input": "", "output": "Two Custom Router application options allow you to control tapoff operations:  custom.route.bus_tap_off_enable  custom.route.bus_tap_off_shielding The custom.route.bus_tap_off_enable application option connects pins to the bus trunk.\n If the custom.route.bus_tap_off_enable application option is turned on, you can use the custom.route.bus_tap_off_shielding application option to add shielding to the bus tapoffs."}
{"instruction": "How do I Discouraging Off-Grid Routing for Vias", "input": "", "output": "By default, wires and vias need to be aligned to the wire track grid for a metal layer only if the onWireTrack or onGrid attribute is set to 1 in its Layer section in the technology file.\n To override the technology file settings, set the following application options:  route.common.wire_on_grid_by_layer_name This option controls off-grid routing for metal layers.\n  route.common.via_on_grid_by_layer_name This option controls off-grid routing for via layers.\n Use the following syntax to set these options: { { layer true|false}...\n } Specify the layers by using the layer names from the technology file.\n Specify true to forbid off-grid routing and false to allow off-grid routing.\n If you use either of these options, the tool ignores all settings for the onWireTrack and onGrid attributes in the technology file and uses only the settings specified by these options.\n If you do not specify a layer in these options, off-grid routing is allowed on that layer, regardless of the setting in the technology file.\n For example, to prevent off-grid routing for wires on the M2 and M3 metal layers and for vias on the V2 via layer, regardless of the settings in the technology file, use the following commands: fc_shell> set_app_options \\ -name route.common.wire_on_grid_by_layer_name \\ -value {{M2 true} {M3 true}} fc_shell> set_app_options \\ -name route.common.via_on_grid_by_layer_name \\ -value {{V2 true}}"}
{"instruction": "How do I Adding Buffers at Specified Locations", "input": "", "output": "To add buffers in an exact configuration, specify the configuration of cells and their locations by using the -user_specified_buffers option.\n With the -user_specified_buffers option, you can add cells only on one net at a time.\n You can insert different types of buffers or inverter pairs by using this option, but you cannot combine both buffers and inverter pairs.\n For each cell you add, use the {instance_name library_cell_name x y layer_name...\n } format with the -user_specified_buffers option to specify the  Instance name  Library cell to use  Coordinates of the exact location  Layer at that location with an existing routing shape to connect to Instead of specifying the routing layer, you can have the tool automatically detect the closest routing shape to that location by using the -detect_layer option.\n The following example adds two cells named ECO1 and ECO2 on net n22.\n The ECO1 cell is of type BUF2, at location (100, 70), connecting to a routing shape on the M3 layer.\n The ECO2 cell is of type BUF4, at location (150, 70), also connecting to a routing shape on the M3 layer.\n fc_shell> add_buffer_on_route net22 \\ -user_specified_buffers {ECO1 BUF2 100 70 M3 ECO2 BUF4 150 70 M3}"}
{"instruction": "How do I Preparing to Run IC Validator In-Design Commands", "input": "", "output": "The IC Validator In-Design feature provides the ability to use the IC Validator tool to perform physical implementation and verification tasks within the Fusion Compiler tool.\n You can use IC Validator In-Design to perform the following tasks:  Signoff design rule checking (the signoff_check_drc command)  Fixing the violations detected during signoff design rule checking (the signoff_fix_drc command)  Augmenting the power grid based on voltage drop information (the signoff_create_pg_augmentation command)  Inserting metal fill (the signoff_create_metal_fill command)  Fixing isolated vias (the signoff_fix_isolated_via command) Note: An IC Validator license is required to run the IC Validator In-Design functions.\n To learn about using these IC Validator In-Design functions, see the following topics:  Preparing to Run IC Validator In-Design Commands  Performing Signoff Design Rule Checking  Automatically Fixing Signoff DRC Violations  Improving Instance Voltage Drop by Augmenting the Power Grid  Inserting Metal Fill With IC Validator In-Design  Automatically Fixing Isolated Vias"}
{"instruction": "How do I Introduction to Physical Datapath With Relative Placement", "input": "", "output": "The physical datapath with relative placement capability provides a way for you to create structures in which you specify the relative column and row positions of instances.\n During placement and legalization, these structures, which are placement constraints called relative placement structures, are preserved and the cells in each structure are placed as a single entity.\n Relative placement is also called physical datapath and structured placement.\n The concepts and tasks necessary for doing relative placement are described in these sections:  Introduction to Physical Datapath With Relative Placement  Relative Placement Flow  Creating Relative Placement Groups  Adding Objects to a Group  Specifying Options for Relative Placement Groups  Changing the Structures of Relative Placement Groups  Generating Relative Placement Groups for Clock Sinks  Performing Placement and Legalization of Relative Placement Groups  Analyzing Relative Placement Groups  Saving Relative Placement Information  Summary of Relative Placement Commands"}
{"instruction": "How do I Synthesizing a Subblock", "input": "", "output": "To partition and plan the top-level design and create the lower-level blocks for subsequent bottom-up synthesis, when the floorplans of subblock and top-level design are available, perform the following steps:  Hierarchical Synthesis Flow When Floorplans are Available 1.\n Read in the full chip design and apply constraints as shown in the following example: fc_shell> set REF_LIBS \"stdcell.ndm macro.ndm\" fc_shell> create_lib TOP -technology techfile \\ -ref_libs $REF_LIBS fc_shell> analyze -format verilog $rtl_files fc_shell> elaborate TOP fc_shell> set_top_module TOP  fc_shell> load_upf fullchip.upf fc_shell> read_sdc fullchip.sdc 2.\n For the design partitions, split the constraints as shown in the following example: fc_shell> set_budget_options -add_blocks {BLOCK1 BLOCK2} fc_shell> split_constraints 3.\n Create the subblock design libraries with design information and enable block-specific reference library setup as shown in the following example: fc_shell> copy_lib -to_lib BLOCK1.nlib -no_design fc_shell> copy_lib -to_lib BLOCK2.nlib -no_design fc_shell> set_attribute -objects BLOCK1.nlib \\       -name use_hier_ref_libs -value true fc_shell> set_attribute -objects BLOCK2.nlib \\ -name use_hier_ref_libs -value true fc_shell> save_lib -all 4.\n Create the subblock design partitions as shown in the following example: fc_shell> commit_block -library BLOCK1.nlib BLOCK1 fc_shell> commit_block -library BLOCK2.nlib BLOCK2 fc_shell> save_lib -all 5.\n Load the UPF and SDC constraints for the unmapped subblocks as well as the top- level, which are generated by the split_constraints command earlier as shown in the following example: fc_shell> set_constraint_mapping_file./split/mapfile fc_shell> load_block_constraints -all_blocks -type SDC \\ -type UPF -type CLKNET fc_shell> save_lib -all 6.\n Load the floorplan for the subblocks and top-level design.\n 7.\n Run the compile_fusion command until logic optimization for the subblocks and top- level design is complete as shown in the following example: fc_shell> compile_fusion -to logic_opto 8.\n Perform standard cell placement followed by the pin assignment, timing estimation and budgeting.\n 9.\n After completing the design planning operations, rebuild the subblock and top-level design by opening the elaborated RTL NDM, loading floorplan, feedthroughs and budgets.\n Note: For details about each of the design planning operation, see the Design Planning User Guide"}
{"instruction": "How do I Analyzing Multivoltage Information", "input": "", "output": "Multivoltage nets are nets that logically or physically cross over more than one voltage area.\n You can enable the use of improved buffering techniques for fixing logical DRC violations on such nets during preroute optimization by setting the opt.buffering.enable_hierarchy_mapping application option to true This application option affects the compile_fusion and clock_opt commands.\n Enabling this feature reduces the number of buffers used to fix logical DRC violations.\n However, it can slightly increase the total negative slack or number of hold violations."}
{"instruction": "How do I Controlling Boundary Optimization", "input": "", "output": "Ungrouping merges subdesigns of a given level of the hierarchy into the parent cell or design.\n It removes hierarchical boundaries and allows Fusion Compiler to improve timing by reducing the levels of logic and to improve area by sharing logic.\n By default, the compile_fusion command automatically ungroups logical hierarchies that do not have a dont_touch attribute setting.\n During optimization, the compile_fusion command performs the following types of automatic grouping:  Area-based automatic ungrouping Before initial mapping, the command estimates the area for unmapped hierarchies and removes small subdesigns.\n Because the command performs automatic ungrouping at an early stage, it has a better optimization context.\n Additionally, datapath extraction is enabled across ungrouped hierarchies.\n These factors improve the area and timing quality of results.\n  Delay-based automatic ungrouping The command ungroups hierarchies along the critical path and is used essentially for timing optimization.\n For ungrouping of DesignWare and datapath cells, see Datapath Implementation You can disable automatic ungrouping by setting the compile.flow.autoungroup application option to false.\n The default is true.\n However, doing do can affect the QoR.\n Therefore, it is better to limit ungrouping, rather than disabling it, using the following methods: Controlling the Automatic Ungrouping of Specific Types of Hierarchies To control the automatic ungrouping of specific types of hierarchies, use the set_autoungroup_options command.\n The following example specifies that parent hierarchies of blocks with UPF and SDC constraints should preserved and that automatic ungrouping of other hierarchies should begin from the third level: fc_shell> set_autoungroup_options -keep_parent_hierarchies UPF fc_shell> set_autoungroup_options -keep_parent_hierarchies SDC fc_shell> set_autoungroup_options -start_level 3       The set_autoungroup_options command is cumulative and the settings are saved in the design library.\n To report information about the hierarchies that are automatically ungrouped or preserved, use the report_ungroup command.\n Controlling the Automatic Ungrouping of Specific Objects To force or prevent specific cell instances or modules being ungrouped during optimization, use the set_ungroup command with the true or false setting.\n The set_ungroup command sets the ungroup attribute to the appropriate value for the specified objects.\n If you set the attribute on a module, all cells that reference the module are affected.\n Note: The set_ungroup command has a higher priority than the set_autoungroup_options command.\n The following example script sets specific modules to be ungrouped and specific cell instances to be preserved during optimization: # Set modules A and B to be ungrouped set_ungroup [get_modules {A B}] true # Set cells u_C and u_D to be preserved set_ungroup [get_cells {u_C u_D}] false # Query the ungroup attribute get_attribute -name ungroup -objects [get_modules {A B}] get_attribute -name ungroup -objects [get_cells {u_C u_D}] compile_fusion"}
{"instruction": "How do I Handling Multiple Levels of Physical Hierarchy", "input": "", "output": "To create an abstract that can be used for signal electromigration analysis at the top-level, set the abstract.enable_signal_em_analysis application option to true before you run the create_abstract command."}
{"instruction": "How do I Introduction to Clock Gating", "input": "", "output": "RTL clock gating is a power optimization feature provided by the Fusion Compiler tool.\n This is a high-level optimization technique that can save a significant amount of power by adding clock gates to registers that are not always enabled and have synchronous load- enable or explicit feedback loops.\n The tool gates flip-flops by extracting common enable signals shared by the flip-flops either in the same hierarchy or across hierarchies.\n This technique greatly reduces dynamic power consumption by reducing the switching activity on the clock inputs to registers and eliminating the multiplexers.\n It can also result in a smaller chip area.\n Clock gating occurs by default during the compile_fusion command using specified clock-gating settings.\n If you do not specify any clock-gating constraints, the default constraints are used.\n While you cannot disable clock gating, you can use the clock-gating commands in this section to prevent insertion of clock gates in specific blocks or the whole design.\n The tool identifies preexisting clock-gating cells during the analyze and elaborate commands.\n The tool inserts one more level of clock-gating cells at the leaf level.\n For more information, see the following topics:  Introduction to Clock Gating  Clock-Gating Prerequisite Conditions  Setting Up Clock Gating  Clock Gating Flows  Replacing Clock Gates  Controlling Clock-Gate Latencies  Controlling the Number of Clock-Gate Levels  Merging Clock Gates  Setting Clock Gating Transformations  Setting Routing Rules for Clock Gates  Clock Gating and Multibit Registers        Placement-Aware Clock Gating  Fanin-Based Sequential Clock Gating  Reporting Clock-Gating Results  Reporting Clock-Gate Enable Signals  Clock Gate Efficiency Reporting  Self-Gating Optimization  Special Naming and Querying for DFT Wrapper Clock Gates"}
{"instruction": "How do I Library Requirements for Self-Gating", "input": "", "output": "illustrates the Fusion Compiler flow for inserting self-gates by using the compile_fusion command.\n        General Self-Gating Flow Fusion Compiler read_saif in.saif or set_switching_activity compile_fusion SAIF RTL/netlist Libraries Reports set_app_options -name compile.clockgate.self_gating -value true"}
{"instruction": "How do I ECO Fusion Flow", "input": "", "output": "When you make ECO changes in the PrimeTime tool and incorporate these changes by performing the Fusion Compiler unconstrained ECO flow, it might be necessary to iterate multiple times between the tools to meet the required QoR goals.\n To reduce the overall turnaround time for the flow, you can reduce the runtime of each iteration by using the incremental signoff ECO flow.\n This flow generates incremental design data from the Fusion Compiler tool, which enables you to perform incremental extraction in the StarRC tool and incremental timing analysis and ECO in the PrimeTime tool, as shown in the following figure.\n        Incremental Signoff ECO Flow StarRC incremental parasitic extraction ECO change list file Fusion Compiler incremental signoff ECO flow PrimeTime timing ECO ECO change list file Incremental parasitics Incremental design data PrimeTime incremental timing analysis and ECO To perform the incremental signoff ECO flow within the Fusion Compiler tool, use the record_signoff_eco_changes command.\n This command incorporates the PrimeTime ECO into the design library, tracks all the changes made to the design, and generates the incremental files that are required to run StarRC incremental extraction and PrimeTime incremental timing analysis and ECO.\n The Fusion Compiler incremental signoff ECO flow consists of the following steps: 1.\n Open the design library by using the open_block command.\n 2.\n Incorporate the PrimeTime ECO changes and begin tracking the ECO changes to the design by using the record_signoff_eco_changes -start -input command as shown in the following example: fc_shell> record_signoff_eco_changes -start -input pt_eco.tcl 3.\n (Optional) Perform additional timing ECO changes to the design by using netlist editing commands.\n Do not perform functional ECO changes to the design.\n If you do so, the tool stops tracking the ECO changes being performed on the design.\n       4.\n Place the ECO cells by using the place_eco_cells command.\n 5.\n Update the routing by using the route_eco command, as described in Performing ECO Routing, or by manually rerouting the affected nets.\n 6.\n Stop tracking the ECO changes and complete the incremental signoff ECO flow by using the record_signoff_eco_changes -stop command as shown in the following example: fc_shell> record_signoff_eco_changes -stop"}
{"instruction": "How do I Viewing the Default Via Mapping Table", "input": "", "output": "You can perform redundant via insertion in the following ways:  Postroute redundant via insertion  Concurrent soft-rule-based redundant via insertion  Near 100 percent redundant via insertion In general, you should start with postroute redundant via insertion.\n If postroute redundant via insertion results in a redundant via rate of at least 80 percent, you can try to improve       the redundant via rate by using concurrent soft-rule-based redundant via insertion.\n If postroute redundant via insertion results in a redundant via rate of at least 90 percent, you can try to improve the redundant via rate by using near 100 percent redundant via insertion.\n Note: As the redundant via rate increases, it becomes more difficult to converge on the routing design rules and you might see a reduction in signal integrity; therefore, you should use near 100 percent redundant via insertion only for those blocks that truly require such a high redundant via rate.\n In addition, achieving very high redundant via rates might require you to modify the floorplan utilization to allow enough space for the redundant vias.\n The following topics describe the default via mapping table, how to define a customized via mapping table, how to insert redundant vias by using various methods, and how to report the redundant via rate.\n  Viewing the Default Via Mapping Table  Defining a Customized Via Mapping Table  Postroute Redundant Via Insertion  Concurrent Soft-Rule-Based Redundant Via Insertion  Near 100 Percent Redundant Via Insertion  Preserving Timing During Redundant Via Insertion  Reporting Redundant Via Rates"}
{"instruction": "How do I Summary Report for Automatic Design Rule Fixing", "input": "", "output": "If your block uses double-patterning technology, you must perform separate signoff DRC fixing runs for the non-double-patterning routing rules and the double-patterning routing rules.\n To perform automatic fixing of the signoff DRC violations for a block that uses double- patterning technology, 1.\n Perform automatic fixing for the non-double-patterning signoff DRC violations by using the process described in Automatically Fixing Signoff DRC Violations When you run the signoff_check_drc and signoff_fix_drc commands, use the -unselect_rules option to ignore the double-patterning rules during signoff design rule checking.\n fc_shell> signoff_check_drc -unselect_rules { list_of_dpt_rules Note: To determine the double-patterning rules for your technology, see the design rule manual (DRM) provided by your vendor.\n 2.\n Perform automatic fixing for the double-patterning odd-cycle violations by using the process described in Automatically Fixing Signoff DRC Violations Before you run the signoff_fix_drc command, set the signoff.fix_drc.custom_guidance application option to dpt fc_shell> set_app_options \\ -name signoff.fix_drc.custom_guidance -value dpt When you run the signoff_check_drc and signoff_fix_drc commands, use the -select_rules option to consider only the double-patterning rules during signoff design rule checking.\n fc_shell> signoff_check_drc -select_rules { list_of_dpt_rules 3.\n Perform signoff design rule checking to verify the results by using the process described in Performing Signoff Design Rule Checking"}
{"instruction": "How do I Reporting Cross-Probing Information for Optimized Registers", "input": "", "output": "To optimize the design by reducing area, the tool can automatically detect and remove constant registers and unloaded registers.\n You can control this behavior as described in the following topics: Removing Constant Registers Certain registers in a design might never change state because they have constant values on one or more input pins.\n These constant values can either be directly at the input or result from the optimization of fanin logic that eventually leads to a constant input of the register.\n Eliminating such registers can improve area significantly.\n By default, the compile_fusion command removes constant registers and prints an information message in the compile log file.\n This table lists cases in which a sequential element can be eliminated.\n Type of register Data Preset Reset                          Type of register Data Preset Reset           To disable constant register removal for the entire block you are synthesizing, set the compile.seqmap.remove_constant_registers application option to false, changing it from its default of true.\n To control constant register removal for a specific cell or module, use the set_constant_register_removal command.\n For example, the following settings prevent the tool from removing the constant register named reg11: fc_shell> set_constant_register_removal reg11 false The following settings globally disables constant register removal for the entire block except for the module named sub5 : fc_shell> set_app_option -name compile.seqmap.remove_constant_registers -value false  fc_shell> set_constant_register_removal [get_module sub5] true Information about removed constant registers is written to the verification guidance.svf file and the compile log file.\n To report the constant registers removed during compile, use the report_constant_registers command.\n Removing Unloaded Registers During optimization, the tool deletes registers with outputs that do not drive any load.\n The combinational logic cone associated with the input of the register can also be deleted if the cell is not used in the design.\n Register outputs can become unloaded because of redundancy in the circuit or as a result of constant propagation.\n In some designs where the registers have been instantiated, the outputs might already be unloaded.\n By default, the compile_fusion command removes unloaded registers in the design and prints an information message in the compile log file.\n To disable this capability for the block you are synthesizing, set the compile.seqmap.remove_unloaded_registers application option to false, changing it from its default of true.\n To control unloaded register removal for a specific cell or module, use the set_unloaded_register_removal command.\n       For example, the following settings prevent the tool from removing the unloaded register named reg22: fc_shell> set_unloaded_register_removal reg22 false The following settings globally disables unloaded register removal for the entire block except for the module named sub5 : fc_shell> set_app_option -name compile.seqmap.remove_unloaded_registers -value false  fc_shell> set_unloaded_register_removal [get_module sub5] true To report the unloaded registers that were removed during compile, use the report_unloaded_registers command."}
{"instruction": "How do I Reverting Changes Made During Resizing", "input": "", "output": "You can resize a cell by using the size_cell command, as shown in the following example: fc_shell> size_cell U21 -lib_cell AND2X4 If you enable the freeze silicon mode by setting the design.eco_freeze_silicon_mode application option to true, by default, the size_cell command checks if a compatible       spare cell is available within a distance of five times the unit site height before sizing the cell.\n If a compatible spare cell is not available, the tool does not size the cell.\n  To control this distance, use the -max_distance_to_spare_cell option.\n  To disable this feature, use the -not_spare_cell_aware option."}
{"instruction": "How do I Reporting the Clock Tree Settings", "input": "", "output": "The tool supports the setup and hold timing on critical endpoints by achieving a sharper transition on the critical launch and capture clock pins.\n The tool populates the ideal timing information from the and uses it to identify the critical endpoints.\n A transition target tighter than the user specified max_transition is applied for the clock pins of timing critical endpoints and the clock tree is built considering the tightened transition which helps to improve the timing.\n To enable timing driven sink transition target, set the cts.compile.timing_driven_sink_transition application option to true before the compile_fusion stage."}
{"instruction": "How do I Running IC Validator on Specific Hosts", "input": "", "output": "By default, the IC Validator In-Design commands use a single process to perform design rule checking.\n To reduce the turnaround time for design rule checking, use multicore processing, which includes multithreading, distributing processing, and parallel command execution.\n After you specify how many cores to use on different hosts, the IC Validator tool determines exactly how to take advantage of multithreading and parallel command execution to achieve the fastest runtimes.\n To enable multicore processing, you must define the configuration by using the set_host_options command.\n By default, the configuration applies to all Fusion Compiler commands that support multicore processing.\n To define a configuration that applies only to the IC Validator In-Design commands, use the -target ICV option.\n       You can enable multicore processing on specific hosts, by using a job scheduler such as the Load Sharing Facility (LSF), or by using a combination of these methods.\n The following topics describe these methods:  Running IC Validator on Specific Hosts  Running IC Validator Using a Job Scheduler  Running IC Validator Using Hybrid Multicore Processing For general information about the set_host_options command, see Enabling Multicore Processing"}
{"instruction": "How do I Controlling Register Merging", "input": "", "output": "You can perform logic optimization by running the compile_fusion -to logic_opto command.\n To identify registers that you want the tool to replicate during logic optimization, use the set_register_replication command.\n This command sets the register_replication attribute on the specified registers and improves the timing by balancing fanout.\n During logic optimization, the specified registers are replicated and the loads of the original registers are evenly distributed among the new replicated registers when the -num_copies option is used.\n When you use the -max_fanout option, the tool computes the number of copies and attempts to distribute loads evenly without violating the maximum fanout constraints.\n You can control the number of copies that are created by using one of the following methods:  Specify the total number of registers after replication, including the original, by using the -num_copies option.\n  Specify the maximum fanout of the replicated registers by using the -max_fanout option.\n If you specify both options, the -max_fanout option is ignored.\n The following example specifies that the tool should replicate the regA and regB registers by creating one additional copy of each register: fc_shell> set_register_replication -num_copies 2 regA fc_shell> set_register_replication -num_copies 2 regB  shows the regA and regB registers before and after optimization.\n  Before and After Register Replication r1 r2 r3 Out1  Out2 r1 r2 r3 RegB regA RegA regB regA regB Out2 Out1       You can include logic in the fanin or fanout of the specified register by using the -include_fanin_logic or -include_fanout_logic option.\n You can specify any startpoint in the fanin or endpoint in the fanout of the register.\n The following example specifies that the tool should replicate the A_reg register and the logic in the path from this register to the U1 cell: fc_shell> set_register_replication -num_copies 2 A_reg \\ -include_fanout_logic U1 To specify endpoint registers or primary output ports that must be driven by the original register after replication, as shown in, use the -driven_by_original_register option.\n The following example specifies that after replication, the r0 register must drive the r1, r2, and r3 registers: fc_shell> set_register_replication -num_copies 3 r0 \\ -driven_by_original_register {r1 r2 r3}  Using the -driven_by_original_register Option r0 r1 r2 r3 Out1  Out2  Out3 r0 r1 r2 r3 Out1  Out2  Out2 r0_rep1 r0_rep2"}
{"instruction": "How do I Preparing the Power Network", "input": "", "output": "During physical implementation, the tool updates the power intent of the design.\n To generate an updated UPF file, use the save_upf command.\n You can use this UPF file in the subsequent steps of the design flow.\n The save_upf command separates the UPF commands based on the input UPF file, and adds a comment indicating the input UPF file and the time it was loaded.\n Assume you load, commit, and save UPF information as shown in the following example script: load_upf top_1.upf load_upf top_2.upf load_upf top_3.upf commit_upf save_upf top.upf       The resulting top.upf file contains the following information: ## Start - load_upf top_1.upf on Wed Oct 26 17:03:18 2016   ## End - load_upf ## Start - load_upf top_2.upf on Wed Oct 26 17:03:49 2016   ## End - load_upf ## Start - load_upf top_3.upf on Wed Oct 26 17:04:22 2016   ## End - load_upf"}
{"instruction": "How do I Working With Macro Models", "input": "", "output": "The Fusion Compiler tool can resize power switch cells to reduce total power and to mitigate the voltage drop in the design.\n Power switch cells have different voltage thresholds (V ) and are classified as high, standard, low, and ultra-low.\n For a power cell with high threshold voltage, the power consumption and leakage current is low, but the timing is also slow with a smaller drive strength.\n For a power cell with a low threshold voltage, the timing is faster but the power consumption and leakage current is high.\n With power switch cell sizing, the tool supports different drive strength and voltage threshold to reduce IR (voltage) drop in the design.\n Note: When the tool switches power cells (swapping), it can only swap cells with the same footprint but with different threshold voltages.\n This means, the cell cannot be swapped if the cell size and pin shapes are different.\n To perform power switch cell swapping, you can use the size_power_switches command to set the maximum IR drop target.\n To meet this target when swapping power cells, the tool tries to find another cell with higher V, but lower R ON value, so that it can reduce the IR drop.\n You can set the ON resistance (R ON ) value to be considered by the tool for swapping using the set_power_switch_resistance command.\n Based on the IR drop target and the set R ON value of the power cell, the tool tries to replace power cells to meet the targeted voltage drop.\n       Note: If the tool does not find any suitable power cell that matches the targeted voltage drop and R ON value, it skips cell resizing.\n To perform voltage driven cell sizing, 1.\n Perform a voltage drop analysis using RH Fusion to get the maximum voltage drop of the cells driven by the power switch in question.\n Refer Performing Voltage Drop Analysis for more information.\n 2.\n Set the RON value limit to be considered for cell switching using the set_power_switch_resistance command.\n Provide the following values as input,  Lib cell name - specifies the name of the library cell.\n  On resistance value - specifies the RON value limit to be considered for cell sizing.\n You can get this information from the switch model files.\n For example, fc_shell> set_power_switch_resistance saed32hvt_pg_ss0p_v125c/HEAD2X2_HVT 0.015 3.\n Run the size_power_switches command to perform cell size switching.\n Provide the value for -max_irdrop option which specifies the allowed maximum effective voltage drop on supply nets for standard cells driven by switches.\n This is a mandatory input.\n For example, fc_shell> size_power_switches -max_irdop 0.05 For the other non-mandatory inputs to further refine the command action, see the man page for the size_power_switches command.\n The tool also provides the following commands to further help with power switch resistance:  get_power_switch_resistance - This command fetches the R ON value of a power switch cell.\n  reset_power_switch_resistance - This command resets the R ON value specified for a power switch cell.\n  report_power_switch_resistance - This command reports all power switch library cells with a set R ON.\n Use the -verbose option with the command to display the group of cells with the same footprint, the R on value and the number of cells instantiated in the design.\n When using the -verbose option, you can also use -same_vt option to report power switch cells and their R ON value from current library with same VT but different driving strength cells."}
{"instruction": "How do I Performing Postroute Optimization", "input": "", "output": "During routing, Zroute can detect possible issues with shielding by checking for the following conditions:  Signal net shapes with a shape_use attribute of shield_route  PG net shapes, which might be a PG strap or rail, but have a shape_use attribute of detail_route  Signal, clock, or PG nets that have a shielding nondefault rule but no associated shield shapes, which might be caused by inappropriate shape_use attributes.\n To enable these checks, set the route.common.check_shield application option to true"}
{"instruction": "How do I Automatically Fixing Signoff DRC Violations", "input": "", "output": "To highlight violations from the Error Browser: 1.\n In the GUI, choose View > Error Browser.\n 2.\n Select the error data generated from the DRC run.\n 3.\n Select violations from the Error Browser to automatically highlight them on the heat map."}
{"instruction": "How do I Setting Clock Gating Transformations", "input": "", "output": "By merging clock gates, the tool attempts to minimize the number of clock gates while maximizing clock-gating coverage.\n The tool looks for clock-gating cells that share a common clock and enable signal.\n This feature is on by default.\n The fanout of clock-gating cells in different hierarchies can be moved to a clock-gating cell in a common ancestor if there is no restriction preventing port punching.\n Note that the the tool also checks clock-gating style and multivoltage constraints.\n Clock-gating cells with the dont_touch or size_only constraints are not optimized.\n To disable this feature, use the set_clock_gate_transformations command."}
{"instruction": "How do I Inserting Clock Drivers", "input": "", "output": "To improve the correlation between the structural multisource clock tree synthesis (SMSCTS) and concurrent clock and data (CCD), the Fusion Compiler tool supports the enhanced CCD engine with more SMSCTS specific offsets.\n This improves the CCD balance point implementation and timing after the SMSCTS step in the clock_opt command.\n To enable the improved correlation between the SMSCTS and CCD in the compile_fusion command and build_clock stage of the clock_opt command, set the cts.multisource.enable_subtree_synthesis_aware_ccd application option to true The improved correlation between the SMSCTS and CCD:  Improves the overall latency implementation after SMSCTS in the clock_opt command.\n  Reduces long and short path violations.\n  Improves timing after the clock_opt command with CCD offsets."}
{"instruction": "How do I Enabling or Disabling Clock Gating on Design Objects", "input": "", "output": "Use the set_clock_gating_options command to define the netlist structure of the inserted clock-gating cells.\n The following options are available: Option Description -minimum_bitwidth   -max_fanout   -objects"}
{"instruction": "How do I Making Changes to a Block After Creating an Abstract", "input": "", "output": "In the Fusion Compiler tool, the create_abstract command includes netlist objects such as the cells, nets, and pins in the abstract view at both the top-level and block-level.\n This command reports the reasons for the inclusion of netlist objects.\n       To report the reasons for including specific netlist objects in the abstract view, you can use the report_abstract_inclusion_reason command for objects across all hierarchies in the abstract.\n For each reason reported, a reason code is displayed.\n These reason codes correspond to the valid reasons for inclusion in the abstract.\n There can be multiple reasons for the same netlist object.\n Note: The report_abstract_inclusion_reason command does not report reasons for the hierarchical cells or pins on hierarchical cells.\n You can perform reason reporting from the top-level design, where the abstract is instantiated, without having the abstract as the current block.\n The following example reports the reasons for abstract inclusion for the pins in abstract view: fc_shell> report_abstract_inclusion_reason [get_pins -of_object [get_cells -filter \"is_hierarchical==false\" -hierarchical *]]  Legend mv - MV Logic comp - Compact interface logic cell - Cell inclusion  Pin name Reason(s) for inclusion  AINV_P_260/I comp, cell AINV_P_260/VDD cell AINV_P_260/VSS cell AINV_P_260/ZN comp, cell AINV_P_262/I comp, cell AINV_P_262/VDD cell AINV_P_262/VSS cell AINV_P_262/ZN comp, cell AINV_P_263/I mv, comp, cell A legend is displayed at the beginning of the report for the queried objects, specifying the detailed reason for each reason code."}
{"instruction": "How do I Generating Relative Placement Groups for Clock Sinks", "input": "", "output": "To modify the structures of existing relative placement groups, use the modify_rp_groups command as follows:  To add a row or column to a relative placement group, use the -add_rows or -add_columns option respectively.\n  To remove a row or column, use the -remove_rows or -remove_columns option respectively.\n  To flip a row or column, use the -flip_row or -flip_column option respectively.\n  To swap two rows or columns, use the -swap_rows or -swap_columns option respectively.\n For example, to swap the first and third columns of the my_rp_group relative placement group, as shown in, enter fc_shell> modify_rp_groups [get_rp_groups my_rp_group] \\ -swap_columns {0 2}  Swapping Columns of a Relative Placement Group Col 0 Col 1 Col 2 Col 0 Col 1 Col 2 Row 2 Row 2 Row 1 Row 1 Row 0 Row 0 C00 C01 C02 C21 C20 C22 C20 C21 C22 C01 C00 C02       To flip the second column of the my_rp_group relative placement group, as shown in, enter fc_shell> modify_rp_groups [get_rp_groups my_rp_group] \\ -flip_column 1  Flipping a Column of a Relative Placement Group Col 0 Col 1 Col 2 Col 0 Col 1 Col 2 Row 2 Row 2 Row 1 Row 1 Row 0 Row 0 C10 C11 C12 C12 C11 C10"}
{"instruction": "How do I Legalizing Relative Placement Groups in a Placed Design", "input": "", "output": "During placement, relative placement groups avoid placement blockages (obstructions) that are defined in the DEF file or created by the create_placement_blockage command.\n A relative placement group can be broken into pieces that straddle obstructions, yet maintain the relative placement structure.\n If the height of the obstruction is below a certain threshold, the relative placement cells are shifted vertically; otherwise, the relative placement column is shifted horizontally.\n  shows the placement of relative placement cells in a design containing obstructions that are either defined in the DEF file or created by create_placement_blockage.\n The obstruction in columns one and two is below the       threshold, so the tool shifts the cells vertically.\n The obstruction in column four is greater than the threshold, so the tool shifts all the cells of the column horizontally.\n  Relative Placement in a Design Containing Obstructions 0 4 0 0 4 4 4 3 4 1 4 2 0 1 4 0 col 0 row 0 row 2 row 4 row 3 row 1 col 1 col 2 col 4 col 3 0 3 0 2 1 4 1 3 1 0 1 1 1 2 2 4 2 3 2 0 2 2 2 1 3 4 3 3 3 1 3 2 3 0 Obstruction Obstruction"}
{"instruction": "How do I Handling Inaccurate Constraints During Clock Tree Synthesis", "input": "", "output": "Clock cells consume more power than cells that are not in the clock network.\n Clock cells that are clustered together in a small area increase the current densities for the power and ground rails, which increases the potential for electromigration problems.\n One way to avoid the problem is to set spacing requirements between clock cells to prevent local clumping of the clock cells along a standard cell power rail between the perpendicular straps.\n To use this method to reduce electromigration in the block, 1.\n Define clock cell spacing rules for the inverters, buffers, and integrated clock-gating cells in the clock network by using the set_clock_cell_spacing command, as described in Defining Clock Cell Spacing Rules 2.\n Perform clock tree synthesis by using the synthesize_clock_trees command, as described in Performing Standalone Clock Trees Synthesis, or the clock_opt command, as described in Synthesizing, Optimizing, and Routing Clock Trees With the clock_opt Command Note: The clock cell spacing rules are also honored by the balance_clock_groups command.\n For information about using this command, see Balancing Skew Between Different Clock Trees       3.\n Check clock cell spacing rule violations by using the check_legality -verbose command.\n You should not see any violations if you set the appropriate clock cell spacing constraints."}
{"instruction": "How do I Inserting Exterior Tap Walls", "input": "", "output": "ad_tap_cel_aray comandcomandsad_tap_cel_aray To add a tap cell array with specific placement, use the create_cell_array command.\n You must specify the name of the library cell to use for tap cell insertion ( -lib_cell option), and the x- and y-pitches.\n For example, fc_shell> create_cell_array -lib_cell myreflib/mytapcell \\ -x_pitch 10 -y_pitch 10 By default, the create_cell_array command inserts a tap cell in each location in the specified grid for the entire block.\n The tool starts inserting the tap cells at the bottom-left of the core area and uses the specified x- and y-pitches to determine the location of the subsequent tap cells.\n The inserted tap cells are snapped to the site rows and placed in the default orientation for the site row.\n If a location is occupied by a fixed cell, hard macro, soft macro, or power-switch cell, the command does not insert a tap cell at that location.\n You can modify the following aspects of the default behavior:  The region in which to insert tap cells Use the -voltage area option to restrict the tap cell insertion to the specified voltage areas.\n Use the -boundary option to restrict the tap cell insertion to the specified region.\n If you use both options, the command inserts tap cells only in the overlapping region.\n  The offset from the insertion region boundary Use the -x_offset option to shift the pattern startpoint to the right by the specified distance in microns.\n Use the -y_offset option to shift the pattern startpoint up by the specified distance in microns.\n        The insertion pattern Use the -checkerboard option to insert a tap cell in every other location in the specified grid.\n  To place a tap cell in the lower-left corner, use the -checkerboard even option.\n  To keep the lower-left corner empty, use the -checkerboard odd option.\n In some design methodologies, you must insert a tap cell in empty spaces in the checkerboard pattern that are above or below specific cells, such as critical area breaker cells or boundary cells.\n To enable the insertion of these extra tap cells, use the -preserve_boundary_row_lib_cells option to specify the affected library cells.\n  The snapping behavior To disable snapping of the inserted tap cells to the site rows, set the -snap_to_site_row false option.\n When you set this option to false, the tool uses R0 as the default orientation for the inserted cells.\n  The orientation of the inserted tap cell instances To specify the orientation of the inserted tap cells, use the -orient option to specify one of the following orientation values: R0 R90 R180 R270 MX MY MXR90, or MYR90  The naming convention used to identify the inserted tap cell instances By default, the tool uses the following naming convention for inserted tap cells: headerfooter__ library_cell_name _R _C number To identify the tap cells inserted in a specific run, use the -prefix option to specify a prefix string.\n When you use this option, the tool uses the following naming convention: prefix __ library_cell_name _R _C number"}
{"instruction": "How do I Specifying the Clock Tree Synthesis Settings", "input": "", "output": "The Fusion Compiler tool supports the following design rule constraints for clock tree synthesis:  clock tre synthesismaximum capacitanceclock tre design rule constraintmaximum capacitance Maximum capacitance To specify maximum capacitance constraints for clock tree synthesis, use the -clock_path option with the set_max_capacitance command.\n If you do not specify this constraint, the clock tree synthesis default is 0.6 pF.\n  clock tre synthesismaximum transition timeclock tre design rule constraintmaximum transition time Maximum transition time To specify maximum transition time constraints for clock tree synthesis, use the -clock_path option with the set_max_transition command.\n If you do not specify this constraint, the clock tree synthesis default is 0.5 ns.\n By default, these constraints apply to all corners associated with the current mode.\n To set the constraint for a specific mode, use the -mode option with the get_clocks command to identify the clocks.\n To set the constraint for specific corners associated with the specified mode, use the -corners option with the constraint command.\n Be careful to apply the correct constraints across all the modes and their associated corners.\n For example, to set a maximum transition time of 0.20 ns on all pins in the CLK clock path for all corners of the current mode, use the following command: fc_shell> set_max_transition 0.20 -clock_path [get_clocks CLK] To set different maximum transition time constraints for different corners associated with a specific mode, use the -mode option with the get_clocks command to specify the mode and use the -corners option to specify the corner.\n For example, to set the maximum transition time to 0.15 ns for corner1 in mode1 and 0.10 ns for corner2 in mode1, use the following commands: fc_shell> set_max_transition 0.15 -corners corner1 \\ -clock_path [get_clocks -mode mode1] fc_shell> set_max_transition 0.10 -corners corner2 \\ -clock_path [get_clocks -mode mode1] To set the same maximum capacitance constraint for different corners associated with a specific mode, use the following command: fc_shell> set_max_capacitance 0.6 \\ -clock_path [get_clocks -mode mode2] \\ -corners [get_corner \\ [get_attribute [get_modes mode2] associated_corners]]"}
{"instruction": "How do I Signoff Metal Fill Result Files", "input": "", "output": "To support designs that require combined foundry fill and track fill in the Fusion Compiler tool, you can use the -mode add option with the signoff_create_metal_fill command.\n fc_shell> signoff_create_metal_fill -mode add For a few advanced nodes, you can add foundry fill on top of track fill, which is the default methodology to create fill on some special layers.\n       You can use the all_runset_layers option with the signoff_create_metal_fill command.\n Set the all_runset_layers option to true to insert fill for all output fill layers defined in the runset settings.\n By default, this option is set to false and inserts fill only for the routing layers (metal and via layers)."}
{"instruction": "How do I Considering the Congestion Effects Due to the Nondefault", "input": "", "output": "When you specify a technology node setting of 7+ s5, or s4 by using the set_technology -node command, you can specify that the tool uses a technology- specific pin-cost model during placement by setting the place.coarse.pin_cost_aware application option to true.\n The default is false.\n When you enable this feature, during placement, the tool tries to improve pin accessibility by using a technology-specific model to predict the routing resources required to access each pin.\n For all other technology nodes, you can control the maximum local pin density during placement by setting the place.coarse.pin_density_aware application option to true The default is false Table 11 Settings for Controlling the Effects of Pins During Placement set_technology -node place.coarse.\n pin_cost_aware place.coarse.\n pin_density_aware The tool does this 7+ s5 s4 true true false   7+ s5 s4 false true   7+ s5 s4 false false    true false true         Table 11 Settings for Controlling the Effects of Pins During Placement (Continued) set_technology -node place.coarse.\n pin_cost_aware place.coarse.\n pin_density_aware The tool does this  true false false"}
{"instruction": "How do I Intra-shield Placement", "input": "", "output": "The custom.route.bus_corner_type application option specifies how the tool routes the bus trunks at corners.\n Valid values are: Valid Value Description auto         Valid Value Description cross  river"}
{"instruction": "How do I Allowing Repeater Groups Over Macros", "input": "", "output": "Use one of the following mutually exclusive options to control repeater group location:  -repeater_distance specifies the distance between repeater groups.\n This option supports the following additional options:  -first_distance specifies the distance between the driver and the center of the first repeater group.\n  -min_distance_repeater_to_load specifies the minimum distance between the last repeater group and the load.\n The default is one half of the value specified for the -repeater_distance option.\n  -tolerance specifies the maximum delta distance over the value specified for -repeater_distance.\n The default is 5 \u03bcm.\n  -relative_distance specifies the distance between the driver and the first repeater group, the distance between the first and second repeater groups, the distance between the second and third repeater group, and so on.\n  -location specifies the location of the center of each repeater group.\n  -number_of_repeater_groups specifies the total number of repeater groups to be inserted evenly along the route.\n  -cutlines specifies the location pairs that define the cutlines.\n Cutlines must be perpendicular to the routes and the repeater groups are centered around the cutlines."}
{"instruction": "How do I Creating a Frame View", "input": "", "output": "When you create an abstract view for a block with the create_abstract command, by default, the tool saves both the abstract and design views of the block and changes the design view to read-only.\n       You can open a read-only block and make changes to it in-memory, but you cannot save these changes by using the save_block or save_lib command.\n To save the change you make to a read-only block, use one of the following methods:  Save it as a different block by using the save_block -as command.\n  Re-create an abstract for the block by using the create_abstract command.\n The tool then automatically saves the design view.\n  Remove the abstract by using the remove_abstract command, if you no longer need the abstract.\n This changes the design view from being read-only to editable, and you can save the block by using the save_block or save_lib command.\n If you create a read-only abstract by using the create_abstract -read_only command, the tool does not make the design view of the block read-only and you can save any subsequent changes to the block by using the save_block or save_lib command, as shown in the following example: fc_shell> create_abstract -read_only fc_shell> change_link [get_cells U25] AND2 fc_shell> save_block"}
{"instruction": "How do I Specifying Orientations for Leaf Cells", "input": "", "output": "To add leaf cells, including physical only cells, to a relative placement group, use the -cells option with the add_to_rp_group command.\n Specify the column and row position within the relative placement group at which to add the cell by using the -column and -row options.\n In a relative placement group, a leaf cell can occupy multiple column positions or multiple row positions, which is known as leaf cell straddling.\n To specify the number of columns and rows the cells occupies, use the -num_columns and -num_rows options respectively.\n If you do not set these options, the default is 1 for both the number of columns and rows.\n For example, to add a leaf cell that occupies two columns and one row at position (0,0), use fc_shell> add_to_rp_group rp1 -cells U23 \\ -column 0 -row 0 -num_columns 2 -num_rows 1 Straddling is for leaf cells only, and not for hierarchical groups or blockages."}
{"instruction": "How do I Defining Routing Constraints", "input": "", "output": "Before you run Custom Router, you must  Ensure the necessary design rules are correctly defined in the technology file, such as layer and via definitions.\n For more information, see the Synopsys Technology File and Routing Rules Reference Manual  Check the connectivity information and look for pin blockages in the design.\n For more information, see the \"Reviewing the Design\" topic in the Custom Compiler Custom Router User Guide  Ensure sure that the power and ground nets in the block have been routed.\n       For more information, see the Fusion Compiler Design Planning User Guide  Check the routability of the placement as explained in Checking Routability"}
{"instruction": "How do I User-Specified Clock-Gate Latency", "input": "", "output": "By default, the tool estimates and updates clock-gate latencies throughout the compile_fusion command flow.\n This ensures that the tool uses accurate clock-gate latencies during datapath optimization and useful-skew optimization performed before clock tree synthesis, when clocks are ideal.\n If you plan to perform structural multisource clock tree synthesis for your design, ensure that the tool estimates clock-gate latencies that are appropriate for structural multisource clock subtrees by setting the opt.clock_latency_estimation.estimate_smscts_subtrees application option to true To improve the accuracy of the estimated clock-gate latencies, specify clock tree synthesis settings before you run the compile_fusion command.\n Such settings include the clock tree buffer library cell list, the clock tree inverter library cell list, and clock nondefault routing rules.\n Estimated clock latencies are more accurate than user-specified clock latencies.\n In addition, the tool updates the estimated clock latencies after steps such as placement and multibit banking, while user-specified latencies are static values.\n To prevent the tool from estimating the latency for a specific clock gate, set the dont_estimate_clock_latency attribute on its clock pin by using the set_attribute command, as shown in the following example: fc_shell> set_attribute \\ [get_pins {ICG21/CK}] dont_estimate_clock_latency true       For best results, do not use the following features with integrated clock-gate latency estimation:  Trial clock tree synthesis To disable this feature, set the compile.flow.trial_clock_tree application option to false  Clock-gate optimization To disable this feature, set the compile.flow.optimize_icgs application option to false  Placement-aware clock gating The tool automatically disables this feature when clock-gate latency estimation is enabled regardless of the compile.clockgate.physically_aware application option setting.\n  User-specified latencies User-specified latencies are set by the set_clock_latency set_clock_gate_latency set_clock_gating_check -setup, or set_path_margin -setup commands.\n The tool does not modify any set_clock_latency constraints that you specify on clock- gating cells.\n Tool-estimated clock-gate latency values are stored as offsets from the user- specified values.\n You can see the offset values by using the write_script -format icc2 command and checking for the set_clock_latency -offset commands in the generated output.\n If you do not set any latency values, the reported offsets are equal to the estimated latency values.\n The estimated latency offset values are not captured in an SDC file generated by the write_sdc command.\n Latency estimates are most accurate if the relative locations of the clock gates and their gated registers do not change during clock tree synthesis.\n To preserve the placement of integrated clock gates, enable the automatic relocation of clock network cells by setting the cts.compile.enable_cell_relocation application option to auto before clock tree synthesis."}
{"instruction": "How do I Creating Abstract Views", "input": "", "output": "In an abstract view, the gate-level netlist for the block is modeled by a partial gate- level netlist that contains only the required interface logic of the block.\n All other logic is removed.\n  shows a block and its abstract view, where the logic is preserved between  The input port and the first register of each timing path  The last register of each timing path and the output port       Logic associated with pure combinational input-port-to-output-port timing paths (A to X) is also preserved.\n Clock connections to the preserved registers are kept as well.\n The register-to-register logic is discarded.\n  A Block and Its Abstract View   CLK   Block   CLK   Abstract View The interface logic of an abstract view consists of the following:  All cells, pins, and nets in timing paths from input ports to registers or output ports  All cells, pins, and nets in timing paths to output ports from registers or input ports  Any logic in the connection from a master clock to generated clocks  The clock trees that drive interface registers, including any logic in the clock tree  The longest and shortest clock paths from the clock ports  All pins with timing constraints that are part of the interface logic In addition to the interface logic, an abstract view contains the following information associated with the interface logic:  Placement information  Timing constraints  Clock tree exceptions  Parasitic information  Transition and case values of dangling pins  Power management cells and UPF constraints  Nondefault routing rule association and minimum and maximum layer constraints, if any, for the nets that are retained in the abstract view"}
{"instruction": "How do I Inserting Boundary Cells", "input": "", "output": "To remove one or more of the boundary cell insertion requirements specified by the set_boundary_cell_rules command, use the remove_boundary_cell_rules command."}
{"instruction": "How do I Scaling CCD Offsets to New Scenarios for Reporting Timing", "input": "", "output": "You analyze the timing effects of concurrent clock and data optimization by using the report_ccd_timing command.\n By default, it reports the setup and hold slack of the worst capture (D-slack) and launch (Q-slack) paths of the five most critical endpoint registers in the block, as shown in the following example report: fc_shell> report_ccd_timing...\n...\n  Setup Hold  D-slack Q-slack D-slack Q-slack Pin  -0.567710 -0.173974 0.524202 0.167434 sink2/CP -0.175268 0.125309 0.168582 0.077625 sink3e/CP -0.173974 0.126836 0.167434 0.076277 sink3a/CP -0.033271 -0.175268 0.068856 0.168582 sink3d/CP 0.017364 -0.033271 -0.022138 0.068856 sink3c/CP       To control the type of report generated, use the -type option as follows:  To report information for the paths in the fanin or fanout of the most critical endpoints, use the -type fanin or -type fanout option.\n  To report information for the previous, current, and next path stages of the most critical endpoints, use the -type stage option.\n  To report information for all the previous, current, and next path stages of the most critical endpoints, use the -type chain option.\n shows the previous, current, and next stages for an endpoint register.\n  Previous, Current, and Next Path Stages By default, the command reports setup slacks at D and Q pins of registers.\n To report the hold slacks at these pins, use the -hold option.\n By default, the command analyzes timing paths across all active scenarios and reports the worst D and Q slacks across scenarios.\n To analyze and report the slack for specific scenarios use the -scenarios option.\n To report the information for specific path endpoints, specify the corresponding clock pins by using the -pins option.\n You can analyze the effects latency adjustments performed during concurrent clock and data optimization by increasing or decreasing the clock arrival of a specific endpoint using the -prepone or -postpone option with the -pins option as shown in the following example: fc_shell> report_ccd_timing -pins Reg11/clk -postpone 0.02 You can also see the effects of changing specific cell and net delay values by using the -annotate_cell_delay and -annotate_net_delay options, as shown in the following example: fc_shell> report_ccd_timing -annotate_cell_delay {{U1/a U1/y 0.2}} \\ -pin I2/d -type stage The delay values you specify with these options are used only for analyzing and reporting the concurrent clock and data timing.\n They are not saved and used during subsequent concurrent clock and data optimization."}
{"instruction": "How do I Clock-Gating Enable Source Selection", "input": "", "output": "You can enable or disable clock gating on certain design objects by overriding all necessary conditions set by the clock-gating style.\n The set_clock_gating_objects command specifies the design objects on which clock gating should be enabled or disabled during the compile_fusion command.\n The following example excludes all registers in the subdesign ADDER, except the out1_reg bank.\n The out1_reg bank is clock gated according to the specified clock-gating style: fc_shell> set_clock_gating_objects -exclude ADDER \\ -include ADDER/out1_reg[*]       The following example sets and then removes the inclusion and exclusion criteria specified by the -include -exclude, and -clear options: fc_shell> set_clock_gating_objects -include ADDER/out1_reg[*] \\ -exclude ADDER/out2_reg[*] fc_shell> set_clock_gating_objects \\ -clear {ADDER/out1_reg[*] ADDER/out2_reg[*]}"}
{"instruction": "How do I Enabling High-Effort Area Mode", "input": "", "output": "You can improve the total negative slack of a block by using path groups to focus the tool on fixing more violating paths during optimization.\n However,using this technique can increase the runtime and degrade the area and power QoR.\n To enable enhanced delay optimization techniques that improve total negative slack, set the compile.flow.enhanced_delay_opto_for_pg application option to.\n This feature has better runtime and better area and power QoR, as compared to the extensive use of path groups to improve total negative slack."}
{"instruction": "How do I Configuring Multithreading", "input": "", "output": "Several functions in the Fusion Compiler tool support multicore processing, whether through multithreading, distributed processing, or parallel command execution.\n Multicore       processing improves turnaround time by performing tasks in parallel much more quickly than if they were run sequentially on a single core.\n Note: A single machine has one or more CPUs and each CPU has one or more cores.\n The total number of cores available for processing on a machine is the number of CPUs multiplied by the number of cores in each CPU.\n When using multicore processing, you need one Fusion Compiler license for every 16 parallel tasks.\n For example, to run 17 parallel tasks, you need 2 Fusion Compiler licenses.\n In most cases, you configure multicore processing by using the set_host_options command.\n The following topics describe how to use the set_host_options command to configure multicore processing:  Configuring Multithreading  Configuring Distributed Processing  Reporting Multicore Configurations  Removing Multicore Configurations The following topic describes how to use distributed processing to run the same task on several blocks in a hierarchical design:  Running Tasks in Parallel The following topic describes how to use parallel command execution for checking and reporting commands:  Running Commands in Parallel on Your Local Host"}
{"instruction": "How do I Calculating the Antenna Ratio for a Pin", "input": "", "output": "0 allowable max-antenna-ratio = layerMaxRatio Table 38 shows the maximum antenna ratio for each diode calculated using this formula.\n Table 38 Calculation of Maximum Antenna Ratio for Each Diode Diode Protection value Maximum Antenna Ratio    v0      v0      v0         The maximum antenna ratio for the net is computed by using the formula for the diode mode from Table 36  For diode mode 2, the maximum antenna ratio for the net is the largest of the maximum antenna ratio values for the diodes, 2300.\n  For diode mode 3, the maximum antenna ratio for the net is the sum of the maximum antenna ratios for the diodes, 400 + 2200 + 2300 = 4900.\n  For diode mode 4, the maximum antenna ratio for the net is computed by using the formula from Table 37 using the sum of the diode-protection values of the diodes, (0.5+1.0+1.5) * 200 + 2000 = 2600.\n Example for Diode Modes 5 and 6 Assume that the antenna area is calculated using surface area in single-layer mode (antenna mode 1), the diode ratio vector for the M1 layer is {0 0 1 0} (the default diode ratio vector), the layerMaxRatio value for the M1 layer is 400, and the following diodes are connected to a single M1 net: diode A with a diode-protection value of 0.5, diode B with a diode-protection value of 1.0, and diode C with a diode-protection value of 1.5.\n In this example, the v4 value is not specified so a value of 0 is used.\n Use the following command to define the antenna rules for this example: fc_shell> define_antenna_rule -mode 1 -diode_mode diode_mode -metal_ratio 400 -cut_ratio 20 Because this example uses the global layerMaxRatio value and the default diode ratio vector, you do not need to define a layer-specific antenna rule.\n As shown in Table 37, for diode modes 5 and 6, the maximum antenna ratio is computed as metal_area / ( gate_area equi_gate_area where the equivalent gate area, equi_gate_area, is computed by using the diode ratio vector: ( diode_protection v1 ) * ( v2 v3 ).\n  For diode mode 5, the equivalent gate area is computed using the maximum diode protection, which is 1.5, so the maximum antenna ratio for the net is metal_area / ( gate_area + ((1.5 + 0) * (1 + 0))) = metal_area /(gate_area + 1.5)  For diode mode 6, the equivalent gate area is computed using the total diode protection, which is 0.5+1.0+1.5=3.0, so the maximum antenna ratio for the net is metal_area / (gate_area + ((3.0 + 0) * (1 + 0))) = metal_area /(gate_area + 3.0)       Example for Diode Modes 7 and 8 Assume that the antenna area is calculated using surface area in single-layer mode (antenna mode 1), the diode ratio vector for the M1 layer is {0.7 0.0 150 800}, the layerMaxRatio value for the M1 layer is 400, and the following diodes are connected to a single M1 net: diode A with a diode-protection value of 0.5, diode B with a diode-protection value of 1.0, and diode C with a diode-protection value of 1.5.\n In this example, the v4 value is not specified so a value of 0 is used.\n Use the following commands to define the antenna rules for this example: fc_shell> define_antenna_rule -mode 1 -diode_mode diode_mode -metal_ratio 400 -cut_ratio 20 fc_shell> define_antenna_layer_rule -mode 1 -layer \"M1\" \\ -ratio 400 -diode_ratio {0.7 0.0 150 800} As shown in Table 37, for diode modes 7 and 8, the maximum antenna ratio is computed as metal_area equi_gate_area ) / gate_area where the equivalent gate area, equi_gate_area, is computed by using the diode ratio vector: ( diode_protection v1 ) * ( v2 v3 ).\n  For diode mode 7, the equivalent metal area is computed using the maximum diode protection, which is 1.5, so the maximum antenna ratio for the net is metal_area - ((1.5 + 0) * (150 + 800))) / gate_area = ( metal_area - 1025) / gate_area  For diode mode 8, the equivalent metal area is computed using the total diode protection, which is 0.5+1.0+1.5=3.0, so the maximum antenna ratio for the net is metal_area - ((3.0 + 0) * 150 + 800)) / gate_area = ( metal_area - 1250) / gate_area Example for Diode Mode 14 With Multiple Gate Oxide Thicknesses Assume that the antenna area is calculated using surface area in single-layer mode (antenna mode 1), the design contains cells with two different gate oxide thicknesses, the diode ratio vector for the first oxide thickness (gate class 0) for the M1 layer is {1 0 1e9 1e9 0 1 2 285 1 285}, the diode ratio vector for the second oxide thickness (gate class 1) for the M1 layer is {1 0 1e9 1e9 1 0 2 165 1 28}, and the layerMaxRatio value for the M1 layer is 285.\n For diode mode 14, the diode ratio vector contains 10 values for each gate class.\n Because the design uses only two gate oxide thicknesses, only the vectors for gate class 0 and gate class 1 are used.\n However, you must specify all 40 values when defining the antenna       rule.\n For unused gate classes (gate class 2 and gate class 3 in this example), specify very large ratios to disable the checks.\n Use the following commands to define the antenna rules for this example: fc_shell> define_antenna_rule -mode 1 -diode_mode 14 \\ -metal_ratio 285 -cut_ratio 10 fc_shell> define_antenna_layer_rule -mode 1 -layer \"M1\" \\ -ratio 285 -diode_ratio {1 0 1e9 1e9 0 1 2 285 1 285 \\ 1 0 1e9 1e9 1 0 2 165 1 28 \\ 1 0 1e9 1e9 0 1 2 1e9 1 1e9 \\ 1 0 1e9 1e9 0 1 2 1e9 1 1e9 } The following table shows the vector usage:                                                                                 Here:  V0: Defines which gate areas are used for the check for the first formula:  0: Gate area of all gates that belong to the current gate class  1: Total gate area regardless of gate class  V1: Defines the diode protection multiplier for the connected to diode case for the first formula  V2: Defines the offset value for the connected to diode case for the first formula        V3: Defines the ratio requirement for the not connected to diode case for the first formula  V4: Defines which gate areas are used for the check for the second formula:  0: Gate area of all gates that belong to the current gate class  1: Total gate area regardless of gate class  V5: Defines the gate area multiplier for the connected to diode case for the second formula  V6: Defines the diode protection multiplier for the connected to diode case for the second formula  V7: Defines the ratio requirement for the connected to diode case for the second formula  V8: Defines the gate area multiplier for the not connected to diode case for the second formula  V9: Defines the antenna ratio requirement for the not connected to diode case for the second formula"}
{"instruction": "How do I Adding Programmable Spare Cells", "input": "", "output": "After placement and optimization, you can add spare cells and legalize them by using the following steps: 1.\n Add spare cells by using the add_spare_cells command and specify the following information:  A name prefix for the spare cells by using the -cell_name option  The type and number of spare cells to insert by using one of the following two methods:  Specify the library cells to use for the spare cell and the number of instances of each library cell by using the -lib_cell and -num_instances options For example, to insert 250 instances each of the AND2 and OR2 library cells, use the following command: fc_shell> add_spare_cells -cell_name spare \\ -lib_cell {AND2 OR2} -num_instances 250  Specify the library cells and a different number of instances for each library cell by using the -num_cells option For example, to insert 200 instances of the NAND2 library cell and 150 instances of the NOR2 library cell, use the following command: fc_shell> add_spare_cells -cell_name spare \\ -num_cells {NAND2 200 NOR 150}  (Optional) A repetitive placement window in which to add the spare cells by using the -repetitive_window option For example, to add 15 instances each of the AND2 and OR2 library cells in a 20 by 20 micron window that is repeated throughout the placement area, use the following command: fc_shell> add_spare_cells -cell_name spare \\ -lib_cell {AND2 OR2} -num_instances 15 \\ -repetitive_window {20 20} To add 20 instances of the NAND2 and 15 instances of the NOR2 library cells in a 25 by 20 micron window that is repeated throughout the placement area, use the following command: fc_shell> add_spare_cells -cell_name spare \\ -num_cells {NAND2 20 NOR 15} -repetitive_window {25 20}       By default, the add_spare_cells command distributes the spare cells evenly throughout the entire design.\n To distribute the spare cells within a specific  Hierarchical block, use the -hier_cell option.\n When you use this option, the tool distributes the spare cells in a rectangular area that encloses all of the cells that belong to the specified hierarchical block.\n  Bounding box, use the -boundary option.\n  Voltage areas, use the -voltage_areas option.\n You can further control the placement of the spare cells as follows:  Ignore specific types of placement blockages by using the -ignore_blockage_types option.\n By default, the add_spare_cells command honors all placement blockage types.\n  Ignore the current cell density and place the spare cells randomly by using the -random_distribution option.\n  Specify a percentage of spare cells to be placed based on the cell density distribution by using the -density_aware_ratio option.\n The rest of the spare cells are placed randomly throughout the design.\n By default, all the spare cells are placed based on the cell density distribution.\n For example, if you specify a setting of -density_aware_ratio 80, the tool places 80 percent of the spare cells based on the cell density distribution and 20 percent randomly across the design.\n  Specify the type of connection for the input pins of the spare cells by using the -input_pin_connect_type option.\n The valid values are tie_low tie_high, or open 2.\n Legalize the spare cells by using the place_eco_cells -legalize_only -cells command."}
{"instruction": "How do I Generating Automatic Group Bounds for Clock Gating Cells", "input": "", "output": "During the early stages of a design cycle, placement constraints can be inaccurate causing the tool to exit during coarse placement.\n To continue with placement when a region is overutilized due to it being too small or being covered by blockages, set the place.coarse.handle_early_data application option to true.\n When you do so, the tool issues warning messages as shown in the following example output: Warning: Utilization of move_bound_0 is 109%.\n (PLACE-089) Warning: Placement continues with over utilized regions in the design.\n (PLACE-083) Warning: Overutilized regions for move_bound_0 are modified by removal of partial and full blockages.\n (PLACE-084) When you enable this feature, the tool prints a warning message if the utilization of a region is more that 90 percent.\n To change this threshold, use the place.coarse.utilization_warning_threshold application option."}
{"instruction": "How do I Mapping of Synchronous Enable Logic to a MUX", "input": "", "output": "During sequential mapping, the tool can map registers to library cells that have both Q and QN pins, if doing so improves the QoR.\n However, you can prevent the tool from using the QN pin of such registers by setting the compile.seqmap.disable_qn_usage application option to true"}
{"instruction": "How do I Reporting Clock-Gate Enable Signals", "input": "", "output": "After compiling your design, you can check the results using the report_clock_gating command.\n An example is shown in  Example of a report_clock_gating Report The report provides a summary of the clock-gating statistics.\n The term \"pre-existing clock gates\" refers to user-instantiated clock gates.\n The preexisting category also includes registers that are gated by a tool-inserted clock gate.\n This includes registers that are gated by both preexisting and tool-inserted clock-gating cells.\n By default, the tool reports multilevel statistics.\n This includes the maximum level of clock- gating cells in the design.\n Clock-gate levels are counted from the clock source toward the register.\n If multibit flip-flops are found in the design, multibit composition is automatically reported.\n        Example of a Multibit Composition Report To report the details on ungated registers, use the -ungated option for report_clock_gating.\n For details on the gated registers, use the -gated option."}
{"instruction": "How do I Specifying a Cell Name Prefix for Optimization", "input": "", "output": "To control the addition of new cells to  Modules or hierarchical cells, use the set_allow_new_cells command  Voltage areas, use the -allow_new_cells option with the create_voltage_area_rule command You can use these settings to prevent new cells from being added to the top-level hierarchies and voltage areas of abutted designs.\n These settings are saved in the design library and honored by the tool throughout the implementation flow.\n The following example prevents new cells from being added to the top-level module: fc_shell> set_allow_new_cells \\ [get_attribute [current_block] top_module] false The following example prevents new cells from being added to the M1 module: fc_shell> set_allow_new_cells [get_modules M1] false       The following example prevents new cells from being added to the U22 hierarchical cell: fc_shell> set_allow_new_cells [get_cells U22] false The following example prevents new cells from being added to the VA1 voltage area: fc_shell> create_voltage_area_rule -name VA1_rule \\ -allow_new_cells false -voltage_areas VA1"}
{"instruction": "How do I Bus Routing Options", "input": "", "output": "The Fusion Compiler tool provides a set of application options to control custom routing results.\n The application options are applied globally.\n To define net-specific routing constraints to override the routing applications options, see Defining Routing Constraints To list all application options that are available for custom routing, use the following command: fc_shell> report_app_options custom.route.* To use the hybrid flow for custom routing, set the custom.route.hybrid_flow application option to on.\n This sets the values for each of hybrid flow related application options automatically.\n Table 56 lists the application options for custom routing.\n       Table 56 Application Options for Running Custom Routing Application option Description custom.route.bus_corner_type custom.route.bus_intra_shield_placement custom.route.bus_pin_trunk_offset custom.route.bus_split_even_bits custom.route.bus_split_ignore_width custom.route.bus_tap_off_enable custom.route.bus_tap_off_shielding   custom.route.match_box    custom.route.layer_grid_mode      custom.route.diffpair_twist_jumper_enable custom.route.diffpair_twist_jumper_interval custom.route.diffpair_twist_jumper_offset custom.route.diffpair_twist_jumper_style    custom.route.net_min_layer_mode custom.route.net_min_layer_mode_soft_cost custom.route.net_max_layer_mode custom.route.net_max_layer_mode_soft_cost     custom.route.routing_area      custom.route.shield_connect_mesh_overlap custom.route.shield_connect_route custom.route.shield_connect_to_supply custom.route.shield_min_open_loop custom.route.shield_min_shield_seg custom.route.shield_min_signal_seg custom.route.shield_net custom.route.shield_second_net         Table 56 Application Options for Running Custom Routing (Continued) Application option Description custom.route.single_loop_match custom.route.single_loop_match_max_spacing custom.route.single_loop_match_min_spacing custom.route.single_loop_match_offset_layer     custom.route.skip_connect_pin_type  custom.route.distance_to_net_pin  skip_connect_pin_type    distance_to_net_pin distance_to_net_pin   netName distance custom.route.balance_mode"}
{"instruction": "How do I Performing Prechecks for Hierarchical Synthesis Flows", "input": "", "output": "The following script shows an example of the hierarchical synthesis flow: # Specify reference libraries set REF_LIBS ${std_cell}.ndm lappend REF_LIBS ${block_design}_etm.ndm create_lib -tech ${TECH_FILE} -ref_libs $REF_LIBS ${DESIGN_NAME}  # Read the RTL design analyze -format verilog [list $rtl_list] elaborate ${DESIGN_NAME}  #ETM.ndm in ref_lib is used to link cell instances set_top_module ${DESIGN_NAME}  # Load UPF for ETM cell instance load_upf ${block_design}.upf -scope cell_instance  # Read test model for ETM cell instance read_test_model ${block_design}.mapped.ctl...\n compile_fusion -to initial_opto  # Query commands # List all macro cell instances get_cells -hierarchical -filter \"is_hard_macro == true\" # ref view name is \"timing\" for cell instances bound to ETM get_attribute ${CELL} is_etm_moded_cell"}
{"instruction": "How do I Shielding Nets", "input": "", "output": "Before you run automatic routing,  Set the common routing application options For information about the common routing application options, see the route.common_options man page.\n  Set the global routing application options For information about the detail routing application options, see the route.global_options man page.\n        Set the track assignment application options For information about the detail routing application options, see the route.track_options man page.\n  Set the detail routing application options For information about the detail routing application options, see the route.detail_options man page.\n To run automatic routing, use the route_auto command.\n By default, the route_auto command ignores existing global routes and sequentially invokes global routing, track assignment, and detail routing.\n The tool does not save the block between routing steps.\n Use the following options to change the default behavior:  To perform incremental global routing, use the -reuse_existing_global_route true option.\n  To stop after track assignment, use the -stop_after_track_assignment true option.\n  To change the number of search and repair iterations during detail routing from the default of 40, use the -max_detail_route_iterations option.\n  To save the block after each routing step, set the -save_after_global_route -save_after_track_assignment, and -save_after_detail_route options to true The tool uses auto as the default prefix for the saved block names.\n To specify the prefix, use the -save_cell_prefix option.\n By default, the route_auto command is not timing-driven or crosstalk-driven and does not fix soft DRC violations, such as bridge rule violations.\n  To enable timing-driven mode, set the route.global.timing_driven route.track.timing_driven, and route.detail.timing_driven application options to true By default, when you enable timing-driving routing, Zroute uses medium effort to assign timing-critical nets to low-resistance metal layers.\n To change the extent to which timing-driven routing prefers lower-resistance metal layers when routing timing-critical nets, set the route.common.rc_driven_setup_effort_level application option.\n To increase the effort level to use more low-resistance metal layers for routing, set the option to high.\n To reduce the effort level, set the option to low.\n To disable resistance- based routing layer preferences, set the option to off        To enable crosstalk-driven mode, set the route.global.crosstalk_driven route.track.crosstalk_driven, and time.si_enable_analysis application options to true  To enable the fixing of soft DRC violations after the final detail routing iteration, set the route.common.post_detail_route_fix_soft_violations application option to true fc_shell> set_app_options \\ -name route.common.post_detail_route_fix_soft_violations \\ -value true"}
{"instruction": "How do I Assigning Nondefault Routing Rules to Critical Nets", "input": "", "output": "To specify the valid via ladders that can be used for a given library pin during preroute optimization, use the set_via_ladder_candidate command.\n You can specify only one via ladder each time you use the set_via_ladder_candidate command.\n To specify multiple via ladder candidates for a library pin, use the command multiple times, as shown in the following example: fc_shell> set_via_ladder_candidate [get_lib_pins lib1/AND2/A1] \\ -ladder_name \"VP4\"       fc_shell> set_via_ladder_candidate [get_lib_pins lib1/AND2/A1] \\ -ladder_name \"VP2\" The order in which you specify multiple via ladders for the same library pin indicates the priority to use when selecting a via ladder to insert for that pin.\n Specify the via ladder candidates starting with the highest priority.\n In the previous example, the tool gives the VP4 via ladder priority over the VP2 via ladder.\n Note: If both the pattern_must_join attribute and a via ladder candidate apply to a pin, the via ladder has a higher priority than the pattern-based must-join connection when both methods have the same estimated delay.\n To specify that a library pin requires an electromigration via ladder, set the is_em_via_ladder_required pin attribute to true, as shown in the following example: fc_shell> set_attribute [get_lib_pins lib1/INV4/A] \\ is_em_via_ladder_required true If you set the is_em_via_ladder_required pin attribute to true for a specific library pin, you must specify an electromigration via ladder as a candidate with the set_via_ladder_candidate command.\n A via ladder can be identified as an electromigration via ladder by using one of the following methods:  By using the forElectromigration=1 construct when defining the via rule in the technology file, as shown in the following example: ViaRule \"EMVP1\" {...\n...\n...\n forElectromigration=1   By setting the for_electro_migration attribute to true, as shown in the following example: fc_shell> set_attribute [get_via_rules EMVP1] \\ for_electro_migration true See Also  Defining Via Ladder Rules"}
{"instruction": "How do I Analyzing the Clock Mesh", "input": "", "output": "After you create clock straps for a clock net by using the create_clock_straps command, you can route the drivers and loads of the clock net to the clock straps by using the route_clock_straps command and specify the clock net name by using the -nets option.\n The tool connects the drivers and the loads only to clock straps with the shape_use attribute setting of stripe.\n It does not connect to clock straps with the shape_use attribute setting of user_route       To specify a topology for connecting the clock drivers and sinks to the clock straps, use the -topology option as follows:  To create a fishbone routing topology, the default topology, use the -topology fishbone option.\n In a fishbone topology, each driver pin is individually connected to the nearest stripe.\n To minimize wire length, multiple load pins are connected using comb routing to a single finger, which is connected to the stripe, as shown in  Fishbone Topology Fanout Clock mesh strap ( shape_use user_route Clock mesh strap ( shape_use stripe Comb route  Driver pins Load pins Finger route If you are using a fishbone topology, you can specify  A maximum fanout for the loads of a finger, as shown in, by using -fishbone_fanout option.\n  Fanout of a Finger of the Fishbone Topology Finger Fanout  A maximum span between any two loads connected to a finger, as shown in, by using -fishbone_span option.\n       The span is measured orthogonal to the direction of the finger.\n  Span and Subspan of the Fishbone Topology Subspan  Span Subfinger Finger  A maximum subspan between any two loads connected to a subfinger, as shown in, by using -fishbone_sub_span option.\n The subspan is measured orthogonal to the direction of the subfinger.\n  The layers to use for routing the fingers and subfingers by using the -fishbone_layers option.\n  To create a comb routing topology, use the -topology comb option.\n In a comb topology, each driver and load pin is directly routed to the nearest stripe.\n However, if the Manhattan distance from a pin to the nearest stripe is more than the comb distance, the tool routes the pin to the nearest net shape using a Steiner topology, as shown in.\n The default comb distance is two global routing cells.\n To change the comb distance, use the route.common.comb_distance application option.\n        Comb Topology Load pins Driver pins  Clock mesh strap ( shape_use stripe Clock mesh strap ( shape_use user_route Comb route Pin that is further than the comb distance Comb routing is suitable when there are a large number of driver and load pins directly under the clock-mesh stripes.\n The tool uses stacked vias to connect the pins to the stripes, which are usually in the higher routing layers.\n However, this can contribute to physical DRC violations due to many adjacent stacked vias, as shown in  Vias in the Comb Topology Driver and load pins Clock mesh strap on Stacked vias  higher routing layer  To create a substrap routing topology, use the -topology sub_strap option.\n In a substrap topology, additional straps that are parallel to the stripes are created on intermediate routing layers.\n This reduces the number of stacked vias, as compared to the comb topology.\n        Vias in the Substrap Topology Driver and load pins Clock mesh stripe on Substrap on intermediate  higher routing layer routing layers For the substrap topology, you  Must specify the layers in which to create the substrap by using the -sub_strap_layers option  Can specify the maximum RC delay allowed for the substrap by using the -sub_strap_max_delay option The default is 2 ps.\n This delay constrains the length and the number of loads connected to the substrap."}
{"instruction": "How do I Removing Metal Fill With the IC Validator Tool", "input": "", "output": "The method you use to remove metal fill depends on the extent of the metal fill removal:  To remove all metal fill from a block, specific layers of a block, or specific regions of a block, use the signoff_create_metal_fill command with the -mode remove option.\n This command uses the IC Validator tool to remove metal fill from the current block, as described in Removing Metal Fill With the IC Validator Tool  To remove specific fill shapes, use the remove_shapes or remove_fill_cells commands.\n You would typically use this method when manually fixing DRC violations related to the metal fill.\n You might also be able to fix the DRC violation by modifying the metal fill shapes, as described in Modifying Metal Fill"}
{"instruction": "How do I Reporting Relative Placement Constraint Violations", "input": "", "output": "Before you run placement and optimization, you can check if a specific relative placement group can be placed by using the check_rp_constraints -analyze_placement command.\n  To limit the analysis to a specific region or a specific number of random sites of the core area, use the -region or -trials option, respectively.\n  To ignore physical design constraints, advanced design rules, or relative placement constraints during placement analysis, use the -no_pdc -no_adv, or -no_rp_constraints option, respectively.\n  To report only the relative placement groups that do not meet a specific threshold, use the -threshold option.\n The tool reports a group only if the percentage of sites the group can be placed, relative to the total number of sites analyzed, is less than the specified threshold."}
{"instruction": "How do I Performing Minimum Path Resistance Analysis", "input": "", "output": "When PG electromigration analysis is complete, you can check the generated error files in the error browser or write the generated errors to an output file.\n The tool does not write the generated errors to the working directory, meaning the error data is deleted if you exit the tool without saving the block.\n Ensure that you save the block to keep the generated error data if you want to examine the errors in another session.\n       You can examine the generated error files in the error browser or write the generated errors to an output ASCII file.\n  To examine the generated errors in the error browser, 1.\n Choose View > Error Browser from the GUI.\n 2.\n In the Open Error Data dialog box, select the errors to examine and click Open Selected.\n 3.\n Check the details of the errors in the Error Browser (see ).\n        Opening Generated Errors in the Error Browser  To write errors to an ASCII file, use the following script: set fileName \"errors.txt\" set fd [open $fileName w] set dm [open_drc_error_data rail_pg_em.VDD.err] set all_errs [get_drc_errors -error_data $dm *] foreach_in_collection err $all_errs { set info [ get_attribute $err error_info]; puts $fd $info } close $fd       Here is an example of the output error file: M1 (414.000,546.584 414.344,546.584) em ratio 23585.9% width 0.06 blech length 0.000 um current 0.0024963 M1 (414.344,546.584 416.168,546.584) em ratio 23401.3% width 0.06 blech length 0.000 um current 0.0024768 M1 (416.168,546.584 416.776,546.584) em ratio 23241.7% width 0.06 blech length 0.000 um current 0.0024599 M1 (416.776,546.584 417.992,546.584) em ratio 23082.2% width 0.06 blech length 0.000 um current 0.0024430 M1 (417.992,546.584 419.208,546.584) em ratio 22897.7% width 0.06 blech length 0.000 um current 0.0024235 M1 (419.208,546.584 419.512,546.584) em ratio 22731.1% width 0.06 blech length 0.000 um current 0.0024059 M1 (419.512,546.584 421.184,546.584) em ratio 22564.4% width 0.06 blech length 0.000 um current 0.0023882 M1 (421.184,546.584 421.488,546.584) em ratio 22349% width 0.06 blech length 0.000 um current 0.0023654 M1 (421.488,546.584 423.160,546.584) em ratio 22133.5% width 0.06 blech length 0.000 um current 0.0023426 M1 (423.160,546.584 423.616,546.584) em ratio 21967.2% width 0.06 blech length 0.000 um current 0.0023250"}
{"instruction": "How do I Saving Relative Placement Information", "input": "", "output": "The Fusion Compiler GUI provides tools to help you visualize and analyze the relative placement groups in your design:  Relative Placement (RP) Groups Visual Mode  Relative Placement (RP) Net Connection Visual Mode For more information about analyzing relative placement groups in the GUI, see the Using Map and Visual Modes topic in the Fusion Compiler Graphical User Interface User Guide."}
{"instruction": "How do I Before Using Custom Router", "input": "", "output": "The Fusion Compiler tool provides a set of Tcl commands to run Custom Router in the Fusion Compiler environment.\n These commands define routing constraints for adding wires on one or more nets at the block level or adding differential pair routing, bus routing, shielding, and other routing features.\n A set of application options is also available for specifying the parameters for performing custom routing.\n For more information about how to define routing constraints and application options, see Defining Routing Constraints and Using Custom Routing Application Options The route_custom command uses the specified routing constraints, as well as information from the technology file and the design, to perform automatic routing using the Custom Router.\n For more information about using the route_custom command, see Routing With the Custom Router Custom Router supports the following constraints:  Net shielding and differential pairs  Matched length  Variable width and space  Pin width matching and tapering Custom Router supports the following Fusion Compiler nondefault routing rules:  Routing layers and vias  Routing width and spacing  Routing grids  Inter- and intra-group spacing  Taper halo  Routing blockage        Routing corridors  Shielding  shows the basic Custom Router flow.\n  Basic Custom Router Flow Define routing constraints Perform custom routing Block with routed power nets and unrouted clock trees Technology file with design rule definitions Perform postroute optimization Analyze routing results"}
{"instruction": "How do I Defining Checkpoint Behaviors", "input": "", "output": "For a checkpoint to affect your flow, you must associate it with a flow change or report that you want to run before, after, or in place of the code the checkpoint enwraps.\n The process of defining flow changes and reports and associating them with individual checkpoints is described in the following topics:  Defining Checkpoint Behaviors  Associating Checkpoints and Checkpoint Behaviors"}
{"instruction": "How do I Specifying Logical Design Rule Constraints", "input": "", "output": "Timing constraints describe the timing requirements of a design.\n An essential part of timing constraints are accurately specified clocks and clock effects, such as latency and uncertainty.\n When you specify clocks, the tool automatically constrains the paths between the registers driven by these clocks.\n However, you can change the default behavior of these timing paths by specifying timing exceptions such as paths that should not be analyzed (false paths), paths that require multiple clock cycles (multicycle paths), and so on.\n In addition, you can constrain the boundary timing paths by specifying input and output delays for input and output ports.\n The Fusion Compiler tool uses on-chip variation (OCV) mode to perform timing analysis, which models the effects of variation in operating conditions across the chip.\n This mode performs a conservative timing analysis that allows both minimum and maximum delays to apply to different paths at the same time.\n For a setup check, it uses maximum delays for the launch clock path and data path and minimum delays for the capture clock path.\n For a hold check, it uses minimum delays for the launch clock path and data path and maximum delays for the capture clock path.\n A block might operate under several different conditions, such as different temperatures and voltages, and might operate in several different functional modes.\n For timing analysis, each set of conditions is represented by a corner and each functional mode is represented by a mode.\n A scenario is a combination of a corner and mode used to perform timing analysis and optimization.\n Before you start working with a block, you must define the modes, corners, and scenarios that are used for the block, as well as the delay calculation       model and routing layers to use.\n The routing layer information you specify is used for RC estimation during timing analysis.\n For detailed information about specifying  Clock and clock effects, see the \"Defining Clocks\" topic in the Fusion Compiler Timing Analysis User Guide  Exceptions for timing paths and constraints for boundary paths, see the \"Constraining Timing Paths\" topic in the Fusion Compiler Timing Analysis User Guide  Modes, corners, and scenarios, see the \"Defining Modes, Corners, and Scenarios\" topic in the Fusion Compiler Timing Analysis User Guide  Operating conditions and on-chip variation (OCV) related settings, see the \"Specifying Operating Conditions\" topic in the Fusion Compiler Timing Analysis User Guide  Parasitic information for RC estimation and extraction, see the \"Performing Parasitic Extraction\" topic in the Fusion Compiler Timing Analysis User Guide  Routing layers, see Specifying the Routing Resources"}
{"instruction": "How do I Routing Critical Nets", "input": "", "output": "To route clock nets before routing the rest of the nets in the block, use the route_group command with the appropriate option to select the nets.\n  To route all clock nets, use the -all_clock_nets option.\n  To route specific clock nets, use the -nets option.\n For example, to route all clock nets, use the following command: fc_shell> route_group -all_clock_nets The route_group command runs global routing, track assignment, and detail routing on the clock nets.\n  When routing clock nets, the route_group command uses A-tree routing to minimize the distance between each pin and driver.\n  When routing clock mesh nets, by default, the route_group command uses Steiner routing, which minimizes the total wire length.\n To use comb routing, set the route.common.clock_topology application option to comb before routing the clock mesh nets.\n  By default, if the block contains existing global routes, the route_group command ignores them during global routing.\n To perform incremental global routing by reusing existing global routes, use the -reuse_existing_global_route true option.\n  If the block contains existing detail routes for the clock nets, the route_group command performs incremental detail routing.\n  By default, the detail router performs a maximum of 40 search and repair iterations.\n To modify the maximum number of detail routing iterations, use the -max_detail_route_iterations option.\n Note: Zroute stops before completing the maximum number of iterations if it determines that all violations have been fixed or that it cannot fix the remaining violations."}
{"instruction": "How do I Controlling How Buffers are Added", "input": "", "output": "To add buffers to the individual nets of a bus in a specific pattern, use the following steps: 1.\n Define buffer patterns for the nets of a bus by using the create_eco_bus_buffer_pattern command.\n This command allows you to define patterns for staggering buffers on bused nets, which prevent clumping and overlapping of buffers.\n Table 72 Commands Associated With Bus Patterns To do this Use this command  create_eco_bus_buffer_pattern       To do this Use this command  report_eco_bus_buffer_patterns  get_eco_bus_buffer_patterns  remove_eco_bus_buffer_patterns The following example creates the buffer pattern shown in for a horizontal bus, where the first buffer is placed on the topmost net.\n The buffers are staggered by a distance of 2, starting from the left, and the pattern is repeated after every three buffers.\n fc_shell> create_eco_bus_buffer_pattern -name top_left \\ -first_buffer top -measure_from left -distance 2 -repeat_after 3  Buffer Patterns for a Horizontal Bus   First buffer The following example creates the buffer pattern shown in for a vertical bus, where the first buffer is placed on the rightmost net.\n Starting from the top, the second buffer is staggered by a distance of 2 from the first, the third a distance of 3 from the second, and the fourth a distance 2 from the third.\n After that, the pattern is repeated.\n fc_shell> create_eco_bus_buffer_pattern -name right_top \\ -first_buffer right -measure_from top \\ -user_specified_distance {2 3 2}        Buffer Patterns for a Vertical Bus   First buffer  2.\n Add buffers on the nets of the bus by using the add_buffer_on_route -user_specified_bus_buffers command.\n For each group of buffers that you add on the bused nets, use the {buffer_pattern_name library_cell_name x y....\n } format with the -user_specified_bus_buffers option to specify the  Name of the buffer pattern that you defined using the create_eco_bus_buffer_pattern command  Library cell to use  Coordinates of the location of the first buffer in the pattern The following example adds a group of buffers of type BUF2 on the bus named data_A, using a buffer pattern named BP1.\n The first buffer of the pattern is placed at location (100, 70).\n fc_shell> add_buffer_on_route [get_net data_A*] \\ -user_specified_bus_buffers {BP1 BUF2 100 70}"}
{"instruction": "How do I Place and Route Design Flow Overview", "input": "", "output": "The Formality tool uses formal techniques to prove or disprove the functional equivalence of two designs.\n It performs RTL-to-RTL, RTL-to-gate, and gate-to-gate verifications.\n Functional equivalence checking does not take into account static timing verification.\n The       following figure shows an overview of the design flow from the Fusion Compiler tool to the Formality tool: Synthesis RTL/ netlist Ref.\n lib (NDM) Tech.\n data TLU+ files UPF files DEF/Tcl (Optional) SDC files (Optional) SVF files Design (NDM) Verilog UPF files Formality"}
{"instruction": "How do I Routing Signal Nets", "input": "", "output": "If the secondary power and ground pins have the appropriate attributes in the frame view, you can use Zroute to route the secondary power and ground pins.\n By default, Zroute performs four topology ECO iterations to fix secondary PG cluster fanout violations.The topology ECO iterations can be time consuming, so you can control the effort level, and therefore the runtime, by setting the route.detail.topology_eco_effort_level application option to one of the following values:  off, which disables topology ECO; this is the default  low, which peforms one topology ECO iteration  medium, which peforms four topology ECO iterations  high, which peforms eight topology ECO iterations For example, to connect the secondary power pins to the VDD1 net, run the following command: fc_shell> route_group -nets VDD1 The following example add new cells, which have secondary power and ground pins, to a block; logically connects the power and ground pins; and then connects the secondary power pins to the VDD2 net.\n fc_shell> add_buffer {TOP/U1001/Z} {libA/BUFHVT} \\ -new_cell_names mynewHVT fc_shell> add_buffer {TOP/U1002/Z} {libA/BUFHVT} \\ -new_cell_names mynewHVT fc_shell> legalize_placement       fc_shell> connect_pg_net -automatic fc_shell> route_group -nets {VDD2}"}
{"instruction": "How do I Removing Global Layer Constraints", "input": "", "output": "To report the ignored layers, use the report_ignored_layers command.\n For example, fc_shell> report_ignored_layers **************************************** Report : Ignored Layers Design : my_design Version: J-2014.12 Date : Wed Oct 22 15:58:23 2014 **************************************** Layer Attribute Value  Min Routing Layer M2 Max Routing Layer M7 RC Estimation Ignored Layers PO M1 M2 M8 M9 MRDL"}
{"instruction": "How do I Performing Voltage Optimization", "input": "", "output": "When you detail route the clock nets of a block, its clock tree QoR can degrade due to the differences between the clock global routes used during clock tree synthesis and the clock detail routes.\n Clock tree QoR can further degrade when you detail route the signal nets due to coupling capacitance and crosstalk effects.\n You can perform clock tree optimization on a postroute design by using the synthesize_clock_trees -postroute command.\n When you do so, you should specify the type of routing performed on the design by using the -routed_clock_stage option.\n For example, to perform clock tree optimization on a design that has completed clock routing, use the following command: fc_shell> synthesize_clock_trees -postroute \\ -routed_clock_stage detail To perform clock tree optimization on a design that has completed both clock and signal routing, use the following command: fc_shell> synthesize_clock_trees -postroute \\ -routed_clock_stage detail_with_signal_routes Note: When you enable concurrent clock and data optimization, including power or area recovery that uses this technique, the tool does not optimize the clock latency or skew during postroute clock tree optimization; it only fixes the logical DRC violations on the clock network."}
{"instruction": "How do I Implementing a Regular Multisource Clock Tree", "input": "", "output": "A multisource clock tree is a custom clock structure that has more tolerance to on-chip variation and has better performance across corners than traditional clock tree structures.\n These custom clock trees consist of  A global clock structure, which includes  The clock root  A global clock tree, which is usually an H-tree structure  Clock mesh drivers  A clock mesh  Local subtrees that can be driven  By a predefined set of drivers, called tap drivers, that are connected to the clock mesh, as shown in Because the subtrees are built using regular clock tree synthesis commands such as the synthesize_clock_trees or clock_opt command, such a structure is called a regular multisource clock tree.\n  Directly from multiple points of the clock mesh, as shown in Because the subtrees are built preserving the user-defined structure and optimized by merging and splitting the clock cells, such a structure is called a structural multisource clock tree.\n        Regular Multisource Clock Tree Clock root  Global clock tree  Clock mesh  Mesh drivers  Clock gates Tap drivers  and sinks Global clock structure Local subtrees  Structural Multisource Clock Tree Clock root  Global clock tree  Clock mesh  Mesh drivers  Clock gates and sinks Global clock structure Local subtrees"}
{"instruction": "How do I Enabling Area Recovery in Regions of High Utilization", "input": "", "output": "To improve timing QoR, you can enable the tool to automatically assign nondefault routing rules to nets with critical timing.\n When you enable this feature, the tool  Chooses candidate nets based on slack and length  Uses timing-driven optimization to assign nondefault routing rules to the nets  Optimizes buffering for these nets To enable the feature, follow these steps.\n 1.\n Define nondefault routing rules by using the create_routing_rule command.\n For details, see Defining Nondefault Routing Rules 2.\n Report routing rules by using the report_routing_rules command.\n 3.\n Specify a prioritized list of nondefault routing rules for automatic assignment by using the opt.common.optimize_ndr_user_rule_names application option.\n 4.\n Enable the automatic assignment of nondefault routing rules during optimization by setting the compile.flow.optimize_ndr application option.\n The following script example creates two nondefault routing rules, ndr_1w2s and ndr_2w3s, and automatically assigns these nondefault routing rules to timing-critical nets during optimization: create_routing_rule \"ndr_1w2s\" -multiplier_spacing 2 create_routing_rule \"ndr_2w3s\" -multiplier_spacing 3 -multiplier_width 2 # Enables feature, defines priority set opt.common.optimize_ndr_user_rule_names { ndr_1w2s ndr_2w3s } set compile.flow.optimize_ndr compile_fusion Controlling Nondefault Routing Rule Assignment To specify a critical range for nondefault routing rule for optimization, set the compile.flow.optimize_ndr_critical_range application option to a value between 0 and 1.\n The default is 0.05.\n The specified value, as shown in the following example, is used to calculate the slack range [ *WNS, WNS] where WNS is the worst negative slack.\n Nets in the timing paths that have a slack value within the range are considered for nondefault routing rule assignment.\n fc_shell> set_app_options \\ -name compile.flow.optimize_ndr_critical_range -value       To set the maximum number of nets considered for automatic nondefault routing rules assignment, set the compile.flow.optimize_ndr_max_nets application option to an integer.\n The default is 2000.\n For example, fc_shell> set_app_options \\ -name compile.flow.optimize_ndr_max_nets -value 5000 When both the compile.flow.optimize_ndr_critical_range and compile.flow.optimize_ndr_max_nets application options are specified, the stricter constraint has priority."}
{"instruction": "How do I Inserting Dense Tap Arrays", "input": "", "output": "tap mesh is a two-dimensional array of tap cells with a tap cell in each mesh window.\n A tap mesh is typically inserted outside of a place and route block, but you can insert one in any area that is either completely inside or completely outside the core area.\n To insert a tap mesh, use the create_tap_meshes command.\n You must specify the following information:  The tap cell used in the tap mesh (the -lib_cell option) By default, the command inserts the tap cells with R0 orientation.\n To place the cells with a different orientation, use the -orientation option.\n By default, the tool uses the following naming convention for inserted tap cells: headerfooter__ library_cell_name _R _C number To identify the tap cells inserted in a specific run, use the -prefix option to specify a prefix string.\n When you use this option, the tool uses the following naming convention: prefix __ library_cell_name _R _C number  The tap insertion region (the -bbox option) You specify the tap insertion region by specifying the lower-left and upper-right corners of its bounding box: -bbox {{ llx lly } { urx ury }}  The mesh window (the -mesh_window option) You specify the size of the mesh window by specifying an integer value for the pitch in the x- and y-directions.\n -mesh_window { x_pitch y_pitch The mesh window is aligned with the lower-left corner of the tap insertion region.\n When inserting the tap mesh, the command does not honor the standard cell rows or sites.\n It places a tap cell in the lower-left corner of each mesh window.\n If that location is blocked, the command uses the closest available location, which is measured as the Manhattan distance to the lower-left corner of the mesh window.\n If there is no available location, the command issues a warning and does not place a tap cell in that mesh window.\n After insertion, the command fixes the placement of the inserted tap cells."}
{"instruction": "How do I Controlling the Placement of Multivoltage Cells", "input": "", "output": "The Fusion Compiler tool automatically associates power strategies with existing multivoltage cells when you run the associate_mv_cells or commit_upf command.\n If this automatic association is not correct, you can manually modify the associations by using the set_power_strategy_attribute command.\n To determine the power strategies for a power domain, use the get_power_strategies command."}
{"instruction": "How do I Mask Constraints", "input": "", "output": "At the 20-nm process node and below, printing the required geometries is extremely difficult with the existing photolithography tools.\n To address this issue, a new technique, multiple patterning, is used to partition the layout mask into two or more separate masks, each of which has an increased manufacturing pitch to enable higher resolution and better printability.\n shows an example of double-patterning, where the layout mask is partitioned into two separate masks, MASK A and MASK B.\n  Double-Patterning Example To use multiple patterning, you must be able to decompose the layout into two or more masks, each of which meets the multiple-patterning spacing requirements.\n A multiple- patterning violation occurs if your layout contains a region with an odd number of neighboring shapes where the distance between each pair of shapes is smaller than the multiple-patterning minimum spacing.\n This type of violation, which is called an odd cycle is shown in        Odd-Cycle Violation If the spacing between any pair in the loop is greater than the multiple-patterning minimum spacing, no violation occurs and the layout can be decomposed.\n For example, in if the spacing,, between segments B and C is greater than the multiple-patterning minimum spacing, there is no odd cycle and the layout can be decomposed.\n  No Odd-Cycle Violation The Fusion Compiler tool ensures that the generated layout is conducive to double patterning by considering the multiple-patterning spacing requirements during placement and routing and preventing odd cycles.\n In general, double patterning is performed only on the bottom (lowest) metal layers, which are referred to as multiple-patterning layers.\n The metal shapes on the multiple-patterning layers must meet the multiple-patterning spacing requirements, whether they are routing shapes or metal within the standard cells and macros.\n The metal shapes on other layers do not need to meet the stricter multiple-patterning spacing requirements.\n       Multiple-patterning considerations affect all parts of the place and route flow.\n Depending on your standard cell library, you follow either an uncolored or precolored multiple- patterning flow.\n  You use the uncolored flow if the cells in your standard cell library have sufficient spacing to the cell boundaries to ensure that multiple-patterning violations do not occur during placement.\n This type of library is referred to as a correct-by-construction library most multiple-patterning libraries are correct-by-construction libraries.\n In the uncolored flow, the tool determines the appropriate mask settings for the pins and net shapes.\n  You use the precolored flow if the cells in your standard cell library have assigned masks on the metal shapes inside the cells.\n These assigned masks are often referred to as colors and this type of library is referred to as a precolored library In the precolored flow, the tool must consider these mask assignments to ensure that multiple-patterning violations do not occur during placement.\n The tool also uses the mask assignments to determine the appropriate mask settings for the pins and net shapes.\n The mask assignments are represented as mask constraints in the Fusion Compiler tool.\n You must ensure that the mask constraints are properly set before starting place and route.\n For information about the mask constraints, see Mask Constraints"}
{"instruction": "How do I Controlling High-Fanout-Net Synthesis", "input": "", "output": "The compile_fusion command mergers registers and removes constant and unloaded registers during register optimization.\n To print the RTL file name and line number of these optimized register in the compile_fusion command log file and in the report_transformed_registers command output, set the compile.seqmap.print_cross_probing_info_for_removed_registers application option to true The following table shows the messages printed out during the different types of register optimization.\n Table 21 Cross-Probing Information Messages Printed During Register Optimization Register Optimization Information Message  Information: The register 'a/b_reg { {/user/test/design.v:49} }' is removed as constant 0 (SQM-4100)  Information: The register 'a/c_reg{ {/user/test/design.v:193} }' is removed because it is unloaded.\n (SQM-4101)  Information: The register 'a_reg[1] { {/user/testcases/top.v:113} }' is removed because it is merged to 'a_reg[0]'.\n (SQM-4102)       The following example output shows the cross probing information that is reported by the report_transformed_registers command: Legend: c0r - Constant 0 Register Removed c1r - Constant 1 Register Removed ul - Unloaded Removed inv - Inverted rep - Replicated mrg - Merged register mb - Mutibit mbd - Mutibit debanked Register Transformation Filename: Line number   mega_shift_reg c0r {{a/b/test1.v:112}} Carry_Flag_reg ul {{a/b/test2.v:117}} mega_shift_reg[10] mrg (mega_shift_reg[31]) {{a/b/test3.v:121 }}"}
{"instruction": "How do I Removing Voltage Areas", "input": "", "output": "A net is considered to be native to a voltage area if one or more segments of that net are in a logical hierarchy of that voltage area.\n If a net must physically route over a nonnative voltage area, then it is a physical feedthrough net of that voltage area, as shown in the following figure.\n        Physical-Feedthrough Nets of Voltage Areas VA0 (PD0 ) VA1 (PD1) N1 Top (PD0) Block1 (PD1) N1 Physical view Logical view By default, physical-feedthrough nets are allowed in voltage areas.\n To prevent physical-feedthrough nets in a voltage area, define a voltage area rule using the create_voltage_area_rule -allow_pass_through false command, as shown in the following example: fc_shell> create_voltage_area_rule -allow_pass_through false \\ -name VA1_rule -voltage_areas VA1 With this rule, the N1 net detours around the VA1 voltage area, as shown in  Physical-Feedthrough Nets Disabled for a Voltage Area VA0 N1 VA1 By default, optimization does not insert buffers on physical-feedthrough nets of voltage areas.\n Inserting a buffer on a physical-feedthrough net can cause a mismatch between the       logical and physical view of the buffer.\n The tool resolves such mismatches by supporting the following types of optimization for such nets:  Physical-feedthrough buffering During physical-feedthrough buffering the tool performs the following: 1.\n Inserts buffers in the logical hierarchy of its drivers or loads and applies the power domain of the nonnative voltage area on the buffer using power-domain-on-instance (PDOI) constraints 2.\n Places the buffers within the nonnative voltage area To allow physical-feedthrough buffering, use the create_voltage_area_rule -allow_physical_feedthrough true command, as shown in the following example: fc_shell> create_voltage_area_rule \\ -allow_physical_feedthrough true \\ -name VA1_rule -voltage_areas VA1  Logical-feedthrough buffering During logical-feedthrough the tool performs the following: 1.\n Adds buffers in the logical hierarchy corresponding to the nonnative voltage area 2.\n Places the buffers within the nonnative voltage area To allow logical-feedthrough buffering, use the create_voltage_area_rule -allow_logical_feedthrough true command.\n To specify logical hierarchies in which feedthrough buffers are allowed or not allowed, use the -include_logical_feedthrough_hierarchy or -exclude_logical_feedthrough_hierarchy option, respectively.\n The following example enables logical-feedthrough buffering for the VA1 voltage area and limits the feedthrough buffers to only the U1 and U2 hierarchical cells: fc_shell> create_voltage_area_rule \\ -allow_logical_feedthrough true \\ -include_logical_feedthrough_hierarchy {U1 U2}\\ -name VA1_rule -voltage_areas VA1 The following example enables logical-feedthrough buffering for the VA2 voltage area and excludes the U3 hierarchical cell from feedthrough buffering: fc_shell> create_voltage_area_rule \\ -allow_logical_feedthrough true \\ -exclude_logical_feedthrough_hierarchy {U3}\\ -name VA2_rule -voltage_areas VA2       Note: To add a logical-feedthrough buffer to a lower-level block, the tool has to add new boundary ports to that block.\n Therefore, freezing the boundary of a block by using the set_freeze_ports command prevents the tool from adding a logical-feedthrough buffer to that block.\n  Difference in the Logical View After Physical- and Logical-Feedthrough Buffering To create a default rule that applies to all voltage areas that do not have a specific rule, use the create_voltage_area_rule command with the -default_rule option, as shown in the following example: fc_shell> create_voltage_area_rule -default_rule \\ -allow_physical_feedthrough true If a voltage area does not have a specific rule, and there is no default rule, feedthrough buffering for that voltage area is controlled by the opt.common.allow_physical_feedthrough application option setting.\n To report voltage area rules, use the report_voltage_area_rules command.\n To remove voltage area rules, use the remove_voltage_area_rules command."}
{"instruction": "How do I Linking to ETMs at the Top Level", "input": "", "output": "After you create an ETM for each mode and corner combination, use the Library Manager tool to combine the ETMs with the corresponding physical information and create a cell library.\n For more information, see the Library Manager User Guide The following Library Manager scripts combine the ETMs created for every mode and corner of the AMS_BLK design with the corresponding physical information and creates the corresponding cell library: # Create a library work space create_workspace -flow etm_moded AMS_BLK  # Read the physical data (frame view) read_ndm -views frame AMS_BLK.ndm  # Read the ETMs for every scenario read_db -mode_label m1 AMS_BLK_m1_c1.db read_db -mode_label m1 AMS_BLK_m1_c2.db read_db -mode_label m1 AMS_BLK_m1_c3.db read_db -mode_label m2 AMS_BLK_m2_c1.db read_db -mode_label m2 AMS_BLK_m2_c2.db read_db -mode_label m2 AMS_BLK_m2_c3.db   # Check the compatibility of the libraries you read in check_workspace  # Generate cell library commit_workspace -output AMS_BLK_ETM.ndm When you create a cell library for an ETM, you can use one of the following methods to obtain the physical data:  Read the frame view of the corresponding block by using the read_ndm -views frame command.\n  Read a LEF file for the block by using the read_lef command.\n  Read a GDSII file for the block by using the read_gds command  Read an OASIS file for the block by using the read_oasis command."}
{"instruction": "How do I Setting Checking Options", "input": "", "output": "Note: Missing via and unconnected pin checking is supported in both RedHawk Fusion and RedHawk-SC Fusion analysis flows.\n Before you check for missing vias and unconnected pins, make sure you have the required input files as described in Preparing Design and Input Data for Rail Analysis Check for missing vias or unconnected pins in the block during voltage drop analysis or in a separate run.\n For example, you can perform the checking after completing the power       structure and before running the place_opt command.\n This allows you to find power network errors in the design before rail analysis.\n By default, RedHawk/RedHawk-SC Fusion detects the following types of power network errors:  Unconnected pin shapes: Pin shapes that are not contiguously and physically connected to an ideal voltage source.\n  Missing vias: If two overlapping metal shapes do not contain a via within their overlapping area, it is considered a potential missing via error.\n To check for and fix missing vias or unconnected pins, 1.\n Define configuration options by using the set_missing_via_check_options command, as described in Setting Checking Options 2.\n Save the block by using the save_block command.\n 3.\n Perform the checking by using the analyze_rail -check_missing_via command, as described in Checking Missing Vias and Unconnected Pins.\n In the RedHawk-SC Fusion flow, you must specify the -check_missing_via option together with the -voltage_drop option.\n 4.\n Examine the checking results by opening the generated error files in the error browser or writing the checking results to an output file, as described in Viewing Error Data and Writing Analysis and Checking Reports 5.\n Insert PG vias to fix the reported missing vias, as described in Fixing Missing Vias"}
{"instruction": "How do I Reporting Routing Corridors", "input": "", "output": "You can make the following modifications to an existing routing corridor:  Add new shapes to the routing corridor.\n To add new shapes, use the create_routing_corridor_shape command.\n Use the -routing_corridor option to specify the routing corridor that you want to update.\n You must specify the boundary of the routing corridor by using the -boundary or -path option (for details about these options, see Defining Routing Corridors ).\n You can also specify the minimum and maximum routing layers for the shape by using the -min_layer_name and -max_layer_name options.\n  Remove rectangles from the routing corridor.\n To remove shapes, use the remove_routing_corridor_shapes command.\n  Change the nets and supernets associated with the routing corridor.\n To add nets or supernets to a routing corridor, use the add_to_routing_corridor command.\n To remove nets or supernets from a routing corridor, use the remove_from_routing_corridor command.\n You can also modify routing corridors in the GUI by using the Create Route Corridor tool, the Move/Resize tool, or the Delete tool, or by editing the attributes in the Properties dialog box."}
{"instruction": "How do I Performing Metal Fill Insertion", "input": "", "output": "Before you run the signoff_create_metal_fill command, configure the run by setting the application options shown in Table 49.\n To set the application options, use the set_app_options command.\n To see the current settings, use the report_app_options command.\n Table 49 Application Options for Signoff Metal Fill Insertion Application option Default Description Options that apply to all flows signoff.create_metal_fill.\n apply_nondefault_rules false   signoff.create_metal_fill.\n auto_eco_threshold_value    -auto_eco true  signoff.create_metal_fill.\n flat false  false true signoff.create_metal_fill.\n read_design_views       signoff.create_metal_fill.\n read_layout_views             Table 49 Application Options for Signoff Metal Fill Insertion (Continued) Application option Default Description signoff.create_metal_fill.\n run_dir    signoff_create_metal_fill     signoff.create_metal_fill.\n user_defined_options         signoff.physical.\n merge_exclude_libraries     signoff.physical.\n merge_stream_files  signoff.physical.\n merge_stream_files        Options that apply only to pattern-based metal fill insertion signoff.create_metal_fill.\n runset      signoff.physical.\n layer_map_file           Options that apply only to track-based metal fill insertion       Table 49 Application Options for Signoff Metal Fill Insertion (Continued) Application option Default Description signoff.create_metal_fill.\n max_density_threshold     layer_name max_density   -mode add  Options that apply only to timing-driven metal fill insertion (pattern-based or track-based) signoff.create_metal_fill.\n fill_over_net_on_adjacent_la yer false      signoff.create_metal_fill.\n fill_shielded_clock false    signoff.create_metal_fill.\n fix_density_errors false    signoff.create_metal_fill.\n space_to_clock_nets       -nets   signoff.create_metal_fill.\n space_to_nets        signoff.create_metal_fill.\n space_to_nets_on_adjacent_la yer"}
{"instruction": "How do I Removing Cell Spacing Constraints", "input": "", "output": "To report the cell spacing rules, use the report_spacing_rules comandcomandsreport_spacing_rules report_placement_spacing_rules command.\n This command reports the cell spacing labels defined in the current session, the library cells (and their sides) to which they apply, and the rules defined for them."}
{"instruction": "How do I Improving Latency-Driven Clock-Gate Splitting", "input": "", "output": "The Fusion Compiler tool enables you to construct a faster clock tree to specified endpoints without any additional detouring or delay balancing.\n This improves clock latency to the specified endpoints.\n To achieve a faster clock tree to specific endpoints, you must create a user skew group for the required endpoints.\n To create a skew group for a specific endpoint, use the following command before running the synthesize_clock_tree or the build_clock stage of the clock_opt command: fc_shell> create_clock_skew_group -objects <endpoints> Note:  If there is a balancing requirement between the fast endpoints, you must put them in the same skew group.\n  If there is no balancing requirement between the endpoints, you must create separate skew groups for each endpoint.\n The upstream clock logic of the required skew group endpoints is shared with the endpoints of the default group or other user skew groups.\n This leads to detouring for balancing among the endpoints of the default group or other user skew group.\n        Faster Clock Tree to Specific Endpoints If a gate drives both skew group and non-skew group endpoints, the skew group endpoints might be delayed due to the balancing requirements of the non-skew group endpoints.\n In such cases, in addition to creating skew groups, you must also completely isolate the skew group endpoints from the rest of the clock tree by cloning the gates along the path to these endpoints.\n To create a separate clock path from the clock source to the required endpoints, use the -to option with the split_clock_cells command before clock tree synthesis, that is, before using the synthesize_clock_trees or the build_clock stage of the clock_opt command.\n fc_shell> split_clock_cells -to $endpoints The Fusion Compiler tool enables you to construct a faster clock tree by identifying the optimal cloning solution to achieve the best latency for the group of specified sinks.\n To achieve a fast clock tree to certain endpoints, use the split_clock_cells -to < $sinks> -latency_driven command.\n The endpoints must also be created as part of the user skew group using the create_clock_skew_group command."}
{"instruction": "How do I Implementing a Regular Multisource Clock Tree With an", "input": "", "output": "A regular multisource clock tree has tap drivers that are driven by a clock mesh and drive clock-gating cells and sinks.\n If you insert these tap drivers before you run the compile_fusion command, the tool can assign sinks to these drivers such that it improves the overall timing QoR of the block.\n It also allows the tool to improve the timing of clock-gating paths.\n To perform regular multisource clock tree synthesis integrated with placement and optimization, use the following steps: 1.\n Specify the placement and optimization constraints and settings.\n 2.\n Specify the clock tree constraints and settings.\n       3.\n Perform the following steps for multivoltage designs: a.\n Enable multivoltage support by setting the cts.multisource.enable_full_mv_support application option to true b.\n Enable the addition of physical feedthrough cells, which belong to a voltage area physically but not logically, by setting the opt.common.allow_physical_feedthrough application option to true c.\n Run the check_mv_design command and fix all multivoltage issues.\n 4.\n Enable cell electromigration fixing by setting the cts.multisource.cell_em_aware application option to true 5.\n Insert the tap drivers by using the create_clock_drivers command, as described in Inserting Clock Drivers 6.\n Create the clock mesh by using the create_clock_straps command, as described in Creating Clock Straps 7.\n Insert the mesh drivers by using the create_clock_drivers command, as described in Inserting Clock Drivers 8.\n Build the global clock tree structure by using the synthesize_multisource_global_clock_trees command, as described in Synthesizing the Global Clock Trees 9.\n Route the connections between the clock mesh and the tap drivers by using the route_clock_straps command, as described in Routing to Clock Straps 10.\n Analyze the clock mesh by using the analyze_subcircuit command, as described in Analyzing the Clock Mesh 11.\n Specify options and settings for tap assignment as described in Specifying Tap Assignment Options and Settings 12.\n Enable integrated tap assignment by setting the compile.flow.enable_multisource_clock_trees application option to true and run the compile_fusion -from initial_place command.\n fc_shell> set_app_options \\ -name compile_flow.flow.enable_multisource_clock_trees -value true fc_shell> compile_fusion -initial_place 13.\n Synthesize the entire clock tree, from the clock root, by using the following command: fc_shell> clock_opt -from build_clock -to route_clock During this step, the tool builds the local subtrees that are driven by the tap drivers.\n It also fixes DRC violations beyond ignore exceptions of the multisource clock tree, if applicable.\n You can also synthesize other clocks in the design that are not synthesized.\n       For more information about the clock_opt command, see Synthesizing, Optimizing, and Routing Clock Trees With the clock_opt Command 14.\n Analyze the clock tree results as described in Analyzing the Clock Tree Results"}
{"instruction": "How do I Avoiding Crosstalk During Group Repeater Insertion", "input": "", "output": "Use the -honor_special_cell option to avoid overlapping with existing tap cells when inserting repeaters."}
{"instruction": "How do I Setting Options for Signoff DRC Fixing", "input": "", "output": "The autofix configuration file has the following format: mask_layer_name \" \" full_IC_Validator_DRC_rule_comment You can put comments in the configuration file by starting the line with the pound sign (#).\n       If you have already run the signoff_check_drc command, you can generate a configuration file by running the $ICV_HOME_DIR/contrib/generate_layer_rule_map.pl script.\n This script has the following syntax: generate_layer_rule_map.pl -dplog IC_Validator_log_file -tech_file technology_file -o config_file_name The IC Validator log file that you specify in the -dplog option is the log file that was generated by the previous signoff_check_drc run.\n This file is called runset.dp.log and is located in the run_details subdirectory of the signoff_drc_run directory (or the directory specified by the signoff.check_drc.run_dir application option.\n To specify the location of the configuration file you created, use the signoff.fix_drc.config_file application option."}
{"instruction": "How do I Reporting the Boundary Cell Insertion Requirements", "input": "", "output": "If your technology has minimum jog or minimum separation requirements, you can create placement blockages for these requirements during boundary cell insertion with the compile_boundary_cells command by using the following options with the set_boundary_cell_rules command:  -min_horizontal_jog If a horizontal edge of a macro, including its hard keepout margin, hard placement blockage, or voltage area, including its guard band, is less than the specified value, the compile_boundary_cells -add_placement_blockage command creates a placement blockage to prevent a violation.\n  -min_vertical_jog If a vertical edge of a macro, including its hard keepout margin, hard placement blockage, or voltage area, including its guard band, is less than the specified value, the compile_boundary_cells -add_placement_blockage command creates a placement blockage to prevent a violation.\n  -min_horizontal_separation If the continuous horizontal placement area is less than the specified value, the compile_boundary_cells -add_placement_blockage command creates a placement blockage to prevent a violation.\n  -min_vertical_separation If the continuous vertical placement area is less than the specified value, the compile_boundary_cells -add_placement_blockage command creates a placement blockage to prevent a violation.\n Note: By default, the compile_boundary_cells command checks the specified rules, but does not create the placement blockages.\n To create the placement blockages, you must use the -add_placement_blockage option with the compile_boundary_cells command.\n This feature is supported only by the compile_boundary_cells command; it is not supported by the compile_targeted_boundary_cells command."}
{"instruction": "How do I Running Tasks in Parallel", "input": "", "output": "To remove the multicore configurations defined by the set_host_options command, use the remove_host_options command.\n To remove all multicore configurations, use the -all option.\n To remove a specific multicore configuration, specify the configuration name with the -name option."}
{"instruction": "How do I Voltage Hotspot Analysis", "input": "", "output": "When you use the RedHawk mesh add command to add virtual PG meshes in the top block for the current subblock, by default the virtual PG meshes created by the RedHawk mesh add command are not shown as layout objects in the GUI.\n Therefore, these GUI shapes cannot be queried using layout object collection commands (such as, get_shapes ), and are not saved to the design library when you save the block.\n To save ECO shapes along with rail results and display them in the GUI, enable the rail.display_eco_shapes application option, like set_app_options -name rail.display_eco_shapes -value true Note: The mesh add command is available only in RedHawk, not in RedHawk SC.\n Therefore, the tool issues an error message when you enable both the rail.enable_redhawk_sc and rail.display_eco_shapes application options.\n To display ECO shapes in GUI, you must first enable the RedHawk signoff license key by setting the rail.allow_redhawk_license_checkout application option to true.\n Otherwise, an error message is issued.\n To add virtual PG meshes and display the created ECO shapes in the GUI, 1.\n Prepare a script file (such as, mesh.tcl) that contains the mesh add command.\n 2.\n Run design setup with the mesh.tcl script file.\n fc_shell> set_rail_command_options -script_file mesh.tcl \\ -command setup_design -order after_the_command The tool generates the RedHawk script file and sources the mesh.tcl file after design setup.\n 3.\n Perform rail analysis using the following command: fc_shell> analyze_rail -nets -voltage_drop 4.\n Run the open_rail_result command to load the design data and analysis results.\n 5.\n Run the gui_start command to open the Fusion Compiler GUI.\n In the GUI layout window, choose View > Map and select the map to display.\n You can now display and query the ECO shapes that are created by the RedHawk mesh add command in the GUI.\n Script Examples       The following script displays ECO shapes in the GUI by using the set_rail_command_option command.\n ## Open design ## open_block link_block ## Specify taps ## create_taps ## Specify RedHawk Fusion input files or variables ## set_app_options -name rail.enable_redhawk -value 1 set_app_options -name rail.redhawk_path -value set_app_options -name rail.disable_timestamps -value true set_app_options -name rail.display_eco_shapes -value true set_rail_command_options -script_file mesh.tcl -command \\ setup_design...\n ## Analyze## analyze_rail -voltage_drop...\n -nets {VDD VSS} ## Check GUI ## open_rail_result The following script displays ECO shapes in the GUI by using a RedHawk script.\n ## Open design ## open_block link_block ## Specify taps ## create_taps ## Specify RedHawk Fusion input files or variables ## set_app_options -name rail.enable_redhawk -value 1 set_app_options -name rail.redhawk_path -value set_app_options -name rail.disable_timestamps -value true set_app_options -name rail.display_eco_shapes -value true ## Analyze ## analyze_rail -voltage_drop...\n -nets {VDD VSS} -script_only ## Modify the script.tcl containing mesh add command ## analyze_rail -redhawk_script_file ## Check GUI ## open_rail_result"}
{"instruction": "How do I Controlling Congestion-Driven Restructuring During Placement", "input": "", "output": "You can control the placement density of a block as described in the following table.\n Table 10 Application Options for Controlling Placement Density To do this Use this application option    place.coarse.auto_density_control -enhanced  place.coarse.auto_density_control false place.coarse.auto_density_control  place.coarse.max_density         place.coarse.congestion_driven_max_util 0.93 When specifying the maximum density or maximum utilization, choose a value between 1 and the overall utilization of the block.\n For example, if the utilization of a block is 40 percent, you can choose values between 1 and 0.4, as shown in the following example: fc_shell> set_app_options -name place.coarse.max_density -value 0.6 fc_shell> set_app_options \\ -name place.coarse.congestion_driven_max_util -value 0.8 If you do not specify a value for the maximum density or maximum utilization, the tool automatically derives a value for the maximum density and maximum utilization based on the stage of the design flow, considering the place.coarse.auto_density_control application option is not set to false By default, the place.coarse_auto_density_control application option is enabled and set to enhanced.\n This improves the total power and wire length for the design.\n Note: It is not necessary to disable the place.coarse.auto_density_control application option to override the max_density and congestion_driven_max_util values.\n User settings always take precedence over the tool derived values.\n Throughout placement, the tool prints PLACE-027 information messages indicating the values it uses for these settings.\n       Information: Automatic density control has selected the following settings: max_density 0.60, congestion_driven_max_util 0.77.\n (PLACE-027) The tool applies the maximum density and congestion-driven maximum utilization settings independently to each placeable area such as a voltage area or exclusive move bound, rather than taking an average over the entire block."}
{"instruction": "How do I Defining Placement Bounds", "input": "", "output": "To remove placement blockages from a block, use the remove_placement_blockage comandcomandsremove_placement_blockage remove_placement_blockages command.\n To remove all placement blockages from a block, use the -all option.\n To remove specific placement blockages from a block, specify the placement blockage names."}
{"instruction": "How do I Inserting Diodes During Detail Routing", "input": "", "output": "antena rulescheckingenabling If the design library contains antenna rules, Zroute automatically analyzes and antena violationsenablingfixing fixes antenna violations.\n antena violationsfixingdisablingantena rulescheckingdisabling To disable the analysis and correction of antenna rules during detail routing, set the route.detail.antenna application option to false Just like other design rules, antenna rules are checked and corrected during detail routing.\n This concurrent antenna rule correction architecture reduces total runtime by minimizing the iterations.\n By default, Zroute  Checks antenna rules and corrects violations for all clock and signal nets To disable fixing of antenna violations on specific nets, set the route.detail.skip_antenna_fixing_for_nets application option.\n Note that Zroute analyzes the antenna rules and reports the antenna violations on these nets, but does not fix the violations.\n  Does not check or correct antenna rules for power and ground nets To check and correct antenna rules for power and ground nets, set the route.detail.check_antenna_on_pg application option to true  Starts fixing antenna violations in the second iteration, after initial routing is complete and the basic DRC violations have been fixed To change the iteration in which Zroute starts fixing antenna violations, set the route.detail.antenna_on_iteration application option.\n        Performs layer hopping to fix antenna violations Layer hopping decreases the antenna ratio by splitting a large metal polygon into several upper-level polygons.\n Zroute performs the following types of layer hopping:  Breaking the antenna with a higher-level metal segment Zroute uses this technique to fix most antenna violations.\n For antenna violations that happen at metal-N, inserting a small segment of metal-(N+1) close to the gate reduces the ratio between the remaining metal-N, making the ratio much lower.\n This approach is not suitable for fixing top-metal layer antenna violations when the output pin can provide only limited protection because there is no way for the router to break antenna violations at the topmost metal layer.\n  Moving down to a lower-level metal Zroute uses this technique to fix only topmost layer antenna violations when output pins provide only limited protection.\n For antenna violations that happen at metal-N, replace part of the metal-N with metal-(N-1 or lower) to reduce the ratio.\n However, splitting the metal layer into many pieces might have a negative impact on RC and timing delay.\n Zroute can also insert diodes to fix antenna violations.\n To enable the insertion of diodes to fix antenna violations, set the route.detail.insert_diodes_during_routing application option to true.\n To force Zroute to fix antenna violations by inserting diodes, disable layer hopping by setting the route.detail.hop_layers_to_fix_antenna application option to false.\n For information about inserting diodes to fix antenna violations, see Inserting Diodes During Detail Routing If both layer hopping and diode insertion are enabled, by default, Zroute first tries to use layer hopping to fix the antenna violation.\n To change the preference to diode insertion, set the route.detail.antenna_fixing_preference application option to use_diodes To delete the redundant diodes that are not needed to fix antenna violations, set the route.detail.delete_redundant_diodes_during_routing application option to true Spare diodes are not deleted as they do not connect to any nets.\n Port protection diodes that are inserted by detail route are not deleted.\n As with other design rule violations, antenna violations are reported at the end of each detail routing iteration.\n For example, DRC-SUMMARY: @@@@@@@ TOTAL VIOLATIONS = 506 @@@@ Total number of instance ports with antenna violations = 1107 antena ruleschecking To check for antenna violations, use the check_routes -antenna true command."}
{"instruction": "How do I Writing Analysis and Checking Reports", "input": "", "output": "RedHawk Fusion or RedHawk-SC Fusion does not support RedHawk signoff analysis capabilities, such as signal electromigration or inrush current analysis.\n If you have the RedHawk signoff licenses, you can enable the following RedHawk signoff analysis features in the Fusion Compiler environment by using the rail.allow_redhawk_license_checkout application option.\n Signoff analysis features not included in the following list can only be run with the RedHawk standalone tool.\n        Dynamic analysis with custom macro models  Dynamic analysis with package models  Analysis with RTL-level VCD files To perform RedHawk signoff analysis, 1.\n Modify your GSR file or RedHawk run script to include the related configuration settings.\n 2.\n Set the following application option to enable the RedHawk signoff features: fc_shell> set_app_options \\ -name rail.allow_redhawk_license_checkout -value true Setting the rail.allow_redhawk_license_checkout application option to true allows the RedHawk tool to retrieve the related RedHawk licenses for performing signoff features inside the Fusion Compiler tool.\n The default is off 3.\n Run the analyze_rail command with the -extra_gsr_option_file or -redhawk_script_file option to perform the analysis defined in the GSR or RedHawk script file.\n Specify other settings as necessary.\n Note: You cannot display the RedHawk signoff analysis results in the Fusion Compiler GUI.\n 4.\n Proceed to other steps in the analysis flow.\n See Also  An Overview for RedHawk Fusion and RedHawk-SC Fusion"}
{"instruction": "How do I Viewing Metal Fill in the GUI", "input": "", "output": "Metal fill shapes are stored in fill cells, which use the following naming convention: FILL_INST_#.\n  To query the fill cells in the block, use the get_fill_cells command.\n By default, this command reports only the fill cells in the top-level design.\n To report all of the fill cells in the hierarchy, use the -hierarchical option.\n  To query the fill shapes, use the get_shapes -include_fill command.\n To return only the fill shapes associated with specific fill cells, use the -of_objects option to specify the fill cells."}
{"instruction": "How do I Physical Synthesis Design Flow Overview", "input": "", "output": "Physical Synthesis Design Flow Overview  Formal Verification  Place and Route Design Flow Overview  Fusion Compiler Concepts  Working With the 3DIC Compiler User Interfaces  Entering fc_shell Commands  Using Application Options  Using Variables  Viewing Man Pages  Using Tcl Scripts  Adding Changes to a Script With Checkpoints  Using Setup Files  Using the Command Log File  Enabling Multicore Processing For information about working with design data in the Fusion Compiler tool, see the Fusion Compiler Data Model User Guide"}
{"instruction": "How do I Protecting Via Ladders", "input": "", "output": "To insert via ladders, use the insert_via_ladders command.\n By default, this command  Does not remove existing via ladders in the block To remove the existing via ladders before via ladder insertion, use the -clean true option.\n  Targets all pins specified in the via ladder constraints, whether on signal nets or clock nets To restrict via ladder insertion to specific nets, use the -nets option.\n  Inserts a single via ladder on each target pin If the constraints specify more than one via ladder template for a pin, the command inserts the via ladder using the first template that does not cause a DRC violation.\n  By default, if all of the via ladder templates cause a DRC violation, the command inserts the via ladder with the lowest DRC cost.\n  To ignore detail routing shapes when checking for DRC violations, use the -ignore_routing_shape_drcs true option.\n  To ignore the line-end cut enclosure rule, use the -relax_line_end_via_enclosure_rule true option.\n  To ignore metal spacing rules on the pin layer when the via ladder shape is fully enclosed within the pin, use the -relax_pin_layer_metal_spacing_rules true option.\n  To prevent the insertion of via ladders that cause DRC violations, use the -allow_drcs false option.\n Note: If a pin is too small to accommodate any of the via ladders specified for it, the insert_via_ladders command does not insert a via ladder for that pin.\n  Uses via cuts other than those specified in the template if they improve compliance with fat via rules To require the command to use only those via cuts specified in the template, use the -strictly_honor_cut_table true option.\n  Centers the via ladder cuts at the intersection of routing tracks between adjacent layers To shift the via ladder cuts on transition layers off the routing tracks for improved DRC compliance, use the -shift_vias_on_transition_layers true option.\n        Honors nondefault width rules for the wires on all layers of the via ladder To use the default width for wires on lower layers of the via ladder and honor the nondefault width only for the top layer, use the -ndr_on_top_layer_only true option.\n  Does not require the via inserted above the pin layer to be contained within the pin shape To prevent changes to the pin shape boundary, you can require that the via inserted above the pin layer be fully contained within the pin shape boundary, which includes the pin shape and its extensions.\n To specify this requirement, use one or both of the following options:  -connect_within_metal true This option controls via enclosures for all types of via ladders: performance, electromigration, and pattern-must-join.\n  -connect_within_metal_for_via_ladder true This option controls the via enclosures only for performance and electromigration via ladders.\n By default, the tool uses the global setting specified by the -connect_within_metal option.\n To explicitly specify the behavior for performance and electromigration via ladders, set this option to true or false  Does not extend the via enclosures to meet the minimum length rule for the layer To enable patching of the via enclosures to meet the minimum length rule and allow additional stacking of the via ladder, use the -allow_patching true option.\n  Uses staggering only when it is defined for the via ladder rule Via ladder staggering extends the row metal of a via ladder level in the preferred direction of the upper layer for the level to avoid obstructions above or near the pin, which can increase the success rate of via ladder insertion.\n In many cases, the via ladder rule specifies the maximum number of stagger tracks.\n To allow staggering if the maximum number of stagger tracks is not defined for the via ladder rule, use the -auto_stagger true option.\n  Does not report on the insertion status To report the via ladder insertion status for each target pin, use the -verbose true option.\n       To output detailed information about insertion failures, use the -user_debug true option.\n The detailed information reports the following types of violations:  Internal DRC violations These are DRC violations internal to a via ladder.\n These violations are never allowed.\n  Hard DRC violations These are DRC violations between a via ladder and a fixed shape, such as a pin, preroute, or obstruction.\n These violations are never allowed.\n  Soft DRC violations These are DRC violations between a via ladder and a detail routing shape, which can be rerouted.\n These violations are allowed if one or more of the following options are true -allow_drcs -ignore_rippable_shapes, or -ignore_routing_shape_drcs After inserting the via ladders, the command reports statistics on the inserted ladders and any DRC violations caused by the insertion.\n See Also  Defining Via Ladder Constraints  Verifying Via Ladders  Updating Via Ladders  Querying Via Ladders  Removing Via Ladders"}
{"instruction": "How do I Removing Multicore Configurations", "input": "", "output": "To report the values set by the set_host_options command, use the report_host_options command."}
{"instruction": "How do I Using the compile_fusion Command", "input": "", "output": "The Fusion Compiler tool performs unified physical synthesis.\n The benefits of unified physical synthesis include  Reduced runtime Unified physical synthesis streamlines the iterations of placement and optimization, and utilizes the initial placement and buffer trees built during synthesis to achieve the best runtime.\n  Improved quality of results (QoR) The unified preroute optimization framework uses consistent costs in optimization algorithms to improve QoR.\n The best technologies, such as logic restructuring, concurrent clock and data optimization, multibit mapping, and advanced legalizer, are shared throughout the preroute flow.\n The Fusion Compiler compilation includes both top-down and bottom-up (hierarchical) compile strategies.\n In a top-down compile, all environment and constraint settings are       defined with respect to the top-level design.\n Top-down compile provides a push-button approach and handles interblock dependencies automatically, but it requires that all designs must reside in memory at the same time.\n In a hierarchical compile flow, you start optimization of design subblocks using a top-down approach and then continue incorporating the subblocks into higher-level blocks until the top-level design is complete.\n The following topics describe how perform unified physical synthesis:  Using the compile_fusion Command  Performing Prechecks  Generating Verification Checkpoints During Compilation"}
{"instruction": "How do I Viewing Your Checkpoint History", "input": "", "output": "To return a list of all the checkpoints or checkpoint behaviors in the current session, or to return detailed information for any checkpoint or checkpoint behavior, use the get_checkpoint_data command.\n  To return a list of all the checkpoints that have been executed, specify the -list_names option: fc_shell> get_checkpoint_data -list_names remove_buffers place_opt_to_initial_drc incr_placement place_opt_to_initial_opto place_opt_to_final_opto  To return detailed information for a checkpoint, specify the checkpoint with the -name option.\n The tool returns the output as a Tcl dictionary: fc_shell> get_checkpoint_data -name place_opt_to_initial_drc memory 173.85 start_time 2.34 end_time 2.34 before_runtime 2.34 before_report_runtime 0.00 after_report_runtime 0.00 self_runtime       0.00 before_reports {} after_reports {app_options timing} before_actions {gropto} after_actions {} replace_actions {}  To return a list of all the checkpoint behaviors defined in your configuration, specify the -list_reports or -list_actions option: fc_shell> get_checkpoint_data -list_reports timing app_options  fc_shell> get_checkpoint_data -list_actions gropto placer_high_effort_congestion  To return the contents and associations of a checkpoint report or action, specify the report or action with the -report or -action option: fc_shell> get_checkpoint_data -report app_options contents { set name [get_current_checkpoint -name] set pos [get_current_checkpoint -position]  report_app_options -non_default \\ >./checkpoint/$name.$pos.app_options.rpt } before_patterns {*} after_patterns {} See Also  Viewing Your Checkpoint History"}
{"instruction": "How do I Automatically Fixing Isolated Vias", "input": "", "output": "To enable metal fil extractionreal real metal fill extraction, 1.\n Associate non-emulation TLUPlus files with the timing corners by using the set_parasitic_parameters command, as described in the Fusion Compiler Timing Analysis User Guide 2.\n Enable real metal fill extraction by using the following command: fc_shell> set_extraction_options \\ -real_metalfill_extraction floating For more information about performing extraction in the Fusion Compiler tool, see the Fusion Compiler Timing Analysis User Guide"}
{"instruction": "How do I Preparing for Top-Level Closure With Abstracts", "input": "", "output": "For designs with physical hierarchy, you can specify if changes can be made to lower-level blocks by using the set_editability command at the top level.\n You can change the editability of  Specific blocks by using the -blocks option  All blocks starting from a specific level of the physical hierarchy by using the -from_level option  All blocks up to a specific level of the physical hierarchy by using the -to_level option For top-level implementation flows with read-only abstracts, after you link the design, explicitly set the lower-level blocks as read-only by using the following command: fc_shell> set_editability -blocks [get_blocks -hierarchical] \\ -value false By default, if you change a library or block name with one of the following commands, the tool does not propagate the editability settings from the old reference to the new reference:  change_abstract -lib  link_block -rebind -force  set_reference To retain the editability settings, set the design.preserve_reference_editability application option to true before you change a library or block name.\n When you save the hierarchical design with the save_block command, the editability settings are saved."}
{"instruction": "How do I Performing Multibit Optimization", "input": "", "output": "The tool performs high-fanout-net synthesis during the initial_drc stage of the compile-fusion command.\n To use globally routed nets to determine the topology for the buffers inserted during high-fanout synthesis, set the compile.initial_drc.global_route_based application option to true.\n After high- fanout synthesis, which is performed during the DRC fixing stage of the compile_fusion command, the global routes are deleted.\n Global-route-based buffering benefits designs with high congestion in routing channels such as the channels between banks of macros.\n This feature increases the runtime of the compile_fusion command.\n Therefore, you should enable it only for designs with highly congested routing channels."}
{"instruction": "How do I Manually Instantiating Spare Cells", "input": "", "output": "The Fusion Compiler ECO Fusion power integrity flow allows you to use the RedHawk Fusion rail analysis feature to identify power integrity issues and the PrimeTime physically aware ECO capabilities to fix them.\n       The Fusion Compiler ECO Fusion power integrity flow consists of the following steps: 1.\n Specify the path for saving the RedHawk Fusion rail analysis results by setting the rail.database application option.\n 2.\n Perform dynamic-vectorless or dynamic-vector-based voltage-drop analysis by using the analyze_rail -voltage_drop dynamic_vcd or analyze_rail -voltage_drop dynamic_vectorless command.\n 3.\n Specify the settings required to run the PrimeTime ECO capabilities within the Fusion Compiler tool by using the set_pt_options command.\n 4.\n Fix the identified power integrity issues by using the eco_opt -type power_integrity command.\n To fix power integrity (voltage drop) violations, the tool downsizes aggressor cells while considering the timing and DRC QoR.\n However, for this optimization to be effective,  The aggressor cells should not be timing- or DRC-critical  The cell library should have smaller versions of the aggressor cells without dont_use or dont_touch attribute settings.\n The following is an example script for running the ECO Fusion power integrity flow: set_app_options -name rail.database -value RAIL_DATABASE_BASELINE analyze_rail -voltage_drop dynamic_vcd -all_nets \\ -switching_activity {VCD./top.vcd top} set_pt_options -pt_exec /snps_tools/PT/pt_shell eco_opt -type power_integrity"}
{"instruction": "How do I Creating Hierarchical Relative Placement Groups", "input": "", "output": "Hierarchical relative placement allows relative placement groups to be embedded within other relative placement groups.\n The embedded groups then are handled similarly to leaf cells.\n       You can use hierarchical relative placement to simplify the expression of relative placement constraints.\n With hierarchical relative placement, you do not need to provide relative placement information multiple times for a recurring pattern.\n Using hierarchical relative placement provides these benefits:  Allows you to organize your relative placement in a manner that is easier to maintain and understand.\n For example, you can create the relative placement group to parallel your Verilog or VHDL organization.\n  Allows reuse of a repeating placement pattern, such as an adder.\n  Can reduce the number of lines of relative placement information you need to write.\n  Allows integrating blocks.\n  Provides flexibility for the configuration you want."}
{"instruction": "How do I Manual Via Ladder Insertion", "input": "", "output": "You can use the following methods to update existing via ladders in a design:  Use the refresh_via_ladders command  Automatic updates during the route_group route_auto, and route_eco commands is enabled by default by setting the route.auto_via_ladder.update_during_route application option to true Both of these methods perform the following tasks:  Verifies the existing via ladders based on the current via ladder constraints  Removes invalid via ladders  Inserts via ladders where needed based on the settings of the route.auto_via_ladder application options By default, the tool  Updates all pins with via ladder constraints, whether on signal nets or clock nets By default, during the route_group command, the via ladder updates is restricted to only those nets specified in the route_group command by setting the route.auto_via_ladder.update_on_route_group_nets_only application option to true  Does not require the via inserted above the pin layer to be contained within the pin shape To prevent changes to the pin shape boundary, you can require that the via inserted about the pin layer be fully contained within the pin shape boundary, which includes       the pin shape and its extensions.\n To specify this requirement, set the following application options:  route.auto_via_ladder.connect_within_metal This application option controls via enclosures for all types of via ladders: performance, electromigration, and pattern-must-join.\n To prevent changes to the pin shape boundary, set this application option to true  route.auto_via_ladder.connect_within_metal_for_via_ladder This application option controls the via enclosures only for performance and electromigration via ladders.\n By default, the tool uses the global setting specified by the route.auto_via_ladder.connect_within_metal application option.\n To explicitly specify the behavior for performance and electromigration via ladders, set this application option to true or false  Uses staggering only when it is defined for the via ladder rule Via ladder staggering extends the row metal of a via ladder level in the preferred direction of the upper layer for the level to avoid obstructions above or near the pin, which can increase the success rate of via ladder insertion.\n In many cases, the via ladder rule specifies the maximum number of stagger tracks.\n To allow staggering if the maximum number of stagger tracks is not defined for the via ladder rule, the route.auto_via_ladder.auto_stagger application option is set to true by default.\n  Reports the via ladder insertion status for each target pin To limit the number of via ladders reported for each category to 40, set the route.auto_via_ladder.report_all_via_ladders application option to false See Also  Inserting Via Ladders  Verifying Via Ladders  Querying Via Ladders  Removing Via Ladders"}
{"instruction": "How do I Datapath Extraction", "input": "", "output": "Datapath design is commonly used in applications that contain extensive data manipulation, such as 3-D, multimedia, and digital signal processing (DSP).\n By default, when you run the compile_fusion command, datapath optimization is enabled and both standard and DesignWare Foundation libraries are automatically linked.\n During datapath optimization, the tool performs the following tasks:  Shares (or reverses the sharing) of datapath operators  Uses the carry-save arithmetic technique  Extracts the data path  Performs high-level arithmetic optimization on the extracted data path  Explores better solutions that might involve a different resource-sharing configuration  Makes tradeoffs between resource sharing and datapath optimization Topics in this section  Datapath Extraction Transforms arithmetic operators, such as addition, subtraction, and multiplication, into datapath blocks.\n  Datapath Implementation Uses a datapath generator to generate the best implementations for the extracted components.\n  Optimizing Datapaths"}
{"instruction": "How do I Controlling Concurrent Clock and Data Optimization", "input": "", "output": "This figure shows how concurrent clock and data optimization allows for clock skew to remove negative slack and potential area and leakage recovery."}
{"instruction": "How do I Specifying the Tapering Method", "input": "", "output": "Pin tapering is the method used to connect wires with nondefault routing rules to pins.\n Zroute supports pin tapering for both hard and soft nondefault routing rules; the tapering implementation is the same for both types of nondefault routing rules.\n Note: By default, if a net has both a nondefault routing rule defined by the create_routing_rule command and voltage-based spacing rules defined in the technology file, Zroute performs pin tapering on that net based on the nondefault routing rule.\n To disable pin tapering on these nets, set the route.detail.enable_ndr_tapering_on_voltage_rule application option to false You can specify the method used for pin tapering and control the tapering width, as described in the following topics:  Specifying the Tapering Method  Controlling the Tapering Width"}
{"instruction": "How do I Using Automatic Floorplanning", "input": "", "output": "The read_def command extracts physical constraint information from DEF files and annotates it on the block.\n However, only the following physical constraints are extracted and annotated:  Placement Area  Port Locations  Cell Locations  Placement Blockages        Site Rows  Routing Tracks  Placement Bounds  Routing Blockages  Preroutes To visually inspect the extracted physical constraints, use the layout view in the GUI.\n All physical constraints extracted from the DEF file are automatically added to the layout view.\n Placement Area Placement area is computed based on the site array information.\n Port Locations For each port with the location specified in the DEF file, the tool sets the location on the corresponding port in the block.\n Note: If the DEF file does not contain port-location information, the tool inherits the port locations from the locations of the pad cells, as described in.\n Example 1 DEF Port Location Information PINS 2 ; -Out1 + NET Out1 + DIRECTION OUTPUT + USE SIGNAL + LAYER M3 (0 0) (4200 200) + PLACED (80875 0) N; -Sel0 + NET Sel0 + DIRECTION INPUT + USE SIGNAL + LAYER M4 (0 0)(200 200) + PLACED (135920 42475) N; END PINS Ports with changed names and multiple layers are supported.\n Example 2 DEF Port Locations With Changed Names and Multiple Layers PINS 2 ; - sys_addr\\[23\\].extra2 + NET sys_addr[23] + DIRECTION INPUT +USE SIGNAL + LAYER METAL4 ( 0 0 ) ( 820 5820 ) + FIXED ( 1587825 2744180 ) N ; - sys_addr[23] + NET sys_addr[23] + DIRECTION INPUT + USE SIGNAL + LAYER METAL3 ( 0 0 ) ( 820 5820 ) + FIXED ( 1587825 2744180 ) N ; END PINS Example 3 DEF Port Orientation Information PINS 1; - OUT + NET OUT + DIRECTION INPUT + USE SIGNAL + LAYER m4 ( -120 0 ) ( 120 240 )       + FIXED ( 4557120 1726080 ) S ; END PINS Cell Locations For each cell with a location and the FIXED attribute specified in the DEF file, the tool sets the location on the corresponding cell in the block.\n Example 4 shows DEF macro location and orientation information, where the letters E and W denote east rotation and west rotation respectively.\n Example 4 DEF Cell Location Information COMPONENTS 2 ; - macro_cell_abx2 + FIXED ( 4350720 8160 ) E ; - macro_cell_cdy1 + FIXED ( 4800 8160 ) W ; END COMPONENTS Placement Blockages The read_def command imports hard, soft, and partial placement blockages defined in the DEF file.\n Note: DEF versions before version 5.7 did not support partial blockages.\n In addition, if your floorplanning tool creates a DEF file with DEF version 5.6, you need to manually add the #SNPS_SOFT_BLOCKAGE pragma to specify a soft blockage, as shown in Example 7 Example 5 DEF Hard Placement Blockage Information BLOCKAGES 50 ;...\n - PLACEMENT RECT ( 970460 7500 ) ( 3247660 129940 )...\n END BLOCKAGES Example 6 DEF Version 5.7 Soft Placement Blockage Information BLOCKAGES 50 ;...\n - PLACEMENT + SOFT RECT ( 970460 7500 ) ( 3247660 129940 ) ;...\n END BLOCKAGES Example 7 DEF Version 5.6 Soft Placement Blockage Information BLOCKAGES 50 ;...\n - PLACEMENT RECT ( 970460 7500 ) ( 3247660 129940 ) ; #SNPS_SOFT_BLOCKAGE...\n END BLOCKAGES       Example 8 DEF Partial Placement Blockage Information BLOCKAGES 50 ;...\n - PLACEMENT + PARTIAL 80 RECT ( 970460 7500 ) ( 3247660 129940 ) ;...\n END BLOCKAGES Site Rows Site row information in the DEF file defines the placement area.\n Example 9 DEF Site Row Information ROW ROW_0 core 0 0 N DO 838 BY 1 STEP 560 0; Routing Tracks The track information in the DEF file defines the routing grid for designs based on standard cells.\n This information can be used during routing, and track support can enhance congestion evaluation and reporting to make it match more closely with the routing results.\n Example 10 DEF Routing Track Information TRACKS X 330 DO 457 STEP 660 LAYER METAL1 ; TRACKS Y 280 DO 540 STEP 560 LAYER METAL1 ; Placement Bounds If REGIONS defining bounds exist in the DEF file, the read_def command imports those placement bounds.\n Also, if any cells in the related GROUP are attached to the region, fuzzy cell matching occurs between these cells and the ones in the block.matched cells are attached to the bounds in the following ways:  If there are regions in the block with the same name as in the DEF, the cells in the related group are attached to the region by the add_to_bound command in incremental mode.\n  If the region does not exist in the block, it is created with the same name as in the DEF file by applying the create_bound command; matched cells in the related group are also attached.\n Example 11 DEF Placement Bound Information REGIONS 1 ; - c20_group ( 201970 40040 ) ( 237914 75984 ) + TYPE FENCE ; END REGIONS GROUPS 1 ; - c20_group cell_abc1 cell_sm1 cell_sm2       + SOFT + REGION c20_group ; END GROUPS Routing Blockages Routing blockages are extracted similar to the way that placement blockages are extracted.\n Preroutes The tool extracts preroutes that are defined in the DEF file.\n Example 12 DEF Preroute Information SPECIALNETS 2 ; - vdd + ROUTED METAL3 10000 + SHAPE STRIPE ( 10000 150000 ) ( 50000 * ) + USE POWER ;...\n END SPECIALNETS"}
{"instruction": "How do I Querying Voltage Areas", "input": "", "output": "Gas stations are small voltage areas that are created for the purpose of minimizing the use of dual-rail buffers on physical feedthrough paths.\n Only single-rail repeaters are allowed in gas stations, and only optimization steps can add cells to gas stations.\n You create gas stations by using the create_voltage_area command during the design planning stage.\n The tool recognizes gas stations automatically and uses them by trading off the cost of a routing detour with the cost of using a dual-rail cell.\n Use the -nwell and -pwell options of the create_voltage_area command to specify n-well and p-well supply nets for a voltage area.\n Defining well supplies for gas station voltage areas provides flexibility for using gas stations for different design styles.\n If you do not use these options for a voltage area, the well bias values are assumed to be the same as the domain supply values.\n The report_voltage_areas command lists the n-well and p-well supply nets regardless of whether they are explicitly set."}
{"instruction": "How do I Defining Minimum Wire Lengths", "input": "", "output": "Use the net priority constraint to define the routing order for specific nets.\n Nets with a higher priority are routed first.\n To define the priority for a collection of nets or bundles, create a constraint group by using the create_net_priority command.\n You need to specify the following information:  The name of the constraint group If you do not specify a name, the command assigns a name.\n By default, the command returns an error if the assigned name already exists.\n To delete the existing constraint group, use the -force option.\n  The nets, bundles, and topology edges to which the constraint group applies Use the -for option.\n  The net priority To specify the priority, use the -priority option.\n You can specify an integer between -128 and 128, inclusive.\n The following example creates a constraint group named abc for the nets and bundles that match the pattern pr*.\n The net priority is set to 15.\n fc_shell> create_net_priority abc \\ -for [get_nets -physical_context pr*] \\ -priority 15 {abc}"}
{"instruction": "How do I Report Clock Gating Options", "input": "", "output": "The set_clock_gating_enable command provides an interface to specify signals to be excluded from clock-gating enable.\n The report_clock_gating -ungated command reports a new reason for registers that are not gated due to enable exclusion.\n The options and syntax of the set_clock_gating_enable command are identical to the corresponding Design Compiler command.\n Use this command to control the signals to enable clock gating during clock-gate insertion.\n This command provides a mechanism to specify why a given signal should not be used as an enable condition for clock gating.\n The syntax of the set_clock_gating_enable command is as follows: status set_clock_gating_enable [-exclude objects_to_exclude] [-undo objects_to_remove_exclusion]       The following figure shows a circuit before enable exclusion: In this step, a forward traversal is performed from all user-specified exclusion signals until endpoints (primary output ports, input ports of sequential cells, including clock gating cells, black boxes, and so on) are reached.\n If the endpoint is a clock-gate-enable pin, then all pins on the path are marked with a temporary design attribute for use in subsequent steps.\n       The following figure shows a circuit after enable exclusion:"}
{"instruction": "How do I Defining Ignore Pins", "input": "", "output": "To override the default phase delay of zero for sink pins (derived or user-specified), use the -delay option with the set_clock_balance_points command to specify the       insertion delay requirements.\n The tool adds the specified delay (positive or negative) to the calculated insertion delay up to the specified sink pin.\n The syntax to define the insertion delay requirement for one or more sink pins is set_clock_balance_points [-clock clock -delay delay [-rise] [-fall] [-early] [-late] -balance_points pins [-corners corners The -clock option is not required.\n If you do not use it, the insertion delay requirement applies to all clocks.\n Clock tree synthesis uses the specified delay value for the path delay calculations used to build the clock tree.\n By default, the specified delay value is used for both longest-path and shortest-path calculations for both rising-edge and falling-edge paths.\n To control when the specified delay value is used, use the -rise -fall -early, and -late options.\n When you define insertion delay requirements, by default, they apply to all corners.\n To apply the definition to specific corners, use the -corners option.\n For example, to specify that clock tree synthesis should use an insertion delay of 2.0 ns for rising-edge shortest-path calculations for the U2/CLK pin for the CLK clock in the current corner, use the following command: fc_shell> set_clock_balance_points -clock [get_clocks CLK] \\ -rise -early -delay 2.0 -balance_points [get_pins U2/CLK] To report the user-defined insertion delay for sink pins, use the report_clock_balance_points command.\n This command reports the user-defined sink pins, the derived sink pins with user-specified insertion delay, and the user-defined ignore pins."}
{"instruction": "How do I Reading DEF Files", "input": "", "output": "A floorplan contains physical constraints such as the core area and shape, port locations, macro locations and orientations, and so on, which are required for performing physical synthesis and optimization.\n       If you have a floorplan for your block, read in the floorplan as a DEF file, as described in Reading DEF Files If you do not have a floorplan for your block, you can perform design planning and generate a floorplan as described in the Fusion Compiler Design Planning User Guide During the early stages of design process, you can use automatically-generated floorplan information to run physical synthesis, as described in Using Automatic Floorplanning"}
{"instruction": "How do I Preserving Ports of Existing Hierarchies", "input": "", "output": "By default, optimization can change the pin names of leaf cells during sizing if the functionality of the resulting circuit is equivalent.\n The Fusion Compiler tool automatically updates its version of the internal constraints to reflect the new pin name if the pin is part of an exception constraint.\n If this occurs, it means that the original constraints used to constrain the block in the Fusion Compiler tool cannot be used for signoff purposes; instead, you must write out the constraints from the Fusion Compiler tool and include these constraints with the resulting block.\n This behavior provides the optimization engine with the most flexibility to select cells and improve the cost functions.\n You can restrict this sizing capability so that the constraints remain unchanged through optimization by setting the opt.common.preserve_pin_names application option.\n This application option defaults to the setting of never, and accepts values of either never or always       To restrict sizing to pin-name equivalent cells, use the following command: fc_shell> set_app_options \\ -name opt.common.preserve_pin_names -value always To restore the default behavior, use the following command: fc_shell> set_app_options \\ -name opt.common.preserve_pin_names -value never"}
{"instruction": "How do I Using Hierarchical Relative Placement for Straddling", "input": "", "output": "To create a hierarchical relative placement group by adding a group to another group, use the -rp_group option with the add_to_rp_group command.\n Specify the column and row position within the relative placement group by using the -column and -row options.\n The group you specify with the -rp_group option must be in the same design as the hierarchical group in which you are including it.\n When you include a relative placement group in a hierarchical group, it is as if the included group is directly embedded within its parent group.\n An included group can be used only in a group of the same design and only one time.\n However, a group that contains an included group can be further included in another group in the same design or can be instantiated in a group of a different design.\n The script in Example 43 creates a hierarchical group (rp4) that contains three included groups (rp1, rp2, and rp3).\n Groups rp1, rp2, rp3, and rp4 are all in the design top.\n The contents of groups rp1, rp2, and rp3 are treated as leaf cells when they are included in group rp4.\n You can further include group rp4 in another group in the design top, or you can instantiate group rp4 in a group of a different design.\n The resulting hierarchical relative placement group is shown in Example 43 Including Groups in a Hierarchical Group create_rp_group -name rp1 -columns 2 -rows 1 add_to_rp_group rp1 -cells U1 -column 0 -row 0 add_to_rp_group rp1 -cells U4 -column 1 -row 0  create_rp_group -name rp2 -columns 2 -rows 1 add_to_rp_group rp2 -cells U2 -column 0 -row 0 add_to_rp_group rp2 -cells U5 -column 1 -row 0        create_rp_group -name rp3 -columns 2 -rows 1 add_to_rp_group rp3 -cells U3 -column 0 -row 0 add_to_rp_group rp3 -cells U6 -column 1 -row 0  create_rp_group -name rp4 -columns 1 -rows 3 add_to_rp_group rp4 -rp_group rp1  -column 0 -row 0 add_to_rp_group rp4 -rp_group rp2  -column 0 -row 1 add_to_rp_group rp4 -rp_group rp3  -column 0 -row 2  Including Groups in a Hierarchical Group rp3 rp2 rp1 U3 U2 U4 U5 U6 U1 rp4 rp1 U4 U1 U3 U6 rp2 U2 U5 rp3 col 0 col 1 col 0 col 1 col 0 col 1 row 0 row 0 row 0 row 2 row 0 row 1 col 0"}
{"instruction": "How do I Changing the Block From Abstract to Design View", "input": "", "output": "For a top-level design to link to an abstract view of a lower-level block, the design library containing the abstract view of the lower-level block must be one of the reference libraries of the top level.\n For example, assume you have a top-level design named TOP with two lower-level blocks named BLK1 and BLK2, as shown in the following figure.\n        Top-Level Design With Instantiated Blocks BLK1 TOP BLK2 To link to the abstracts of the BLK1 and BLK2 blocks, the design libraries containing the abstract views of these block must be reference libraries of the TOP design.\n If not, you can add it to the reference libraries of the TOP design by using the set_ref_libs -add command as shown in the following example: current_block TOP set_ref_libs -add../BLK1/BLK1.nib set_ref_libs -add../BLK2/BLK2.nib To report the reference libraries for the current design, use the report_ref_libs command.\n When you link a top-level design, if an abstract view for a lower-level block is available, the tool links to that abstract view by default.\n The default precedence of the different views used when linking lower-level blocks is as follows: 1.\n Abstract view 2.\n Design view 3.\n Frame view 4.\n Outline view"}
{"instruction": "How do I Excluding Specific Path Groups", "input": "", "output": "By default, the tool performs concurrent clock and data optimization on all paths in a block.\n However, you might want to exclude paths that are connected to boundary registers, which are registers in the transitive fanout of input ports and transitive fanin of output ports.\n To exclude boundary paths from concurrent clock and data optimization, set the ccd.optimize_boundary_timing application option to false When you do so, you can selectively ignore the boundary paths of some ports during boundary path exclusion by using the ccd.ignore_ports_for_boundary_identification application option.\n The following example disables concurrent clock and data optimization for all boundary paths except for those connected to the ports named IN_A and OUT_A: fc_shell> set_app_options -name ccd.optimize_boundary_timing \\ -value false fc_shell> set_app_options \\ -name ccd.ignore_ports_for_boundary_identification \\ -value {IN_A OUT_A} Even when you disable concurrent clock and data optimization on boundary registers, the tool can still optimize the clock tree fanin cone of these boundary registers, when doing so improves the timing QoR of other internal registers that share the same clock paths.\n To prevent the clock tree fanin of boundary registers from changing, set the ccd.optimize_boundary_timing_upstream application option to false.\n However, doing so heavily restricts the scope of concurrent clock and data optimization.\n These application option settings affect concurrent clock and data optimization performed during the place_opt clock_opt, and route_opt commands."}
{"instruction": "How do I Specifying Physical Constraints for Placement and Legalization", "input": "", "output": "By default, the tool identifies preexisting clock-gating cells during the execution of the analyze and elaborate commands.\n During compile, the tool inserts one more level of clock-gating cells at the leaf level and implements clock-gating logic using the default       clock-gating settings if you do not specify any clock-gating constraints.\n You cannot disable clock gating.\n Before you set up clock-gating constraints, load your design and libraries in memory and then use the following commands to set up the constraints:  The set_clock_gate_style command To select the type of integrated clock-gating cell, specify the following options: Option Description -test_point  none before after -observation_output  -target  pos_edge_flip_flop neg_edge_flip_flop -objects   The set_clock_gating_options command To define the netlist structure of the inserted clock-gating cells, specify the following options: Option Description minimum_bitwidth   max_fanout    The set_clock_gating_objects command To control clock gating for specified objects and override the default clock-gating behavior that is set during compile, specify the following options.\n An object can be a hierarchical cell, register, power domain, or module.\n Option Description -include   -exclude        Option Description -clear   -reset  set_clock_gating_objects   The default During clock-gating optimization, the tool uses the following setup defaults: Setup Default         For more information, see Setting Up Clock Gating"}
{"instruction": "How do I Inserting Clock Drivers for Designs With Multiple Levels of", "input": "", "output": "Before you insert clock drivers,  Specify all clock tree synthesis settings, including clock routing rules  Specify that the inserted clock drivers should be aligned with the clock straps of the mesh structure by setting the cts.multisource.enable_clock_driver_snapping application option to true During multisource clock tree synthesis, you can use the create_clock_drivers command for the following:  Insert mesh drivers for both regular or structural multisource clock trees.\n  Tap drivers for regular multisource clock trees.\n When you insert mesh or tap drivers,  Specify the loads to drive by using the -loads option.\n The load can be a net or a set of pins or ports connected to the same net.\n  Specify where to add the clock drivers as exact locations by using the -location option.\n Alternatively, you can add the clock drivers in an X by Y grid pattern over the entire core area by using the -boxes option.\n You can limit the X by Y grid pattern to a specific area by using the -boundary option with the -boxes option.\n  Specify a list of cells that can be used as clock drivers by using the -lib_cells option.\n You can specify both single-rail and dual-rail library cells.\n Alternatively, you can select the existing driver of the load net as a template for the clock drivers by using the -template option.\n If the template cell you specify is a buffer or inverter, you can also specify different versions of this buffer or inverter as clock drivers by using the -lib_cells option with the -template option.\n When you insert mesh drivers, to specify that the outputs of all the clock drivers must be connected together to drive the mesh net, use the -short_outputs option.\n The following example creates a grid of eight by eight mesh drivers, places them in a regular pattern that covers the full core area, shorts the outputs of the clock drivers and connects to the net named clk_mesh, and transfers preexisting routing shapes and vias from the net named clk to this net: fc_shell> create_clock_drivers -loads [get_nets clk1_mesh] \\ -boxes {8 8} -lib_cells [get_lib_cells my_lib/CKBUF8X] \\ -short_outputs -output_net_name clk_mesh \\ -transfer_wires_from [get_nets clk]       The following example creates a grid of five by five tap drivers that are placed within the rectangle bounded at the lower-left by (200, 200) and upper right by (1000, 1000).\n fc_shell> create_clock_drivers -loads [get_nets clkA]  -lib_cells [get_lib_cells my_lib/CKBUF8X] \\ -boxes {5 5} -boundary [list {{200 200} {1000 1000}} ] In this example, one of the tap drivers drives all the loads of the original clock net named clkA.\n To distribute the loads among all the clock drivers, you must subsequently perform automated multisource tap assignment by using the synthesize_multisource_clock_taps command, as described in Specifying Tap Assignment Options and Settings You can use the create_clock_drivers to add multiple levels of clock drivers by using the -configuration option and specifying the configuration of drivers at each level.\n The following example inserts three levels of clock drivers.\n The first level consists of one buffer, the second level consists of four inverters in a 2x2 grid, and the third level consists of 16 inverters in a 4x4 grid.\n fc_shell> create_clock_drivers -loads [get_nets clk] \\ -configuration [list \\ [list -level 1 -boxes {1 1} -lib_cells buf32x ] \\ [list -level 2 -boxes {2 2} -lib_cells inv16x ] \\ [list -level 3 -boxes {4 4} -lib_cells inv8x ]] For this example, because there is no bounding box specified at any level, the drivers at each level are evenly distributed in the core area.\n The buffer in the first level is placed at the center of the core area, the four buffers in the next level are placed at the center of the four quadrants of the core area, and so on, resulting in an evenly placed clock drivers that can be routed to form an H-tree structure as shown in  Clock Drivers Placed in an H-Tree Structure After it inserts the clock drivers, the tool marks the clock drivers that it inserts as fixed and don't touched.\n The tool avoids overlapping the clock drivers with other fixed cells, blockages, and macros.\n However, the clock drivers can overlap with cells that are not       fixed.\n The create_clock_drivers command does not legalize the design.\n To do so, run the legalize_placement command.\n If you run the create_clock_drivers command multiple times, to reduce runtime, run the legalize_placement command one time, after you complete all the create_clock_drivers command runs.\n When routing the H-trees, the tool uses the highest routing layers available, based on the routing rules specified for the clock nets.\n These same layers can contain prerouted nets such as power and ground nets.\n To prevent placing clock drivers under these preroutes, which can cause pin accessibility issues, the create_clock_drivers command creates temporary placement blockages for the preroutes on the same layers it uses for routing.\n After it completes routing, it removes these temporary placement blockages, as shown in the following example output.\n...\n Net: clk; Rule: htree_ndr; Min Layer: M8; Max Layer: M9...\n Information: Using routing H/V layer pair 'M9'/'M8' for net 'clk'.\n (CTS-659)...\n Converting metal shapes in horizontal layer M9 and vertical layer M8 into placement blockages.\n In total 703 placement blockages created.\n...\n Successfully deleted all temporary placement blockages and the cell map.\n If you specify multirow-height cells with the -lib_cell option of the create_clock_drivers command, the tool might not be able to place them due to the temporary placement blockages it creates for the preroutes.\n If so, you can prevent the tool from creating the temporary placement blockages by setting the cts.multisource.enable_pin_accessibility_for_global_clock_trees application option to false To remove clock drivers inserted by the create_clock_drivers command, use the remove_clock_drivers command."}
{"instruction": "How do I Adding Buffers on Routed Nets", "input": "", "output": "To add buffers on nets connected to specified pins, use the add_buffer command.\n Specify the following options when using the add_buffer command:  -new_net_names : Specifies the names of the new nets to add.\n You must specify one net name per buffer when adding buffers, and two net names per inverter pair when adding inverter pairs.\n  -new_cell_names : Specifies the names of the new cells to be added.\n You must specify one cell name per buffer when adding buffers, and two cell names per inverter pair when adding inverter pairs.\n  -inverter_pair : Adds inverter pairs instead of buffer cells.\n If you specify this option, you must supply a library cell that has an inverting output.\n  -respect_voltage_areas : Inserts buffers within the hierarchy of the voltage area such that the buffer is physically in the layout.\n This option is mutually exclusive with the -snap option.\n  -no_of_cells : Specifies the number of buffer cells or inverter pairs to be inserted per net.\n  -object_list : Specifies a list of nets, pins, or ports that must be buffered.\n The new buffer cells or inverter pairs are placed close to the specified pins or ports if their cells are placed.\n        -lib_cell : Specifies the library cell to be used as buffer.\n In this case, the object is either a named library cell or a library cell collection.\n This is a required option.\n This option is mutually exclusive with buffer_lib_cell  -snap : Puts the new buffer cells next to the target cell, then snaps the buffer cells to the closest site, and flips the buffer cells when needed."}
{"instruction": "How do I Pattern-Based Metal Fill Insertion", "input": "", "output": "You can use the signoff_create_metal_fill command to perform the following tasks:  Pattern-Based Metal Fill Insertion  Track-Based Metal Fill Insertion  Using an IC Validator Parameter File  Typical Critical Dimension Metal Fill Insertion  Timing-Driven Metal Fill Insertion  Incremental Metal Fill Insertion  Foundry Fill and Track Fill Combined for Different Flows of Foundry"}
{"instruction": "How do I Checking Missing Vias and Unconnected Pins", "input": "", "output": "Before checking for missing vias or unconnected pins in the design, use the set_missing_via_check_options command to specify the necessary options.\n To check or remove the option settings, use the report_missing_via_check_options or remove_missing_via_check_options command, respectively.\n Table 68 lists the commonly used options for the set_missing_via_check_options command.\n For a detailed description about each option, see the man page.\n Table 68 Commonly Used Options for the set_missing_via_check_options Command Option Description -redhawk_script_file        Table 68 Commonly Used Options for the set_missing_via_check_options Command (Continued) Option Description exclude_stack_via   check_valid_vias  -exclude_cell -exclude_instance    -exclude_regions  sort_by_hot_inst   -threshold     -min_width  -pitch   -gds     -ignore_inter_met -toplayer -bottomlayer"}
{"instruction": "How do I Saving Route Information", "input": "", "output": "You can get information about DRC violations by using the get_drc_errors command to create a collection of DRC violations and then using the get_attribute command to query the attributes of the errors.\n Some attributes that provide information about DRC errors are type_name bbox objects, and shape.\n Note that the availability of attribute values depends on the error type and the verification method used.\n For a list of all attributes associated with DRC errors, use the list_attributes -application -class drc_error command.\n Before you run the get_drc_errors command, you must load the error data files that you want to query.\n To determine the error data files attached to the current block, use the get_drc_error_data command.\n You must use the -all option to include both the opened and unopened error data files in the result.\n fc_shell> get_drc_error_data -all       To load an error data file, use the open_drc_error_data command.\n fc_shell> open_drc_error_data zroute.err To determine the error data types included in the error data file, use the get_drc_error_types command.\n fc_shell> get_drc_error_types -error_data zroute.err By default, the get_drc_errors command creates a collection that contains all DRC violations contained in the specified error data file.\n Use the -filter option to restrict the returned errors.\n For example, to return only \"Diff net spacing\" errors, use the following command: fc_shell> get_drc_errors -error_data zroute.err \\ -filter {type_name == \"Diff net spacing\"} To get the nets associated with an error, use the get_attribute command.\n For example, fc_shell> get_attribute [get_drc_errors -error_data zroute.err 859] \\ objects {u0_1/n237}"}
{"instruction": "How do I Using Nondefault Routing Rules for Critical Nets During", "input": "", "output": "By default, the clock_opt command does not consider voltage drop information when inserting clock buffers.\n Therefore, the tool might place clock buffers in locations with high PG resistance, which increases the voltage drop.\n To prevent this, you can use the       RedHawk Fusion feature to analyze the PG network and use this information with the clock_opt command, as shown in the following flow: 1.\n Use the RedHawk Fusion feature to analyze the PG network by using the analyze_rail -min_path_resistance command.\n 2.\n Load the RedHawk Fusion results by using the open_rail_result command.\n 3.\n Enable voltage-drop-aware clock tree synthesis by setting the clock_opt.flow.enable_voltage_drop_aware application option to true 4.\n Run the clock_opt command."}
{"instruction": "How do I Setting Application Option", "input": "", "output": "To set the constraints of variant cell: 1.\n Use the set_repeater_group_constraints -type variant_cell_swapping command to set the variant for cell swapping.\n 2.\n Use the report_repeater_group_constraints command to view the report of the repeater group constraint settings, after setting the variant for cell swapping.\n The report provides the  Track pattern of the variant cell group  Variant site ids  Honor cell group not sharing timing  First track alignment 3.\n Use the remove_repeater_group_constraints command to remove the constraints and reset the settings based on the report."}
{"instruction": "How do I Enabling Improved Buffering for Multivoltage Nets", "input": "", "output": "A net that connects a multivoltage cell, such as a level-shifter cell or an isolation cell, to at least one cell in another voltage area is referred to as a multivoltage net.\n To reduce the length of multivoltage nets, enable advanced multivoltage cell placement by setting the place.coarse.enable_advanced_mv_cell_placement application option to true When you enable this feature, the tool reduces the length of the multivoltage nets by placing the multivoltage cells closer to the voltage area boundaries.\n To optimize multivoltage nets, the tool uses dual-rail buffers.\n By reducing the length of multivoltage nets, the tool can reduce the number of dual-rail buffers used during optimization and prevent unoptimized multivoltage nets due to the unavailability of dual-rail buffers."}
{"instruction": "How do I Performing Signoff Analysis", "input": "", "output": "Macro models are used to reduce total turnaround time for designs with advanced process technology.\n When violations are reported in the macros during full-chip signoff voltage drop analysis, it might be time-consuming to fix the violations at the block level and then rerun full-chip analysis to verify the fix.\n To reduce the turnaround time, you can create a context model for the macro block.\n A context model depicts the relationship between the block and the full-chip design, including both physical and electrical information, and enables you to verify the fixes by running block-level analysis rather than full-chip analysis.\n To create a context model, use the create_context_for_sub_block command.\n To run voltage drop analysis with the generated block contexts, set the rail.pad_files and rail.block_context_model_file application options before running the analyze_rail command.\n Note: Creating full-chip contexts for blocks is supported only in RedHawk Fusion.\n The following figure shows that a fix in the block can effectively improve the performance of the top design.\n        Fixing Violations in Block Contexts Content of Block Contexts The create_context_for_sub_block command models the full-chip context of a block, including physical models and electrical models.\n  Physical model, which describes the locations of the connections between the block and full-chip design.\n These tap locations are derived by tracing the RedHawk parasitics data.\n The physical model is generated when the create_context_for_sub_block command process is complete, in the.ploc format.\n  Electrical model, which is an effective resistance- and capacitance-based model.\n The tool uses the native resistance engine to calculate the effective resistance, which is used to calculate equivalent current during peak dynamic voltage drop analysis.\n By default, the tool models the full-chip context for the specified block.\n The capacitance data is derived by the RedHawk extraction engine.\n To create an electrical model for the specified block, specify the -electric_model_file option with the create_context_for_sub_block command.\n The electrical model is written in the.context format.\n       Examples The following example creates a block context for the top/cell_inst_1 block.\n The generated physical and electrical models are block.ploc and block.context, respectively.\n ##Create block context in top block ## open_block top create_context_for_sub_block \\ -block_instance top/cell_inst_1 block.ploc \\ -nets {VDD_TOP VSS_TOP} \\ -electric_model_file block.context close_block  ##Run IR analysis with block context information in block-level design ## open_block block set_app_options -name rail.pad_files -value block.ploc set_app_options -name rail.block_context_model_file \\ -value block.context analyze_rail -nets {VDD_BLOCK VSS_BLOCK} \\ -voltage_drop static -extra_gsr_option_file extra.gsr...\n The following example creates only a physical model for the top/cell_inst_1 block.\n ##Create block context in top block ## open_block top create_context_for_sub_block \\ -block_instance top/cell_inst_1 block.ploc \\ -nets {VDD_TOP VSS_TOP} close_block  ##Run IR analysis with block context information in block-level design ## open_block block set_app_options -name rail.pad_files -value block.ploc analyze_rail -nets {VDD_BLOCK VSS_BLOCK} \\ -voltage_drop static -extra_gsr_option_file extra.gsr..."}
{"instruction": "How do I Generating a Report of Early Data Check Records", "input": "", "output": "You can report the policy set for all checks or for a specific set of checks by using the report_early_data_checks command.\n Use the following options to configure the report:  -policy : Reports configuration details for all checks.\n It shows the selected policy and strategy for each check.\n Using the -hierarchical option with the -policy option reports the configuration of the subblocks.\n  -checks : Reports the current policy, supported policies, strategies, and help text for each check.\n It supports an asterisk wildcard character (*) as the name of a check.\n Using the -hierarchical option with the -checks option reports the checks of the subblocks.\n  -verbose : Reports a summary of all failed checks, policies, strategies applied, and checked objects with comments.\n Using the -hierarchical option with the -verbose       option reports the records of all failed checks, policies, strategies applied, and checked objects from the subblocks.\n  -hierarchical : Traverses the hierarchy and reports the check, policy, strategy, and fail count for each design in the hierarchy.\n In the following example, the report_early_data_checks -hierarchical -policy command reports the policy settings for the hierarchical checks set on the top-level design and the block-level designs: fc_shell> report_early_data_checks -policy **************************************** Report : report_early_data_checks Design : top Version: R-2020.09 Date : Thu Aug 20 23:53:54 2020 **************************************** Design Check Policy Strategy   unit_des.nlib:top.design hier.block.instance_bound_to_frame error  unit_des.nlib:top.design hier.block.instance_unlinked error  unit_des.nlib:top.design hier.block.instance_with_design_type_macro error  unit_des.nlib:top.design hier.top.estimated_corner error   blk hier.block.abstract_missing_design_view error  blk hier.block.abstract_target_use_non_implementation error  blk hier.block.abstract_type_non_timing error  blk hier.block.leaf_cell_outside_boundary error   In the following example, the report_early_data_checks -hierarchical -verbose command reports the repair performed on the checked object and also provides the details of policies, strategies applied, and checked objects on the subblocks.\n fc_shell> report_early_data_checks -hierarchical -verbose ****************************************************** Design Check Policy Strategy Checked Comment Object  blk hier.block.missing_leaf_cell_location repair eco_cell Instance location updated to X:150145 Y:69620  While running the top-level early flow, you might run into top-level errors or warnings because of the design data violations in the subblocks or top-level design.\n To use the data checks and policies capability for exploring early flows, you need to know       the check name corresponding to the top-level errors seen on the design.\n The report_hier_check_description command provides the required information about the checks associated with each top-level error.\n The following example shows the report generated by the report_hier_check_description command: fc_shell> report_hier_check_description ********************************************* Report : Hier check description Design : top Version: R-2020.09 Date : Fri Aug 21 00:08:32 2020 *********************************************  Legend E - error R - repair T - tolerate TOP - Top-Design BLK - Respective Block-Ref    Check Error Tolerate Repair Allowed Allowed Description ID ID ID Policies References   hier.block.missing_frame_view TL-101 TL-401 TL-501 E|R|T BLK Missing frame view hier.block.abstract_missing_design_view TL-101 TL-401 N/A E|T BLK Missing design view for abstract hier.block.reference_missing_port_location TL-126 TL-426 TL-526 E|R|T BLK Missing location of physical  hierarchy boundary pin hier.block.reference_port_outside_boundary TL-127 TL-427 TL-527 E|R|T BLK Location of physical hierarchy boundary  pin outside physical hierarchy boundary"}
{"instruction": "How do I Defining Voltage Areas", "input": "", "output": "A floating logical supply net is a power or ground net that is not connected to any logical pin or port in the block.\n For example, a power feedthrough net created during power planning is a floating supply net.\n To create a floating supply net with the connect_pg_net command, use the following steps: 1.\n Specify the supply net type in the UPF file by using the supply_net_pg_type attribute, as shown in the following UPF file example:  create_supply_net VDD   set_design_attributes -elements VDD \\ -attribute supply_net_pg_type power  2.\n Enable floating supply nets by setting the mv.pg.create_floating_pg application option to true before you create supply nets with the connect_pg_net command, as shown in the following script example:   # Apply the UPF load_upf $upf_input_file_name commit_upf   # create logical supply nets set_app_options -as_user_default \\ -name mv.pg.create_floating_pg -value true connect_pg_net -automatic  check_mv_design The tool creates the logical supply nets in the topmost hierarchy of the current UPF scope for domain-independent supply nets.\n For domain-dependent supply nets, the tool creates the logical supply nets in the topmost hierarchy of the domain.\n The tool does not create a logical supply net in the following situations:  There is a conflict between the connection and the supply net type specified with the supply_net_pg_type attribute.\n  The supply net is connected to a PG pin of an instance.\n  The connect_pg_net command is not specified in the current or parent physical hierarchy."}
{"instruction": "How do I Running Commands in Parallel", "input": "", "output": "To improve runtime, you can run checking and reporting commands in the background while you run other commands in the foreground.\n This is useful in interactive sessions when you want to continue your work while the tool generates a report.\n To run commands in the background, use the redirect command with the -bg option.\n When you use this command, fc_shell returns immediately to execute the next command.\n If you issue an exit command in the parent process, the tool waits for all redirect -bg commands to complete before exiting.\n To list the commands supported by the redirect -bg command, use the list_commands -bg command.\n You can run either a single command or source a Tcl script that contains       only supported commands.\n If the command or script includes a redirect -bg command, the -bg option is ignored.\n You can run at most two jobs in the background.\n If you specify more than two background jobs, they are queued.\n To specify the maximum number of cores to use for the background jobs, use the -max_cores option with the redirect command.\n The number of cores available for the parent process (as specified by the -max_cores option of the set_host_options command) is reduced by the number of cores allocated for background jobs.\n The following example redirects a Tcl script to run in the background: fc_shell> set_host_options -max_cores 8 fc_shell> redirect -bg -max_cores 3 -file bg_log.out \\ {source bg_script.tcl} Information: redirect -bg with max_cores 3 started.\n The maximum number of cores available in parent is reduced to 5.\n (BGE-004) Reporting Background Jobs To report the background jobs submitted with the redirect -bg command, use the report_background_jobs command.\n This command reports both the completed jobs and the jobs currently running in the background, as shown in the following example: fc_shell> report_background_jobs JOB'redirect -bg -file {background.log} source bg_script.tcl -max_cores 4'completed JOB(pid:13010)'redirect -bg -file {background_1.log} source bg_script_1.tcl -max_cores 3'is running To omit the completed jobs, use the -reset option with the report_background_jobs command."}
{"instruction": "How do I Defining Equivalent Cell Groups", "input": "", "output": "Some libraries provide sets of functionally equivalent cells that can be legalized in different locations.\n This allows the tool to legalize a cell by replacing it with a variant, instead of moving the cell.\n The following figure shows functionally equivalent cells with varying pin locations.\n With variant-aware legalization, if a cell pin does not align with a track, the tool can try using its variant to align the cell pin to the track.\n  Equivalent Cells With Different Pin Locations       The following figure shows functionally equivalent cells with different pin colors, which need to be aligned to a track of the same color.\n With variant-aware legalization, if a cell pin does not align with the corresponding colored track, the tool can try using its variant with a different colored pin and try aligning it to the appropriately colored track.\n  Equivalent Cells With Pins of Different Colors The following topics describe the tasks you need to perform to set up the reference libraries and design for variant-aware legalization:  Defining Equivalent Cell Groups  Enabling Variant-Aware Legalization"}
{"instruction": "How do I Defining Guard Bands", "input": "", "output": "You can modify the stacking order of the voltage area shapes by using the set_voltage_area_shape command.\n  To raise a voltage area shape one position, use the -raise option.\n  To lower a voltage area shape one position, use the -lower option.\n  To move a voltage area shape to the top, use the -top option.\n  To move a voltage area shape to the bottom, use the -bottom option.\n  To move a voltage area shape directly above another shape, use the -above option.\n  To move a voltage area shape directly below another shape, use the - below option."}
{"instruction": "How do I Useful Skew", "input": "", "output": "Changing clock latencies can balance the slack in successive timing path stages to optimize clock and data paths.\n This capability is called concurrent clock and data (CCD) optimization, which can reduce total negative slacks (TNS), worst negative slacks (WNS), area, and leakage power.\n In addition, it can improve correlation between physical synthesis and place-and-route."}
{"instruction": "How do I Inserting Clock Gates in Multivoltage Designs", "input": "", "output": "The Fusion Compiler tool inserts clock-gating cells during the compile_fusion command.\n The following topics describe clock-gating flows:  Inserting Clock Gates in Multivoltage Designs  Inserting Clock Gates in an RTL Design"}
{"instruction": "How do I Handling Design Data Using the Early Data Check Manager", "input": "", "output": "You can check a hierarchical design that contains abstracts for possible issues such as the application option consistency issues and the design issues related to top-level closure by using the check_hier_design command.\n Identifying issues and fixing them before you perform top-level closure can help reduce turnaround time.\n When you use the check_hier_design command, you can specify  The references to check by using the -reference option.\n If you do not specify this option, the command checks all the references in the physical hierarchy.\n  The type of checks to perform by using the -stage option as follows:  Use the -stage timing option to perform timing related checks.\n If you do not specify the -stage option, by default, the tool performs timing related checks.\n  Use the -stage pre_placement to perform both timing and preplacement related checks.\n The check_hier_design command can check the consistency between the top-level constraints and the constraints of every lower-level instance linked to an abstract.\n To enable this feature, set the abstract.check_constraints_consistency application option to true before you run the check_hier_design command.\n The tool saves the settings of the predetermined list of timer application options during the create_abstract command.\n At the top-level design, the check_hier_design command compares these timer application option settings with those of the block-level design.\n The following application options are automatically saved during the create_abstract command and verified by the check_hier_design command:  time.case_analysis_propagate_through_icg  time.case_analysis_sequential_propagation  time.clock_gating_propagate_enable  time.clock_gating_user_setting_only        time.clock_marking  time.clock_reconvergence_pessimism  time.create_clock_no_input_delay  time.crpr_remove_clock_to_data_crp  time.delay_calc_waveform_analysis_mode  time.delay_calculation_style  time.disable_case_analysis_ti_hi_lo  time.disable_clock_gating_checks  time.disable_cond_default_arcs  time.disable_internal_inout_net_arcs  time.disable_recovery_removal_checks  time.edge_specific_source_latency  time.enable_auto_mux_clock_exclusivity  time.enable_ccs_rcv_cap  time.enable_clock_propagation_through_preset_clear  time.enable_clock_propagation_through_three_state_enable_pins  time.enable_clock_to_data_analysis  time.enable_non_sequential_checks  time.enable_preset_clear_arcs  time.enable_si_timing_windows  time.gclock_source_network_num_master_registers  time.special_path_group_precedence  time.use_lib_cell_generated_clock_name  time.use_special_default_path_groups For more information about the issues the check_hier_design command identifies and how to fix them, see the man page for the corresponding message ID.\n In addition to generating a report, the check_hier_design command generates an enhanced messaging system (EMS) database that you can view by using the message       browser in the Fusion Compiler GUI.\n Create the EMS database before you run the check_hier_design command, as shown in the following example: fc_shell> create_ems_database check_hier.ems fc_shell> check_hier_design -stage timing fc_shell> save_ems_database In the Fusion Compiler GUI message browser, you can sort, filter, and link the messages to the corresponding man page, as shown in the following figure.\n  Viewing the EMS Database in the Message Browser You can also output the information in the EMS database in ASCII format by using the report_ems_database command.\n The checks performed by the check_hier_design command are also available in the check_design command.\n You can perform  Timing checks specific to top-level closure by using the check_design -checks hier_timing command  All timing checks, including those specific to top-level closure, by using the check_design -checks timing command  Preplacement checks specific to top-level closure by using the check_design -checks hier_preplacement command  All preplacement checks, including those specific to top-level closure, by using the check_design -checks preplacement command       You can generate an EMS database for the check_design command and view it in the GUI, similar to the check_hier_design command."}
{"instruction": "How do I Supporting RedHawk-SC Customized Python Script Files", "input": "", "output": "You can run the analyze_rail -script_only command to generate the settings required for running rail analysis based on the information in the design library without actually executing the analysis run.\n The analyze_rail -script_only command writes data to the working directory.\n The output files that are saved to the directory are Table 67 Output data when running analyze_rail -script_only File Name Description RedHawk RedHawk-SC             analyze_rail"}
{"instruction": "How do I Prerequisites for Handling Early Design Data", "input": "", "output": "While you identify the issues related to top-level closure and fix them, the tool enables you to explore the top-level flow in the presence of design data violations.\n The hierarchical checks can detect different types of mismatched, incomplete, and inconsistent data in designs.\n You can configure the following policies for the hierarchical checks to manage the design data violations:  Error: When you apply this policy, the tool does not mitigate the violation, but issues an error message.\n  Tolerate: When you apply this policy, the tool makes predictable and explainable assumptions to continue with the flow.\n No changes are made to the design or setup.\n  Repair: When you apply this policy, the tool mitigates the violation using one or more repair strategies and records it.\n Repair can include changes in design and setup.\n  Strict: When you apply this policy, depending on the policy supported by the check, the tool applies the policies in this order \u2013 error, tolerate, repair.\n  Lenient: When you apply this policy, depending on the policy supported by the check, the tool applies the policies in this order \u2013 repair, tolerate, error.\n Note:  Quality of results of the top-level flow might get impacted due to the toleration and repair of errors.\n Error toleration and repair is mainly provided for managing data violations during the early design exploration phases.\n In the implementation flow, you should use the strict policy for all checks."}
{"instruction": "How do I Adding Objects to a Group", "input": "", "output": "A relative placement group is an association of cells, other relative placement groups, and blockages.\n A group is defined by the number of rows and columns it uses.\n Use the create_rp_group command to create a relative placement group.\n When you do so, you must specify a name for the relative placement group by using the -name option.\n You can specify the number of columns and rows for the relative placement group by using the -columns and -rows options.\n If you do not do so, the tool create a relative placement group with one row and column.\n       For example, to create a relative placement group named RP1 that has six columns and six rows, use the following command: fc_shell> create_rp_group -name RP1 -columns 6 -rows 6  shows the relative placementpositions for data positions of columns and rows in a relative placement group.\n  Relative Placement Column and Row Positions 0 5 0 4 0 2 1 5 1 4 1 3 3 5 3 4 3 3 4 5 2 5 3 2 2 3 4 2 0 0 1 0 3 0 4 0 2 0 5 5 5 4 5 2 5 0 2 4 5 3 0 1 1 1 3 1 2 1 5 1 col 0 row 0 row 2 row 4 row 5 row 3 row 1 col 1 col 2 col 4 col 5 col 3 1 2 2 2 4 4 4 3 For the relative placement group in  The column count begins from column 0 (the leftmost column).\n  The row count begins from row 0 (the bottom row).\n  The width of a column is the width of the widest cell in that column.\n  The height of a row is the height of the tallest cell in that row.\n  All positions in the structure are not used.\n For example, positions 0 3 (column 0, row 3) and 4 1 (column 4, row 1) are not used.\n By default, the tool creates the relative placement group in the current design.\n You can create it in a different design by specifying its name using the -design option.\n If you create a relative placement group in a design that has multiple instantiations in a top-level design, the changes you make to the relative placement group in one instance is reflected in all its instances.\n To remove relative placement groups, use the remove_rp_groups command."}
{"instruction": "How do I Inserting Metal Fill", "input": "", "output": "Filler cells inserted by the Fusion Compiler tool have instance names that start with the string xofiller.\n To remove all the filler cells from a block, use the remove_cells command, as shown in the following example: fc_shell> remove_cells [get_cells xofiller*] If you used the standard filler cell insertion flow and want to remove only those filler cells with DRC violations, use the remove_stdcell_fillers_with_violation command, as described in Checking for Filler Cell DRC Violations"}
{"instruction": "How do I Identifying Why Multibit Banking Is Not Performed", "input": "", "output": "After you run the compile_fusion command, you can report the registers that are transformed due to multibit banking and debanking by using the report_transformed_registers -multibit command.\n The following example shows a portion of a report generated by the report_transformed_registers -multibit command.\n fc_shell> report_transformed_registers -multibit...\n **************************************** Attributes: mb - multibit mbd - multibit debanked...\n Register Optimization  MB_2_MB_1 mb (source:MB_2, MB_1) MB_0_MB_3 mb (source:MB_0, MB_3) MB_5, MB_6 mbd (source: MB_5_MB_6)"}
{"instruction": "How do I Reporting and Querying Self-Gating Cells", "input": "", "output": "To override the default behavior of selecting self-gating objects, use the set_self_gating_objects command with the following options:  -force object_list Directs the tool to perform self-gating on the listed objects, even if the objects do not pass internal checks.\n  -exclude object_list Prevents the tool from self-gating the listed objects, even if they pass the internal checks.\n  -include object_list Specifies that the listed objects should be self-gated according to the insertion rules set by the set_clock_gating_options command.\n This is the default setting for all registers in the design.\n  -clear object_list Removes all inclusion, exclusion, and forced criteria on the listed objects.\n  -reset Removes all configurations previously set by the set_self_gating_objects command.\n  -tree_type combinational_logic_type Specifies the type of combinational cells to use.\n Valid arguments are xor (the default), or nand, and auto.\n When auto is specified, the tool automatically decides which comparison logic to use based on switching activity information.\n This might help to reduce the area while optimizing power.\n You must use this option with the -include or -force options.\n To report self-gating objects, use the report_self_gating_objects command."}
{"instruction": "How do I Routing Nets Within a Specific Region", "input": "", "output": "The derive_perimeter_constraint_objects command places the inserted constraint objects in an group named PC_Edit_Group_ blockname.\n Each time you run the derive_perimeter_constraint_objects command, it creates a unique group.\n To remove a group of constraint objects, use the following command: fc_shell> remove_objects [get_attribute \\ [get_edit_groups PC__Edit_Group_ blockname ] objects]"}
{"instruction": "How do I Track-Based Metal Fill Insertion", "input": "", "output": "Pattern-based metal fill insertion is the default mode for the signoff_create_metal_fill command.\n It inserts metal and via fill by using a foundry runset.\n Before running the signoff_create_metal_fill command, you must specify the runset by setting the signoff.create_metal_fill.runset application option.\n During pattern-based metal fill insertion, the signoff_create_metal_fill command performs the following tasks: 1.\n Loads the block into the IC Validator tool, as described in Reading Blocks for Signoff Metal Fill Insertion 2.\n Removes existing metal and via fill By default, the command removes the metal and via fill from the entire block.\n For information about performing incremental metal fill insertion, see Incremental Metal Fill Insertion 3.\n Inserts metal and via fill in the empty regions By default, the command uses the spacing rules defined in the technology file to perform metal fill insertion on all metal and via layers for the entire block.\n The       command uses the metal fill mode specified in the runset file, which is either hierarchical or flat.\n To modify the default behavior,  Set one or more of the following application options before running the signoff_create_metal_fill command:  signoff.create_metal_fill.apply_nondefault_rules To enable the use of nondefault spacing rules in addition to the spacing rules defined in the technology file, set this application option to true  signoff.create_metal_fill.flat To force the use of the flat metal fill mode, set this application option to true  Use one or more of the following command options with the signoff_create_metal_fill command:  -select_layers or -all_runset_layers To restrict the layers on which to insert metal fill, use these options, as described in Specifying the Layers for Metal Fill Insertion  -coordinates and -excluded_coordinates To restrict the regions on which to insert metal fill, use these options, as described in Specifying the Regions for Metal Fill Insertion 4.\n Removes floating via fill from the specified via layers and from the via layers associated with the specified metal layers Floating via fill is via fill that does not have both an upper and lower metal enclosure.\n 5.\n Saves the fill data in the design view 6.\n Writes the result files to the run directory For information about the generated result files, see Signoff Metal Fill Result Files See Also  Timing-Driven Metal Fill Insertion       Reading Blocks for Signoff Metal Fill Insertion By default, the IC Validator tool reads the design view for the top-level block and the library cell instances, and the pin information from the frame view for the macro cell and I/O pad cell instances.\n  To read the GDSII or OASIS data for specific reference cells, specify the stream files by setting the signoff.physical.merge_stream_files application option.\n When you use this option, the GDSII or OASIS data replaces the cell library view for the cells defined in the specified stream files, except for cells in the cell libraries specified with the signoff.physical.merge_exclude_libraries application option.\n  To read the layout view for specific reference cells, specify the cells by setting the signoff.create_metal_fill.read_layout_views application option.\n The layout view is identical to the GDSII or OASIS data that was used to create them, but reduces the runtime for signoff design rule checking.\n Note: By default, layout views are not included in the cell libraries To save the layout views, you must set the lib.workspace.save_layout_views application option to true during library preparation, as described in the Library Manager User Guide  To read the design view for specific reference cells, specify the cells by setting the signoff.create_metal_fill.read_design_views application option.\n Note: By default, design views are not included in the cell libraries To save the design views, you must set the lib.workspace.save_design_views application option to true during library preparation, as described in the Library Manager User Guide The order of precedence for child cell data is 1.\n GDSII or OASIS data specified with the signoff.physical.merge_stream_files application option 2.\n Layout views specified with the signoff.create_metal_fill.read_layout_views application option If the IC Validator tool cannot find a layout view and the cell is specified in the signoff.check_drc.read_design_views application option, it reads the design view.\n Otherwise, it reads the frame view.\n If it cannot find the frame view, the cell is ignored during design rule checking.\n       3.\n Design views specified with the signoff.create_metal_fill.read_design_views application option If the IC Validator tool cannot find a design view, it reads the frame view instead.\n If it cannot find the frame view, the cell is ignored during design rule checking.\n 4.\n Frame views Specifying the Layers for Metal Fill Insertion By default, the signoff_create_metal_fill command inserts metal fill on all the metal routing layers and via fill on all the via layers.\n To modify the layers for metal fill insertion, use one of the following options:  -select_layers metal_fill_layers This option restricts metal fill insertion to the specified set of metal and via layers.\n  -all_runset_layers true This option enables metal fill insertion on all the fill layers specified in the runset.\n This option applies only to pattern-based metal fill insertion.\n When you specify the layers for metal fill insertion, the signoff_create_metal_fill command 1.\n Removes the existing metal and via fill from the block By default, the command removes all existing metal and via fill.\n For incremental metal fill insertion, the command removes metal and via fill only from the specified layers.\n 2.\n Inserts metal and via fill only on the specified layers 3.\n Removes floating via fill from the specified via layers and from the via layers associated with the specified metal layers For example, if you specify -select_layers {M2}, the tool removes floating via fill from the V1 and V2 layers.\n       Specifying the Regions for Metal Fill Insertion By default, the signoff_create_metal_fill command inserts metal and via fill for the whole chip.\n To modify the regions for metal fill insertion, use one or both of the following options:  -coordinates This option restricts metal fill insertion to the specified regions.\n Note: The bounding box coordinates passed to the IC Validator tool in the METAL_FILL_SELECT_WINDOW parameter are enlarged by 1 um to avoid DRC violations on the boundary of the specified regions during metal fill insertion.\n The actual metal fill insertion occurs within the regions specified by the -coordinates option.\n In addition, when you use the -coordinates option, the signoff_create_metal_fill command always uses the flat metal fill mode.\n  -excluded_coordinates This option prevents metal fill insertion in the specified regions.\n If you specify this option with the -coordinates option, the command does not perform metal fill insertion in the overlapping regions.\n To specify the coordinates, use the following format: {{x } {x }...\n {x }} Note that there must be a space between each set of coordinates.\n When you specify the regions for metal fill insertion, the signoff_create_metal_fill command 1.\n Removes all existing metal and via fill from the block For standard metal fill insertion, the command removes all existing metal and via fill.\n For incremental metal fill insertion, the command removes metal and via fill only from the specified regions.\n 2.\n Inserts metal and via fill only on the specified regions 3.\n Removes floating via fill from the specified regions For example, to remove all metal fill from the block and then fill all empty regions outside the bounding box with corners at (100,150) and (300,200), use the following command: fc_shell> signoff_create_metal_fill \\ -excluded_coordinates {{100 150} {300 200}}"}
{"instruction": "How do I Deriving Mask Colors", "input": "", "output": "To save the route information, use the write_routes command.\n This command generates a Tcl script that reproduces the metal shapes and vias for a block, including their attribute settings.\n Note that the write_routes command reproduces routes, but not routing blockages.\n To generate a Tcl script that reproduces routing blockages, use the write_floorplan command."}
{"instruction": "How do I Typical Critical Dimension Metal Fill Insertion", "input": "", "output": "You can customize the metal fill insertion by using an IC Validator parameter file.\n  For pattern-based metal fill insertion, you pass the parameter file to the IC Validator tool by setting the signoff.create_metal_fill.user_defined_options application option.\n fc_shell> set_app_options \\ -name signoff.create_metal_fill.user_defined_options \\ -value {-D INDESIGN_USER_DEFINED_PARAM_FILE= file_name  For track-based metal fill insertion, you pass the parameter file to the IC Validator tool by using the -track_fill_parameter_file option with the signoff_create_metal_fill command.\n fc_shell> signoff_create_metal_fill -track_fill true \\ -track_fill_parameter_file file_name When you run track-based metal fill insertion, the IC Validator tool writes a parameter file named track_fill_params.rh into the run directory.\n This file contains the default settings for the supported parameters.\n You can modify this file as necessary and use it on subsequent signoff_create_metal_fill runs.\n Note: In some cases, the IC Validator behavior can be controlled by either an IC Validator parameter or an application option.\n In these cases, if both are specified, the IC Validator parameter setting overrides the application option setting.\n Table 50 shows the commonly used parameters supported in the parameter file.\n Unless otherwise specified, these parameters apply only to track-based metal fill insertion.\n The mx or vx prefix in the parameters refers to the mask name, which can be found in the layer section of the technology file.\n       For example, if layer M11 has the mask name metal12 in the technology file, the prefix m12 would replace mx in the parameters below.\n Therefore, the M11 layer would have the parameters m12_spare_fill m12_fill_staggering, and so on.\n Table 50 IC Validator Metal Fill Insertion Parameters Parameter Default Description Metal fill insertion mode parameters _sparse_fill     Note: -fill_all_tracks true   _fill_staggering   _ignore_route_guide     _ignore_system_blockage    _via_enclosure  ContactCode    Fill size and spacing parameters _fill_width defaultWidth  _min_fill_length  minArea  _max_fill_length      _fill2fill_side_ spacing  _fill2route_side_ spacing   9.\n Attribute setting from the Layer section of the technology file       Table 50 IC Validator Metal Fill Insertion Parameters (Continued) Parameter Default Description _fill2fill_end_ spacing   _fill2route_end_ spacing   _fill2Blockage_x _fill2Blockage_y   _fill2routeGuide_x _fill2routeeGuide_y   _fill2chipBoundary_x _fill2chipBoundary_y   _min_area_with_via   _EXCLUDED_CELLS_ OVERSIZE_VALUE   _EXCLUDED_CELLS   _EXCLUDED_CELLS   _EXCLUDED_CELLS_OVERSIZE_VALUE  _iso_via_distance    _min_spacing minSpacing       _per_metal         Exclude layer parameters       Table 50 IC Validator Metal Fill Insertion Parameters (Continued) Parameter Default Description _exclude_layer_ datatype     Fill output mode parameters _compress_fill _compress_fill    Note: -mode add    _fill_datatype _fill_datatype   output_colored_fill     Note: -output_colored_fill true  _fill_datatype_ color1 _fill_datatype_ color1 _fill_datatype_ color2 _fill_datatype_ color2    Note:   Density calculation parameters exclude_bounding_box_ blockage_for_density_ computation   -excluded_coordinates   exclude_route_guides_ for_density_computation    exclude_system_metal_ blockages_for_density_ computation          Table 50 IC Validator Metal Fill Insertion Parameters (Continued) Parameter Default Description _density_gradient_ window 10 windowSize     signoff.report_metal_density.gradient_windo w_size  Note: windowSize   signoff.report_metal_density.gradient_wi ndow_size   _max_density_window    _max_open_area_rule 10    _min_density 10 minDensity     signoff.report_metal_density.min_density  Note: minDensity   signoff.report_metal_density.min_density   _min_density_window 10 windowSize     signoff.report_metal_density.density_window  Note: windowSize   signoff.report_metal_density.density_win dow   10.\n This parameter applies to timing-driven metal fill insertion, whether pattern-based or track-based.\n       Table 50 IC Validator Metal Fill Insertion Parameters (Continued) Parameter Default Description _window_step_size        signoff.report_metal_density.density_window _step"}
{"instruction": "How do I Defining the Search Path", "input": "", "output": "The Fusion Compiler tool uses a design library to store your design and its associated library information.\n This topic describes how to create a design library and how to prepare and save your design.\n These steps are explained in the following topics:  Defining the Search Path  Setting Up Libraries  Using Pin Access Checker Utility  Analyzing Libraries  Reading the Design  Mitigating Design Mismatches  Importing the Floorplan Information  Setting Up Multivoltage Designs  Specifying Timing Constraints and Settings  Specifying Logical Design Rule Constraints  Specifying Clock Gating Constraints  Specifying Physical Constraints for Placement and Legalization  Specifying Placement Settings  Specifying Legalization Settings  Controlling the Optimization of Cells, Nets, Pins, and Ports  Specifying Settings for Preroute Optimization  Setting Up for Power-Related Features  Specifying the Routing Resources        Handling Design Data Using the Early Data Check Manager  Applying Mega-Switch Command Settings"}
{"instruction": "How do I Querying Attributes", "input": "", "output": "This topic provides examples about how to use the hotspot analysis capability to identify the root cause of voltage violations.\n Cells with overlapping timing windows might switch at the same time, leading to high voltage drops.\n shows how to identify if the cell has overlapping timing windows.\n        Checking High Voltage Drop Caused by Overlapping Timing Window (I) Next, run the report_hot_spots command with the -index option to find which grid has overlapping timing windows that contribute to large current.\n As shown in when the -index option is set to 5, the grid 5 is reported with small total current.\n When the -index option is set to 2, the grid 2 is reported with large total current.\n In this case, overlapping time windows contribute to the high effective voltage drop in grid 2.\n        Checking High Voltage Drop Caused by Overlapping Timing Window (II) See Also  Generating Hotspots  Reporting Hotspots  Removing Hotspots"}
{"instruction": "How do I Enabling Advanced Legalization Algorithms", "input": "", "output": "For advanced technology nodes, such as 12 nanometer, 7 nanometer, or smaller technology nodes, you can enable advanced physical DRC checks between cells and prerouted PG nets by setting the place.legalize.enable_advanced_prerouted_net_check application option to true When you enable this feature, you can further control the accessibility checks performed for standard cell pins by using the place.legalize.advanced_layer_access_check and place.legalize.advanced_libpin_access_check application options.\n However, the default behavior of this feature is suitable for most designs."}
{"instruction": "How do I Controlling Standard Filler Cell Insertion", "input": "", "output": "The standard filler cell insertion flow uses the create_stdcell_fillers command to insert filler cells in the block and the remove_stdcell_fillers_with_violation command to perform design rule checking on the filler cells and remove filler cells with violations.\n During filler cell insertion, the create_stdcell_fillers command uses the placement legalizer to ensure that the inserted filler cells honor advanced node placement rules and physical constraints such as placement blockages and keepout margins.\n When the place.legalize.enable_advanced_legalizer application option is set to true the command uses the advanced legalization algorithms for 2D rule checking and cell interaction, which can reduce filler cell insertion runtime.\n To insert filler cells in your block, 1.\n Ensure that the block is legalized by using the check_legality command.\n When verifying that a block is fillable, the check_legality command assumes that all standard cells that have a design_type attribute of filler can be used as filler cells.\n However, the create_stdcell_fillers command inserts only the cells specified in       the -lib_cells option.\n This difference might result in unfillable gaps after filler cell insertion even though the check_legality command did not report any issues.\n 2.\n (Optional) Insert required filler cells to abut specific standard cells by using the create_left_right_filler_cells command, as described in Abutting Standard Cells With Specific Filler Cells 3.\n (Optional) Enable multithreaded filler cell insertion by using the set_host_options command to specify the multicore configuration.\n fc_shell> set_host_options -max_cores The create_stdcell_fillers command uses the multicore configuration only when the technology file contains minimum threshold voltage, oxide-diffusion (OD), or trimpoly (TPO) rules.\n Note: The multicore configuration specified with the set_host_options command applies to all Fusion Compiler commands that support multicore processing.\n 4.\n Insert metal filler cells by using the create_stdcell_fillers command.\n Use the -lib_cells option to specify the metal filler library cells to insert.\n The command tries to insert the filler cells in the order that you specify; for the best results, specify them from the largest to the smallest.\n To determine the filler cells available in a cell library, use the following command: fc_shell> get_lib_cells ref_lib /* -filter \"design_type==filler\" To sort the filler cells by decreasing size, use the sort_collection command, as shown in the following example: fc_shell> set FILLER_CELLS \\ [get_object_name [sort_collection -descending \\ [get_lib_cells ref_lib /* -filter \"design_type==filler\"] area]] For more information, see Controlling Standard Filler Cell Insertion 5.\n Connect the inserted filler cells to the power and ground (PG) network by using the connect_pg_net -automatic command.\n 6.\n Remove the metal filler cells with DRC violations by using the remove_stdcell_fillers_with_violation command.\n Removing the filler cells can expose new violations; therefore, you must sometimes run this command multiple times to remove all violating filler cells.\n For more information, see Checking for Filler Cell DRC Violations       7.\n Insert nonmetal filler cells by using the create_stdcell_fillers command.\n Use the -lib_cells option to specify the nonmetal filler library cells to insert.\n The tool tries to insert the filler cells in the order that you specify; for the best results, specify them from the largest to the smallest.\n For more information, see Controlling Standard Filler Cell Insertion 8.\n Connect the inserted filler cells to the PG network by using the connect_pg_net -automatic command.\n See Also  Generic ECO Flow for Timing or Functional Changes  Removing Filler Cells"}
{"instruction": "How do I Defining Antenna Rules", "input": "", "output": "In chip manufacturing, gate oxide can be easily damaged by electrostatic discharge.\n The static charge that is collected on wires during the multilevel metalization process can damage the device or lead to a total chip failure.\n The phenomenon of an electrostatic charge being discharged into the device is referred to as either antenna or charge- collecting antenna problems.\n       To prevent antenna problems, the tool verifies that for each input pin the metal antenna area divided by the gate area is less than the maximum antenna ratio given by the foundry: (antenna-area)/(gate-area) < (max-antenna-ratio) This check is based on the global and layer-specific antenna rules defined for the block and the antenna properties of the cells in the block.\n The antenna flow consists of the following steps: 1.\n Define the antenna rules.\n 2.\n Specify the antenna properties of the pins and ports.\n 3.\n Analyze and fix the antenna violations."}
{"instruction": "How do I Fixing Remaining Mask Design Rule Violations", "input": "", "output": "By default, the remove_stdcell_fillers_with_violation command checks for DRC violations between all cell instances with xofiller in their name and top-level signal routing objects and removes the violating filler cells.\n You can control the following aspects of the filler cell checking:  The string used to identify the filler cells To specify a different string to identify the filler cells, use the -name option.\n  The region in which to check for violations To restrict filler cell check to specific regions, use the -boundary option.\n        The checked objects To check for violations between filler cells and all neighboring objects, such as signal net shapes, other filler cells, standard cells, PG rails, and terminals, use the -check_between_fixed_objects true option.\n  Restrict DRC to shorts-checking only By default, the command removes all filler cells that cause a routing DRC violation.\n To remove only those filler cells that cause a short by overlapping a routing shape, use the -shorts_only true option.\n  Whether double-patterning rules are checked The command checks for double-patterning violations only when the following criteria are met:  The route.common.color_based_dpt_flow application option is true  The technology file defines double-patterning rules To check for DRC violations on filler cells before removing them, use the -check_only true option.\n When you use this option, this command writes a report to a file named block _fillers_with_violation.rpt in the current working directory.\n The report lists the filler cells with violations and reports the first violation for each filler cell."}
{"instruction": "How do I Using Routing Guides to Prioritize Routing Regions", "input": "", "output": "By default, the maximum track utilization is 100 percent.\n You can use a routing guide to control the routing density within the routing guide boundary.\n  To set the maximum track utilization for layers with a horizontal preferred direction, use the -horizontal_track_utilization option with the create_routing_guide command.\n  To set the maximum track utilization for layers with a vertical preferred direction, use the -vertical_track_utilization option with the create_routing_guide command.\n By default, when you create these routing guides, they apply to all layers within the routing guide boundary.\n To set the routing density only for specific layers, use the -layers option to specify the affected layers.\n For example, to set a maximum track utilization of 50 percent for all layers with a horizontal preferred direction and a maximum track utilization of 30 percent for all layers with a vertical preferred direction within the rectangle with its lower-left corner at (0, 0) and its upper-right corner at (100, 100), use the following command: fc_shell> create_routing_guide -boundary {{0.0 0.0} {100.0 100.0}} \\ -horizontal_track_utilization 50 -vertical_track_utilization 30"}
{"instruction": "How do I Inserting Clock Gates in an RTL Design", "input": "", "output": "In a multivoltage design, the different hierarchies of the design might have different operating conditions and use different target library subsets.\n When inserting clock-gating cells in a multivoltage design, the tool chooses the appropriate library cells based on the specified clock-gating style as well as the operating conditions that match the operating conditions of the hierarchical cell of the design.\n If you do not specify a clock-gating style, the tool uses a default style where the test point is before and the observation output is false If the tool does not find a library cell that suits the clock-gating style and the operating conditions, the tool issues a warning message and does not insert a clock-gating cell.\n To check whether there are integrated clock-gate cells available for clock-gate insertion, use the check_clock_gate_library_cell_availability command."}
{"instruction": "How do I Measuring Quality of Results", "input": "", "output": "This table shows some reporting commands to report QoR, timing, area, and so on.\n Use this command To do this report_area    report_qor   report_timing        Use this command To do this report_power   report_clock_gating  report_logic_levels  report_area Example fc_shell> report_area -designware -hierarchy -physical **************************************** Report : area Design : test...\n **************************************** Information: Base Cell (com): cell XNOR2ELL, w=3300, h=6270 (npin=3) Information: Base Cell (seq): cell LPSDFE2, w=10560, h=6270 (npin=5) Number of cells: 8 Number of combinational cells: 4 Number of sequential cells: 4 Number of macros/black boxes: 0 Number of buf/inv 0 Number of references 2 Combinational area: 49.66 Buf/Inv area: 0.00 Noncombinational area: 231.74 Macro/Black box area: 0.00 Total cell area: 281.40 ____________________________________________________________________  Hierarchical area distribution  Global cell area Local cell area  Hierarchical cell Absolute Percent Combi- Noncombi- Black- Total Total national national boxes Design  test 281.40 100.0 37.24 231.74 0.00 test mult_10 12.41 4.4 12.41 0.00 0.00 DW_mult_uns_J1_H3_D1  Total 49.66 231.74 0.00  Area of detected synthetic parts  Perc.of Module Implem.\n Count Area cell area        DW_mult_uns pparch 1 12.41 4.4%  Total: 1 12.41 4.4%  Estimated area of ungrouped synthetic parts  Estimated Perc.\n of Module Implem.\n Count area cell area  DW_mult_uns pparch 3 48.28 17.2%  Total: 3 48.28 17.2%  Total synthetic cell area: 60.69 21.6% (estimated) ____________________________________________________________  Core Area: 1767 Aspect Ratio: 1.0902 Utilization Ratio: 0.1593  The above information was from the logic library.\n The following information was from the physical library:  Total moveable cell area: 281.4 Total fixed cell area: 0.0 Total physical cell area: 281.4 Core area: 0.000, 0.000, 40.260, 43.890 report_qor Example fc_shell> report_qor *********************************************** Report : qor Design : top...\n *********************************************** Scenario 'SC1' Timing Path Group 'clk'  Levels of Logic: 6 Critical Path Length: 0.28 Critical Path Slack: -0.19 Critical Path Clk Period: 0.30 Total Negative Slack: -0.80 No.\n of Violating Paths: 11 Worst Hold Violation: -0.07 Total Hold Violation: -0.13 No.\n of Hold Violations: 32.00   Cell Count        Hierarchical Cell Count: 2 Hierarchical Port Count: 24 Leaf Cell Count: 287 Buf/Inv Cell Count: 45 Buf Cell Count: 4 Inv Cell Count: 41 CT Buf/Inv Cell Count: 0 Combinational Cell Count: 208 Sequential Cell Count: 79 Macro Count: 1   Area  Combinational Area: 419.34 Noncombinational Area: 736.26 Buf/Inv Area: 67.60 Total Buffer Area: 9.66 Total Inverter Area: 57.94 Macro/Black Box Area: 2337.90 Net Area: 0 Net XLength: 0 Net YLength: 0  Cell Area (netlist): 3493.49 Cell Area (netlist and physical only): 3493.49 Net Length: 0  Design Rules  Total Number of Nets: 363 Nets With Violations: 0 Max Trans Violations: 0 Max Cap Violations: 0  report_transformed_registers Example The report_transformed_registers command reports the following information about registers that are modified or removed during compile:  Sequential output ports inversion  Constant registers that are removed and the constant values  Constant registers that are preserved by users  Unloaded registers that are removed  Unloaded registers that are preserved by users  Register merging        Register replication  Shift registers  Multibit registers  Summary of register optimizations Use the report_transformed_registers command only on a mapped design generated by compile or incremental compile.\n If you run the command on an unmapped design, the report_transformed_registers command reports no information.\n The information of transformed or optimized registers is stored in the design library, so you can query the same information by using the report_transformed_registers command in different sessions.\n For example, fc_shell> report_transformed_registers **************************************** Report: report_transformed_registers Version:...\n Date:...\n **************************************** Legend: C0p - constant 0 register preserved C1p - constant 1 register preserved C0r - constant 0 register removed C1r - constant 1 register removed ulp - preserved unloaded register ulr - removed unloaded register mrg - merged register srh - shift register head srf - shift register flop mb - multibit rep - replicated inv - inverted   Register Optimization  I_RISC_CORE/I_DATA_PATH/PSWL_Carry_reg C0r I_RISC_CORE/I_STACK_TOP/I3_STACK_MEM/Stack_Mem_reg[0][0] C0r I_RISC_CORE/I_STACK_TOP/I3_STACK_MEM/Stack_Mem_reg[1][0] C0r To report the count of each type of register transformation, specify the -summary option as shown in the following example: fc_shell> report_transformed_registers -summary **************************************** Report: report_transformed_registers Version:...\n Date:...\n       **************************************** Legend: C0p - constant 0 register preserved C1p - constant 1 register preserved C0r - constant 0 register removed C1r - constant 1 register removed ulp - preserved unloaded register ulr - removed unloaded register mrg - merged register srh - shift register head srf - shift register flop mb - multibit rep - replicated inv - inverted  Register Count  Constant Registers Deleted 12 Constant Registers Preserved 1 Unloaded Registers Deleted 101 Unloaded Registers Preserved 38 Shift Registers 170 Merged 12 Multibit 10 Inverted 15 Replicated 4"}
{"instruction": "How do I Querying Shield Shapes", "input": "", "output": "After you run the create_shields command, you can  Query the shield shapes associated with a shielded net by using the get_shapes -shield_only command, as described in Querying Shield Shapes  Report the shielding statistics by using the report_shields command, as described in Reporting Shielding Statistics"}
{"instruction": "How do I Defining Blockages That Exclude Relative Placement Groups", "input": "", "output": "To define partial blockages that exclude registers, specify the boundary, type ( -type register option), blockage percentage ( -blocked_percentage option), and optionally the name for the placement blockage.\n For example, to define a partial blockage that excludes registers, but allows a cell density of 50 percent for other cells, use the following commands: create_placement_blockage -boundary {{10 20} {100 200}} \\ -type register -blocked_percentage 50 Note: Register blockages are honored during placement, but not during legalization, optimization, or clock tree synthesis."}
{"instruction": "How do I Routing Secondary Power and Ground Pins", "input": "", "output": "To route a group of critical nets before routing the rest of the nets in the block, use the route_group command.\n To specify the nets to route, use one of the following options:  -nets Use this option to specify the nets on the command line: fc_shell> route_group -nets collection_of_critical_nets  -from_file Use this option to specify the nets in a file: fc_shell> route_group -from_file file_name If the specified nets are associated with a routing corridor, the nets are routed within the defined region.\n Note that global routing considers routing corridors as a hard constraint, while track assignment and detail routing consider routing corridors as a soft constraint and might route nets slightly outside of the routing corridor to fix DRC violations.\n By default, the route_group command ignores existing global routes, reuses dangling wires and existing detail routes on the specified nets, and runs global routing, track assignment, and detail routing on the specified nets.\n  To perform incremental global routing, set the -reuse_existing_global_route option to true Note: You cannot use the -reuse_existing_global_route true option when routing the nets in a routing corridor.\n If you use this option, the global router ignores the routing corridors.\n  To disable the reuse of dangling wires, set the -utilize_dangling_wires option to false  To stop after global routing, set the -stop_after_global_route option to true By default, the detail router performs a maximum of 40 iterations search and repair iterations.\n If Zroute determines before then that all violations have been fixed or that it cannot fix the remaining violations, it stops.\n To change the maximum number of detail routing iterations, use the -max_detail_route_iterations option.\n By default, the route_group command does not fix soft DRC violations, such as bridge rule violations.\n To enable the fixing of soft DRC violations after the final detail routing       iteration, set the route.common.post_group_route_fix_soft_violations application option to true fc_shell> set_app_options \\ -name route.common.post_group_route_fix_soft_violations \\ -value true"}
{"instruction": "How do I Recording the Changes Made to a Layout", "input": "", "output": "To update the supply nets for ECO cells, use the eco_update_supply_net command.\n By default, it updates the supply nets for all cell added by using the following ECO commands:  size_cell in freeze-silicon mode  add_buffer  add_eco_repeater  split_fanout  add_buffer_on_route To update the supply nets for specific cells, use the eco_update_supply_net -cells command."}
{"instruction": "How do I Disabling Design Rule Checking (DRC)", "input": "", "output": "If a block does not contain clock trees, you should perform optimization using ideal clocks.\n To mark all clock networks as ideal, use the following command: foreach_in_collection mode [all_modes] { current_mode $mode set_ideal_network [all_fanout -flat -clock_tree]  To model the clock tree effects for placement, you should also define the uncertainty, latency, and transition constraints for each clock by using the set_clock_uncertainty set_clock_latency, and set_clock_transition commands.\n Before performing clock tree synthesis, you must use the remove_ideal_network command to remove the ideal setting on the fanout of the clock trees."}
{"instruction": "How do I Applying the Multivoltage Power Intent", "input": "", "output": "The following topics describe the tasks you need to perform when setting up multivoltage designs:  Applying the Multivoltage Power Intent  Preparing the Power Network        Defining Voltage Areas  Inserting Multivoltage Cells  Controlling the Placement of Multivoltage Cells  Enabling Improved Buffering for Multivoltage Nets  Analyzing Multivoltage Information"}
{"instruction": "How do I Defining the Clock Trees", "input": "", "output": "This topic details the prerequisites required before running the clock tree synthesis.\n Before you run clock tree synthesis on a block, it should meet the following requirements:  The clock sources are identified with the create_clock or create_generated_clock commands.\n  The block is placed and optimized.\n Use the check_legality -verbose command to verify that the placement is legal.\n Running clock tree synthesis on a block that does not have a legal placement might result in long runtimes and reduced QoR.\n The estimated QoR for the block should meet your requirements before you start clock tree synthesis.\n This includes acceptable results for  Congestion If congestion issues are not resolved before clock tree synthesis, the addition of clock trees can increase congestion.\n If the block is congested, you can rerun the       create_placement command with the -congestion and -congestion_effort high options, but the runtime can be long.\n  Timing  Maximum capacitance  Maximum transition time  The power and ground nets are prerouted.\n  High-fanout nets, such as scan enables, are synthesized with buffers.\n  The active scenarios are defined.\n By default, the Fusion Compiler tool synthesizes and optimizes all clocks in all active scenarios that are enabled for setup or hold analysis."}
{"instruction": "How do I Specifying the Library Cells for Boundary Cell Insertion", "input": "", "output": "To specify the boundary cell insertion requirements, use the set_boundary_cell_rules command.\n You use this command to specify the following requirements:  The library cells to use for the boundary cells, as described in Specifying the Library Cells for Boundary Cell Insertion  The rules used to place the boundary cells, as described in Specifying Boundary Cell Placement Rules  The naming convention used for the inserted cells, as described in Specifying the Naming Convention for Boundary Cells  The creation of routing guides to honor the metal cut allowed and forbidden preferred grid extension rules, as described in Creating Routing Guides During Boundary Cell Insertion       Note: This feature is supported only by the compile_boundary_cells command; it is not supported by the compile_targeted_boundary_cells command.\n  The creation of placement blockages to honor the minimum jog and minimum separation rules, as described in Creating Placement Blockages During Boundary Cell Insertion Note: This feature is supported only by the compile_boundary_cells command; it is not supported by the compile_targeted_boundary_cells command.\n Note: The settings specified by the set_boundary_cell_rules command are saved in the design library.\n See Also  Reporting the Boundary Cell Insertion Requirements"}
{"instruction": "How do I Specifying Constraints for Clock Nets", "input": "", "output": "By default, initial clock tree synthesis uses the virtual router to build the clock trees.\n For designs with complex floorplans, this can introduce congestion hotspots and degrade clock QoR after clock routing.\n The tool can use the global router during the initial clock tree synthesis stage of the synthesize_clock_trees and clock_opt commands.\n To enable this feature, set the cts.compile.global_route_aware_buffering application option to true"}
{"instruction": "How do I Specifying Ideal Voltage Sources as Taps", "input": "", "output": "_integrity -value true  clock_opt -from final_opto The following example uses a customized python script for concurrent clock and data optimization on a multicorner and multimode design on a grid system.\n       #Required.\n Specify the below options to enable MCMM-based optimization.\n set_app_options -list { rail.enable_new_rail_scenario true rail.enable_parallel_run_rail_scenario true  set_app_options -name rail.generate_file_type -value python  #Use user-defined test_def_files for DEF variable to match #with customized python set_app_options -name rail.generate_file_variables \\ -value {DEF test_def_files }  create_rail_scenario -name func.ss_125c -scenario func.ss_125c set_rail_scenario -name func.ss_125c -voltage_drop dynamic  create_rail_scenario -name func.ff_125c_ -scenario func.ff_125c set_rail_scenario -name func.ff_125c \\ -generate_file {PLOC DEF SPEF TWF LEF} \\ -custom_script_file./customized.py  set_host_options -submit_protocol sge \\ -submit_command {qsub -V -notify -b y -cwd -j y \\ -P bnormal -l mfree=16G} # Enable IR-driven CCD analysis before running route_opt set_app_options -name opt.common.power_integrity -value true route_opt See Also  Specifying RedHawk and RedHawk-SC Working Directories"}
{"instruction": "How do I Placing and Mapping ECO Cells to Spare Cells", "input": "", "output": "To minimize the disturbance to the existing placement during ECO placement, use the following options with the place_eco_cells command:  -legalize_mode minimum_physical_impact  -max_displacement_threshold  -remove_filler_references"}
{"instruction": "How do I Controlling Clock Latencies, Path Groups, and Boundary Paths", "input": "", "output": "By default, the tool performs concurrent clock and data optimization during the compile_fusion command.\n You can disable it by setting the compile.flow.enable_ccd application option to false.\n The default is true During concurrent clock and data optimization,  The tool uses ideal clock tree, and it updates clock latencies and balance points for all active scenarios.\n  The tool adjusts the register latencies within the delayed and advanced latency settings for the most critical sequential elements in the design.\n By default, the maximum delayed latency is 100 ps, and the maximum advanced latency is 300 ps.\n  All path groups are considered for useful skew computation.\n  All boundary timing paths are considered during concurrent clock and data optimization.\n Note: In an incremental compile, enabling concurrent clock and data optimization has no effect.\n For best results, you should also do the following:  Enable the hold scenario even during compile to reduce hold violations that are caused by clock latency insertions during concurrent clock and data optimization.\n The tool considers the hold scenario only for concurrent clock and data optimization, but not for other optimization steps.\n  Enable concurrent clock and data optimization for the clock_opt command."}
{"instruction": "How do I Optimizing the Fanout of a Net", "input": "", "output": "When you set the design.eco_freeze_silicon_mode application option to true and run the add_buffer_on_route command in the freeze silicon mode, the tool checks if a spare cell is available within a distance of five times the unit site height before adding a buffer.\n If a spare cell is not available, the tool does not add the buffer.\n  To control this distance, use the -max_distance_to_spare_cell option.\n  To disable this feature, use the -not_spare_cell_aware option."}
{"instruction": "How do I Making ECO Changes Using the eco_netlist Command", "input": "", "output": "The Fusion Compiler tool supports programmable spare cells, also known as gate array filler cells, if they are provided by your vendor.\n These cells can be programmed by metal mask changes for ECO implementation, reducing mask costs and time-to-results.\n To insert programmable spare cells, use the following steps: 1.\n Specify the programmable spare cell that a standard cell can map to by setting the same psc_type_id attribute setting on the corresponding library cell for the standard cell and the programmable spare cell.\n The following example specifies that  The standard cells named BUF1 and INV1 can map to the programmable spare cell named fill1x by setting the psc_type_id attribute to for the corresponding library cells.\n  The standard cells named NAND2 and NOR2 can map to the programmable spare cell named fill2x by setting the psc_type_id attribute to for the corresponding library cells.\n fc_shell> set_attribute [get_lib_cells fill_lib/fill1x] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells stdcell_lib/BUF1] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells stdcell_lib/INV1] \\ psc_type_id 1 fc_shell> set_attribute [get_lib_cells fill_lib/fill2x] \\ psc_type_id 2 fc_shell> set_attribute [get_lib_cells stdcell_lib/NAND2] \\ psc_type_id 2 fc_shell> set_attribute [get_lib_cells stdcell_lib/NOR2] \\ psc_type_id 2 2.\n Insert the programmable spare cells by using the create_stdcell_fillers command.\n The following example inserts programmable spare cells named fill1x and fill2x: fc_shell> create_stdcell_fillers \\ -lib_cells {fill_lib/fill1x fill_lib/fill2x} During the ECO flow, the tool swaps an ECO cell with a programmable spare cell based on the psc_type_id attribute setting, cell width, and voltage area.\n If the tool removes an ECO cell from the design, it can reuse the corresponding programmable spare cell."}
{"instruction": "How do I Controlling Placement When Using the place_eco_cells Command", "input": "", "output": "For the unconstrained ECO flow, place the ECO cells by using the place_eco_cells command.\n This command derives the placement of each ECO cell based on the connectivity and the delays associated with the cell.\n The tool then legalizes each ECO cell to the closest unoccupied site.\n The existing cells are untouched to minimize the impact to their placement.\n To place  All unplaced cells, use the -unplaced_cells option.\n  Specific cells, use the -cells option.\n  Only the cells that have changed due to ECO operations, use the -eco_changed_cells option.\n A cell is considered changed if the eco_change_status attribute of the cell has one of the following values:  create_cell  change_link  add_buffer  size_cell When you run the eco_netlist command, the tool automatically sets the eco_change_status attribute on the changed cells.\n If you update the design without using the eco_netlist command, you can manually set the eco_change_status attribute by using the set_attribute command."}
{"instruction": "How do I Shielding Example", "input": "", "output": "To perform postroute shielding, you use the same command, create_shields, that is used for preroute shielding.\n If you want to use the default spacings and widths from the technology file for postroute shielding, you do not need to define and assign nondefault routing rules.\n If you specify the nets to be shielded by using the -nets option, the create_shields command shields these nets with the default spacing and widths.\n Postroute shielding should introduce few to no DRC violations.\n If DRC violations are created during shielding, the create_shields command triggers five detail routing iterations to fix them.\n If this does not fix the DRC violations, you can fix the remaining violations by running incremental detail routing with the route_detail -incremental true command.\n It is possible that postroute shielding might break some tie-off connections during the shield trimming process.\n In this case, use the route_eco command instead of the route_detail command to rebuild the tie-off connections and to fix the DRC violations."}
{"instruction": "How do I Swapping Variant Cell", "input": "", "output": "Cell input mode enables to set distance based on route placement.\n By default, repeaters are placed based on even distance on route for one repeater path.\n This mode allows you to set the -first_distance or -last_distance option to control the distance at driver side or load side.\n 1.\n Use the place_group_repeaters -cells -blockage_aware command, to internally enable auto grouping.\n To internally enable auto grouping use the create_repeater_groups -cells command and derive cutline based on distance.\n Then, define repeater group by using the set_repeater_group -group_id -cutline command, After placing group repeaters, you can query repeater groups using the report_repeater_groups command and repeater paths info using the get_repeater_paths_info command.\n 2.\n To establish cell connection based on real connection and apply distance based placement, use the place_group_repeaters -cells command.\n No repeater group is created."}
{"instruction": "How do I Defining Keepout Margins", "input": "", "output": "During placement and legalization, the floorplan information dictates where cells are placed.\n The following topics describe how to specify additional physical constraints that affect placement and legalization:  Defining Keepout Margins  Defining Area-Based Placement Blockages  Defining Placement Bounds  Defining Placement Attractions  Specifying Locations for Unmapped Cells  Defining Cell Spacing Constraints for Legalization"}
{"instruction": "How do I Reporting Multicore Configurations", "input": "", "output": "Distributed processing performs tasks in parallel by using multiple machines; each process uses its own process memory image.\n When using distributed processing, each parallel task is called a process.\n For the best performance during distributed processing, you should limit the number of processes to the number of available cores, which is the sum of the number CPUs times the number of cores per CPU for each host machine.\n The following commands support distributed processing configured by the set_host_options command:  analyze_rail  create_placement -floorplan  signoff_check_drc  signoff_fix_drc  signoff_create_metal_fill  signoff_fix_isolated_via When you configure distributed processing, you can specify one or more of the following settings:  The job submission command (the -submit_command option) If you do not specify this option, the tool uses the rsh command to submit the parallel processes.\n  The list of host machines (the host_names argument)  The maximum number of processes (the -num_processes option) By default, the tool assigns a name to each configuration you define with the set_host_options command.\n To specify the configuration name, use the -name option.\n You use the configuration name to select the configuration to use for specific commands and to remove a configuration.\n For example, to specify a distributed processing configuration that uses the qsub command to submit the parallel processes, use the following command: fc_shell> set_host_options -name dp_config \\ -submit_command [list qsub -P bnormal -cwd]"}
{"instruction": "How do I Controlling Legalization When Using the place_eco_cells", "input": "", "output": "The place_eco_cells command places and legalizes each ECO cell based on the connectivity and the delays associated with the cell.\n You can control the placement of the ECO cells as follows:  To use the channels areas between macro cells for ECO placement, use the -channel_aware option.\n By default, the command avoids channel areas during ECO placement.\n        To place ECO cells closer to fixed points, such as I/O pads or macro cells, specify a weight for the nets connected to the fixed cells by using the -fixed_connection_net_weight option.\n By default, nets connected to fixed cells have a weight of one.\n To place ECO cells closer to fixed points, specify an integer value greater than one for the corresponding net.\n  To prioritize specific nets during ECO placement by specifying net weights, 1.\n Specify net weights by using the set_eco_placement_net_weight command.\n 2.\n Specify that the tool honors the net weights by using the -honor_user_net_weight option with the place_eco_cells command.\n To report the net weights you specify, use the report_eco_placement_net_weight command.\n  To create virtual connections for ECO cells and use them during ECO placement, instead of the actual connections, 1.\n Create virtual connections by using the create_virtual_connection command 2.\n Use these virtual connections during ECO placement by using the -use_virtual_connection option with the place_eco_cells command.\n The following example creates virtual connections for the inputs and output of the cell named ECO17 and performs ECO placement using these virtual connections: fc_shell> create_virtual_connection -name VC_0 \\ -pins {U1/Y ECO17/A} fc_shell> create_virtual_connection -name VC_1 \\ -pins {U2/Y ECO17/B} fc_shell> create_virtual_connection -name VC_2 \\ -pins {ECO17/Y D17_OUT} -weight 3 fc_shell> place_eco_cells -cells ECO17 -use_virtual_connection To remove virtual connections you have created, use the remove_virtual_connections command.\n To query virtual connections, use the get_virtual_connections command.\n  To ignore high-fanout nets connected to ECO cells that exceed a specific threshold, use the -max_fanout option with the place_eco_cells command.\n  To ignore the connections of specific pins on ECO cells, use the -ignore_pin_connection option with the place_eco_cells command."}
{"instruction": "How do I Timing-Driven Metal Fill Insertion", "input": "", "output": "Typical critical dimension (TCD) structures can improve yield for designs.\n These structures are made up of marker and fill layers, both of which are added to the block during TCD metal fill insertion.\n To enable TCD metal fill insertion, set the signoff.create_metal_fill.tcd_fill application option to true before running the signoff_create_metal_fill command.\n By default, the command inserts TCD structures on all metal and via layers for the entire block.\n To modify the default behavior, use the following options:  -select_layers To restrict the layers on which to insert metal fill, use this option, as described in Specifying the Layers for Metal Fill Insertion  -coordinates and -excluded_coordinates To restrict the regions on which to insert metal fill, use these options, as described in Specifying the Regions for Metal Fill Insertion Note: You cannot use timing-driven metal fill insertion with TCD metal fill insertion.\n See Also  Removing Metal Fill"}
{"instruction": "How do I Controlling Register Replication", "input": "", "output": "During compile, the tool can automatically identify shift registers in the design.\n The compile_fusion -to initial_opto command maps the first register to a scan cell and the other cells to nonscan cells, as shown in this figure.\n   ___ RST  SI SE CLK       SO This capability improves the sequential design area and reduces congestion because of fewer scan-signals for routing.\n To disable this capability, set the compile.seqmap.identify_shift_registers application option to false.\n The default is true"}
{"instruction": "How do I Defining Clock Tree Exceptions", "input": "", "output": "When deriving the clock trees, the tool identifies two types of clock endpoints:  Sink pins clock tre synthesistop pindefinedexceptions for clock tre synthesistop pinsclock tre exceptionstop pinclock tre sinksdefining Sink pins are the clock endpoints that are used for delay balancing.\n The tool assigns an insertion delay of zero to all sink pins and uses this delay during delay balancing.\n During clock tree synthesis, the tool uses sink pins in calculations and optimizations for both design rule constraints and clock tree timing (skew and insertion delay).\n Sink pins are also referred to as balancing pins.\n  Ignore pins clock tre synthesisexclude pindefinedexceptions for clock tre synthesisexclude pinsclock tre exceptionsexclude pin Ignore pins are clock endpoints that are excluded from clock tree timing calculations and optimizations.\n The tool uses ignore pins only in calculations and optimizations for design rule constraints.\n During clock tree synthesis, the tool isolates ignore pins from the clock tree by inserting a guide buffer before the pin.\n Beyond the ignore pin, the tool never performs skew or insertion delay optimization, but does perform design rule fixing.\n The tool identifies the following clock endpoints as sink pins:  A clock pin on a sequential cell (a latch or flip-flip), unless that cell drives a generated clock  A clock pin on a macro cell       The tool identifies the following clock endpoints as clock tre synthesisimplicitexclude pin ignore pins:  Source pins of clock trees in the fanout of another clock For example, in, the source pin of the driven clock (clk2) is an ignore pin of the driving clock (clk1).\n Sinks of the driven clock are not considered sinks of the driving clock.\n  Cascaded Clock With Two Source Clocks CLK clk1 clk2  Nonclock input pins of sequential cells  Three-state enable pins  Output ports  Incorrectly defined clock pins (for example, the clock pin does not have trigger edge information or does not have a timing arc to the output pin)  Buffer or inverter input pins that are held constant by using the set_case_analysis command Note: The tool does not synthesize a clock tree if its source is held constant by using the set_case_analysis command.\n  Input pins of combinational cells or integrated clock-gating cells that do not have any fanout or that do not have any enabled timing arcs To verify that the tool has correctly identified the sink pins and ignore pins, examine the clock trees in the GUI, as described in Analyzing Clock Trees in the GUI If the default sink and ignore pins are correct, you are done with the clock tree definition.\n Otherwise, first identify any timing settings, such as disabled timing arcs and case analysis settings, that affect the clock tree traversal.\n To identify disabled timing arcs in the block, use the report_disable_timing command.\n To identify case analysis settings in the block, use the report_case_analysis command.\n Remove any timing settings that cause an incorrect clock tree definition.\n       If necessary, you can override the default balancing and ignore pin settings by using the set_clock_balance_points command, as described in Defining Clock Tree Exceptions The implicit ignore on an intermediate pin can be inferred by any of the following reasons:  All the register pins in the fanout are EXPLICIT IGNORE  Disabled path on a MUX input is caused by the set_case_analysis command  Disabled timing arc on a downstream pin is set by the set_disable_timing set_false_path commands, disable timing due to conditional arcs.\n  set_sense -stop_propogation constraint on a downstream pin  Dangling pin in the downstream path  Downstream path goes to a data pin of a register or a subblock  Downstream path has an unconstrained output port without any set_clock_balance_point -consider_for_balancing true setting  Downstream path has a constrained set_output_delay output port  Pins on the path to cascaded create_clock definition Note: This implicit or explicit ignore does not essentially mean that the clock endpoints are excluded from clock tree calculations.\n Explicit ignore means, the endpoints are not considered for skew balancing with other endpoints of the clock, but the portion is considered for DRC fixing in clock tree."}
{"instruction": "How do I Modifying Routed Nets", "input": "", "output": "To route nets interactively in the active layout view, draw the routes with the layout editing tolsCreate Route tol Create Route tool.\n To activate the Create Route tool, click the button on the Edit toolbar or choose Create > Route.\n To draw route segments, you click points in the layout view.\n The tool displays flylines and target port and pin locations to guide you in drawing the route segments.\n It can also check for routing design rule violations as you draw the route segments.\n You can set options to adjust the routing, control the tool operation, and enable or disable routing aids.\n By default, the Create Route tool  Ignores routing blockages To force the tool to honor these blockages, change the setting in the Mouse Tool Options panel.\n Note: The Create Route tool does not honor routing guides defined by the create_routing_guide command.\n  Uses the metal width and metal spacing requirements defined in the technology file and ignores nondefault routing rules To force the tool to honor nondefault routing rules, change the setting in the Mouse Tool Options panel.\n When you enable this feature, the Create Route tool honors the metal width and metal spacing requirements from the nondefault routing rule and uses these settings to determine the width and pitch of the routes.\n Note: The shielding width and shield spacing defined in the nondefault routing rule are not used by the Create Route tool.\n       If you route on grid and know that there no obstacles, you can reduce runtime by skipping the spacing checks during automatic welding and automatic alignment.\n To force the tool to ignore the spacing requirements during these tasks, change the settings in the Mouse Tool Options panel.\n You can reverse and reapply Create Route tool operations by using the GUI undo and redo capabilities.\n For detailed information about using the Create Route tool, click on the Mouse Tool Options panel after you activate the Create Route tool.\n This opens a Help page in the man page viewer."}
{"instruction": "How do I Using Application Options", "input": "", "output": "Command help consists of either a brief description of aFusion Compiler command or a list of the options and arguments supported by an Fusion Compiler command.\n  To display a brief description of a Fusion Compiler command, enter the help command followed by the command name.\n For example, to display a brief description of the report_timing command, use the following command: fc_shell> help report_timing  To display the options supported by a Fusion Compiler command, enter the command name with the -help option on the command line.\n For example, to see the options supported by the report_timing command, use the following command: fc_shell> report_timing -help"}
{"instruction": "How do I Reducing Critical Areas", "input": "", "output": "During detail routing, Zroute optimizes wire length and via count in the areas where DRC violations occur; however, it does not optimize the layout in areas where no DRC violations occur.\n To improve the manufacturing yield, use the optimize_routes command to perform standalone optimization of wire length and via count after performing detail routing and redundant via insertion.\n By default, Zroute selects the nets to reroute based on the overall cost.\n For each selected net, Zroute determines whether to reroute all the shapes in the net or just a portion of them.\n To select the nets to reroute, use the -nets option to specify the nets.\n When you specify the nets to optimize, you can also use the -reroute_all_shapes_in_nets option to control whether Zroute must reroute all the associated net shapes.\n By default, Zroute performs a maximum of 40 detail routing iterations to fix DRC violations that exist after the optimization.\n You can use the -max_detail_route_iterations option to control the maximum number of detail routing iterations."}
{"instruction": "How do I Inserting Via Ladders", "input": "", "output": "To define instance-specific via ladder constraints, use the set_via_ladder_constraints command.\n You must specify the instance pins and the via ladder templates that can be used for those pins.\n For example, to enable the use of the VL1, VL2, and VL3 via ladder templates for the u1/i1 and u2/i2 pins, use the following command: fc_shell> set_via_ladder_constraints -pins {u1/i1 u2/i2} \\ {VL1 VL2 VL3} To report the via ladder constraints, use the report_via_ladder_constraints command.\n To remove via ladder constraints, use the remove_via_ladder_constraints command."}
{"instruction": "How do I Implementing Clock Trees and Performing Post-CTS Optimization", "input": "", "output": "To report the settings that are used by clock tree synthesis, use the report_clock_settings command.\n By default, the commands reports the clock tree configuration, including the design rule constraints, target skew, and target latency; the clock tree references; the clock cell spacing rules; and the nondefault routing rules for all clocks in the current block.\n To report the settings for specific clocks, use the -clock option to specify the clocks of interest.\n To report specific information, use the -type option with the appropriate keyword.\n  To report the clock tree design rule constraints, target skew, and target latency, use -type configurations  To report the clock tree references, use -type references  To report the clock cell spacing rules, use -type spacing_rules  To report the nondefault routing rule used for the clock nets, use -type routing_rules"}
{"instruction": "How do I Enabling the Embedded Area Optimization Flow", "input": "", "output": "On designs that need further reduction of area, set the compile.flow.high_effort_area application option to true Running high-effort area mode during compile might increase runtime because the tool performs multiple passes of placement and optimization to obtain optimal area results."}
{"instruction": "How do I Checking Routability", "input": "", "output": "To remove via ladders and their associated shapes, use the remove_via_ladders command.\n You must specify the via ladders to remove.\n To remove all via ladders from the block, use the asterisk (*) wildcard character, as shown in the following example: fc_shell> remove_via_ladders * To remove via ladders only from specific nets, use the -nets option."}
{"instruction": "How do I Enabling High-Effort Timing Mode", "input": "", "output": "The following topics provide information settings you can use to improve performance, power, and area (PPA) of a design:  Enabling High-Effort Timing Mode  Enabling Enhanced Delay Optimization to Improve Total Negative Slack  Enabling High-Effort Area Mode  Enabling the Embedded Area Optimization Flow"}
{"instruction": "How do I 13", "input": "", "output": "When rail analysis or missing via checking is complete, use the get_attribute attribute to query the results stored in the RedHawk working directory.\n You can query the attributes shown in Table 71, which are specific to the rail analysis results.\n Table 71 Rail Result Object Attributes Object Attributes  static_power       Table 71 Rail Result Object Attributes (Continued) Object Attributes  static_current peak_current static_power switching_power leakage_power internal_power min_path_resistance voltage_drop average_effective_voltage_drop_in_tw max_effective_voltage_drop (=effective_voltage_drop) max_effective_voltage_drop_in_tw min_effective_voltage_drop_in_tw effective_resistance The following example retrieves the effective_voltage_drop attribute for the cellA/VDD pin.\n fc_shell> get_attribute [get_pins cellA/VDD] effective_voltage_drop -0.02812498807907104 The following example retrieves the static_power attribute for the cellA cell.\n fc_shell> get_attribute [get_cells cellA] static_power 2.014179968645724e-05"}
{"instruction": "How do I Creating Differential Groups", "input": "", "output": "To route a collection of nets or bundles as a single bus structure (trunk), create a constraint group that defines the bus routing style by using the create_bus_routing_style command.\n You need to specify the following information:  The name of the constraint group If you do not specify a name, the command names the constraint group bus_style_ where is a unique integer.\n By default, the command returns an error if the assigned name already exists.\n To delete the existing constraint group, use the -force option.\n  The nets, bundles, and topology edges to which the constraint group applies To specify the objects that constitute the constraint group, use the -for option.\n  The constraints To specify the constraints that are applied per layer to the trunk and override any settings on the individual bits, use the options listed in Table 53       Table 53 Bus Routing Style Constraints Option Description -valid_layers    -layer_spacings  {M1 5.0 M2 6.0} -layer_widths  {M1 5.0 M2 6.0} -shield_placement  default  custom.route.bus_intra_shield_placement double_interleave half_interleave interleave  outside:   -corner_type   auto   cross river   -gap   The following example creates a bus routing constraint group for the nets that match the pattern pr*.\n The command names the constraint group bus_style_1, because no name is assigned to the constraint group.\n The shield placement is interleave.\n The minimum spacing is 3 microns for the M2 layer and 5 microns for the M3 layer.\n fc_shell> create_bus_routing_style \\ -for [get_nets -physical_context pr*] \\ -shield_placement interleave \\ -layer_spacings {M2 3 M3 5} {bus_style_1}"}
{"instruction": "How do I Considering DFT Connections During Placement", "input": "", "output": "To minimize many cells from being displaced during legalization, the tool should be aware of legalization limits and restrictions during coarse placement.\n For example, the tool should be aware that it cannot place a wide cell in a specific location due to the position of the power straps at that location.\n To consider legalization effects during placement, set the place.coarse.enhanced_legalizer_driven_placement application option to true Use this feature for blocks that have many cells with small to medium displacements during legalization and use the RMS displacement values reported during legalization to see if it reduces the displacement."}
{"instruction": "How do I Specifying the Net Names, Buffers Types, and Their Locations", "input": "", "output": "To add buffers or pairs of inverters on a fully routed net, use the add_buffer_on_route command as described in the following topics:  Specifying the Net Names, Buffers Types, and Their Locations  Controlling How Buffers are Added  Specifying Settings for Multivoltage Designs  Specifying Settings for the Freeze Silicon ECO Flow"}
{"instruction": "How do I Controlling Mapping and Optimization", "input": "", "output": "You can generate intermediate verification checkpoints during the compile_fusion command, which you can use in the Formality tool to provide a netlist snapshot of the synthesis design state and verify the intermediate netlists.\n Verification checkpointing allows the Fusion Compiler and Formality tools to synchronize on an intermediate netlist and in turn results in higher completion rates and better QoR.\n To enable verification checkpointing, use the set_verification_checkpoints command as shown in the following example: fc_shell> set_verification_checkpoints Enabling checkpoint \"ckpt_logic_opt\" Enabling checkpoint \"ckpt_pre_map\" When you enable this feature, by default, the compile_fusion command generates verification checkpoints before mapping and during logic optimization, and the corresponding verification-checkpoint stages are named ckpt_pre_map and ckpt_logic_opt.\n You can enable verification checkpointing at only one stage as shown in the following example: fc_shell> set_verification_checkpoints {ckpt_logic_opt} Disabling checkpoint \"ckpt_pre_map\" Enabling checkpoint \"ckpt_logic_opt\" At each verification checkpoint, the tool automatically generates a checkpoint netlist and the guide_checkpoint command in the.svf file that is used by the Formality tool during verification."}
{"instruction": "How do I Controlling Concurrent Clock and Data Optimization", "input": "", "output": "During placement, the tool can use the voltage (IR) drop values of cells to identify areas of high power density and spread the cells with high voltage drop values, which reduces the power density of such areas.\n To perform IR-drop-aware placement during the clock_opt command, use the following steps: 1.\n Run the clock_opt command through the route-clock stage by using the clock_opt -to route_clock command.\n 2.\n Set up for RedHawk Fusion and perform static or dynamic voltage drop analysis by using the analyze_rail -voltage_drop command as shown in the following example: fc_shell> source redhawk_setup.tcl fc_shell> analyze_rail -voltage_drop static -nets {VDD VSS} For more information, see Performing Voltage Drop Analysis 3.\n Enable IR-drop-aware placement by setting the place.coarse.ir_drop_aware application option to true       4.\n (Optional) Specify additional settings for IR-drop-aware placement, as described in Controlling IR-Drop-Aware Placement 5.\n Run the final optimization stage of the clock_opt command by using the clock_opt -from final_opto command."}
{"instruction": "How do I Performing IR-Drop-Aware Placement During the clock_opt", "input": "", "output": "If you enable concurrent clock and data optimization for the clock_opt command, the tool performs clock power recovery on clock cells and registers during the final_opt stage.\n       If you do not enable concurrent clock and data optimization, you can enable clock power recovery by using the following application option setting: fc_shell> set_app_options \\ -name clock_opt.flow.enable_clock_power_recovery \\ -value power Before you perform power recovery using this technique, you must 1.\n Enable scenarios for dynamic, leakage, or total power optimization by using the -dynamic_power and - leakage_power options of the set_scenario_status command 2.\n (Optional) Provide a switching activity by using the read_saif command If you do not provide switching activity, the tool uses default switching activity for dynamic power recovery.\n Instead of power recovery, you can enable area recovery on the clock cells and registers during the final_opt stage clock_opt command by using the following application option setting: fc_shell> set_app_options \\ -name clock_opt.flow.enable_clock_power_recovery \\ -value area"}
{"instruction": "How do I Adding Buffers on a Bus in a Specified Pattern", "input": "", "output": "To add buffers at an interval that is a ratio of the total net length, specify  A list of one or more library cells to select from by using the -lib_cell option  The distance between the driver and the first buffer as a ratio of the total net length by using the -first_distance_length_ratio option  The distance between the buffers as a ratio of the total net length by using the -repeater_distance_length_ratio option The following example adds the BUF1 library cell on the nets n3, n21, and n41 at an interval that is 20 percent of the total net length.\n The distance between the driver and the first repeater cell is 10 percent of the total net length.\n fc_shell> add_buffer_on_route {n3 n21 n41} -lib_cell mylib/BUF1 \\ -first_distance_length_ratio 0.1 \\ -repeater_distance_length_ratio 0.2 By default, the tool uses eco_cell and eco_net ad the name prefix of all new cells and nets.\n To specify a name prefix for the new cells and nets, use the -cell_prefix and -net_prefix options, respectively."}
{"instruction": "How do I Clock-Gating Enable Condition", "input": "", "output": "Before gating the clock signal of a register, the Fusion Compiler tool checks to see if certain clock-gating conditions are satisfied.\n The tool inserts a clock gate only if all the clock-gating conditions are met:  The circuit demonstrates synchronous load-enable functionality.\n  The circuit satisfies the setup condition.\n  The register bank or group of register banks satisfies the minimum number of bits you specify with the set_clock_gating_options -minimum_bitwidth command.\n The default minimum bitwidth is 3.\n After clock gating is complete, the status of clock-gating conditions for gated and ungated register banks appears in the clock-gating report.\n For information about the clock-gating report, see Reporting Clock-Gating Results Clock-Gating Conditions The register must satisfy the following conditions for the tool to gate the clock signal of the registers:  Enable condition If the register bank's synchronous load-enable signal is a constant logic 1, reducible to logic 1, or logic 0, the condition is false and the circuit is not gated.\n If the synchronous load-enable signal is not a constant logic 1 or 0, the condition is true and the setup condition is checked.\n The enable condition is the first condition that the tool checks.\n For more information, see Clock-Gating Enable Condition  Setup condition This condition applies to latch-free clock gating only.\n The enable signal must come from a register that uses the same clock as the register being gated.\n The setup condition is checked only if the register satisfies the enable condition.\n For more information, see Clock-Gating Setup Condition  Width condition The width condition is the minimum number of bits for gating registers or groups of registers with equivalent enable signals.\n The default is 3.\n You can set the width condition by using the -minimum_bitwidth option of the set_clock_gating_options command.\n The width condition is checked only if the register satisfies the enable condition and the setup condition."}
{"instruction": "How do I Performing On Route Placement of Repeaters", "input": "", "output": "Use the preplace_group_repeaters command to place the group repeaters before the routing.\n This command places the group repeaters either in an ascending or descending order, default is ascending.\n The ascending order means that the command places the cells in a group from the lowest or left-most available track to the highest or right-most track.\n Note: A FAA-Base-Beta license is required to run the preplace_group_repeaters command.\n You must specify either the cell collections or group IDs of the group repeaters.\n The following example places group repeaters when the cell collections are specified.\n fc_shell> preplace_group_repeaters \\ -repeater_group_locations { { $cellGroup1, M2, {720 363.5}, \\ north, descending} {$cellGroup2, M2, {920 363.5}, east, ascending} \\       {$cellGroup3, M2, {1230 328.5}, east, ascending} \\ {$cellGroup4, M2, {1430 328.5}}, east} The following example places group repeaters when the group ID is specified.\n fc_shell> preplace_group_repeaters \\ -repeater_group_locations { { 1,M2, {720 363.5}, south, ascending} \\ {2, M2, {920 363.5},west, ascending} {3, M2, {1230 328.5}, \\ south, ascending} {4, M2, {1430 328.5}, west, descending}}"}
{"instruction": "How do I Reporting Shielding Information", "input": "", "output": "By default, Zroute does not perform incremental shielding on nets that are modified after they were shielded.\n However, you can enable this capability for nets that were initially shielded with the create_shields command by changing the route.common.reshield_modified_nets application option from its default of off When you enable incremental shielding, Zroute performs incremental shielding during detail routing by removing the existing shielding from the modified nets and optionally reshielding these nets based on the new topology.\n  To remove the existing shielding only, set the route.common.reshield_modified_nets option to unshield  To remove the existing shielding and reshield the modified nets, set the route.common.reshield_modified_nets option to reshield Note: Zroute automatically detects the nets modified within the Fusion Compiler tool; however, nets modified externally and input by reading a DEF file are not supported by incremental shielding."}
{"instruction": "How do I Inserting Filler Cells", "input": "", "output": "A metal-insulator-metal (MiM) capacitor is made of two special-purpose conducting layers separated by an insulator.\n This capacitor is inserted between two regular metal layers, as shown in  Cross-Section View of MiM Capacitor Layers maskName = \"mimtop\" M9 M9 M9 M8 maskName = \"mimbottom\" via via via M9 layer M8 layer MBOT layer MTOP layer MiM capacitor maskName = \"viaMimtop\" maskName = \"viaMimbottom\"       MiM capacitors are typically connected between power and ground to help maintain a constant supply voltage in the presence of electrical noise.\n The dimensions of a MiM capacitor are usually customized to fit the power strap geometry of a specific power plan.\n To insert an array of MiM capacitors into a block and connect them to the power rails, use the create_mim_capacitor_array command.\n At a minimum, you must specify the MiM capacitor library cell, as well as the x- and y-increments of the array.\n For example, fc_shell> create_mim_capacitor_array \\ -lib_cell my_lib/mim_ref_cell -x_increment 20 -y_increment 20 By default, the create_mim_capacitor_array command  Inserts MiM capacitors for the entire block To restrict the insertion to a specific region, use the -boundary option to specify the rectangular or rectilinear region.\n You can specify only a single region.\n The command does not consider standard cells, macros, placement blockages, or voltage areas when inserting the MiM capacitors.\n  Inserts the MiM capacitors in the R0 orientation To change the orientation, use the -orientation option.\n When you change the orientation, it affects all MiM capacitor cells in the inserted array.\n  Uses the following naming convention for the inserted MiM capacitor cells: mimcap!\n library_cell_name number To identify the MiM capacitor cells inserted in a specific run, use the -prefix option to specify a prefix string.\n When you use this option, the tool uses the following naming convention: mimcap!\n prefix library_cell_name number"}
{"instruction": "How do I Setting Sequential Clock Gating Options", "input": "", "output": "Sequential clock gating is the process of extracting and propagating enable conditions from upstream or downstream sequential elements in the data path for the purpose of enabling clock gate insertion for additional registers.\n The Fusion Compiler tool supports fanin-based sequential clock gating, in which a register can be gated during a given clock cycle if all the registers in the transitive fanin did not change state in the previous clock cycle.\n To enable this feature, set the comple.clockgate.fanin_sequential application option to true (the default is false ).\n This feature requires the FC-New-Tech-Power license.\n  and illustrate the general concept of fanin-based sequential clock gating.\n The circuit in has a register bank R1 whose transitive fanin consists       of register bank R0.\n All registers in bank R0 share the same enable condition EN0 and all registers in both register banks are clocked by clock CLK.\n If in one clock cycle the registers in bank R0 are disabled (in other words, enable signal EN0 is 0), those registers do not change their values.\n Therefore, in the next clock cycle, the registers in bank R1 do not change their values and consequently they are candidates for clock gating.\n  Before Clock Gating  shows the clock-gate implementation.\n Clock gate CG1 is first inserted for register bank R0.\n The tool creates a new register RD1 to generate a delayed version of enable signal EN0.\n The delayed signal drives sequential clock gate SEQCG1 for register bank R1.\n  After Clock Gating The Fusion Compiler tool implements several different clock-gating strategies as needed, based on analyzing the circuit.\n For example, if the fanin registers do not all have the same enable signals, the tool looks for common signals upstream from the enable signals and uses those signals as inputs to the inserted delay register.\n Sequential clock gates are inserted by the compile_fusion command.\n During the initial_map stage, the tool analyzes the local hierarchy of the registers and inserts clock gates as needed.\n During the logic_opto stage, the tool performs analysis across hierarchical boundaries and inserts additional clock gates as needed.\n The name of a clock gate inserted as a result of fanin-based sequential clock gating begins with the prefix seq_clock_gate_fi_.\n The rest of the name follows standard clock-       gate naming conventions.\n If the tool inserts a delay register as a result of this feature, the register name begins with the prefix reg_seqcg_ followed by standard clock-gate naming conventions (in other words, the most common base name among the gated registers)."}
{"instruction": "How do I Finding Invalid Taps", "input": "", "output": "During validation checking, the tool marks each inserted tap object with the is_valid_location attribute to indicate its validity.\n When validation checking is enabled, the attribute value is true for the valid taps and false for the invalid taps.\n When the -nocheck option is used, the attribute value is unknown for all taps.\n The tool updates the tap attributes in subsequent voltage drop and minimum path resistance analyses.\n Each tap belonging to the PG nets is checked against the extracted shape.\n When the analysis is complete, the tool updates the tap validity status based on the rail analysis results.\n The following example first lists the attributes of tap objects that are added to the block in this command run, and then checks if the Tap_1437 tap passes the validation checking.\n fc_shell> list_attributes -application -class tap...\n Properties: A - Application-defined U - User-defined I - Importable from design/library (for user-defined) S - Settable B - Subscripted  Attribute Name Object Type Properties Constraints         context tap string A integrity, rail, session full_name tap string A is_valid_location tap boolean A layer_name tap string A layer_number tap int A net tap string A object_class tap string A parasitics tap string A position tap coord A static_current tap double A type tap string A absolute_coord, auto_pg, cell_coord, cell_pg, lib_cell_coord, lib_cell_pg, signal, terminal, top_pg fc_shell> get_attribute [get_taps Tap_1437] is_valid_location true"}
{"instruction": "How do I Abutting Standard Cells With Specific Filler Cells", "input": "", "output": ", use the -skip_fixed_cells option with the replace_fillers_by_rules command.\n       When the command finds one of the target cells, it replaces it by randomly selecting one of the replacement cells specified for that target cell.\n For example, the following command replaces FILL1 cells with one of FILL1a, FILL1b, or FILL1c and replaces FILL2 cells with one of FILL2a or FILL2b: fc_shell> replace_fillers_by_rules -replacement_rule random \\ -random_replace { {FILL1 {FILL1a FILL1b FILL1c}} {FILL2 {FILL2a FILL2b}} }"}
{"instruction": "How do I Exiting the Fusion Compiler", "input": "", "output": "The CLI interface is a text-only environment in which you enter commands at the command-line prompt.\n It is typically used for scripts, batch mode, and push-button operations.\n Before you start the command-line interface, ensure that the path to the bin directory is included in your $PATH variable.\n To start fc_shell, Enter the fc_shell command in a Linux shell.\n fc_shell You can include other options on the command line when you start the shell.\n For example, you can use  -file script_file_name to execute a script  -x command to execute a command  -output_log_file file_name to create a log file of your session  -help to display a list of the available options (without starting the shell) At startup, the tool : 1.\n Creates a command log file.\n 2.\n Reads and executes the setup files.\n 3.\n Executes any script files or commands specified by the -file and -x options, respectively, on the command line.\n 4.\n Displays the program header and fc_shell> prompt in the shell.\n See Also  Using the Command Log File  Using Setup Files  Using Tcl Scripts"}
{"instruction": "How do I Working With Design Libraries", "input": "", "output": "block is a container for physical and functional design data.\n A design library is a collection of related blocks, together with technology data that applies to the block collection.\n A chip design consists of one or more blocks, often stored in different design libraries.\n A design library uses instances of blocks defined in lower-level libraries, called reference libraries.\n A design library can serve as a reference library for another design library.\n To learn about setting up libraries, see the following topics:  Working With Design Libraries  Setting Up Reference Libraries  Library Configuration  Restricting Library Cell Usage  Restricting the Target Libraries Used  Specifying Library Subset Restrictions"}
{"instruction": "How do I Specifying RedHawk and RedHawk-SC Working Directories", "input": "", "output": "To run RedHawk Fusion or RedHawk-SC Fusion features, enable the features and specify the location of the RedHawk or RedHawk-SC executable by setting the rail.product and rail.redhawk_path application options.\n For example, fc_shell> set_app_options -name rail.product -value redhawk|redhawk_sc fc_shell> set_app_options -name rail.redhawk_path \\ -value /tools/RedHawk_Linux64e5_V19.0.2p2/bin You must ensure that the specified executable is compatible with the Fusion Compiler version that you are using.\n See Also  Specifying RedHawk and RedHawk-SC Working Directories"}
{"instruction": "How do I Working With the 3DIC Compiler User Interfaces", "input": "", "output": "Mask constraints indicate the mask requirements for the metal shapes of the physical pins and nets in a block that uses multiple-patterning technology.\n These mask requirements drive placement and routing to ensure that the resulting layout is multiple-patterning compliant.\n Note: Mask constraints are used only for the precolored flow; they are not necessary in the uncolored flow.\n You can set mask constraints on timing-critical nets (net shapes, routing rules, and routing blockages) and vias.\n For nets, the mask constraint is defined in the mask_constraint attribute.\n For vias, the mask constraints are defined in the lower_mask upper_mask, and cut_mask attributes.\n Table 1 shows the supported values for these attributes.\n Table 1 Mask Constraint Values Attribute value Description same_mask          Table 1 Mask Constraint Values (Continued) Attribute value Description mask_one    mask_two    mask_three    any_mask"}
{"instruction": "How do I Rebudgeting After compile_fusion", "input": "", "output": "The Fusion Compiler tool performs transparent hierarchical optimization during the final_opto stage, which ensures that all interblock and block-top paths are optimized as if the design were flat.\n In this manner, CCD optimizations can address any violations on these paths completely.\n fc_shell> init_hier_optimization -flow pre_route fc_shell> compile_fusion -from final_opto -to final_opto fc_shell> commit_hier_optimization"}
{"instruction": "How do I Grouping Variant Cell", "input": "", "output": "To set application option before using the add group repeaters and place_group_repeaters commands, use the set_app_option -name eco.placement.variant_cell_swapping -value true command.\n Based on the requirement, you can plan the variant cell flow.\n The rules of variant reference swapping differ in various variant cell flows.\n By default, the variant cell support in the eco_change_legal_reference command detects the variant mode and settings automatically.\n The eco_change_legal_reference command checks whether the current reference of the cell is legal or illegal for the placed location.\n If it is illegal, the command attempts to find a legal reference from its variant cell group defined to swap with the illegal one.\n       To use the variant cell features directly for other cells, use the eco_change_legal_reference command.\n For complicated scenarios, you can switch the variant mode and tune the settings manually.\n Following are the new application options for variant cell support:  To set variant mode for different customers and different design flows, use the set_app_options -name eco.placement.variant_cell_group_mode -value command.\n  To honor the variant cell groups, which are not sharing timing view, use the set_app_options -name eco.placement.honor_cell_group_not_sharing_timing -value command."}
{"instruction": "How do I Defining a group of repeaters", "input": "", "output": "To add a group of repeaters, use the add_group_repeaters command as described in the following sections:  Defining a group of repeaters  Grouping a list of repeaters  Setting Constraints for a Group of Repeaters  Adding Voltage Area Aware Group Repeaters        Reporting the Constraints Assigned to a Group of a Repeaters  Removing Constraints for a Group of Repeaters  Placing Group Repeaters Before Routing  Performing On Route Placement of Repeaters  Placing Group Repeaters For Multibit Registers  Specifying Locations for Repeater Groups  Allowing Repeater Groups Over Macros  Specifying Cut Space and Cut Distance for Repeater Groups  Specifying Horizontal and Vertical Spacing for Repeater Groups  Specifying Library Cells as Repeaters  Avoiding Overlapping Repeaters With Existing Tap Cells  Avoiding Crosstalk During Group Repeater Insertion  Previewing Repeater Groups  Unplacing the Repeaters  Removing Repeater Groups"}
{"instruction": "How do I Reporting Sequential Clock Gates", "input": "", "output": "Sequential clock gates inserted by the tool honor the settings in the set_clock_gating_style command, which applies to all clock gates.\n However, the following commands are specific to fanin-based sequential clock gates:  The set_fanin_sequential_clock_gating_options command This command specifies the minimum bitwidth and maximum fanout that a fanin-based sequential clock gate is allowed to gate.\n The default minimum bitwidth is 3 and the default maximum fanout is unlimited.\n You can specify a list of designs, power domains, and hierarchical instances to which the settings apply.\n  The report_fanin_sequential_clock_gating_options command This command lists the settings for design objects.\n  The set_fanin_sequential_clock_gating_objects command This command allows you to specify registers, hierarchical cells, power domains, or modules to be included in or excluded from fanin-based sequential clock gate insertion.\n  The report_fanin_sequential_clock_gating_objects command This command lists the settings explicitly set with the set_fanin_sequential_clock_gating_objects command.\n Regardless of any settings made with the set_fanin_sequential_clock_gating_options and set_fanin_sequential_clock_gating_objects commands, the following conditions prevent fanin-based sequential clock-gate insertion:  A register has asynchronous state changes.\n  A register contains objects other than edge-triggered flip-flops in its transitive fanin.\n Disallowed objects include ports, latches, and registers with asynchronous state changes.\n  The enable signal extracted from registers in the transitive fanin depends on their output pins.\n  The toggle rate of a register's clock signal is so low that clock gating provides no benefit.\n        Registers in the transitive fanin are controlled by a different base clock or triggered by the opposite clock edge.\n  At least one of the registers in the transitve fanin is always enabled.\n  The next state function of the register under analysis or of the registers in its transitive fanin depends on logic that contains a dont_care condition in the RTL.\n  The register under analysis is multibit and its slices do not share the same enable signal."}
{"instruction": "How do I Setting the Preferred Routing Direction for Layers", "input": "", "output": "During clock tree synthesis, you can assign continuous nondefault routing rule to nets based on different levels of the clock tree.\n The tool implements the changes only during the initial clock tree synthesis and assigns nondefault routing rule in the following ways:  Nets connected to pure sink carry sink nondefault routing rule  Nets connected to both sink and intermediate cells (repeaters or clock gates) must receive internal nondefault routing rule  Nets at root level until the branching point downstream must get root nondefault routing rule  Level-Based Clock Nondefault Routing Rule       As shown in the figure, after clock tree synthesis:  Without the level-based clock nondefault routing rule: The n1 and n3 nets must get sink nondefault routing rule after compiling the clock tree because they are connected to at least one sink.\n These nets leave the discontinuous nondefault routing rules applied at different levels.\n  With the level-based clock nondefault routing rule: All the nondefault routing rules in a clock tree path must be in a topologically sorted order from clock root to sinks, that is, Root nondefault routing rule > Internal nondefault routing rule > Sink nondefault routing rule.\n Therefore, n1 and n2 nets are assigned with internal nondefault routing rule while net n3 is assigned to a sink nondefault routing rule.\n To enable the level-based clock nondefault routing rule application, use the following application option: fc_shell> set_app_options -list {cts.compile.topological_ndr true}"}
{"instruction": "How do I Removing Clock Trees", "input": "", "output": "To prevent the Fusion Compiler tool from modifying them, you can mark existing clock trees in your design as synthesized clock tree by using the mark_clock_trees command as shown in Table 27 Table 27 Using the mark_clock_trees Command To do this Use this option   create_clock create_generated_clock  -clocks clock_list   -synthesized   -clear dont_touch   -dont_touch dont_touch   -clear -dont_touch  set_clock_routing_rules  -routing_rules  set_clock_cell_spacing  -clock_cell_spacing  -fix_sinks  -freeze_routing The tool traverses the clock trees and marks the clock trees as specified.\n Clock tree traversal continues until it finds an exception pin or a default sink pin."}
{"instruction": "How do I Early Data Checks, Policies, and Strategies", "input": "", "output": "The tool performs the repairs and saves the records in the subblocks within the check_hier_design command itself.\n For the repairs, which need frame recreation, you need to run the save_lib command to save the frame on disk.\n  To save the repair and its record to the subblock successfully, the following setup is required: 1.\n Enable the reference libraries, containing the subblocks, for open_lib top.nlib -ref_libs_for_edit       2.\n Enable editability on the subblocks, on which repair has to be performed set_editability -blocks <> -value true The check_hier_design command checks for the required editability settings, if repair has to be performed.\n If the required editability settings are not performed,then the check_hier_design command issues TL-170 error message.\n Note: The tool supports the repairs on subblocks, which are linked to the read_only abstracts rather than editable abstracts because managing the mismatched design data mechanism only aims to continue the top-level flow for exploration and as such no repair work performed on the subblock abstracts should get merged with the actual design view.\n If the top-level uses editable design views, the repair work still continues."}
{"instruction": "How do I Removing Taps", "input": "", "output": "If the specified location for a tap is invalid, use the -snap_distance option of the create_taps command to find a substitute location for the tap.\n The command searches the nearby layout shapes within the specified snap distance both horizontally and vertically on the same layer as defined by the -layer option.\n The tap is reinserted at the corner of a valid shape which is within the minimum distance to the specified location.\n Note: You can search for a substitute location only for the invalid taps that are specified by the -point and -layer options.\n For example, fc_shell> create_taps -point {50,50} -layer {M1} \\ -snap_distance 100 Snap tap point from original location (50.000, 50.000) to new location (90.000, 65.910) As shown in, the specified tap location does not touch any layout shape.\n The red rectangle is the shape closest to the specified location and its lower-right corner has       the minimum distance among all the other rectangles.\n The tool reinserts the tap at the lower-right corner of the red rectangle.\n  Finding a Substitute Location for an Invalid Tap Original {x,y} 100 100 100 100 Snapped tap"}
{"instruction": "How do I Performing Preroute Shielding", "input": "", "output": "Before you perform shielding, you must 1.\n Define the shielding rules.\n To define shielding rules, use the -shield_spacings and -shield_widths options of the create_routing_rule command.\n For example, to specify a shielding rule that uses spacing of 0.1 microns and width of 0.1 microns for metal1 through metal5 and spacing of 0.3 microns and width of 0.3 microns for metal6, use the following command: fc_shell> create_routing_rule shield_rule \\ -shield_widths {M1 0.1 M2 0.1 M3 0.1 M4 0.1 M5 0.1 M6 0.3} \\ -shield_spacings {M1 0.1 M2 0.1 M3 0.1 M4 0.1 M5 0.1 M6 0.3} For more information about the create_routing_rule command, see Using Nondefault Routing Rules       2.\n Assign shielding rules to the nets to be shielded.\n To avoid congestion issues and achieve the best balance of DRC convergence and timing closure, you should apply shielding rules only to high-frequency or critical clock nets and apply double-spacing rules to the lower-frequency clock nets.\n  To assign shielding rules to clock nets, use the set_clock_routing_rules command.\n Note: You can use the set_clock_routing_rules command only before clock tree synthesis.\n  To assign shielding rules to signal nets, use the set_routing_rule command.\n For more information about these commands, see Assigning Nondefault Routing Rules to Nets"}
{"instruction": "How do I Using Hierarchical Relative Placement for Compression", "input": "", "output": "A cell can occupy multiple column positions or multiple row positions, which is known as straddling.\n For more information about leaf cell straddling, see Adding Leaf Cells  shows a relative placement group in which cells straddle columns (instance U2) and rows (instance U7).\n  Hierarchical Relative Placement Group With Straddling U3 U4 U6 U1 U2 U7        shows the process of using hierarchical relative placement to build this structure.\n First, define relative placement groups that contain the leaf cells: rp1 contains U1 and U4, rp2 contains U2, and rp3 contains U3 and U6.\n Then define a group (rp4) that contains these groups.\n Finally, define a group (rp5) that contains the hierarchical group rp4 and the leaf cell U7.\n The resulting group includes both the column and the row straddle.\n Example 44 shows the commands used in this process.\n  Straddling With Hierarchical Relative Placement rp3 rp2 rp1 U3 U2 U4 U7 U6 U1 rp4 U4 U1 U2 U3 U6 rp1 rp2 rp3 rp5 U4 U1 U2 U3 U6 col 0 col 1 row 0 col 0 col 1 col 0 col 1 row 0 row 0 row 2 row 1 row 0 col 0 row 2 row 1 row 0 col 0 col 1 U7 col 1 row 2 row 1 row 0 rp4 Example 44 Straddling With Hierarchical Relative Placement create_rp_group -name rp1 -columns 2 -rows 1 add_to_rp_group rp1 -cells U1 -column 0 -row 0 add_to_rp_group rp1 -cells U4 -column 1 -row 0  create_rp_group -name rp2 -columns 1 -rows 1 add_to_rp_group rp2 -cells U2 -column 0 -row 0  create_rp_group -name rp3 -columns 2 -rows 1 add_to_rp_group rp3 -cells U3 -column 0 -row 0 add_to_rp_group rp3 -cells U6 -column 1 -row 0  create_rp_group -name rp4 -columns 1 -rows 3 add_to_rp_group rp4 -rp_group rp1 -column 0 -row 0 add_to_rp_group rp4 -rp_group rp2 -column 0 -row 1 add_to_rp_group rp4 -rp_group rp3 -column 0 -row 2  create_rp_group -name rp5 -columns 2 -rows 1       add_to_rp_group rp5 -rp_group rp4 -column 0 -row 0 add_to_rp_group rp5 -cells U7 -column 1 -row 0"}
{"instruction": "How do I Fanin-Based Sequential Clock Gating", "input": "", "output": "When you enable placement-aware clock gating, the tool performs the following optimizations:  Replication of clock gates with timing-critical enables  Adjustment of clock gates so they are placed closer to their gated registers  Automatic clock network latency annotation for clock-gate cells To enable this feature, set the compile.clockgate.physically_aware application option to true.\n (The default is false.) The example in shows the difference between using and not using physically- aware clock gating.\n In this layout view, you can see that the integrated clock gates (shown in red) are physically closer to their gated registers.\n        Placement-Aware Clock Gating Clock gate cells are shown in red Gated clock nets are shown in blue Placement-Aware Clock Gating Disabled Placement-Aware Clock Gating Enabled"}
{"instruction": "How do I Using Nondefault Routing Rules", "input": "", "output": "To remove routing corridors from the current block, use the remove_routing_corridors command.\n  To remove specific routing corridors, specify the routing corridors, either as a list or collection, such as that returned by the get_routing_corridors command.\n  To remove all routing corridors, specify the -all option.\n You can also remove routing corridors in the GUI by using the Delete tool."}
{"instruction": "How do I Handling Endpoints With Balancing Conflicts", "input": "", "output": "If you have set ideal clock latencies for specific sinks in your design, you can use the derive_clock_balance_points command to convert these ideal latency settings to clock tree exceptions.\n The derive_clock_balance_points command converts ideal latencies specified with the set_clock_latency command on clock sink pins to set_clock_balance_points commands.\n Use the derive_clock_balance_points command before clock tree synthesis, when the design has ideal clocks.\n By default, the derive_clock_balance_points command  Generates balance point constraints for all ideal primary clocks of all active scenarios that are enabled for setup analysis, hold analysis, or both.\n It does not generate balance points for generated clocks.\n To generate the balance points for  Specific primary clocks, use the -clocks option.\n  All active scenarios of specific corners, use the -corners option.\n  Calculates a reference latency for each primary ideal clock, which is the sum of the ideal source and network latency specified on the clock with the set_clock_latency command.\n To specify a different reference latency value, use the -reference_latency option.\n  Applies the derived clock balance point constraints to the design.\n To generate an output file containing set_clock_balance_points commands, instead applying it to the design, use the -output option.\n Balance points that the tool derives are clock and corner specific.\n Ensure that appropriate set_clock_latency constraints are set for all clocks for all modes used for clock tree synthesis in at least the worst corner.\n The tool uses the following formula to derive the delay values for the corresponding set_clock_balance_points commands: (Clock balance point delay value at the sink) = (Reference latency of the clock) - (Total ideal clock latency at the sink) The total ideal clock latency at the sink is the sum of the source latency of the clock and the ideal network latency for the sink.\n       For example, assume you have the following timing constraint settings: fc_shell> set_clock_latency -source 0.5 [get_clocks clk] fc_shell> set_clock_latency 1.0 [get_clocks clk] fc_shell> set_clock_latency 1.5 [get_pins reg1/CK] fc_shell> set_clock_latency 0.5 [get_pins reg2/CK] If you run the derive_clock_balance_points command, the tool derives the following clock balance point constraints: fc_shell> set_clock_balance_points -delay -0.5 \\ -balance_points [get_pins reg1/CK] -clock clk -corners worst fc_shell> set_clock_balance_points -delay 0.5 \\ -balance_points [get_pins reg2/CK] -clock clk -corners worst To avoid conflicts, do not manually apply any clock balance points for a clock that you derive balance points with the derive_clock_balance_points command."}
{"instruction": "How do I Specifying Boundary Cell Placement Rules", "input": "", "output": "Boundary cells include both end-cap cells placed on the left, right, top, and bottom boundaries, and inside and outside corner cells.\n You can specify different library cells for each boundary cell type.\n To specify the library cells, use the following options with the set_boundary_cell_rules command:  -left_boundary_cell and -right_boundary_cell These options specify a single library cell that is used for the end-cap cells for the left and right boundaries, respectively.\n  -top_boundary_cells and -bottom_boundary_cells These options specify a list of library cells that are used for the end-cap cells for the top and bottom boundaries, respectively.\n The command inserts the cells in the specified order.\n If the remaining space is smaller than the current cell, the command inserts the next cell in order that fits in the remaining space.\n For a vertical-row block, rows start at the bottom and end at the top, so the top boundary is along the left side of the block and the bottom boundary is along the right side of the block.\n For the flipped rows in a double-back block, the top boundary cells are used on the bottom boundaries and the bottom boundary cells are used on the top boundaries.\n  -top_tap_cell and -bottom_tap_cell       These options specify a single library cell that is used for the tap cells on the top and bottom boundary rows, respectively.\n The tool inserts the tap cells to ensure that the end-cap cells inserted on the top and bottom boundary rows comply with the maximum diffusion-to-tap distance limit.\n  -top_left_outside_corner_cell -top_right_outside_corner_cell -bottom_left_outside_corner_cell, and -bottom_right_outside_corner_cell These options specify a single library cell that is used for each outside corner location.\n  -top_left_inside_corner_cells -top_right_inside_corner_cells -bottom_left_inside_corner_cells, and -bottom_right_inside_corner_cells These options specify a list of library cells for each inside corner location.\n The tool inserts the first corner cell that matches the size of the inside corner.\n If none matches exactly, it inserts the first cell that can be placed without violating any rules.\n  shows the boundary and corner cell locations for a horizontal-row block with two hard macros or blockages.\n Note that the cell locations are flipped when the row is flipped.\n  Boundary Cell Locations"}
{"instruction": "How do I Improving Instance Voltage Drop by Augmenting the Power Grid", "input": "", "output": "Before you run interactive design rule checking, configure the run by setting the application options shown in Table 47.\n To set the application options, use the set_app_options command.\n To see the current settings, use the report_app_options command.\n Table 47 Application Options for Interactive Design Rule Checking Application option Default Description signoff.check_drc_live.\n check_only_visible_layers all_layers    all_layers visible_involved visible_only signoff.check_drc_live.\n default_layers    signoff.check_drc_live.\n exclude_command_class {{density true} {connectivity true}}    signoff.check_drc_live.\n halo    signoff.check_drc_live.\n hierarchy_level visible   signoff.check_drc_live.\n keep_enclosing_results false      true     signoff.check_drc_live.halo          Table 47 Application Options for Interactive Design Rule Checking (Continued) Application option Default Description signoff.check_drc_live.\n run_dir         signoff.check_drc_live.\n runset     signoff.check_drc_live.\n user_defined_options         signoff.physical.\n layer_map_file          signoff.physical.\n merge_stream_files"}
{"instruction": "How do I Specifying a Design Scenario for Rail Analysis", "input": "", "output": "To control scenarios for RedHawk Fusion or RedHawk-SC Fusion rail analysis:  Use the rail.scenario_name application option to specify a different design scenario for rail analysis.\n For details, see Specifying a Design Scenario for Rail Analysis        You can create rail scenarios and associate them with a design scenario in the current design.\n This allows you to identify possible power integrity issues by running optimization and rail analysis on one or more rail scenarios.\n For details, see Creating and Specifying Rail Scenarios for Rail Analysis"}
{"instruction": "How do I Controlling Off-Grid Routing", "input": "", "output": "The Fusion Compiler tool provides two commands for assigning nondefault routing rules to nets:  set_clock_routing_rules This command assigns nondefault routing rules to clock nets before clock tree synthesis.\n During clock tree synthesis and optimization, the tool propagates the nondefault routing rules to the newly created clock nets.\n  set_routing_rule This command assigns nondefault routing rules to signal nets and to clock nets after clock tree synthesis.\n The following topics describe how to assign nondefault routing rules to nets:  Assigning Nondefault Routing Rules to Clock Nets  Assigning Nondefault Routing Rules to Signal Nets  Reporting Nondefault Routing Rule Assignments Assigning Nondefault Routing Rules to Clock Nets To assign a clock routing rule to a clock net, use the -rule option with the set_clock_routing_rules command.\n The specified rule must be a rule that you previously defined with the create_routing_rule command.\n To reset a clock routing rule to the default routing rule, use the -default_rule option.\n To change a clock routing rule, first reset it to the default routing rule and then use the -rule option after resetting the assignment to the default routing rule.\n       By default, the set_clock_routing_rules command assigns the specified clock routing rule to all clock trees in the block.\n Table 31 shows the options used to restrict the routing rule assignment.\n Table 31 Restricting Clock Routing Rule Assignments To assign a nondefault routing rule to Use this option  -clocks clocks  -net_type root  -net_type sink   -net_type internal  -nets nets  shows the root, internal, and sink nets of a clock tree after clock tree synthesis.\n By default, the root-net routing rule is applied to all the single-fanout clock nets starting from the clock root up to the point where the clock tree branches out to a fanout of more than one.\n Internal-net routing rules are applied to the nets from this point until the sink nets.\n  Root, Internal, and Sink Clock Net Types Sink nets Internal nets Root nets 200 sinks 200 sinks 200 sinks The following example specifies routing rules for the root, internal, and sink nets: fc_shell> set_clock_routing_rules -rules NDR1 -net_type root fc_shell> set_clock_routing_rules -rules NDR2 -net_type internal fc_shell> set_clock_routing_rules -rules NDR3 -net_type sink 2.\n You can use this option with the -clocks option to further restrict the assignment.\n This option is not valid with the -nets option.\n       To specify a transitive fanout limit to use when identifying root nets, use the set_clock_tree_options -root_ndr_fanout_limit command.\n For example, to specify that any clock net with a transitive fanout of more than 300 be considered as a root net, use the following command: fc_shell> set_clock_tree_options -root_ndr_fanout_limit 300  shows the root, internal, and sink nets of the same clock tree when a transitive fanout limit of 300 is used for identifying the clock root nets.\n  Using a Fanout Limit for Selecting Root Nets Sink nets Internal nets Root nets 200 sinks 200 sinks 200 sinks When calculating the transitive fanout of clock nets for the purpose of identifying root nets, the tool includes only the valid clock sinks; It does not include the ignore pins.\n If a net identified as a root net is less than 10 microns, the tool uses internal-net routing rules for that net.\n Note: Specifying a smaller value with the set_clock_tree_options -root_ndr_fanout_limit command increases the number of clock nets that are assigned the root-net routing rule, which can increase routing congestion.\n During clock tree synthesis and optimization, the tool also honors nondefault routing rules set by using the set_routing_rule command.\n However, the tool does not propagate these routing rules to any new clock nets it creates.\n If a net is assigned more than one nondefault routing rule, the tool uses the following priority to determine the effective routing rule: 1.\n Nondefault routing rule set by the set_routing_rule command 2.\n Net-specific clock routing rule set by the set_clock_routing_rules -nets command       3.\n Clock-specific clock routing rule set by the set_clock_routing_rules -clocks command 4.\n Global clock routing rule set by the set_clock_routing_rules command Assigning Nondefault Routing Rules to Signal Nets To assign a nondefault routing rule to a net, use the -rule option with the set_routing_rule command.\n The specified rule must be a rule that you previously defined with the create_routing_rule command.\n You must specify the nets to which to assign the nondefault routing rule.\n You can assign multiple nondefault routing rules to a net.\n To change the routing rule assignment for one or more nets,  Use the -default_rule option to reset the nets to the default routing rule.\n To assign different nondefault routing rules to the nets, use the -rule option after resetting the nets to the default routing rule.\n  Use the -no_rule option to remove all routing rules from the nets and allow the tool to automatically assign a routing rule to them.\n  Use the -clear option to remove all routing rules and net-specific layer constraints from the nets.\n For example, to assign a nondefault routing rule called WideMetal to the CLK net, use the following command: fc_shell> set_routing_rule -rule WideMetal [get_nets CLK] To reset the routing rule for the CLK net to the default routing rule, use the following command: fc_shell> set_routing_rule -default_rule [get_nets CLK] Reporting Nondefault Routing Rule Assignments The Fusion Compiler tool provides commands to report the nondefault routing rules assigned by the set_routing_rule command and the clock routing rules assigned by the set_clock_routing_rules command.\n  To report the nondefault routing rules assigned by the set_routing_rule command, use the -of_objects option with the report_routing_rules command.\n fc_shell> report_routing_rules -of_objects [get_nets *]  To report the clock routing rule assignments, use the report_clock_routing_rules command.\n This command reports only the routing rules assigned by the set_clock_routing_rules command."}
{"instruction": "How do I Identifying the Clock Roots", "input": "", "output": "The Fusion Compiler tool derives the clock trees by tracing through the transitive fanout from the clock roots to the clock endpoints.\n In general, the tracing terminates when it finds a clock pin of a sequential cell or macro; however, the tool traces through sequential cells if they are integrated clock-gating (ICG) cells or their fanout drives a generated clock.\n If the clock-gating logic uses a non-unate cell, such as an XOR or XNOR gate, the tool uses both the positive-unate timing arc and the negative-unate timing arc when tracing the clock path.\n If this does not correspond to the functional mode of the block, use the set_case_analysis command to hold all nonclock inputs of the cell at a constant value, which forces the cell into the required functional mode for clock tree synthesis.\n For example, suppose a block has the gating logic shown in  Non-Unate Gated Clock To force the XOR gate (U0) into functional mode for timing analysis, use the following command: fc_shell> set_case_analysis 0 U0/B"}
{"instruction": "How do I Clock Gating and Multibit Registers", "input": "", "output": "When you set constraints for physical implementation, you can set nondefault routing rules to define stricter (wider) wire width and spacing requirements for specific nets to improve timing and reduce crosstalk.\n To specify nondefault routing rules, use the create_routing_rule and set_routing_rule commands.\n       However, the set_routing_rule command does not allow annotation on the output nets of tool-inserted clock gates, because these nets do not exist until after the execution of the compile_fusion command.\n To specify nondefault routing rules that are applied automatically on nets connected to the output pins of tool-inserted clock gates, use the set_clock_gate_routing_rule command.\n These rules are applied on clock gates after the compile_fusion command.\n For example, create_routing_rule cg_rule...\n set_clock_gate_routing_rule -rule cg_rule...\n compile_fusion...\n report_routing_rules"}
{"instruction": "How do I Controlling Timing Optimization Effort", "input": "", "output": "To exclude a specific sink from concurrent clock and data optimization, 1.\n Apply a cts_fixed_balance_pin attribute on the sink pin by using the set_attribute command as shown in the following example: fc_shell> set_attribute -objects reg21/CK \\ -name cts_fixed_balance_pin -value true 2.\n Set the ccd.respect_cts_fixed_balance_pins application option to true These setting prevents the tool from adjusting the latencies of the sinks that have a cts_fixed_balance_pin attribute set to true.\n However, changes in the clock tree due to concurrent clock and data optimization performed on other sinks can change the latencies of the sinks that have a cts_fixed_balance_pin attribute set to true To prevent the tool from making any changes to the clock paths between the sinks that have a cts_fixed_balance_pin attribute set to true and the clock root, set the       ccd.respect_cts_fixed_balance_pins application option to upstream instead of true This application option setting affects concurrent clock and data optimization performed during the place_opt clock_opt, and route_opt commands."}
{"instruction": "How do I Reporting Global Layer Constraints", "input": "", "output": "To specify the global layer constraints, use the set_ignored_layers command.\n By default, the global layer constraints are used for RC estimation, congestion analysis, and as soft constraints for routing.\n Use the following options to set the constraints and change the default behavior.\n  To specify the minimum and maximum routing layers, use the -min_routing_layer and -max_routing_layer options.\n Specify the routing layers by using the layer names from the technology file.\n For example, to use layers M2 through M7 for routing, RC estimation, and congestion analysis, use the following command: fc_shell> set_ignored_layers \\ -min_routing_layer M2 -max_routing_layer M7  To allow the use of layers beyond the minimum or maximum routing layer only for pin connections, set the route.common.global_min_layer_mode and route.common.global_max_layer_mode application options to allow_pin_connection        To change the constraints to hard constraints, set the route.common.global_min_layer_mode and route.common.global_max_layer_mode application options to hard  To specify additional layers to be ignored for RC estimation and congestion analysis, use the -rc_congestion_ignored_layers option.\n The specified layers must be between the minimum and maximum routing layers.\n For example, to use layers M2 through M7 for routing and layers M3 through M7 for RC estimation and congestion analysis, use the following command: fc_shell> set_ignored_layers \\ -min_routing_layer M2 -max_routing_layer M7 \\ -rc_congestion_ignored_layers {M2}  To change an existing layer constraint setting, simply reset that option.\n When you reset an option, it overrides the existing value of only that option; the other option settings remain unchanged.\n For example, assume that you used the previous command to set the minimum routing layer to M2 and the maximum routing layer to M7.\n To change the maximum routing layer from M7 to M8, but keep the other settings, use the following command: fc_shell> set_ignored_layers -max_routing_layer M8"}
{"instruction": "How do I Introduction to Redundancies, Outliers, Equivalences, and Gaps", "input": "", "output": "The Fusion Compiler tool provides the following library analysis capabilities, which can help achieve the performance, power, and area goals of your design:  Identify redundant, outlier, and equivalent cells in library cell families, which you can exclude from optimization during the subsequent steps in the implementation flow.\n  Identify gaps in library cell families, which you can provide as feedback to your library vendor with the goal of improving the reference libraries.\n  Compare the delay, power, area, and other characteristics of equivalent cells in different libraries or different versions of the same library, which can help you identify the appropriate libraries to use for design implementation.\n A family of library cells is a collection of cells that share some common properties.\n The tool can identify a family of library cells based on  User-identified library cells properties, such function, threshold voltage, and so on  User-defined library cells lexical attributes and settings  User-specified custom library cell families The following figure shows the high-level flow for performing library analysis.\n        Library Analysis Flow For more information about the different types of library analysis flows, see the following topics:  Introduction to Redundancies, Outliers, Equivalences, and Gaps in Library Cell Families  Identifying Redundancies, Outliers, Equivalences, and Gaps in Library Cell Families  Comparing Libraries"}
{"instruction": "How do I Adding Cells Within a Predefined Relative Placement Area", "input": "", "output": "To add a blockage within relative placement groups, use the -blockage option with the add_to_rp_group command.\n When you add a blockage using this command, you can specify  The column and row position within the relative placement group by using the -column and -row options.\n If you do not specify a position, the tool adds the blockage to position (0,0).\n  The size of the blockage by using the -height and -width options.\n If you do not specify the -height or -width option, the tool determines the size based on the tiling type of the relative placement group as follows:  For a tiling type setting of bit_slice, the height default to the height of site row and the width to width of the column.\n  For a tiling type setting of compression, the height default to the height of site row and the width to the width of one site.\n  That the blockages can overlap with other relative placement blockages or other objects that are not relative placement cells by using the -allow_overlap option.\n The following example adds a blockage named gap1 to the rp1 relative placement group at position (0,2) that is one site row high and five site rows wide: fc_shell> add_to_rp_group rp1-blockage gap1 \\ -column 0 -row 2 -width 5 -height 1"}
{"instruction": "How do I Generating Via Ladder Rules for Electromigration Via Ladders", "input": "", "output": "A via ladder rule defines the number of rows and the number of cuts in each row for each layer in a via ladder.\n Via ladder rules can be defined in the technology file or by using the create_via_rule command.\n For example, each of the following methods creates an identical via ladder rule:  Define the rule in the technology file.\n ViaRule \"VL1\" { cutLayerNameTblSize = 2 cutLayerNameTbl = (VIA1, VIA2) # via layer names cutNameTbl = (CUT1A, CUT2A) # cut names numCutRowsTbl = (2, 2) # rows in ladder numCutsPerRowTbl = (2, 2) # columns in ladder upperMetalMinLengthTbl = (L1, L2) # minimum length rule cutXMinSpacingTbl = (X1, X2) # X-direction spacing cutYMinSpacingTbl = (Y1, Y2) # Y-direction spacing maxNumStaggerTracksTbl = (S1, S2) # maximum staggering forHighPerformance = 0       forElectromigration = 1  For details about defining via ladder rules in the technology file, see the \"Via Ladder Rule\" topic in the Synopsys Technology File and Routing Rules Reference Manual  Define the rule on the command line by using the create_via_rule command with a detailed specification and then specifying additional details by setting its attributes.\n fc_shell> create_via_rule -name VL1 -cut_layer_names {VIA1 VIA2} \\ -cut_names {CUT1A CUT2A} -cut_rows {2 2} -cuts_per_row {2 2} fc_shell> set_attribute [get_via_rules VL1] \\ upper_metal_min_length_table {L1 L2} fc_shell> set_attribute [get_via_rules VL1] \\ cut_x_min_spacing_table {X1 X2} fc_shell> set_attribute [get_via_rules VL1] \\ cut_y_min_spacing_table {Y1 Y2} fc_shell> set_attribute [get_via_rules VL1] \\ max_num_stagger_tracks_table {S1 S2} fc_shell> set_attribute [get_via_rules VL1] \\ for_high_performance false fc_shell> set_attribute [get_via_rules VL1] \\ for_electro_migration true  Define the rule on the command line by creating an empty via rule with the create_via_rule command and then specifying the details by setting its attributes.\n fc_shell> create_via_rule -name VL1 fc_shell> set_attribute [get_via_rules VL1] \\ cut_layer_name_table_size 2 fc_shell> set_attribute [get_via_rules VL1] \\ cut_layer_name_table {via1 via2} fc_shell> set_attribute [get_via_rules VL1] \\ cut_name_table {CUT1A CUT2A} fc_shell> set_attribute [get_via_rules VL1] \\ num_cuts_per_row_table {2 2} fc_shell> set_attribute [get_via_rules VL1] num_cut_rows_table {2 2} fc_shell> set_attribute [get_via_rules VL1] \\ upper_metal_min_length_table {L1 L2} fc_shell> set_attribute [get_via_rules VL1] \\ cut_x_min_spacing_table {X1 X2} fc_shell> set_attribute [get_via_rules VL1] \\ cut_y_min_spacing_table {Y1 Y2} fc_shell> set_attribute [get_via_rules VL1] \\ max_num_stagger_tracks_table {S1 S2} fc_shell> set_attribute [get_via_rules VL1] \\ for_high_performance false fc_shell> set_attribute [get_via_rules VL1] \\ for_electro_migration true       The tool treats the via rules the same, regardless of the method used to create them.\n You can perform the following tasks for via rules:  Create a collection of via rules by using the get_via_rules command  Remove via rules by using the remove_via_rules command  Display detailed information about the via rules by using the report_via_rules command  Set or query via rule attributes To see the attributes supported on via rule objects, use the list_attributes -application -class via_rule command.\n  Save the via rules in a technology file by using the write_tech_file command See Also  Generating Via Ladder Rules for Electromigration Via Ladders"}
{"instruction": "How do I Grouping a list of repeaters", "input": "", "output": "To define a group of repeaters for cutline support, use the set_repeater_group or create_repeater_groups command.\n The following examples show the command usage.\n set_repeater_group -group_id group_id -check_connection \\ [-cells cell_list ] \\ [-cutline { {{x1 y} {x2 y}} | {{x y1} {x y2}} } ] \\ [-driver_group_id group_id ] \\ [-path_drivers pin_port_list ] \\ [-path_loads pin_port_list ] \\ [-override] [-lib_cell_input string [-lib_cell_output string [-clear]"}
{"instruction": "How do I Controlling the Number of Clock-Gate Levels", "input": "", "output": "You cannot disable general integrated clock-gate latency estimation.\n However, you can disable the feature for specific clock gates by setting a dont_estimate_clock_latency attribute on their clock pins.\n This topic describes how to manually specify latency values on those clock gates.\n       Specify the clock network latency by using either the set_clock_latency or set_clock_gate_latency command.\n The set_clock_gate_latency command can be used for both gate-level and RTL designs.\n The clock latency specified using the set_clock_gate_latency command is annotated on the registers during the compile_fusion command when the clock-gating cells are inserted.\n However, if you modify the latency values on the clock gates after the compilation, you must manually apply the latency values on the existing clock-gating cells using the apply_clock_gate_latency command.\n Note: After you modify the clock-gate latency using the set_clock_gate_latency command, if you compile your design using the compile_fusion command, it is not necessary to use the apply_clock_gate_latency command to apply the latency values.\n The tool annotates the specified value during compilation.\n To remove clock latency information previously set on clock-gating cells with the set_clock_gate_latency or apply_clock_gate_latency commands, use the reset_clock_gate_latency command.\n This command removes the clock latency values on the specified clocks.\n If you do not specify the clock, the clock latency values on all the clock-gating cells are removed.\n The set_clock_latency Command Use the set_clock_latency command to specify clock network latency for specific clock- gating cells.\n In lat_cgtoreg is the estimated delay from the clock pin of the clock-gating cell to the clock pin of the gated register and lat_reg is the estimated clock-network latency to the clock pins of the registers without clock gating.\n  Clock Latency With Clock-Gating Design Clock- Gating Cell Register Clock tree delay Clock tree delay lat_reg lat_cgtoreg For all clock pins of registers (gated or ungated) in the design that are driven by a specific clock, use the lat_reg value for the set_clock_latency command.\n For clock pins of all the clock-gating cells, use the difference between the lat_reg and lat_cgtoreg       values for the set_clock_latency command.\n Because the purpose of setting the latency values is to account for the different clock-network delays between the registers and the clock-gating cell, it is important to get a reasonably accurate value of the difference lat_cgtoreg ).\n The absolute values used are less important unless you are using these values to account for clock-network delay issues not related to clock gating.\n The set_clock_gate_latency Command When you use the compile_fusion command, clock gates are inserted during the compilation process.\n To specify the clock network latency before the clock-gating cells are inserted by the tool, use the set_clock_gate_latency command.\n This command lets you specify the clock network latency for the clock-gating cells as a function of the clock domain, clock-gating stage, and the fanout of the clock-gating cell.\n The latency that you specify is annotated on the clock-gating cells when they are inserted by the compile_fusion command.\n You can manually annotate the latency values on the existing clock-gating cells in your design using the apply_clock_gate_latency command.\n The set_clock_gate_latency command takes the following arguments:  -clock clock_list  -stage clock_gate_stage  -fanout_latency fanout_list The fanout_list is a list of tuples specifying a fanout range and a delay decrement.\n In the following example, three fanout ranges are created: 1 to 5 with a latency value of 0.5, 6-15 with a latency value of 0.8, and 16 to infinity with a latency value of 0.6.\n set_clock_gate_latency -stage 1 \\ -fanout_latency {{1-5 0.5} {6-15 0.8} {16-inf 0.6}} To specify a clock latency value for clock-gated registers, use the -stage option with a value of 0.\n Because you are specifying the latency value for the clock-gated registers, the value for the -fanout_latency option should be 1-inf (1 to infinity) and the latency is the absolute clock latency value for the registers, as shown in the following example: set_clock_gate_latency -clock CLK -stage 0\\ -fanout_latency {1-inf 1.0} If the -clock option is not specified, the setting applies to all clocks in the design.\n Clock latencies using the set_clock_latency command have higher precedence and are not overwritten.\n  shows an example of clock-gate stages and fanout.\n        Clock-Gating Stages and Latency Calculations The following commands set the latency values for set_clock_gate_latency -stage 0 -fanout_latency {{1-inf 1.0}} set_clock_gate_latency -stage 1 -fanout_latency {{1-inf 0.3}} set_clock_gate_latency -stage 2 -fanout_latency {{1-inf 0.2}} set_clock_gate_latency -stage 3 -fanout_latency {{1-inf 0.1}}  shows another example of fanout and latency calculations.\n        Latency Calculations With Varying Fanout The following commands specify the latency and fanout for set_clock_gate_latency -stage 0 \\ -fanout_latency {{1-inf 1.6}} set_clock_gate_latency -stage 1 \\ -fanout_latency {{1-20 0.6} {21-inf 0.8}} set_clock_gate_latency -stage 2 \\ -fanout_latency {{1-20 0.2} {21-inf 0.3}} set_clock_gate_latency -stage 3 \\ -fanout_latency {{1-30 0.1} {31-65 0.8} {66-inf 0.18}} set_clock_latency \"0.4\" {uicg_a/CK} Note that for the clock gate uicg_a, the latency value of 0.4 is assigned using the set_clock_latency command, which cannot be overwritten."}
{"instruction": "How do I Aligning by the Left Edges", "input": "", "output": "You can align the leaf cells in a column of a relative placement group by using the following alignment methods:  Left alignment (default)  Right alignment  Pin alignment Controlling the cell alignment can improve the timing and routability of your design."}
{"instruction": "How do I Restricting Optimization to Cell Sizing Only", "input": "", "output": "To preserve design objects during optimization, use the set_dont_touch command.\n The command places the dont_touch attribute on cells, nets, references, and subdesigns in the current design to prevent these objects from being modified or replaced during optimization.\n This table shows how the tool preserves the design objects during optimization.\n Design objects Tool behavior         When you use the set_dont_touch command, keep the following in mind:  The command prevents the ungrouping of hierarchy.\n  A child cell must inherit the same dont_touch attribute from its parent module.\n For example, you cannot remove the dont_touch attribute from a child cell when its parent module is marked with the dont_touch attribute.\n  Use the command on fully mapped logic only, including cells, modules, and designs, as well as nets with fully mapped surrounding logic.\n  The dont_touch attribute has higher precedence over the boundary_optimization and the size_only attributes.\n To report design objects marked with the dont_touch attribute, use the report_dont_touch command.\n This command sets the dont_touch attribute on cell A.\n fc_shell> set_dont_touch [get_cells A] true       To remove the dont_touch attribute, use the remove_attributes command or the set_dont_touch command set to false, as shown in the following two commands: fc_shell> remove_attributes [get_cells A] dont_touch fc_shell> set_dont_touch [get_cells A] false The following table summaries the commands that you can use to set, remove, or report the dont_touch attribute on design objects, including modules, cells, nets, and library cells.\n Yes denotes the command operation of the dont_touch attribute is supported, while No denotes the command operation is not supported.\n Table 13 Command Support for the dont_touch Attribute on Design Objects Command Module Cell (Instance) Net Library cell set_dont_touch     set_attribute     define_user_attribute     get_attribute     remove_attributes     report_dont_touch     set_dont_touch unmapped_object"}
{"instruction": "How do I Linking to Subblocks With Multiple Labels", "input": "", "output": "To change a block from its abstract to its design view, use the change_abstract command and specify the view you are changing to by using the -view option.\n When you change a block from its abstract view to its design view, remove the existing constraints and apply full-chip timing constraints.\n The following example changes the BLK1 block from its abstract view to its design view and applies full-chip timing constraints.\n 1.\n Remove any existing timing constraints: fc_shell> remove_scenarios -all fc_shell> remove_modes -all fc_shell> remove_corners -all       2.\n Use the change_abstract command to change from the abstract view to the design view: fc_shell> change_abstract -view design -references BLK1 3.\n Apply the full-chip scenario creation script: fc_shell> source full_chip_scenario_creation.tcl If the abstract view for a block has changed in its design library, you can reload the new abstract view by using the change_abstract -reload command.\n To report the abstract views the design is linking to, use the report_abstracts command.\n More information about specifying reference libraries and linking to abstract views at the top level, see SolvNet article 2436119, Setting Up Designs for Hierarchical Place and Route"}
{"instruction": "How do I Integrated Clock-Gate Latency Estimation", "input": "", "output": "During synthesis, the Fusion Compiler tool assumes that clocks are ideal.\n An ideal clock incurs no delay through the clock network.\n This assumption is made because real clock- network delays are not known until after clock tree synthesis.\n In reality, clocks are not ideal and there is a nonzero delay through the clock network.\n For designs with clock gating, the clock-network delay at the registers is different from the clock-network delay at the clock-       gating cell.\n This difference in the clock-network delay at the registers and at the clock- gating cell results in tighter constraints for the setup condition at the enable input of the clock-gating cell.\n The tool can obtain clock-gate latencies in the following ways:  Integrated latency estimation By default, the tool estimates and updates clock-gate latencies throughout the flow.\n Estimated clock latencies are more accurate than user-specified clock latencies.\n  User-specified latency You can disable integrated latency estimation for specific instances and specify latency values manually."}
{"instruction": "How do I Setting Up for Self-Gating", "input": "", "output": "To perform self-gating, the logic library should contain AND and OR gates to build the self- gating tree.\n The library should also contain XOR, NAND, and OR gates for the comparator gate.\n The integrated clock-gating cells in the library must have the following settings:  Sequential cell: latch  Control point: before  Control signal: scan_enable  Observation point: none If the library does not contain cells with these configurations for the corresponding operating conditions, the tool does not insert self-gating cells.\n If an integrated clock gate compatible with self-gating is specified by the set_clock_gate_style command, the self- gating feature uses the same integrated clock gate or the clock gate that is most similar to the one specified."}
{"instruction": "How do I Checking if Library Cells Are Legally Placeable", "input": "", "output": "To enable variant-aware legalization, set the place.legalize.enable_variant_aware application option to true When variant-aware legalization is enabled, the tool tries to fix legalization DRC violations by swapping the violating cell with a variant, rather than moving it.\n If the cell has no variants, it is moved to a legal location.\n For pin-track and pin-color alignment during variant-aware legalization, set the following application options to true  place.legalize.enable_prerouted_net_check  place.legalize.enable_pin_color_alignment_check In addition, you must specify the layers to be aligned by using the place.legalize.pin_color_alignment_layers application option."}
{"instruction": "How do I Setting Size-Only Settings", "input": "", "output": "To set a don't touch setting on a clock tree, use the set_dont_touch_network command.\n fc_shell> set_dont_touch_network -clock_only [get_pins pin_name The -clock_only option sets the don't touch setting only on the clock network.\n If you do not specify the -clock_only option, the don't touch setting is set on both the data path and the clock path.\n       To remove the don't touch setting from a clock tree, use the -clear option with the set_dont_touch_network command.\n fc_shell> set_dont_touch_network -clock_only [get_pins pin_name ] -clear You can also set don't touch settings on specific objects in the clock network by using the set_dont_touch command.\n The following commands set a don't touch setting on a cell and a net: fc_shell> set_dont_touch [get_cells cell_name ] true fc_shell> set_dont_touch [get_nets -segments net_name ] true To report all the don't touch settings set on the current block, use the report_dont_touch command.\n fc_shell> report_dont_touch -all Note: If a cell has a placement status of fixed, it is treated like a don't touch cell during clock tree synthesis."}
{"instruction": "How do I Shielding Options", "input": "", "output": "The following Custom Router application options allow you to control differential-pair routing: Application Option Description custom.route.diffpair_twist_jumper_enable  custom.route.diffpair_twist_jumper_interval   custom.route.diffpair_twist_jumper_offset    custom.route.diffpair_twist_jumper_style   diagonal  none   orthogonal"}
{"instruction": "How do I Self-Gating Optimization", "input": "", "output": "The clock gate efficiency reporting feature calculates the efficiency of clock-gates by considering toggle rate.\n This computation based on toggle rate has a direct dependency on power saving which can be used to improve gating efficiency.\n The report_clock_gating_efficiency command reports the overall clock gating efficiency of the user specified clock gates for each clock tree.\n The syntax of the command is as follows: - report_clock_gating_efficiency [-by_register] [-by_clock_gate] [-bin_size] [-type clock_gate | self_gate | fanin_seqcg] [-origin tool_inserted | pre_existing] [-sortby name | stage | gating_efficiency]       The following figure shows the report_clock_gating_efficiency command report: The columns of this report include the following parameters:  Toggle Savings Distribution : A histogram of registers efficiency with a bin size of 5 (the bin size can be modified)  Number of Registers : The number of registers in each histogram bin  Accumulated Percentage : The cumulative percentage of registers encountered so far  Number of Bitwidth : The bitwise equivalent for the number of registers in each bin  Accumulated Percentage (of the bitwidth): The cumulative bitwise register equivalent percentage encountered so far       The reports can be refined to report efficiency with respect to the following:  Bin size using the following command: report_clock_gating_efficiency -bin_size #numberofbin  Registers using the following command: report_clock_gating_efficiency -by_register       The columns of this report include the following parameters:  Register : The name of the register under consideration  Bitwidth : Bitwidth of the register  Local Clock Toggle Rate : Toggle rate at the clock pin of the register after gating  D Toggle Rate : Toggle rage at the D-pin of the register  Q Toggle Rate : Toggle rage at the Q-pin of the register  D/Clk Toggle Rate : D toggle rate per local clock toggle rate  Q/Clk Toggle Rate : Q toggle rate per local clock toggle rate  Current Gating Efficiency : Efficiency of the register computed as 1 (local clock toggle by root clock toggle)  Clock gates using the following command: report_clock_gating_efficiency -by_clock_gate The columns of this report include the following parameters:  Clock : The root clock of the clock gate  Clock Gating Stage : Stage of the clock gate  Gating Element  Number of Fanout Registers : The number of registers the clock gate is directly driving  Number of Fanout Bitwidth : The bitwidth equivalent for the number of registers the clock gate is directly driving        Toggle Savings : Toggle saving of the clock-gate  Clock Gating Efficiency : The efficiency of the clock gate  Receiver Clock Gate : The name of clock-gate directly driven by the clock-gate under consideration The output can also be sorted based on name, efficiency, or stage of clock gates in ascending order using the following command: report_clock_gating_efficiency -sort_by [name | stage | gating_efficiency]  name : Alphabetically  stage : By clock gate stage  gating_efficiency : By clock gate or register gating efficiency       The following figure shows an example report:"}
{"instruction": "How do I Enabling Advanced Logic Restructuring", "input": "", "output": "For designs that cannot be legalized due to areas of high utilization, you can specify that the tool perform area recovery in regions where the utilization is high by setting the opt.common.small_region_area_recovery application option to true When you enable this feature, the tool performs the area recovery during the compile_fusion and clock_opt commands.\n However, doing so can slightly degrade the timing QoR."}
{"instruction": "How do I Using Tcl Scripts", "input": "", "output": "To display the man page for a Fusion Compiler command or application option, enter the man command followed by the command or application option name.\n For example, to see the man page for the report_timing command, use the following command: fc_shell> man report_timing To display the man page for a Fusion Compiler application option, enter the man command followed by the option name.\n You can also view the following types of summary pages for application options:  Category summaries To view a man page that summarizes all of the application options for a specific category, enter the man command followed by category _options.\n For example, to see the man page that summarizes all timer application options, use the following command: fc_shell> man timer_options  Subcategory summaries To view a man page the summarizes all of the application options for a specific subcategory, enter the man command followed by category subcategory _options.\n For example, to see the man page that summarizes all common route application options, use the following command: fc_shell> man route.common_options        Command summaries To view a man page the summarizes all of the application options for a specific command, enter the man command followed by command _options.\n For example, to see the man page that summarizes all application options that affect the report_timing command, use the following command: fc_shell> man report_timing_options If you enter the man command on the fc_shell command line, the man page is displayed in the Fusion Compiler shell and in the console log view if the GUI is open.\n If you enter this command on the console command line in the GUI, the man page is displayed in the GUI man page viewer."}
{"instruction": "How do I Routing With the Custom Router", "input": "", "output": "You use the custom.route.balance_mode application option in interposer designs to reduce the skews caused by wire length and via cut count differences.\n This topic describes how to set the custom.route.balance_mode application option and provides examples to demonstrate its use.\n To route using the balance mode: Set the custom.route.balance_mode application option to true.\n set_app_options -name custom.route.balance_mode -value true When custom.route.balance_mode is true:  The Custom Router creates feedthrough routes between two \u03bcbumps and a C4 bump on the same net.\n  The wire lengths between the \u03bcbumps and the C4 bump are equal.\n  The number of via cuts is the same in all related \u03bcbump-C4 bump connections.\n Example 1 shows how the custom.route.balance_mode option is used when routing \u03bcbumps to a C4 bump.\n The designated routing layers are M1 through M5.\n The M2 and M4       layers are used for horizontal connections; and the M1, M3, and M5 layers are used for vertical connections.\n The number of via cuts and the array dimensions are also defined.\n Example 42 Routing with custom.route.balance_mode turned on create_routing_rule skew -widths {M1 6 M2 6 M3 6 M4 6 M5 25} -vias {{VIA12_1cut 3x3 R} {VIA23_1cut 3x3 R} {VIA34_1cut 3x3 R} {VIA45_1cut 2x5 R}} set_routing_rule $nets_to_route -rule skew -max_routing_layer M5 -min_routing_layer M1 set_attribute [get_layers \"M2 M4\"] routing_direction horizontal set_attribute [get_layers \"M1 M3 M5\"] routing_direction vertical set_ignored_layers -min_routing_layer M1 -max_routing_layer M5 set_app_options -list {custom.route.balance_mode true} route_custom -nets $nets_to_route The results from the example show that the wire lengths and the number of vias are the same in each of the four \u03bcbump-to-C4 bump connections."}
{"instruction": "How do I Controlling Sequential Output Inversion", "input": "", "output": "When mapping sequential cells, the tool uses the nextstate_type attribute of a library cell pin to identify its synchronous pin type.\n The valid values for this attribute are data preset clear load scan_in, and scan_enable If this attribute is not specified or is incorrectly specified in the logic library, you can manually specify it by using the set_attribute command, as shown in the following example: fc_shell> set_attribute [get_lib_pins my_lib/dffsrst/RN] \\ nextstate_type clear"}
{"instruction": "How do I Specifying Block-Level Power Intent", "input": "", "output": "You can provide test model of the subblocks for the top-level design implementation when running hierarchical synthesis.\n To do so, generate the test model for the mapped block       design during block-level synthesis and then annotate the test model to the subblock during top-level synthesis.\n For example,  Generating the test model during block-level synthesis # Read block RTL and constraints # Specify DFT configuration set_dft_signal...\n set_scan_configuration...\n create_test_protocol...\n compile_fusion -to initial_opto insert_dft write_test_model -output block_des.mapped.ctl  Running top-level synthesis with the block-level test model # Read RTL and constraints # Specify DFT configuration  read_test_model block_des.mapped.ctl set_dft_signal...\n set_scan_configuration...\n create_test_protocol...\n compile_fusion -to initial_opto insert_dft"}
{"instruction": "How do I Running Commands in Parallel on Your Local Host", "input": "", "output": "To efficiently run the same task on several blocks in your design, you can use the run_block_script command to enable distributed processing and perform the tasks in parallel.\n The run_block_script command accepts a Tcl script and applies the commands in the script to the blocks you specify.\n For example, to compile the subblocks of a design in parallel in a hierarchical flow, 1.\n Write the script to run on the blocks in the design.\n The following example performs the steps necessary to compile each subblock in the design.\n You can add additional commands to the script as needed to perform other operations.\n open_lib $block_libfilename open_block $block_refname.design commit_upf set_editability -from_level 1 -value false compile_fusion -from initial_map -to initial_map create_abstract -read_only create_frame save_block $block_refname -as $block_refname/initial_map compile_fusion -from logic_opto -to logic_opto create_abstract -read_only create_frame save_block $block_refname -as $block_refname/logical_opt compile_fusion -from initial_place -to initial_place create_abstract -read_only create_frame save_block $block_refname -as $block_refname/initial_place save_block save_lib close_block -force -purge close_lib 2.\n Run the run_block_script command to process each block in parallel by using the script created in step 1.\n The following example specifies the settings for distributed processing with the set_host_options command and runs the block_synthesis.tcl synthesis script on the subblocks in parallel by using the Load Sharing Facility (LSF).\n The command writes data and log files to the my_work_dir directory.\n fc_shell> set block_list [get_attribute \\ [get_cells -hierarchical -filter \"is_soft_macro==true\" \\ ref_full_name] fc_shell> set block_list [lsort -unique $block_list] fc_shell> set_host_options -name myhost -submit_command \"sh\" fc_shell> run_block_script \\ -host_options myhost \\       -script block_synthesis.tcl \\ -blocks $block_list \\ -work_dir my_work_dir To control the order in which blocks are processed, use the -run_order option with the top_down or bottom-up argument.\n When you specify the -run_order top-down option, the tool delays processing for a child block until all parent blocks for the child block are processed.\n When you specify the -run_order bottom-up option, the tool begins by processing the child blocks, then processes the parent blocks.\n By default, blocks are processed in a bottom-up order."}
{"instruction": "How do I Specifying Optimization Targets at the Preroute Stage", "input": "", "output": "The concurrent clock and data optimization performed during the route_opt command can reduce the peak dynamic voltage drop without affecting the timing QoR.\n To enable this feature, set the route_opt.flow.enable_voltage_drop_opt_ccd application option to true When you enable this feature, during the route_opt command, the tool uses the RedHawk Fusion feature to identify dynamic-voltage-drop hotspots and cells rows where the cells from the hotspots can be relocated.\n Then, during optimization, the tool relocates or downsizes the cells in the hotspots to reduce the peak dynamic voltage drop without hurting the timing QoR.\n You can specify one of the following thresholds for selecting cells for optimization:  A threshold for the cell voltage drop, as a percentage of the supply voltage, by using the ccd.voltage_drop_voltage_threshold application option.\n Any cell with a voltage drop that exceeds this threshold is selected for optimization.\n  A threshold for the number of cells, as a percentage of the total number of cells, by using the ccd.voltage_drop_population_threshold application option.\n Cells are selected starting with the worst violator.\n During postroute optimization, the tool can perform RedHawk dynamic voltage drop analysis to identify cells involved in voltage drop violations, and then use optimization techniques on those cells to improve the dynamic voltage drop.\n You can further improve the dynamic voltage drop by enabling IR-driven sizing, which uses the RedHawk dynamic voltage drop analysis results to identify cells involved in voltage drop violations, and then tries to replace those cells with cells having smaller leakage current.\n To enable this feature, set the route_opt.flow.enable_irdrivenopt application option to true, in addition to setting the route_opt.flow.enable_voltage_drop_opt_ccd application option to true"}
{"instruction": "How do I Updating Supply Nets for ECO Cells", "input": "", "output": "When the tool maps an ECO cell, it looks for a spare cell with the same library cell name.\n If it is unable to find such a spare cell, the ECO cells remains unmapped.\n To identify and map such ECO cells to logically equivalent (LEQ) spare cells, use the following steps: 1.\n Identify the ECO cells that do not have matching spare cells by using the check_freeze_silicon command.\n 2.\n Find logically equivalent spare cells for these ECO cells by using the create_freeze_silicon_leq_change_list command.\n Specify  The names of the unmapped ECO cells by using the -cells option  A name for the output file by using the -output option This output file is a Tcl script with netlist editing commands.\n The commands replace each ECO cell with logically equivalent spare cells, which can be a  Single spare cell with the same logical functionality, but a different library cell name  Combination of up to two spare cells that give the same logical functionality 3.\n View the output Tcl file to ensure that the ECO mapping is satisfactory, and it if necessary.\n 4.\n Source the Tcl file in the tool to replace the ECO cells with their logical equivalents."}
{"instruction": "How do I Unplacing the Repeaters", "input": "", "output": "Use the -preview option to preview repeater group locations before insertion.\n The repeaters are represented by annotation objects at the same locations where they are to be inserted, with the same width and height as a repeater lib cell, and with the group ID eco_preview_ id Note: Before previewing the repeater groups again, you must first remove the annotation objects using either the gui_remove_annotations or gui_remove_all_annotations command."}
{"instruction": "How do I Finding Substitute Locations for Invalid Taps", "input": "", "output": "To find taps that fail the validity checking, use the get_taps command to report the taps that have is_valid_location attribute set to false For example, fc_shell> get_attribute [ get_taps Tap_1437 ] is_valid_location false"}
{"instruction": "How do I Enabling Multicore Processing", "input": "", "output": "The command log file records the commands processed by the Fusion Compiler tool, including setup file commands and application option settings.\n By default, the Fusion Compiler tool writes the command log to a file named fc_command.log in the directory from which you invoked fc_shell.\n You can change the name of the command log file by setting the sh_command_log_file variable in your.synopsys_fc.setup file.\n You should make any changes to this variable before you start the Fusion Compiler tool.\n If your user-defined or project-specific setup file does not contain this variable, the Fusion Compiler tool automatically creates the fc_command.log file.\n Each Fusion Compiler session overwrites the command log file.\n To save a command log file, move it or rename it.\n You can use the command log file to  Produce a script for a particular implementation strategy  Record the physical implementation process  Document any problems you are having"}
{"instruction": "How do I Performing Power or Area Recovery on the Clock Network", "input": "", "output": "The following topics describe how you can control the multibit optimization performed during the clock_opt command:  Enabling the Rewiring of Mixed-Drive-Strength Multibit Cells  Enabling Post-Clock-Tree-Synthesis Multibit Debanking Enabling the Rewiring of Mixed-Drive-Strength Multibit Cells Some logic libraries have mixed-drive-strength multibit cells where some bits have a higher drive strength than others.\n For designs that use such cells, if a violating path goes through a lower-drive-strength bit, the tool can rewire the mixed-drive-strength multibit cell such that the violating path goes through a higher-drive-strength bit.\n To enable this feature for the clock_opt command, set the clock_opt.flow.enable_multibit_rewiring application option to true Enabling Post-Clock-Tree-Synthesis Multibit Debanking To improve the timing QoR, the clock_opt command can debank multibit registers during the final_opt stage, if doing so does not introduce hold timing violations.\n To enable this feature, set the clock_opt.flow.enable_multibit_debanking application option to true before you run the final_opt stage of the clock_opt command."}
{"instruction": "How do I Running the signoff_fix_drc Command", "input": "", "output": "Before you run the signoff_fix_drc command, configure the run by setting the application options shown in Table 44.\n To set the application options, use the set_app_options command.\n To see the current settings, use the report_app_options command.\n Table 44 Application Options for Signoff DRC Fixing Application option Default Description signoff.fix_drc.\n advanced_guidance_for_rules all   signoff_fix_drc  all off signoff.fix_drc.check_drc local    local global signoff.fix_drc.\n custom_guidance    dpt           Table 44 Application Options for Signoff DRC Fixing (Continued) Application option Default Description signoff.fix_drc.\n fix_detail_route_drc global signoff_fix_drc  global  local signoff.fix_drc.\n init_drc_error_db    signoff_fix_drc signoff_check_drc   signoff.fix_drc.\n last_run_full_chip false    false true signoff.fix_drc.\n max_detail_route_iterations     signoff.fix_drc.\n max_errors_per_rule    signoff.fix_drc.run_dir    signoff_fix_drc      signoff.fix_drc.\n target_clock_nets false  false true signoff.fix_drc.\n user_defined_options"}
{"instruction": "How do I Reporting Redundant Via Rates", "input": "", "output": "When you insert redundant vias, it changes the timing of your block.\n Short nets tend to slow down due to increased capacitance, whereas long nets tend to speed up due to decreased resistance.\n Zroute redundant via insertion has a timing-preservation mode that allows you to perform redundant via insertion without affecting the block timing by preventing insertion of redundant vias on critical nets.\n To enable timing-preservation mode for redundant via insertion, define the timing preservation constraints by using the following options of the add_redundant_vias command:  -timing_preserve_setup_slack_threshold  -timing_preserve_hold_slack_threshold  -timing_preserve_nets You should timing-preservation mode only at the end of the flow, after using the normal redundant via insertion flows, which converge both the redundant via rate and the timing QoR.\n Timing-preservation mode can slightly increase the redundant via rate while maintaining timing.\n However, if you use timing-preservation mode earlier in the flow, before timing is met, it might severely reduce the redundant via rate due to critical nets."}
{"instruction": "How do I Inserting Metal-Insulator-Metal Capacitors", "input": "", "output": "After you have performed detail routing, redundant via insertion, and wire spreading, you can perform wire widening to increase the average width of the wires, which reduces the critical area open faults and therefore improves yield.\n To perform wire widening, use the widen_zrt_wires comandcomandswiden_zrt_wireswire widening widen_wires command.\n By default, the widen_wires command widens all wires in the block to 1.5 times their original width.\n For more flexibility, you can use the -widen_widths_by_layer_name option to define up to five possible wire widths to use for each layer.\n For example, to define possible wire widths of 0.07 and 0.06 microns for the M1 layer; wire widths of 0.08 and 0.07 microns for the M2 layer; and 1.5 times the existing wire width for all other layers, enter the following command: fc_shell> widen_wires \\ -widen_widths_by_layer_name {{M1 0.07 0.06} {M2 0.08 0.07}} When you perform wire widening, the spacing between neighboring wires is decreased, which can reduce the improvement in critical area shorts gained from wire spreading.\n You can control the tradeoff between wire spreading and wire widening by using the       -spreading_widening_relative_weight option.\n By default, wire spreading and wire widening are given equal priority.\n To weight the priority toward wire widening and reduced critical area open faults, set this option to a value between 0.0 and 0.5.\n To weight the priority toward wire spreading and reduced critical area short faults, set this option to a value between 0.5 and 1.0.\n After widening, the widen_wires command performs detail routing iterations to fix any DRC violations caused as a result of widening.\n Note that the widened wires do not trigger fat wire spacing rules.\n When you widen the wires, it changes the timing of your block.\n Wire widening has a timing-preservation mode that allows you to perform wire widening without affecting the block timing.\n To enable timing-preservation mode for wire widening, define the timing preservation constraints by using the following options with the widen_wires command:  -timing_preserve_setup_slack_threshold threshold  -timing_preserve_hold_slack_threshold threshold  -timing_preserve_nets nets The threshold values are floating-point numbers in library units.\n Wire widening is not performed on nets with slack less than the specified values or the nets specified in the -timing_preserve_nets option."}
{"instruction": "How do I Selectively Removing or Preserving Constant and Unloaded", "input": "", "output": "By default, the tool performs register merging during optimization.\n To disable register merging, set the compile.seqmap.enable_register_merging application option to false.\n The default is true.\n Information about which registers are being merged is stored in the.svf file, and the output log file shows information messages about register merging.\n       You can also control register merging based on the attribute setting by using the set_register_merging command.\n To enable register merging during optimization, specify a true value to set the register merging attribute on the specified object list.\n To disable register merging, specifies a false value for the specified object list.\n The objects can include leaf cells, hierarchical cells, and modules.\n This example specifies not to merge registers that have the u1/count_reg string in the instance names: fc_shell> set_register_merging \"u1/count_reg*\" false This example specifies not to merge all flip-flops of u1 instance name: fc_shell> set_register_merging \"u1\" false"}
{"instruction": "How do I Performing THO During route_opt or hyper_route_opt", "input": "", "output": "After running the compile_fusion command with THO, the block interface timing changes as compared to when the blocks were implemented individually.\n Because the blocks are now optimized based on the top-level timing picture, you must create updated timing constraints for the blocks before you continue with the clock_opt command for the blocks.\n The CCD useful skew balance point offsets might have been modified during THO.\n These offsets need to also be made block-context-specific so that the clock tree synthesis step can take them into account.\n This example shows how you can perform budgeting: fc_shell> set budget_instances \"u1/a/i1 u1/b/i2 \u2026\" fc_shell> set_budget_options -reset -all fc_shell> set_budget_options -add_blocks $budget_instances fc_shell> set_app_options -list { plan.budget.use_ccd_latency true } fc_shell> set_app_options -list { plan.budget.write_hold_budgets false } fc_shell> set_app_options -list { plan.budget.estimate_timing_mode false } fc_shell> compute_budget_constraints -setup_delay -latency_targets actual -balance false -boundary fc_shell> write_budgets -output budgets-post-compile -force -nosplit -include_balance_points -include_pin_latency"}
{"instruction": "How do I Removing Via Ladders", "input": "", "output": "To query the via ladders in a block, use the get_via_ladders command.\n To report on the via ladders in a block, use the report_via_ladders command."}
{"instruction": "How do I Introduction to Multisource Clock Trees Structures", "input": "", "output": "The following topics introduce the different types of multisource clock trees and provides detailed information about the different multisource clock tree implementation flows and steps:  Introduction to Multisource Clock Trees Structures  Implementing a Regular Multisource Clock Tree  Implementing a Regular Multisource Clock Tree Using Integrated Tap Assignment  Implementing a Regular Multisource Clock Tree With an H-Tree-Only Global Clock Tree Structure  Implementing MSCTS Multiple H-Tree  Implementing Power Aware Flexible H-Tree  Implementing a Structural Multisource Clock Tree  Implementing a Structural Multisource Clock Tree Using Integrated Subtree Synthesis  Improving the Correlation Between the SMSCTS and CCD  Inserting Clock Drivers  Synthesizing the Global Clock Trees  Creating Clock Straps  Routing to Clock Straps  Analyzing the Clock Mesh        Performing Automated Tap Insertion and H-Tree Synthesis  Specifying Tap Assignment Options and Settings  Building the Local Clock Subtree Structures"}
{"instruction": "How do I Assigning Nondefault Routing Rules to Nets", "input": "", "output": "For specific signal nets, use the -ignore_spacing_to_shield false option when you define the nondefault spacing rule with the create_routing_rule command  To ignore nondefault spacing rules between signal nets and PG nets  For all signal nets, set the route.common.ignore_var_spacing_to_pg application option to true  For specific signal nets, use the -ignore_spacing_to_pg true option when you define the nondefault spacing rule with the create_routing_rule command Note: When you ignore nondefault spacing rules between signal nets and PG nets, the rules are also ignored between the signal nets and the shield wires regardless of the setting of the route.common.ignore_var_spacing_to_shield and create_routing_rule -ignore_spacing_to_shield options.\n  To ignore nondefault spacing rules between signal nets and blockages  For all signal nets, set the route.common.ignore_var_spacing_to_blockage application option to true  For specific signal nets, use the -ignore_spacing_to_blockage true option when you define the nondefault spacing rule with the create_routing_rule command       You can relax the nondefault spacing checks for specific nondefault routing rules by ignoring violations for short parallel distances, as shown in.\n This technique increases the flexibility of DRC convergence without adversely affecting crosstalk.\n  Ignoring Nondefault Spacing Rule Violations To define the length thresholds within which to ignore the nondefault spacing violations, use the -spacing_length_thresholds option when you create the nondefault routing rule with the create_routing_rule command.\n The length threshold values are in microns and must have a one-to-one correspondence with the spacing entries specified in the -spacings option.\n For example, fc_shell> create_routing_rule new_rule \\ -spacings { M1 {0.09 0.15 0.2} M2 {0.09 0.15 0.2} M3 {0.09 0.15 0.2} } \\ -spacing_weight_levels { M1 {hard medium low} M2 {hard medium low} M3 {hard medium low} } \\ -spacing_length_thresholds { M1 {0.01 0 0} M2 {0.01 0 0} M3 {0.01 0 0} } Specifying the Routing Effort for Soft Spacing Violations By default, Zroute does not fix soft spacing violations.\n To fix soft spacing violations, you must assign a routing effort to each of the soft weight levels.\n These assignments apply to all soft spacing rules.\n       To assign a routing effort for each weight level, use the following syntax to set the route.common.soft_rule_weight_to_effort_level_map application option: set_app_options -name route.common.soft_rule_weight_to_effort_level_map -value { { weight effort }...\n } Each weight argument can be one of low medium, or high.\n You should specify each weight level only one time; if you specify a weight level multiple times, the router uses the last specification.\n Each effort argument can be one of the following values:  off (the default) Zroute does not fix the soft spacing rule violations.\n  low Zroute uses a small number of rip-up and reroute passes to resolve soft spacing rule violations.\n  medium Zroute uses a medium number of rip-up and reroute passes to resolve soft spacing rule violations.\n Note that you cannot specify this effort level for the low weight level.\n  high Zroute treats soft spacing rule violations the same as regular design rule violations during rip up and reroute.\n Note that you cannot specify this effort level for the low weight level.\n For example, to assign low routing effort to low-weight soft spacing rules, medium routing effort to medium-weight soft spacing rules, and high routing effort to high-weight soft spacing, use the following command: fc_shell> set_app_options \\ -name route.common.soft_rule_weight_to_effort_level_map \\ -value { {low low} {medium medium} {high high} } Defining Minimum Via Spacing Rules You can define nondefault minimum via spacing rules that are stricter than the rules defined in the technology file.\n Nondefault via spacing rules are hard constraints that must be met.\n To define a minimum via spacing rule, use the -via_spacings option with the create_routing_rule command using the following syntax: create_routing_rule rule_name -via_spacings { { layer1 layer2 spacing }...\n }       Specify the routing layers by using the layer names from the technology file.\n You can define a single width value per layer pair.\n The minimum via spacing for vias between any unspecified layer combinations is determined from the technology file.\n For vias on the same layer, the router uses the minimum spacing defined in the Layer section for the via layer.\n For vias on different layers, the router uses the minimum spacing defined in the DesignRule section for the via layer combination.\n For example, to define a nondefault routing rule named via_spacing_rule that defines nondefault spacing rules between vias on the V1 layer and between vias on the V1 and V2 layers, and uses the minimum spacing rules defined in the technology file between vias on all other layer combinations, use the following command: fc_shell> create_routing_rule via_spacing_rule \\ -via_spacings {{V1 V1 2.3} {V1 V2 3.2}} Specifying Nondefault Vias By default, when routing nets with nondefault routing rules, Zroute selects vias based on the design rules.\n To specify the vias to use when routing nets with nondefault routing rules, use the create_routing_rule command.\n You can define the nondefault vias by using either the -cuts option or the -vias option.\n  When you use the -cuts option, the tool determines the suitable via definitions from the technology file based on the specification in the -cuts option and the rules defined in the technology file.\n  When you use the -vias option, you explicitly specify the nondefault via definitions, including the allowed cut numbers and rotation for each via definition.\n Specifying Nondefault Vias Using the -cuts Option The syntax for specifying nondefault vias using the -cuts option is create_routing_rule rule_name -cuts { { cut_layer1 cut_name1 ncuts } { cut_name2 ncuts } \u2026} cut_layer2 cut_name1 ncuts } { cut_name2 ncuts } \u2026}...\n cut_layern cut_name1 ncuts } { cut_name2 ncuts } \u2026}  The cut_layer arguments refer to the via layer names in the technology file and the cut_name arguments refer to the cut names defined in the cutNameTbl attribute in the associated Layer section in the technology file.\n You can specify multiple cut names per layer.\n For each cut name, you must specify the minimum number of cuts, which must be an integer between 1 and 255.\n       The tool searches for the vias defined in the ContactCode section of the technology file that meet the rules defined in the technology file for the specified cut name, such as the cutWidthTbl cutHeightTbl, and fatTblFatContactNumber rules.\n In addition, the width of the via enclosure must meet the nondefault width and the xLegalWidthTbl and yLegalWidthTbl rules defined in the technology file for the adjacent metal layers.\n If the fat metal contact rule is not defined for a via layer, the tool searches for the default vias that meet the cut width, cut height, and via enclosure width requirements.\n The minimum number of cuts required is the larger of the ncuts value in the -cuts option and the value defined in the fatTblFatContactMinCuts attribute.\n For the selected vias, the tool always allows both the rotated and unrotated orientations for the via.\n For example, assume the following information is defined in the technology file: Layer \"VIA1\" { fatTblThreshold = ( 0, 0.181, 0.411 ) fatTblFatContactNumber = ( \"2,3,4,5,6 \",\"5,6,20\", \"5,6,20\" ) fatTblFatContactMinCuts = ( \"1,1,1,1,1\", \"1,1,1\", \"2,2,2\" )  cutNameTbl = ( Vsq, Vrect ) cutWidthTbl = ( 0.05, 0.05 ) cutHeightTbl = ( 0.05, 0.13 )    ContactCode \"VIA12_LH\" { contactCodeNumber = 5 cutWidth = 0.13 cutHeight = 0.05   ContactCode \"VIA12_LV\" { contactCodeNumber = 6 cutWidth = 0.05 cutHeight = 0.13   ContactCode \"VIA12_P\" { contactCodeNumber = 20 cutWidth = 0.05 cutHeight = 0.05   If you use the"}
{"instruction": "How do I Controlling the Addition of New Cells to Modules, Hierarchical", "input": "", "output": "Multiple-port nets include  Feedthrough nets, where an input port feeds into an output port  Nets connected to multiple output ports, logically equivalent outputs       Feedthrough net Logically equivalent outputs Multiple-port nets, which are represented with the assign statement in the gate-level netlist, might cause design rule violations further in the design flow.\n By default, the tool does not fix multiple-port nets during optimization.\n You can enable multiple-port-net fixing for  The logic_opto and initial_drc stages, which are earlier stages of the compile_fusion command, by using the set_fix_multiple_port_nets command This command sets the fix_multiple_port_nets attribute on the specified objects.\n  The initial_opto and final_opto stages, which are later stages of the compile_fusion command, by setting the opt.port.eliminate_verilog_assign application option to true This application option is false by default.\n If you want to fix multiple-port nets, use the set_fix_multiple_port_nets command and fix these issues during the early stages of the compile_fusion command, when the tool can perform more optimization techniques.\n If you do not explicitly specify a multiple-port-net setting using the set_fix_multiple_port_nets command, but you set the opt.port.eliminate_verilog_assign application option to true, the tool issues the following message and uses the set_fix_multiple_port_nets -all -buffer_constants command setting to fix multiple-port nets during the logic_opto and initial_drc stages of the compile_fusion command: Information: The opt.port.eliminate_verilog_assign application option has been set to true.\n The tool will run set_fix_multiple_port_nets -all -buffer_constant by internally enabling MPN fixing due to the absence of explicit set_fix_multiple_port_nets constraint.\n (MPN-0004) However, if you explicitly specify a multiple-port-net setting using the set_fix_multiple_port_nets command, the tool honors it.\n For example, the following settings enable multiple-port-net fixing for feedthrough nets during the logic_opto and       initial_drc stages and all nets during the initial_opto and final_opto stages of the compile_fusion command: fc_shell> set_fix_multiple_port_nets -feedthroughs fc_shell> set_app_options \\ -name opt.port.eliminate_verilog_assign -value true The following settings disable multiple-port-net fixing for all nets during the logic_opto and initial_drc stages and enables it for all nets during the initial_opto and final_opto stages of the compile_fusion command: fc_shell> set_fix_multiple_port_nets -default fc_shell> set_app_options \\ -name opt.port.eliminate_verilog_assign -value true During multiple-port-net fixing, the tool performs the following tasks in sequence: 1.\n Rewires the connections to ensure that each net is connected to one hierarchical port, if possible.\n The tool first performs rewiring to avoid unnecessary buffering.\n The tool also rewires across the hierarchy because boundary optimization is enabled by default.\n 2.\n Inserts buffers or inverters to the nets that are not fixed by rewiring."}
{"instruction": "How do I Using ECO Scripts for Netlist Editing", "input": "", "output": "If the ECO changes are minimal, you can update the design by using netlist editing commands given.\n If you are using the freeze silicon ECO flow, enable ECO changes in the freeze silicon mode by setting the design.eco_freeze_silicon_mode application option to true, before you run the netlist editing commands.\n For a list of netlist editing commands, see the Common Design Objects topic in the Fusion Compiler Data Model User Guide"}
{"instruction": "How do I Routing to Clock Straps", "input": "", "output": "You can create clock straps, which are straight metal shapes in a single routing layer, by using the create_clock_straps command.\n You can use this command to implement  A clock mesh, which is a two-dimensional grid in a horizontal and a vertical layer, where the straps are connected by vias at the intersection points, as shown in   A clock spine, which can be either a one- or two-dimensional structures.\n One-dimensional spines are straps in a single direction.\n Two-dimensional spines consists of one-dimensional spines connected to multiple stripes in the orthogonal direction.\n Stripes connected to one spine do not connect to stripes of a different spine and the minimum distance between the stripes of different spines is called the backoff, as shown in        Clock Mesh Structure Boundary of clock mesh Start distance for Incremental step for subsequent straps (0,0) Start distance for Incremental step for subsequent straps first vertical strap first horizontal strap End distance for last vertical strap End distance for last horizontal strap        Clock Spine Structure Boundary of clock spine (0,0) Backoff Stripes Spine The horizontal and vertical  start, end, and step distances are applicable for  clock spine structures too.\n When you use the create_clock_straps command,  To specify the clock net for which to create the structure, use the -net option.\n  To specify the bounding box to confine the structure, use the -boundary option.\n  To specify keepouts, use the -keepouts option.\n If part of a strap is over a keepout, by default, the tool splits the strap and creates the portions that are outsides the keepouts.\n However, you can disable splitting and specify that the tool not create a strap if it is over a keepout by using the -allow_splitting false option.\n  To specify the layers on which create the straps, use the -layers option.\n The tool determines the direction of the straps based on the metal layer you specify.\n To create straps in both directions, specify a layer for each direction.\n  To specify the width of the straps, use the -width option.\n        To specify where to create the straps, use the -grid option as follows:  To create a single horizontal or vertical strap, specify the distance from the x- or y- axis.\n  To create a multiple straps in a single direction, specify an iterator list consisting of the start distance to the first strap, the end distance to the last strap, and the incremental distance between straps.\n  To create a multiple straps in a both the horizontal and vertical directions, specify an iterator list for each direction, starting with the horizontal direction.\n Use the -grid option to create multiple straps for both the clock mesh and clock spine structures.\n  To specify a margin within which the tool can move a strap from the position it derives based on the -grid option settings, use the -margins option.\n If the tool cannot create a strap within the specified margin, due to obstructions or keepouts, it does not create the strap.\n By default, the tool uses a margin of zero and only creates the strap if it can do so at the exact position it derives.\n  To create straps on the boundary specified with the -boundary option, use the -create_ends option.\n  To allow straps that are unconnected to orthogonal straps, use the -allow_floating true option.\n By default, the tool does not allow orthogonal straps that are unconnected.\n  To specify the type of straps to create, use the -type option and specify user_route stripe, or detect When you create straps in both direction, specify a list of two values starting with the type for the horizontal direction.\n You can specify detect as the type only when you are creating a clock spine structure and want the tool to detect and use existing spines.\n When you do so, you must specify a list consisting of the type for the orthogonal stripes and detect, with the type for the horizontal direction specified first in the list.\n In addition, when the tool detects existing spine, you can specify a minimum length for the spine, by using the -detect_length option.\n  To specify the maximum length of the orthogonal stripes, when creating a clock spine structure, use the -length option.\n  To specify the backoff distance between stripes of different spines, when creating a clock spine structure, use the -backoff option.\n        To specify the direction of the spines, when creating a clock spine structure and the tool is not detecting and using existing spines, use the -spine_direction option.\n  To shield the straps with power and ground nets, use the -bias option.\n  To shield the straps with specific nets, use the -bias_to_nets with the list of nets.\n  To specify the distance from the shielding nets, use the -bias_margins option.\n  To remove the straps of a specific clock net, use the -clear option.\n For example, the following command creates a clock mesh for the net named clk1_mesh that is bounded by coordinates (0,0) and (1200, 980).\n The straps are on layers M7 and M8 with a width of 2.4 units and of type stripe.\n The horizontal straps on layer M7 start at a distance of 20 units from the x-axis and repeat every 100 units, until they reach 1200 units from the x-axis.\n The vertical straps on layer M8 start at a distance of 60 units from the y- axis and repeat every 150 units, until they reach 980 units from the x-axis.\n fc_shell> create_clock_straps -nets [get_nets clk1_mesh] \\ -layers {M7 M8} -widths {2.4 2.4} -types {stripe stripe} \\ -grids {{20 1200 100} {60 980 150}}-boundary {{0 0} {1200 980}} The following example creates a two-dimensional spine structure that has spines on the vertical layer M8 with a width of 3.6 units and of type user_route and stripes on the horizontal layer M7 with a width of 2.4 units and of type stripe.\n The vertical spines on layer M8 start at a distance of 60 units from the y-axis and repeat every 150 units, until they reach 980 units from the x-axis.\n The horizontal stripes on layer M7 have a length of 120 units and they start at a distance of 20 units from the x-axis and repeat every 100 units, until they reach 1200 units from the x-axis.\n The minimum distance (backoff) between stripes of different spines is 5 units.\n fc_shell> create_clock_straps -nets [get_nets clk1_mesh] \\ -layers {M7 M8} -widths {2.4 3.6} -types {stripe user_route} \\ -grids {{20 1200 100} {60 980 150}} -length 120 -backoff 5"}
{"instruction": "How do I Hierarchical Synthesis Flow When Floorplans are Available", "input": "", "output": "To partition and plan the top-level design and create the lower-level blocks for subsequent bottom-up synthesis, when the floorplans of subblock and top-level design are not available, perform the following steps:  Hierarchical Synthesis Flow When Floorplans are not Available 1.\n Read in the full chip design and apply constraints as shown in the following example: fc_shell> set REF_LIBS \"stdcell.ndm macro.ndm\" fc_shell> create_lib TOP -technology techfile \\ -ref_libs $REF_LIBS fc_shell> analyze -format verilog $rtl_files fc_shell> elaborate TOP fc_shell> set_top_module TOP  fc_shell> load_upf fullchip.upf fc_shell> read_sdc fullchip.sdc 2.\n Identify the design partitions and split the constraints as shown in the following example: fc_shell> set_budget_options -add_blocks {BLOCK1 BLOCK2} fc_shell> split_constraints 3.\n Create the subblock design libraries with design information and enable block-specific reference library setup as shown in the following example: fc_shell> copy_lib -to_lib BLOCK1.nlib -no_design fc_shell> copy_lib -to_lib BLOCK2.nlib -no_design       fc_shell> set_attribute -objects BLOCK1.nlib \\ -name use_hier_ref_libs -value true fc_shell> set_attribute -objects BLOCK2.nlib \\ -name use_hier_ref_libs -value true fc_shell> save_lib -all 4.\n Create the subblock design partitions as shown in the following example: fc_shell> commit_block -library BLOCK1.nlib BLOCK1 fc_shell> commit_block -library BLOCK2.nlib BLOCK2 fc_shell> save_lib -all 5.\n Load the UPF and SDC constraints for the unmapped subblocks and the top-level, which are generated by the split_constraints command earlier as shown in the following example: fc_shell> set_constraint_mapping_file./split/mapfile fc_shell> load_block_constraints -all_blocks -type SDC \\ -type UPF -type CLKNET fc_shell> save_lib -all 6.\n Run the compile_fusion command until logic optimization with auto floorplanning for sub blocks is complete as shown in the following example: fc_shell> compile_fusion -to logic_opto 7.\n Run the compile_fusion command until technology mapping for the top-level design is complete as shown in the following example: fc_shell> compile_fusion -to initial_map 8.\n Perform design planning operations starting from floorplan initialization, subblock and voltage area shaping, hard macro and standard cell placement, power network creation until pin assignment.\n 9.\n Run the compile_fusion command until logic optimization for the top-level design is complete as shown in the following example: fc_shell> compile_fusion -from logic_opto -to logic_opto 10.\n Perform incremental top-level only standard cell placement as shown in the following example: fc_shell> create_placement -floorplan -use_seed_locs 11.\n Perform timing estimation and budgeting steps to generate the block budgets needed for block level implementation.\n 12.\n After completing the design planning operations, rebuild the subblock and top-level design by opening the elaborated RTL NDM, loading floorplan, feedthroughs and budgets."}
{"instruction": "How do I Viewing Error Data", "input": "", "output": "Use the analyze_rail -check_missing_via command to check for missing vias and unconnected pins in the block based on the settings specified by the set_missing_via_check_options command.\n Use the -voltage_drop option if you want to run missing via and unconnected pin checks together with voltage drop analysis.\n The tool then reports all missing vias found in the geometry with voltage values in the missing via report.\n To set a voltage threshold across layers for filtering the report, use the -threshold option of the set_missing_via_check_options command.\n You can set multiple threshold values in a missing via check.\n       Note: In the RedHawk-SC Fusion flow, you must specify the -check_missing_via option together with the -voltage_drop option.\n In cases you want to report missing vias with and without setting a voltage threshold in one analyze_rail run, first set the desired threshold value (for example, 0.001) and then reset the value to -1.\n Setting -threshold option to -1 disables voltage checking.\n When the missing via check is completed, open the generated error data to examine the errors in the error browser.\n For details, see Viewing Error Data Example 48 fc_shell> set_missing_via_check_options -exclude_stack_via \\ -threshold 0.0001 fc_shell> analyze_rail -voltage_drop static -check_missing_via \\ -nets {VDD VSS} The above example sets the threshold value to 0.0001 and compares voltages across two ends of a via, excluding stack vias.\n It then runs the missing via check during static rail analysis.\n The RedHawk script file includes the following commands: perform extraction -power -ground perform analysis -static mesh vias -report_missing -exclude_stack_via -threshold 0.0001 \\ -o apache.missingVias1 Example 49 fc_shell> set_missing_via_check_options -exclude_stack_via \\ -threshold -1 fc_shell> analyze_rail -voltage_drop -check_missing_via \\ -nets {VDD VSS} The above example sets the threshold value to -1 to disable the voltage checking, and runs the missing via check.\n The RedHawk script file includes the following commands: perform extraction -power -ground mesh vias -report_missing -exclude_stack_via -threshold -1 \\ -o apache.missingVias1.nothreshold Example 50 fc_shell> set_missing_via_check_options -exclude_stack_via \\ -threshold 0.001 fc_shell> set_missing_via_check_options -exclude_stack_via \\ -threshold 0.002 fc_shell> set_missing_via_check_options -exclude_stack_via \\       -threshold -1 fc_shell> analyze_rail -voltage_drop static -check_missing_via \\ -nets {VDD VSS} fc_shell> report_rail_result -type missing_vias -supply_nets \\ {VDD VSS} rpt.missing_vias Information: writing rpt.missing_vias.no_threshold done Information: writing rpt.missing_vias done In the above example, three different threshold values are set for a missing via check.\n The tool generates two missing via reports: rpt.missing_vias.no_threshold includes all missing vias found in the geometry, while rpt.missing_vias includes the vias whose voltage difference across two ends is less than 0.001 and 0.002."}
{"instruction": "How do I Timing Driven Sink Transition Target", "input": "", "output": "I/O ports are usually constrained using real or virtual clocks.\n After clock tree synthesis, the latencies of the clocks constraining the I/O ports need to be updated to ensure that the boundary constraints are accurate.\n When you run clock tree synthesis with the clock_opt command, the tool automatically updates the latencies of the clocks constraining the I/O ports.\n You can control these latency adjustments by using the set_latency_adjustment_options command.\n The following example specifies that the clock latencies of the VCLK1 and VCLK2 clocks should be updated based on the latency of the PC_CLK clock, and that the latency of the VCLK3 clock should not be updated: fc_shell> set_latency_adjustment_options \\ -reference_clock PC_CLK -clocks_to_update {VCLK1 VCLK2} \\ -exclude_clocks VCLK3 If you perform clock tree synthesis with the synthesize_clock_trees command, the tool does not automatically update the clock latencies.\n To do so, use the compute_clock_latency command after clock tree synthesis.\n This command honors the settings you specify with the set_latency_adjustment_options command."}
{"instruction": "How do I Checking for Filler Cell DRC Violations", "input": "", "output": "By default, the create_stdcell_fillers command fills all empty space in the horizontal standard-cell rows of the entire block by inserting instances of the filler library cells specified by the -lib_cells option.\n You can control the following aspects of the filler cell insertion:  The selection of filler cells based on lowest leakage current By default, the command does not consider leakage current when inserting filler cells.\n To reduce the leakage current, use the -leakage_vt_order option to specify the threshold voltage layers in order of decreasing leakage current.\n The command uses this information to select the filler cells with the lowest leakage current that meet the legalization requirements.\n To check whether the filler cells inserted in the current block result in the lowest possible leakage power, use the -leakage_vt_check option with the create_stdcell_fillers command.\n When you use this option, the command only checks the existing filler cells; it does not insert filler cells.\n You must also use the -lib_cells and -leakage_vt_order options to specify the available filler cells.\n The check reports the filler cells that could be replaced with a specified filler cell with lower leakage current without causing legalization errors.\n By default, the command reports a maximum of 100 violations.\n To change the maximum number of reported violations, set the chf.create_stdcell_fillers.max_leakage_vt_order_violations application option.\n       Note: This feature has the following requirements:  You must enable the advanced legalizer by setting the place.legalize.enable_advanced_legalizer application option to true.\n For more information about the advanced legalizer, see Enabling Advanced Legalization Algorithms  The threshold voltage layers must be defined in the technology file.\n A threshold voltage layer is identified by a maskType attribute of implant in the Layer section of the technology file.\n  The percentage of empty space to fill By default, the command fills all the empty space.\n To leave some empty space, use the -utilization option to specify the percentage of empty space to fill.\n To control the relative amount of various types of filler cells, such as ULVT and LVT decoupling capacitor cells, use the -type_utilization option to specify the insertion percentage for each type of cell.\n The sum of all the percentages specified in this option must be less than or equal to 100.\n When you use this option, the create_stdcell_fillers command might leave some empty spaces.\n To fill these empty spaces, use the -fill_remaining option.\n When you use this option, the command uses the cells specified in the -lib_cells option but not the -type_utilization option to fill the empty spaces.\n For example, # insert 70 percent ULVT cells, 30 percent LVT cells fc_shell> create_stdcell_fillers -lib_cells $FILLER_CELLS \\ -type_utilization { {*/DCAP16*ULVT */DCAP8*ULVT */DCAP4*ULVT */DCAP2*ULVT */DCAP1*ULVT} 70 {*/DCAP16*LVT */DCAP8*LVT */DCAP4*LVT */DCAP2*LVT */DCAP1*LVT} 30 } \\ -fill_remaining By default, the tool randomly selects the filler cells from the specified library cells.\n To control the cell selection priority, use the -prefer_type_ordering option with the -type_utilization option.\n When you use the -prefer_type_ordering option, the tool selects the library cells in each group specified in the -type_utilization option in priority order, from left to right.\n If some of the cells are difficult to insert due to legalization rules or library cell size, specify them before cells that are easier to insert to       ensure that they are selected.\n For example, when the -prefer_type_ordering option is used with the previous example, the tool  First tries to insert the ULVT decoupling capacitors to a utilization of 70 percent, first trying to insert */DCAP16*ULVT cells, then */DCAP8*ULVT cells, and so on  Then tries to insert the LVT decoupling capacitors to a utilization of 30 percent, first trying to insert */DCAP16*LVT cells, then */DCAP8*LVT cells, and so on  Then filling the remaining gaps using the remaining cells specified in the -lib_cells option # insert 70 percent ULVT cells, 30 percent LVT cells fc_shell> create_stdcell_fillers -lib_cells $FILLER_CELLS \\ -type_utilization { {*/DCAP16*ULVT */DCAP8*ULVT */DCAP4*ULVT */DCAP2*ULVT */DCAP1*ULVT} 70 {*/DCAP16*LVT */DCAP8*LVT */DCAP4*LVT */DCAP2*LVT */DCAP1*LVT} 30 } \\ -prefer_type_ordering -fill_remaining  The region in which to insert the filler cells  Use the -bboxes option to restrict filler cell insertion to the specified bounding boxes.\n  Use the -voltage_area option to restrict filler cell insertion to the specified voltage areas.\n  Whether to check for power net violations By default, the command does not check for power net violations, as this check increases runtime and is required only for metal filler cell insertion.\n However, this might result in the remove_stdcell_fillers_with_violation command removing many decoupling capacitors.\n To prevent power net violations caused by filler cell insertion, use the -rules check_pnet option.\n  Gap prevention By default, the command assumes that the smallest cell size is one unit site.\n If the smallest cell in your design is larger than one unit site, this could cause gaps during filler cell insertion.\n To prevent the command from inserting filler cells that leave a gap, specify the smallest cell size as a multiple of the unit site by using the -smallest_cell_size option.\n You can specify a value of 1, 2, or 3.\n       Note: The advanced legalizer automatically prevents gaps; therefore, this option is ignored when place.legalize.enable_advanced_legalizer application option is set to true  The handling of hard placement blockages By default, the command honors hard placement blockages and does not insert filler cells in regions affected by these blockages.\n To ignore the hard placement blockages and insert filler cells in those regions, use the -ignore_hard_blockages option.\n  The naming convention used to identify the inserted filler cell instances By default, the command uses the following naming convention for inserted filler cells: xofiller!\n filler_library_cell_name number To identify the filler cells inserted in a specific run, use the -prefix option to specify a prefix string.\n When you use this option, the command uses the following naming convention: xofiller!\n prefix filler_library_cell_name number  The behavior when the legalizer detects errors By default, the command fails if the legalizer detects an error.\n To force the command to complete filler cell insertion even when errors occur, use the -continue_on_error option.\n When you use this option, the resulting block might have legalization errors that must be fixed manually."}
{"instruction": "How do I Removing Perimeter Constraint Objects", "input": "", "output": "You can insert routing blockages along the preferred-direction edges, the nonpreferred- direction edges, or all edges.\n  To insert routing blockages along the preferred-direction edges of the current block, use the derive_perimeter_constraint_objects command with the -perimeter_objects route_blockage_preferred option.\n  To insert routing blockages along the preferred-direction edges of the current block, use the derive_perimeter_constraint_objects command with the -perimeter_objects route_blockage_nonpreferred option.\n  To insert routing blockages along all edges of the current block, use the derive_perimeter_constraint_objects command with the -perimeter_objects {route_blockage_preferred route_blockage_nonpreferred} option.\n When you specify this object type, the command creates routing blockages parallel to the boundary edges in the specified directions.\n       By default,  The routing blockages are inserted only in the top-level block To insert routing blockages in all soft macros in a hierarchical block, use the -hierarchical option.\n  The routing blockages are inserted on all routing layers To insert the routing blockages only on specific routing layers, use the -layers option.\n  The routing blockages have the default metal width specified for the routing layer in the technology file To specify a different width in microns for preferred-direction routing blockages, use the -width_preferred option.\n To specify a different width in microns for nonpreferred- direction routing blockages, use the -width_nonpreferred option.\n For information about specifying the default metal width for a layer, see the \"Default Width Rule\" topic in the Synopsys Technology File and Routing Rules Reference Manual  The routing blockages are offset from the boundary edges by the minimum spacing specified for the routing layer in the technology file To specify a different offset in microns tor preferred-direction routing blockages, use the -spacing_preferred option.\n To specify a different offset in microns tor nonpreferred- direction routing blockages, use the -spacing_nonpreferred option.\n For information about specifying the minimum spacing for a layer, see the \"Minimum Spacing Rule\" topic in the Synopsys Technology File and Routing Rules Reference Manual  The routing blockages do not have pin cutouts To generate pin cutouts, use the -pin_cutout option.\n  Cut layer routing blockages are not created To create via blockages, specify the via layers in the -layers option.\n By default, the width of the cut layer routing blockages is the maximum width for the preferred-direction and nonpreferred-direction routing blockages.\n  To specify the width for cut layer routing blockages along the horizontal edges, use the -width_cut_layer_horizontal option.\n  To specify the width for cut layer routing blockages along the vertical edges, use the -width_cut_layer_vertical option.\n       By default, the cut layer routing blockages are offset from the boundary edges by the is the maximum spacing for the preferred-direction and nonpreferred-direction routing blockages  To specify the spacing for cut layer routing blockages along the horizontal edges, use the -spacing_cut_layer_horizontal option.\n  To specify the spacing for cut layer routing blockages along the vertical edges, use the -spacing_cut_layer_vertical option."}
{"instruction": "How do I Adding Relative Placement Groups", "input": "", "output": "The add_to_rp_group command supports hard macro cells.\n When you add macro cells to a relative placement group, you cannot specify  A pin name with which to align the macro cell by using the -pin option  An orientation for the macro cells by using the -orientation option When you add hard macro cells, use the following steps: 1.\n Create the relative placement groups, add the cells, including the hard macro cells, and specify the relative placement options and settings.\n 2.\n Place the design by using the create_placement -floorplan command.\n 3.\n Fix the placement of the hard macro cells in the relative placement groups by using the set_placement_status fixed command.\n Before you can perform further placement and optimization,  The hard macro cells in the relative placement groups must be placed and fixed  The other cells in the relative placement groups that contain hard macro cells must be placed 4.\n Perform further placement and optimization."}
{"instruction": "How do I Analyzing Relative Placement Groups", "input": "", "output": "You can create a new relative placement group of cells in a placed design and relative placementincremental relative placement place the relative placement groups incrementally by using the place_opt -from final_place command.\n If you select cells that are placed far apart in the initial placement for the same relative placement group, performing incremental relative placement might degrade the QoR.\n The following example shows how to add a new relative placement group to a design that is already placed and optimized, and performs incremental placement and optimization: fc_shell> create_rp_group -name new_rp -columns 1 -rows 2 fc_shell> add_to_rp_group new_rp -cells U1 -column 0 -row 0 fc_shell> add_to_rp_group new_rp -cells U2 -column 0 -row 1...\n fc_shell> place_opt -from final_place For a placed design, if you create a new relative placement group and specify an anchor location by using the -x_offset and -y_offset options with the set_rp_group_options command, you can use the legalize_rp_groups command to legalizes the newly created group anchored by the specified x- and y-coordinates.\n Using the       legalize_rp_groups command for incremental relative placement groups reduces the turnaround time."}
{"instruction": "How do I Querying Checkpoints and Checkpoint Behaviors", "input": "", "output": "After defining checkpoints with the eval_checkpoint command and checkpoint behaviors with the create_checkpoint_* command, you must associate the two so that the tool executes the behaviors when it encounters the checkpoints in a run, as described in the following topics:  Associating Checkpoints With Checkpoint Reports  Associating Checkpoints With Checkpoint Actions See Also  Defining Checkpoints  Defining Checkpoint Behaviors Associating Checkpoints With Checkpoint Reports To associate a checkpoint report with one or more checkpoints, use the associate_checkpoint_report command in your checkpoint.config.tcl file.\n        Specify the checkpoint report name with the -enable option.\n  To enable the report to run before one or more checkpoints, specify the checkpoint names with the -before option.\n  To enable the report to run after one or more checkpoints, specify the checkpoint names with the -after option.\n When associating a report with multiple checkpoints, you can list the checkpoint names or specify the asterisk wildcard character.\n Assume you have defined three checkpoints in your script named remove_buffers, place_opt_to_initial_opto, and place_opt_to_final_opto using the eval_checkpoint command, and two checkpoint reports named timing and app_options using the create_checkpoint_report command.\n To enable your timing report to run after the place_opt_to_initial_opto and place_opt_to_final_opto checkpoints, use the following command: associate_checkpoint_report -enable timing \\ -after { place_opt_to_initial_opto place_opt_to_final_opto } To enable your app_options report to run before all the checkpoints in your script, use the following command: associate_checkpoint_report -enable app_options -before * You can also add the command like this, which enables the report to run before the remove_buffers checkpoint and any checkpoints beginning with the phrase place_opt: associate_checkpoint_report -enable app_options -before remove_buffers place_opt* The following is a portion of your checkpointed script showing what happens when the script runs.\n Note that the highlighted code is not actually in your script, but shows the checkpoint behaviors that are executed when the tool encounters the checkpoints in the run.\n  The code that is commented out identifies the original script body  The blue code identifies the checkpointed Tcl commands in your golden flow  The green code identifies the app_options report, configured to run before each checkpoint  The code identifies the timing report, configured to run after the place_opt_to_final_opto checkpoint       Associating Checkpoints With Checkpoint Actions To associate a checkpoint action with one or more checkpoints, use the associate_checkpoint_action command in your checkpoint.config.tcl file.\n  Specify the checkpoint action name with the -enable option.\n  To enable the action to run before one or more checkpoints, specify the checkpoint names with the -before option.\n  To enable the action to run after one or more checkpoints, specify the checkpoint names with the -after option.\n  To enable the action to run instead of the code block enwrapped by the checkpoint, specify the checkpoint names with the -replace option.\n When associating an action with multiple checkpoints, you can list the checkpoint names or specify the asterisk wildcard character.\n Assume you have defined two checkpoints in your script named place_opt_to_initial_drc and incr_placement with the eval_checkpoint command, and two checkpoint actions named gropto and placer_high_effort_cong with the create_checkpoint_action command.\n To enable your gropto action to run before the place_opt_to_initial_drc checkpoint, use the following command: associate_checkpoint_action -enable gropto \\ -before place_opt_to_initial_drc       To enable your placer_high_effort_cong action to run instead of the code block enwrapped by the incr_placement checkpoint, use the following command: associate_checkpoint_action -enable placer_high_effort_cong \\ -replace incr_placement The following is a portion of your checkpointed script showing what happens when the script runs.\n Note that the highlighted code is not actually in your script, but shows the checkpoint behaviors that are executed when the tool encounters the checkpoints in a run.\n  The code that is commented out identifies the original script body  The blue code identifies the checkpointed Tcl commands in your golden flow  The green code identifies the gropto action, configured to run before the place_opt_to_initial_drc checkpoint  The code identifies the placer_high_effort_cong action, configured to run in place of the code block enwrapped by the incr_placement checkpoint       Associating a Checkpoint With Multiple Behaviors When you associate a checkpoint with multiple behaviors, the tool executes those behaviors in the following order:  Any reports configured to run before the checkpoint  Any actions configured to run before the checkpoint  The checkpointed code block, or any actions configured to replace it  Any actions configured to run after the checkpoint  Any reports configured to run after the checkpoint If multiple reports or multiple actions are configured to run at the same position ( -before -after, or -replace ) in the same checkpoint, the tool executes them in the order in which they were associated with the checkpoint with the associate_checkpoint_* commands.\n For example, suppose you define a timing report before an area report.\n If you associate the area report to run after a checkpoint before you associate the timing report to run after the same checkpoint, the tool runs the area report first when it encounters the checkpoint.\n See Also  Defining Checkpoints  Defining Checkpoint Behaviors  Querying Checkpoints and Checkpoint Behaviors"}
{"instruction": "How do I Defining Routing Blockages", "input": "", "output": "Routing constraints provide guidance during routing.\n Table 29 describes the types of guidance provided by the routing constraints.\n Table 29 Routing Constraints Guidance Description                                 onWireTrack onGrid      physical_status  locked                 Table 29 Routing Constraints (Continued) Guidance Description                           derive_metal_cut_route_guides"}
{"instruction": "How do I Specifying Legalization Settings", "input": "", "output": "If a block has many chains of repeater cells (buffers, inverters, or pipelined registers) along the edges or corners of macro cells or blockages, the tool might clump the cells along the edges or corners, increasing congestion.\n You can reduce this congestion by setting the place.coarse.spread_repeater_paths application option to true before you run the create_placement compile_fusion, or clock_opt command.\n Then, the tool reduces congestion along the edges and corners by spreading the repeater cells in an orthogonal direction."}
{"instruction": "How do I Running the signoff_fix_isolated_via Command", "input": "", "output": "Before you run the signoff_fix_isolated_via command, configure the run by setting the application options shown in Table 51.\n To set the application options, use the set_app_options command.\n To see the current settings, use the report_app_options command.\n       Table 51 Application Options for Signoff Isolated Via Fixing Application option Default Description signoff.\n fix_isolated_via.\n isolated_via_max_range           signoff.\n fix_isolated_via.\n avoid_net_types {clock pg}    clock pg none signoff.\n fix_isolated_via.\n run_dir    signoff_fix_isolated_via      signoff.\n fix_isolated_via.\n user_defined_options"}
{"instruction": "How do I Threshold-Voltage-Based Filler Cell Insertion", "input": "", "output": "If your technology requires specific filler cells on the left or right sides of certain standard cells, use the create_left_right_filler_cells command to insert these filler cells.\n After you run the create_left_right_filler_cells command, perform standard filler cell insertion to fill the remaining gaps in the design.\n You must use the -lib_cells option to specify the rules for filler cell insertion.\n Each rule uses the following format: standard_cells } { left_filler_cells } { right_filler_cells  For the standard_cells argument,  You must specify a list of one or more standard cells  You can use wildcards to specify the cell names  If you specify multiple rules for the same standard cell, the command uses only the last rule specified for that cell.\n  For the left_filler_cells and right_filler_cells arguments,  At least one of these arguments must contain one or more filler library cells If only one of the arguments contains filler library cells, the tool inserts filler cells only on that side of the standard cells.\n  The specified filler cells must meet the following requirements:  They must have a design_type attribute of lib_cell or filler  They must use the same site definition as the standard cells.\n  The height of the standard cells must be an integer multiple of the filler cell heights.\n  The command tries to insert the filler cells in the order that you specify; for the best results, specify them from the largest to the smallest.\n        You can use wildcards for the filler cell names.\n When you use wildcards, the order of the cells is the same as that returned by the get_lib_cells command.\n The following example inserts filler cells on the left and right sides of the SC1 standard cells and only on the left side of the SC2 standard cells.\n The left and right filler cells are specified in decreasing size order, so that the command inserts the largest possible filler cell.\n fc_shell> create_left_right_filler_cells \\ -lib_cells { { {mylib/SC1} {mylib/LF4X mylib/LF2X} {mylib/RF2X mylib/RF1X} } { {mylib/SC2} {mylib/LF2X mylib/LF1X} {} } } When the create_left_right_filler_cells command inserts the filler cells,  It checks for legal locations for the filler cells, but does not check advanced rules such as minimum threshold voltage, oxide-diffusion (OD), or trimpoly (TPO) rules.\n  It inserts the filler cells immediately next to the standard cells; it does not consider the intercell spacing rules.\n See Also  Standard Filler Cell Insertion Controlling Cell-Based Filler Cell Insertion You can control the following aspects of the filler cell insertion with the create_left_right_filler_cells command:  The region in which to insert the filler cells By default, the command inserts filler cells next to the specified standard cells in the entire block.\n  Use the -boundaries option to restrict filler cell insertion to the specified regions.\n You can specify one or more rectangular or rectilinear regions.\n  Use the -voltage_areas option to restrict filler cell insertion to the specified voltage areas.\n  The orientation of the filler cells By default, the command uses the allowable orientations of the row to place the filler cells.\n To use the same orientation as the standard cell, use the -follow_stdcell_orientation option.\n When you use this option, the tool swaps the right and left filler cells when the cell is flipped.\n        The handling of one-unit-tile gaps By default, the command does not consider the one-unit-tile rule.\n To prevent the tool from inserting filler cells that would leave a one-unit-tile gap, use the -rules no_1x option.\n When you enable this rule, do not include filler library cells with a width of one- unit tile in the -lib_cells option; otherwise, the tool ignores this rule.\n  The naming convention used to identify the inserted filler cell instances By default, the command uses the following naming convention for inserted filler cells: xofiller!\n filler_library_cell_name number To identify the filler cells inserted in a specific run, use the -prefix option to specify a prefix string.\n When you use this option, the command uses the following naming convention: xofiller!\n prefix filler_library_cell_name number"}
{"instruction": "How do I 10", "input": "", "output": "This topic describes how to use Custom Router to perform routing on a double data rate (DDR) design.\n  illustrates the basic steps to create interconnects (routes) between a DDR net and the core.\n        A Basic DDR Flow Prepare input data Run custom routing Insert buffer cells Run ECO routing Define single-loop lengthening Trimming Define net bundles Define and apply routing rules route_custom -nets Set the length match route_custom -nets add_buffer_on_route route_eco -nets route_detail -incremental remove_redundant_shapes create_bundle create_routing_rule set_routing_rule Run custom routing create_wire_matching To perform routing on a DDR design with Custom Router, 1.\n Before you run Custom Router to perform routing on a DDR design,  Group the nets as a bundle for length matching.\n fc_shell> create_bundle -name GRP1 [get_nets $nets]  Create and apply the routing rules for the DDR nets.\n fc_shell> create_routing_rule DDR -widths { layer value \u2026} \\ -spacings { layer value \u2026 } fc_shell> set_routing_rule -rule DDR -min_routing_layer M4 \\ -max_routing_layer M5 $all_ddr_nets       Set two or more layers for initial routing and length matching.\n For postroute length matching, you should include two additional layers.\n For example, if you use the M4 and M5 layers for initial routing, set the M3, M4, M5, and M6 layers for postroute length matching.\n  Add routing blockages as needed.\n Add routing blockages to prevent Custom Router from routing in the area that is covered by placement blockages.\n This helps avoid possible unplaceable buffers during buffer insertion.\n 2.\n Create routes for the DDR nets by running the route_custom command.\n fc_shell> route_custom -nets $all_ddr_nets 3.\n Define the routing constraints.\n  Use the following application options to set the single-loop constraints.\n fc_shell> set_app_options \\ -name custom.route.single_loop_match -value true fc_shell> set_app_options \\ -name custom.route.single_loop_match_min_spacing -value 0.75 fc_shell> set_app_options \\ -name custom.route.single_loop_match_max_spacing -value 10.0 fc_shell> set_app_options \\ -name custom.route.single_loop_match_offset_layer -value true  Set the wire matching constraint for the group.\n fc_shell> create_wire_matching -for [get_bundles GRP1] \\ -match_type length -tolerance 20 -force match1 fc_shell> create_wire_matching -for [get_bundles GRP2] \\ -match_type length -tolerance 20 -force match2  Set the bounding box for the wire matching routing constraint.\n fc_shell> set_app_options -name custom.route.match_box \\ -value {{1000 1000} {1500 1500}} By default, the bounding box coordinates are set to {{0 0} {0 0}}.\n When set to other values, the router limits matching routes to the area inside the box, which might result in connections that do not meet the matching constraint.\n The matching box should cover the entire available channel space.\n 4.\n Perform custom routing with length matching.\n fc_shell> route_custom -nets [get_bundles {GRP1}]       If you define multiple matching boxes at step 3, you need to run the route_custom command on each matching box.\n 5.\n Report the list of mismatching nets.\n grep \"Mismatch\" log_file 6.\n Reduce the number of the mismatched nets (if any) by using one of the following methods:  Rebalance the net routing of one or more groups in the same channel among the available routing layers by a.\n Resetting the routing layers.\n For example, - Set the routing layers for route group A to the M3 and M4 layers, and for group B to the M5 and M6 layers, or - Set the routing layers for route groups A and B to the M3 and M4 layers, and route all mismatched nets on the M5 and M6 layers.\n b.\n Rerunning initial routing with the rebalanced routing layer settings.\n  Set a larger value for the custom.route single_loop_match_max_spacing application option.\n  Allow different layers for the single loop.\n fc_shell> set_app_options \\ -name custom.route.single_loop_match_offset_layer -value true 7.\n Insert buffer cells on the routed DDR net.\n In the following example, ddr_BUF is used as the prefix for the names of the added ECO nets and buffers for easy identification.\n The buffer cells are added at an interval that is 20 percent of the total net length.\n fc_shell> set ddr_buf \"BUF001\" fc_shell> add_buffer_on_route -net_prefix ddr_BUF \\ -cell_prefix ddr_BUF \\ -repeater_distance_length_ratio 0.2 \\ -respect_blockages [get_nets $ddr_nets] $ddr_BUF 8.\n Perform legalization and verify that the placement is legal.\n fc_shell> legalize_placement -cells $ddr_BUF fc_shell> check_legality -cells $ddr_BUF       Note: Using the -cells option with the legalize_placement and check_legality commands is supported only in the standard legalizer.\n 9.\n Preserve the route shapes of the clock nets by setting the shape_use attribute of the clock nets to user_route fc_shell> set_attribute [get_shapes -of_object $all_nets] \\ shape_use \"user_route\" fc_shell> set_attribute [get_vias -of_object $all_nets] \\ shape_use \"user_route\" 10.\n Limit rerouting to minor changes by setting the physical_status attribute on the nets to minor_change fc_shell> set_attribute [get_shapes -of_object $all_nets] \\ physical_status \"minor_change\" fc_shell> set_attribute [get_vias -of_object $all_nets] \\ physical_status \"minor_change\" 11.\n Run ECO routing to reconnect the nets by using the route_eco command.\n fc_shell> route_eco -nets $all_nets -reroute modified_nets_only 12.\n Run incremental detail routing to fix the DRC violations by running the route_detail -incremental true command.\n After routing is complete, clean up the routed nets by running the remove_redundant_shapes command.\n fc_shell> set_attribute \\ [get_shapes -of_object $all_nets] shape_use \"detail_route\" fc_shell> set_attribute \\ [get_vias -of_object $all_nets] shape_use \"detail_route\" fc_shell> route_detail -incremental true fc_shell> remove_redundant_shapes -nets $all_nets 13.\n Check the routing results by using one of the following methods:  Check for DRC violations.\n fc_shell> check_routes By default, the tool checks for DRC violations between signal shapes.\n To check for DRC violations between signal shapes and user shapes, use the following command: fc_shell> check_routes -check_from_user_shapes true  View the ECO nets and buffers in the GUI.\n  Report the lengths of the DDR nets and the ECO nets."}
{"instruction": "How do I Cell Input Mode", "input": "", "output": "Auto grouping is based on the create_repeater_groups command.\n Use manual grouping in case the auto grouping command result does not meet the requirement.\n Check your target cell connection before placement.\n Perform the following steps for manual grouping: 1.\n Group your cells by -cells, cell order intends the cell connection.\n 2.\n Define group virtual connection by -driver_group_id or -path_drivers.\n 3.\n Specify the cutline by -cutline.\n       1.\n To define register path connections (virtual connection) for each set of repeater groups, use the set_repeater_group command.\n  To check cell connection before placement, use the set_repeater_group -check_connection command.\n If the cell connection is not placed as expected, repeat a reset repeater group for desired cell connection.\n It also adds new options to record pin pair for the -check_connection option.\n The pin_pair option is used to find cell real connection based on specified lib cell pin name.\n 2.\n Verify the path with the get_attribute report_repeater_group, and get_repeater_path_info commands.\n The get_attribute command reports the attributes of the group_repeater_driver or group_repeater_loads commands.\n The report_repeater_groups command reports group information for specified groups or all groups, and reports repeater paths for all groups if -verbose is on.\n The command get_repeater_paths_info returns cells on repeater paths that include input cells.\n 3.\n Check and report invalid paths based on real connection with the set_repeater_group -check connection command.\n 4.\n Place interconnect repeaters on route with cutline for input repeater groups."}
{"instruction": "How do I Creating Relative Placement Groups", "input": "", "output": "The relative placement flow consists of the following steps: 1.\n Prepare the design as described in Preparing the Design 2.\n Specify placement constraints as described in Setting Up Multivoltage Designs 3.\n Define the relative placement constraints and settings.\n a.\n Create the relative placement groups by using the create_rp_group command, as described in Creating Relative Placement Groups b.\n Add relative placement objects to the groups by using the add_to_rp_group command.\n See Adding Objects to a Group c.\n Specify options for the relative placement groups by using the set_rp_group_options command, as described in Specifying Options for Relative Placement Groups 4.\n Perform placement and optimization.\n 5.\n Analyze the relative placement results as described in Analyzing Relative Placement Groups If the relative placement is not what you want, modify the relative placement group or the constraints and settings, and rerun placement and optimization."}
{"instruction": "How do I Aligning by Pin Location", "input": "", "output": "To align a group by aligning the right edges, use the -alignment right option of the set_rp_group_options command.\n Note: For hierarchical relative placement groups, the bottom-right alignment does not propagate through the hierarchy.\n  shows cells that are right aligned.\n        Bottom-Right-Aligned Relative Placement Group U4 U3 U2 U1 row 3 row 2 row 1 row 0 col 0"}
{"instruction": "How do I Creating an Autofix Configuration File", "input": "", "output": "Zroute can use the signoff DRC results generated by the signoff_check_drc command to automatically fix the detected design rule violations.\n To perform automatic fixing of the signoff DRC violations, 1.\n Set up the IC Validator environment as described in Setting Up the IC Validator Environment 2.\n Set up the physical signoff options as described in Setting Options for Signoff Design Rule Checking 3.\n (Optional) Generate a configuration file that aids in the fixing process.\n The configuration file defines the layer-to-DRC-rule mapping.\n The signoff_fix_drc command processes only those rules that are specified in the configuration file.\n By default, the signoff_fix_drc command automatically generates the configuration.\n You can manually generate a configuration file, as described in Creating an Autofix Configuration File 4.\n Set the application options for signoff DRC fixing.\n For information about the options for signoff DRC fixing, see Setting Options for Signoff Design Rule Checking 5.\n Save the block to disk.\n When you run the signoff_fix_drc command, the IC Validator tool uses the on- disk information for the block, not the information in memory.\n To ensure accurate information, use the save_block command to save the current state of the block before running the signoff_fix_drc command.\n 6.\n Run signoff DRC fixing by using the signoff_fix_drc command as described in Running the signoff_fix_drc Command If your block uses double-patterning technology, first perform signoff DRC fixing for all other routing design rules, and then perform signoff DRC fixing for only the double- patterning rules, as described in Automatically Fixing Double-Patterning Odd-Cycle Violations"}
{"instruction": "How do I Viewing the Violations in an ICV Heat Map", "input": "", "output": "The signoff_check_drc command writes its output files to the run directory, which is specified by the signoff.check_drc.run_dir application option (or the signoff_check_drc_run directory if you do not use this option).\n The following files are written to the run directory:  The error data file By default, the error data generated by the signoff_check_drc command is saved in a file named signoff_check_drc.err, which is stored in the design library.\n To specify the name for the error data file, use the -error_data option.\n The error data shows child-       level errors on only one of the cell instances, which makes it easier to identify lower- level errors.\n You can use the error data file to report or display the DRC violations in the error browser.\n For information about using the error browser, see Using the Error Browser.\n  The IC Validator results files You can use the following IC Validator results files for debugging the signoff DRC results:  block.LAYOUT_ERRORS This file contains details about the errors detected during the signoff_check_drc run.\n  block.RESULTS This file contains a summary of the signoff_check_drc run results.\n  signoff_check_drc.log This file contains a summary of the signoff_check_drc run environment.\n  icv_config_out This file contains the paths to the top-level block and the cell libraries.\n  icv_sdrc.conclude This file contains an error summary report.\n  layer.map This file contains the layer mapping file generated by the signoff_check_drc command.\n  signoff_check_drc.rc This file contains the IC Validator runset environment variables.\n ./run_details directory This directory contains all the data generated by the IC Validator tool for the signoff DRC run.\n For more information about these files, see the IC Validator documentation."}
{"instruction": "How do I Creating Abstracts for Minimum Pulse Width Analysis", "input": "", "output": "For designs with multiple levels of physical hierarchy, before you create an abstract for a block with an abstract instantiated in it,  Bind the lower-level blocks that you want represented by abstracts to the specific abstracts For example, assume you have a design with multiple levels of physical hierarchy as shown in the following figure.\n  A Design With Multiple Levels of Physical Hierarchy BOT MID TOP       To create an abstract for the block named BOT, use the following commands: fc_shell> open_block BOT fc_shell> create_abstract To create an abstract for the block named MID, use the following commands: fc_shell> open_block MID fc_shell> change_abstract -view abstract -references BOT fc_shell> create_abstract By default, the create_abstract command preserves all levels of the physical hierarchy.\n To flatten the physical hierarchy and retain only the relevant logic of the lower-level blocks, use the -preserve_block_instances false option, as shown in the following example: fc_shell> open_block MID fc_shell> create_abstract -preserve_block_instances false Use flattened abstracts in top-level implementation flows where the lower-level blocks are treated as read-only.\n Using flattened abstracts reduces the netlist size at the top level.\n It also simplifies data management because you do not need to include the lower-level blocks in the reference library list of the top level."}
{"instruction": "How do I Controlling the Optimization of Cells, Nets, Pins, and Ports", "input": "", "output": "After you specify all your placement and legalization constraints and settings, you can check if specific library cells can be legally placed in the block by using the analyze_lib_cell_placement -lib_cells command.\n  To limit the analysis to a specific region of the core area, use the -region option.\n By default, the command searches the entire core area.\n  To limit the analysis to a specific number of sites, use the -trials option.\n By default, the command analyzes 1000 random sites to see if the specified library cells can be placed.\n If you specify a value of 0, the command analyzes all free sites, which can increase runtime  To ignore physical design constraints or advanced design rules during placement analysis, use the -no_pdc or -no_adv option.\n  To report only the relative placement groups that do not meet a specific threshold, use the -threshold option.\n The tool reports a cell only if the percentage of sites the cell       can be placed, relative to the total number of sites analyzed, is less than the specified threshold.\n  To limit the report to a maximum number of cells, use the -max_cells option.\n The default is 100, starting with the library cell with the worst pass rate.\n If your block contains library cells that have a very low pass rate, they will cause large displacements and increased runtime during legalization.\n In such cases, review the power plan, cell layout, and the legalization settings to improve the pass rate.\n The following example analyzes all the library cells that have instances in the current block: fc_shell> analyze_lib_cell_placement -lib_cells [add_to_collection \\ -unique \"\" [get_attribute [get_cells -physical_context] ref_phys_block]]  Analyzing 215 lib cell(s).\n Warning: Routing direction of metal layer PO is neither \"horizontal\" nor \"vertical\".\n PDC checks will not be performed on this layer.\n (PDC-003)  PDC app_options settings ========= place.legalize.enable_prerouted_net_check: 1 place.legalize.num_tracks_for_access_check: 1 place.legalize.use_eol_spacing_for_access_check: 0 place.legalize.allow_touch_track_for_access_check: 1 place.legalize.reduce_conservatism_in_eol_check: 0 place.legalize.preroute_shape_merge_distance: 0.0  Layer M1: cached 0 shapes out of 869 total shapes.\n Layer M2: cached 773 shapes out of 773 total shapes.\n Cached 41757 vias out of 99466 total vias.\n 0.0%...2.0%...4.0%...6.0%...8.0%...10.0%........\n  Lib Cell Pass Rate  saed32_lvt_lsup:LSUPX1_LVT.frame 0.4450 saed32_lvt_lsup:LSUPX8_LVT.frame 0.4780 saed32_lvt_lsup:LSUPX2_LVT.frame 0.4810 saed32_lvt_lsup:LSUPX4_LVT.frame 0.5040..............\n..............\n..............\n saed32_rvt_std:HADDX1_RVT.frame 0.8600 saed32_lvt_std:OA21X1_LVT.frame 0.8610"}
{"instruction": "How do I Creating Logical Power and Ground Connections", "input": "", "output": "To learn about preparing the power network for physical implementation, see  Creating Logical Power and Ground Connections  Creating Floating Logical Supply Nets"}
{"instruction": "How do I Removing Boundary Cell Insertion Requirements", "input": "", "output": "To report the boundary cell insertion requirements specified by the set_boundary_cell_rules command, use the report_boundary_cell_rules command.\n This command reports only the user-specified settings; it does not report any default settings."}
{"instruction": "How do I Specifying the Boundary Cell Insertion Requirements", "input": "", "output": "Before placing the standard cells, you can add boundary cells to the block.\n Boundary cells consist of end-cap cells, which are added to the ends of the cell rows and around the boundaries of objects such as the core area, hard macros, blockages, and voltage areas, and corner cells, which fill the empty space between horizontal and vertical end-cap cells.\n End-cap cells are typically nonlogic cells such as a decoupling capacitor for the power rail.\n Because the tool accepts any standard cell as an end-cap cell, ensure that you specify suitable end-cap cells.\n To insert boundary cells, 1.\n Specify the boundary cell insertion requirements by using the set_boundary_cell_rules command, as described in Specifying the Boundary Cell Insertion Requirements 2.\n Insert the boundary cells based on the specified rules by using the compile_boundary_cells or compile_targeted_boundary_cells command, as described in Inserting Boundary Cells 3.\n Verify the boundary cell placement by using the check_boundary_cells or check_targeted_boundary_cells command, as described in Verifying the Boundary Cell Placement This process supports a single site definition per run.\n The compile_boundary_cells and compile_targeted_boundary_cells commands determine the site definition based on the library cells specified with the set_boundary_cell_rules command, and insert boundary cells only in the regions that use this site definition.\n If your design has multiple site definitions, you must run this process one time for each site definition."}
{"instruction": "How do I Setting Up Reference Libraries", "input": "", "output": "You can create, open, query, save, or close a design library, using an absolute path, a relative path, or no path, by using the following commands:  create_lib This command creates the library in memory and sets it as the current library.\n When you run this command to create a new design library, you must specify the library name.\n Slash (/) and colon (:) characters are not allowed in library names.\n The following command creates the my_libA library using a relative path: fc_shell> create_lib../my_lib_dir/my_libA {my_libA}  open_lib This command opens the specified library, makes that library the current library, and opens all its associated reference libraries.\n Opening a library means loading it into memory and making its blocks accessible.\n The following example opens the my_libA library saved on disk: fc_shell> open_lib my_libA Information: Loading library file '/usr/lib/StdCells.ndm' Information: Loading library file '/usr/lib/RAMs.ndm' Information: Loading library file '/usr/lib/PhysicalOnly.ndm' {my_libA}  current_lib By default, the library most recently opened is the current library.\n You can explicitly set any open library to be the current library by using the current_lib command.\n For example, fc_shell> current_lib my_libA {my_libA}  save_lib When you create or change a library, the changes are stored in memory only.\n To save a library to disk, use this command.\n For example, fc_shell> save_lib lib_A Saving library 'lib_A'   close_lib       When you no longer need access to data in a library, you can close it by using the close_lib command.\n Be sure to save the changes in the library before you close it.\n For example, fc_shell> close_lib Closing library 'lib_A'  In addition, you can use the current_lib get_libs, and report_lib commands to query design libraries.\n For more information, see the Design Libraries topic in the Fusion Compiler Data Model User Guide"}
{"instruction": "How do I Analyzing Clock Timing", "input": "", "output": "You can create a collection of the pins on a clock network by using the get_clock_tree_pins command.\n By default, this command returns all the pins on all the clock networks of all scenarios.\n You can limit the selection by using one of the following methods  Limit it to specific clock by using the -clocks option.\n  Limit it to specific scenario, scenarios of modes, or scenarios of corners by using the -scenarios -modes, or -corners option.\n You can further limit the selection by  Considering only the clock paths that go from, through, and to specific pins by using the -from -through, and -to options.\n  Filtering the pins based on their attributes by using the -filter option.\n For more information about all the available options and all the supported pin attributes, see the man page for the get_clock_tree_pins command.\n The following example creates a collection named clk1_icg_pins, which consists of the pins of integrated-clock-gating cells that are on the clock network of the clock named clk1: fc_shell> set clk1_icg_pin \\ [get_clock_tree_pins -filter is_on_ICG -clocks clk1]"}
{"instruction": "How do I Getting Information About Commands", "input": "", "output": "If you enter the wrong options for a command or enter the wrong command, you can interrupt command processing and remain in fc_shell.\n interupting comandscomandsinteruptingterminatingterminating comands To interrupt or terminate a command, press Ctrl+C.\n Some commands and processes cannot be interrupted.\n To stop these commands or processes, you must terminate fc_shell at the system level.\n When you terminate a process or the shell, no data is saved.\n When you use Ctrl+C, keep the following points in mind:  If a script file is being processed and you interrupt one of its commands, the script processing is interrupted and no further script commands are processed.\n  If you press Ctrl+C three times before a command responds to your interrupt, fc_shell is interrupted and exits with this message: Information: Process terminated by interrupt."}
{"instruction": "How do I Defining the Shielding Rules", "input": "", "output": "Zroute shields routed nets net shieldingshielding nets by generating shielding wires that are based on the shielding widths and spacing defined in the shielding rules.\n In addition to shielding nets on the same layer, you also have the option to shield one layer above and one layer below.\n Shielding above or below the layer is called coaxial shielding shows an example of coaxial shielding.\n Coaxial shielding provides even better signal isolation than same-layer shielding, but it uses more routing resources.\n  Coaxial Shielding You can perform shielding either before or after signal routing.\n Shielding before signal routing, which is referred to as preroute shielding, provides better shielding coverage but can result in congestion issues during signal routing.\n Preroute shielding is typically used to shield critical clock nets.\n Shielding after signal routing, which is referred to as postroute shielding, has a very minimal impact on routability, but provides less protection to the shielded nets.\n        shows the Zroute shielding flow, which is described in the topics that follow.\n  Zroute Shielding Flow"}
{"instruction": "How do I Enabling Route-Driven Estimation (RDE) for Preroute", "input": "", "output": "To improve correlation with the postroute stage of the design flow, you can enable global-route-layer-based RC estimation during the compile_fusion and clock_opt commands.\n With this feature, the tool uses global routes for all nets and identifies the layers with the most appropriate per-unit resistance values.\n The nets are then constrained to minimum and maximum layers for preroute RC estimation.\n To enable this feature, use the opt.common.use_route_aware_estimation application option.\n If you set it to  auto, the feature is enabled only when there is a variation in the per-unit resistance value of the different routing layers  true, the feature is enabled irrespective of the variation in the per-unit resistance value of the different routing layers If you enable this feature, use the remove_route_aware_estimation command to remove all global route based estimation before you perform routing."}
{"instruction": "How do I Verifying the Boundary Cell Placement", "input": "", "output": "The Fusion Compiler tool provides several methods for inserting boundary cells, depending on where you want to insert the boundary cells.\n  To insert boundary cells around the core area, hard macros, and placement blockages, use the compile_boundary_cells command.\n  To insert boundary cells only around one or more voltage areas, use the compile_boundary_cells command with the -voltage_area option.\n  To insert boundary cells only around specific objects, use the compile_targeted_boundary_cells command with the -target_objects option.\n You can use this command to insert boundary cells around specific macros, voltage areas, voltage area shapes, placement blockages, routing blockages, and the core area.\n The set_boundary_cell_rules command configures boundary cell insertion for both the compile_boundary_cells and compile_targeted_boundary_cells commands.\n However, some rules are honored only by the compile_boundary_cells command.\n For details, see Specifying the Boundary Cell Insertion Requirements"}
{"instruction": "How do I Specifying the Routing Resources", "input": "", "output": "To manually enable  Dynamic power shaping for the compile_fusion command, use the following settings: fc_shell> set_app_option -name opt.common.power_integrity \\ -value false fc_shell> set_app_option -name compile.flow.enable_dps \\ -value true  Voltage-drop-aware placement for the compile_fusion command, use the following settings: fc_shell> set_app_option -name opt.common.power_integrity \\ -value false fc_shell> set_app_option -name compile.flow.enable_irap \\ -value true  Voltage-drop-aware placement for the clock_opt command, use the following settings: fc_shell> set_app_option -name opt.common.power_integrity \\ -value false fc_shell> set_app_option -name clock_opt.flow.enable_irap \\ -value true"}
{"instruction": "How do I Overriding the Alignment When Adding Objects", "input": "", "output": "To align a group by pin location, use the -alignment pin and -pin_name options with the set_rp_group_options command.\n The tool looks for the specified alignment pin in each cell in the column.\n If the alignment pin exists in a cell, the cell is aligned by using the pin location.\n If the specified alignment pin does not exist in a cell, the cell is aligned by the left edge, and the tool issues an information message.\n If the specified alignment pin does not exist in any cell in the column, the Fusion Compiler tool issues a warning message.\n The script in Example 46 creates a relative placement group rp1, adds cells to it, and specifies that the cells are aligned by pin A.\n Example 46 Definition for Relative Placement Group Aligned by Pins create_rp_group -name rp1 -columns 1 -rows 4 set_rp_group_options -alignment pin -pin_name A add_to_rp_group rp1 -cells U1 -column 0 -row 0 add_to_rp_group rp1 -cells U2 -column 0 -row 1 add_to_rp_group rp1 -cells U3 -column 0 -row 2 add_to_rp_group rp1 -cells U4 -column 0 -row 3 When aligning by pins, the tool tries different orientations for the cells and selects the orientation for each cell that gives the minimum column width.\n For example, changing the orientation of cell U2, as shown in, reduces the width of column 0.\n However, if you specify an orientation when adding a cell to a relative placement group by using the -cells and -orientation options with the add_to_rp_group command, the tool honors the orientation you specify.\n        Minimizing the Column Width of a Relative Placement Group Aligned by Pins U4 U3 U2 U1 row 3 row 2 row 1 row 0 column 0 Pin A U4 U3 U2 U1 row 3 row 2 row 1 row 0 column 0 When you specify an alignment pin for a group, the pin applies to all cells in the group.\n You can override the group alignment pin for specific cells in the group by specifying the -pin_name option when you use the add_to_rp_group command to add the cells to the group.\n The script in Example 47 defines relative placement group rp2, and specified pin A as the group alignment pin.\n However, instances I5 and I6 use pin B as their alignment pin, rather than the group alignment pin.\n The resulting structure is shown in Example 47 Definition for Aligning a Group and Leaf Cells by Pins create_rp_group -name rp2 -columns 1 -rows 6 set_rp_group_options rp2 -alignment pin -pin_name A add_to_rp_group rp2 -cells I3 -column 0 -row 0 add_to_rp_group rp2 -cells I4 -column 0 -row 1 add_to_rp_group rp2 -cells I5 -column 0 -row 2 -pin_name B add_to_rp_group rp2 -cells I6 -column 0 -row 3 -pin_name B add_to_rp_group rp2 -cells I7 -column 0 -row 4 add_to_rp_group rp2 -cells I8 -column 0 -row 5        Relative Placement Group Aligned by Different Pins I5 I4 I3 row 3 row 2 row 1 row 0 col 0 I6 I7 I8 row 4 row 5 Pin A Pin B"}
{"instruction": "How do I Checking PG Electromigration Violations", "input": "", "output": "A PG electromigration map is a visual display of the color-coded electromigration values overlaid on the physical supply nets.\n For static analysis, the map displays average electromigration values.\n For dynamic analysis, the map displays average electromigration values, peak electromigration values, or root mean square electromigration values.\n  shows an example of the electromigration map in which problem areas are highlighted in different colors.\n Use the options to investigate the problem areas by       selecting layers or nets to display.\n For example, to examine the current value of one specific net, deselect all the nets and then select the net to display from the list.\n To analyze only the shapes on a layer, select the layer from the list.\n For a detailed procedure about how to display an electromigration map, see Displaying Maps in the GUI  Displaying a PG Electromigration Map"}
{"instruction": "How do I Defining Cell Spacing Constraints for Legalization", "input": "", "output": "You can specify fixed locations for unmapped leaf cells, so the tool does not move these cells during compile.\n To do so, specify the location coordinates for leaf cells by using the set_cell_location command.\n To prevent the tool from moving a cell during compile, specify a fixed location for the cell by using the -fixed option with the set_cell_location command to set the physical_status attribute to fixed on the cell.\n To change the location of a cell marked with fixed placement status, specify the -ignore_fixed option with the command.\n The specified coordinates indicate the lower-left corner of the cell boundary.\n After you run the set_cell_location command, the unmapped cell gets the specified location in memory, but the location is not reflected in the layout view before compile.\n To view the placement of the cell, run the compile_fusion command.\n This example sets the lower-left corner of the out1_reg cell to (20 10) and sets the fixed physical placement status to prevent the tool from moving the cell during compile.\n       fc_shell> set_cell_location -coordinates { 20 10 } out1_reg -fixed"}
{"instruction": "How do I Performing Postroute Shielding", "input": "", "output": "Zroute considers shielding rules as soft rules during signal routing.\n If a net has a shielding rule and is not shielded before signal routing, by default, Zroute reserves shielding space during the whole routing process: global routing, track assignment, and detail routing.\n At the end of detail routing, it reports the shielding space violations and the locations where shielding wires cannot be established.\n The following log file example shows shielding soft spacing violations, which are highlighted in text: DRC-SUMMARY: @@@@@@@ TOTAL VIOLATIONS = 13 Diff net var rule spacing : 2 Same net spacing : 2 Less than minimum area : 4 Short : 1 Soft spacing (shielding) : 2 Signal routing only reserves space for the shielding; it does not actually insert it.\n You must run create_shields after signal routing to physically place the shielding wires.\n The reported soft rule violations help you to understand the shielding rate.\n Note that the router reserves space only for same-layer shielding and not for coaxial shielding; therefore, postroute coaxial shielding can produce a very low shielding rate.\n       If you want to use power and ground nets for shielding, and do not want Zroute to reserve space for the shielding when power and grounds nets are available, set the route.common.allow_pg_as_shield application option to true before running signal routing."}
{"instruction": "How do I Clock-Gate Levels and Stages", "input": "", "output": "The compile log file contains information about clock gating, which can help you debug simple setup issues.\n By default, the clock-gating information is displayed after  Clock-gate insertion n the initial_map stage  Clock-gate restructuring in the logic_opto stage  Clock-gate restructuring in the initial_opto stage  Self-gating insertion in the initial_opto stage, if enabled The reported information includes the following:  The number of preexisting clock gates that have a dont_touch attribute, which prevents optimization and might lead to unsatistactory results  The number of clock-gating library cells that have a CTS purpose, which is required for insertion  Clock-gating metrics such as the number of elements and the number of gated registers  A summary of reasons for registers that could not be gated If the self-gating or fanin-based sequential clock gating features are enabled, their metrics are displayed separately.\n An example of the compile log for a run that includes self-gating is as follows: Number of Pre-Exsting Clock Gates with dont_touch Attribute: 0 Number of ICG library cells with CTS purpose: 12  Tool Gated Register Summary  Clock Gating Type | Number of Clock Gates | Register Count | Bitwidth | | | Equivalent  Regular Clock Gating | 1 | 2 | 2 Self Gating | 0 | 0 | 0   Regular Clock Gating Ungated Register Summary  Ungated Reason | Count | Bitwidth  Enable is Constant One | 2 | 2         Self Gating Ungated Register Summary  Ungated Reason | Count | Bitwidth  Register is clock gated, and ineraction | | is set to none | 2 | 2 The tool as not able to find enough registers | | that are compatible to this register | | in order to be gated.\n | 2 | 2"}
{"instruction": "How do I Viewing Multibit Components in the GUI", "input": "", "output": "During the initial_mapping stage, the tool can prioritize the use of sequential library cells that have functionally equivalent multibit library cells.\n After you run the compile_fusion command, if the report_multibit -ignored_cells command indicates that many cells were not banked due to a lack of multibit cells in the library, you can enable this feature by setting the compile.seqmap.prefer_registers_with_multibit_equivalent application option to true and rerunning the compile_fusion command.\n However, enabling this feature might degrade the design QoR."}
{"instruction": "How do I Performing Design Rule Checking Using Zroute", "input": "", "output": "To display the global route congestion map, choose View > Map > Global Route Congestion in the GUI.\n If the design library contains global route congestion information for the block, the tool generates the congestion map based on this information; otherwise, you must click Reload to generate the congestion map.\n When you click Reload, the tool opens a dialog box that contains the following command: route_global -congestion_map_only true Note: By default, only the hard congestion data is saved in the design library.\n To also save the soft congestion data, set the route.global.export_soft_congestion_maps application option to true before performing global routing.\n When you click OK in this dialog box, the tool generates a new congestion map.\n If you want to use different options for the route_global command, you can modify this command before clicking OK.\n        shows an example of a congestion map.\n  Global Route Congestion Map By default, the congestion map displays only the hard congestion data.\n To display soft congestion data, select the desired rule level in the Rule level drop-down list.\n The rule level refers to the spacing weight level of the soft nondefault spacing rules.\n The congestion map displays the sum of the hard and soft congestion with a weight level of at least the selected rule level.\n The congestion map shows the borders between global routing cells highlighted with different colors that represent the congestion values.\n The congestion map supports two methods for calculating the congestion value:  Sum of overflow for each layer (the default) In this mode, the tool calculates the congestion value as the sum of the overflow for all selected layers.\n Underflow is not considered; if a layer has underflow, it contributes zero overflow to the total overflow calculation.\n  Total demand minus total supply In this mode, the tool calculates the congestion value by subtracting the supply for all selected layers from the demand for all selected layers.\n Note that because this calculation considers the underflow, it produces a more optimistic congestion result in regions that contain both overflow and underflow.\n For example, assume that the METAL2 layer is heavily congested with a demand of 13 routing tracks and a supply of only 7 tracks for an overflow of 6.\n The METAL4 is       moderately congested with an overflow of 4, and the METAL6 layer is not congested and contains an underflow of 3.\n Other layers are not used in the congestion calculation in this example.\n  The sum of overflow calculation results in a congestion value of 6+4=10.\n The underflow of 3 for the METAL4 layer is not used in the calculation.\n  The total demand calculation results in a congestion value of 6+4-3=7.\n In this case, the underflow of 3 for the METAL4 layer is used in the calculation.\n To select the mode, select either \"Sum of overflow for each layer\" or \"Total demand minus total supply\" in the \"Congestion calculation\" section of the Map Mode panel.\n By default, all metal layers are selected in the congestion map, except those specified as ignored layers with the set_ignored_layers command.\n To display the congestion map for a subset of layers, select (or deselect) the layers on the Map Mode panel.\n For example, if the global routing report shows that the maximum overflow occurs on the METAL2 layer, you can deselect all layers, except for METAL2, to display only the METAL2 congestion.\n The Map Mode panel also displays a histogram showing the number of global routing cells in different ranges (bins) of congestion values for the selected layers.\n If your block contains global routing cells that have no available routing resources, an additional bin named Blocked is displayed that shows the number of global routing cells with no routing resources.\n You can select which bins to display in the congestion map by selecting or deselecting them on the Map Mode panel.\n If the block shows congested areas, zoom into the congested area to see the congestion value on the global routing cell.\n For example, in, the red highlight on the edge of the global routing cell shows 18/9.\n This means there are 9 wire tracks available, but 18 tracks are needed.\n        Global Route Overflow on Global Routing Cell"}
{"instruction": "How do I Running Rail Analysis with Multiple Rail Scenarios", "input": "", "output": "To fix the missing vias by inserting PG vias based on the specified DRC error objects written by the analyze_rail command, use the fix_pg_missing_vias command.\n When the tool inserts a PG via for a missing via error object, it sets the status of the error object to fixed in the error browser (see ).\n This allows you to quickly check if all the missing vias are fixed.\n        Error Status After Inserting PG Vias for Missing Vias The tool marks the error object as fixed The following example inserts PG vias for all error objects of the VDD nets reported by the analyze_rail -check_missing_via command: fc_shell> set errdm [open_drc_error_data rail_miss_via.VDD.err] fc_shell> set errs [get_drc_errors -error_data $errdm] fc_shell> fix_pg_missing_vias -error_data $errdm $errs"}
{"instruction": "How do I Missing Via and Unconnected Pin Checking", "input": "", "output": "To remove taps that were previously created using the create_taps command, use the remove_taps command.\n To remove specific taps, use the -filter option with the get_taps command.\n The following example removes all the supply pin taps that are of the type top_pg: fc_shell> remove_taps [get_taps -filter type==top_pg] To remove all the taps in the block, run the following command: fc_shell> remove_taps"}
{"instruction": "How do I Performing Signoff Design Rule Checking", "input": "", "output": "A layer mapping file maps the technology layers to the layers used in the runset file.\n You specify the location of the layer mapping file by setting the signoff.physical.layer_map_file application option.\n Each line in the layer mapping file specifies a layer in the technology file and the corresponding layer in the runset file using the following syntax: tf_layer [: tf_purpose ][: use_type ][: mask_type runset_layer [: runset_data_type Each line can contain the following items:  tf_layer \u2013 Layer number in the technology file, a required item  tf_purpose \u2013 Purpose number in the technology file  use_type \u2013 Fusion Compiler usage of the geometries in the design       Valid values are power ground signal clock boundary hard_placement_blockage soft_placement_blockage routing_blockage area_fill, and track  mask_type \u2013 Multiple-patterning mask constraint of the geometries Valid values for metal layers are mask_one mask_two mask_three, and mask_same Via layers support the additional values MASK_FOUR through MASK_FIFTEEN  runset_layer \u2013 Layer number in the runset file, a required item  runset_data_type \u2013 Layer data type number in the runset file Any text in a line that comes after a semicolon character ( ) is considered a comment and has no effect on layer mapping."}
{"instruction": "How do I Deriving Clock Tree References for Preexisting Gates", "input": "", "output": "The buffer and inverter cells that can be used to build a clock tree and the reference cells of the preexisting gates of the clock tree are referred to as clock tree references To specify the clock tree references, use the set_lib_cell_purpose -include cts command.\n Ensure that the list of cells you specify as clock tree references meets the following criteria:  The list contains at least one buffer or one inverter  The list contains the library cells of the preexisting gates If they are not included in the list, the tool is unable to resize these preexisting gates during clock tree synthesis and optimization.\n You can automatically derive equivalent cells for all the preexisting clock tree cells that are not buffers or inverters and specify them as clock references by using the derive_clock_cell_references command, as described in Deriving Clock Tree References for Preexisting Gates  For multivoltage designs with always-on buffering requirements, the list contains always-on cells  The library cells in the reference list do not have a dont_touch attribute If library cells have the dont_touch attribute set on them, they are not used by clock tree synthesis even if you specify them as clock tree references.\n To ensure that clock tree synthesis uses only the specified set of cells and that these cells are not used by any optimization step other than clock tree synthesis, run the following commands: fc_shell> set cts_cells list_of_cells fc_shell> set_lib_cell_purpose -exclude cts [get_lib_cells] fc_shell> set_lib_cell_purpose -include none [get_lib_cells $cts_cells] fc_shell> set_lib_cell_purpose -include cts [get_lib_cells $cts_cells]"}
{"instruction": "How do I Using Custom Router in the Fusion Compiler", "input": "", "output": "Custom Router is a shape-based router that supports gridded or gridless routing for custom digital, mixed signal, and analog routing needs.\n In the Fusion Compiler environment, you can use Custom Router to create interconnects (or routes) for critical signals between blocks and continue with the Fusion Compiler tool to complete the physical implementation.\n You can also preroute critical signals or clock nets by using a set of custom routing constraints.\n In the Fusion Compiler environment, Custom Router provides the following features:  Prerouting in the batch mode  Tcl-based routing constraint management  Automatic routing with custom routing constraints  Hybrid flow for prerouting  Double Data Rate (DDR) net routing flow Custom Router is fully integrated with the Fusion Compiler tool, and supports advanced design rules for 20 nm and below technologies.\n To learn about using Custom Router in the Fusion Compiler environment, see the following topics:  Using Custom Router in the Fusion Compiler Tool  Before Using Custom Router  Defining Routing Constraints  Managing Constraint Groups  Using Custom Routing Application Options  Routing With the Custom Router  Shielding the Nets  Checking the Routing Results        Using a Hybrid Routing Flow  Using a DDR Routing Flow These topics describe how to use Custom Router to create custom routes in the Fusion Compiler environment.\n For detailed information about Custom Router, see the Custom Compiler Custom Router User Guide"}
{"instruction": "How do I Incremental Global Routing", "input": "", "output": "By default, the route_global command is not crosstalk-driven.\n To enable crosstalk-driven global routing, set the route.global.crosstalk_driven and time.si_enable_analysis application options to true before you run the route_global command.\n When you enable crosstalk-driven global routing, the tool calculates the net delays before invoking the global router.\n If the design library contains global route information, the tool uses the global route information to calculate the net delays; otherwise, it uses virtual routing to calculate the net delays.\n The global routing results can vary depending on whether the initial net delays were calculated by using global route information or virtual routing.\n To remove existing global route information from the signal nets in the block, use the -global_route and -net_types signal options with the remove_routes command, as shown in the following example: fc_shell> remove_routes -global_route -net_types signal"}
{"instruction": "How do I Creating ETM Cell Libraries in the Library Manager Tool", "input": "", "output": "You can create an ETM for design by using the extract_model command in the PrimeTime tool.\n For multicorner-multimode designs, you must create an ETM for each scenario by applying the appropriate corner and mode constraints for each scenario.\n The following PrimeTime script creates an ETM for the S3 scenario, which consists of the m1 mode and c3 corner, of the AMS_BLK design: #Read in the design read_verilog./AMS_BLK.v link  # Apply parasitics read_parasitics./AMS_BLK.spef        #For multivoltage designs, apply UPF data and settings load_upf./AMD_BLK.upf set extract_model_include_upf_data true  # Apply the mode (m1) and corner (c3) constraints for the scenario (S3) source m1_constraints.tcl source c3_constraints.tcl  # Enable clock latencies for designs with synthesized clock trees set extract_model_with_clock_latency_arcs true set extract_model_clock_latency_arcs_include_all_registers false  # Create the ETM extract_model -library_cell -format db -output AMS_BLK_m1_c3 For more information about creating ETMs in the PrimeTime tool, see the Extracted Timing Models chapter in the PrimeTime User Guide"}
{"instruction": "How do I Partitioning and Planning the Full Chip Design", "input": "", "output": "The hierarchical synthesis flow using abstracts consists of the following major steps: 1.\n Partition the design at the top-level and generate the design libraries for the lower-level blocks as described in Partitioning and Planning the Full Chip Design 2.\n Synthesize each lower-level block and generate the block-level information required for top-level synthesis as described in Synthesizing a Subblock 3.\n Synthesize the top-level as described in Synthesizing the Top-Level See Also  Overview of Abstract Views  Creating Abstract Views  Creating a Frame View"}
{"instruction": "How do I Implementing a Regular Multisource Clock Tree Using Integrated", "input": "", "output": "To implement a regular multisource clock tree, use the following steps: 1.\n Specify your clock tree constraints and settings.\n 2.\n Perform the following steps for multivoltage designs: a.\n Enable multivoltage support by setting the cts.multisource.enable_full_mv_support application option to true b.\n Enable the addition of physical feedthrough cells, which belong to a voltage area physically but not logically, by setting the opt.common.allow_physical_feedthrough application option to true c.\n Run the check_mv_design command and fix all multivoltage issues.\n 3.\n Enable cell electromigration fixing by setting the cts.multisource.cell_em_aware application option to true 4.\n Insert the tap drivers by using the create_clock_drivers command, as described in Inserting Clock Drivers 5.\n Create the clock mesh by using the create_clock_straps command, as described in Creating Clock Straps 6.\n Insert the mesh drivers by using the create_clock_drivers command, as described in Inserting Clock Drivers 7.\n Build the global clock tree structure by using the synthesize_multisource_global_clock_trees command, as described in Synthesizing the Global Clock Trees 8.\n Route the connections between the clock mesh and the tap drivers by using the route_clock_straps command, as described in Routing to Clock Straps 9.\n Analyze the clock mesh by using the analyze_subcircuit command, as described in Analyzing the Clock Mesh 10.\n Specify options and settings for tap assignment as described in Specifying Tap Assignment Options and Settings       11.\n Perform tap assignment by using the synthesize_multisource_clock_taps command, which  Merges equivalent clock cells to remove any artificial boundaries between clusters of sinks  Assigns endpoints to the closest tap driver and split cells along the path honoring any sink groups defined  Copies the UPF and SDC constraints and the user-specified attributes onto the newly created cells across the active scenarios 12.\n Synthesize the entire clock tree, from the clock root, by using the following command: fc_shell> clock_opt -from build_clock -to route_clock During this step, the tool builds the local subtrees that are driven by the tap drivers.\n It also fixes DRC violations beyond ignore exceptions of the multisource clock tree, if applicable.\n You can also synthesize other clocks in the design that are not synthesized.\n For more information about the clock_opt command, see Synthesizing, Optimizing, and Routing Clock Trees With the clock_opt Command 13.\n Analyze the clock tree results as described in Analyzing the Clock Tree Results"}
{"instruction": "How do I Example Script of Hierarchical Synthesis Using ETMs", "input": "", "output": "To run hierarchical synthesis flow, follow the steps described in.\n In contrast to the feasibility flow, the hierarchical synthesis flow uses the following block-level information generated post place and route:  Floorplanning information to generate the ETM in step 3  Power intent and test model (CTL) information for top-level synthesis in step 5        Hierarchical Synthesis Flow 1.\n Synthesize the block in the front-end tool compile_fusion RTL Block UPF (back end) Block CTL (back end) UPF SDC, floorplan Design (ndm) 3.\n Create an ETM in the back-end tool extract_model Block_ETM (ndm) 4.\n Synthesize the top-level design in the front-end tool compile_fusion RTL Top-level UPF SDC, floorplan SDC Netlist UPF '' CTL Design (frame) 2.\n Implement the block design in the back-end tool SPEF (back end or StarRC)"}
{"instruction": "How do I Defining a Hard Macro Placement Blockage", "input": "", "output": "To define a hard placement blockage, specify the boundary and optionally a name for the placement blockage.\n For example, to create a hard placement blockage in the area enclosed by a rectangle with corners at (10, 20) and (100, 200), use the following command: create_placement_blockage -boundary {10 20 100 200} Note: Hard placement blockages are honored during placement, legalization, optimization, and clock tree synthesis.\n       Hard placement blockages can also be defined in the DEF as shown in Example 13 Example 13 Placement Blockages in DEF BLOCKAGES 2 ; - PLACEMENT RECT ( 0 327600 ) ( 652740 327660 ) ; - PLACEMENT RECT ( 0 327600 ) ( 652740 327660 ) ; END BLOCKAGES"}
{"instruction": "How do I Introduction to Zroute", "input": "", "output": "This topic describes the routing capabilities of Zroute, which is the router for the Fusion Compiler tool.\n Zroute is architected for multicore hardware and efficiently handles advanced design rules for 45 nm and below technologies and design-for-manufacturing (DFM) tasks.\n It also describes the postroute optimization features supported by the Fusion Compiler tool.\n To learn about routing and postroute optimization, see the following topics:  Introduction to Zroute  Basic Zroute Flow  Prerequisites for Routing  Defining Vias  Inserting Via Ladders  Checking Routability  Routing Constraints  Routing Application Options  Routing Clock Nets  Routing Critical Nets  Routing Secondary Power and Ground Pins  Routing Signal Nets  Shielding Nets  Performing Postroute Optimization  Analyzing and Fixing Signal Electromigration Violations  Performing ECO Routing  Routing Nets in the GUI  Cleaning Up Routed Nets        Analyzing the Routing Results  Saving Route Information  Deriving Mask Colors  Inserting and Removing Cut Metal Shapes"}
{"instruction": "How do I Applying Mega-Switch Command Settings", "input": "", "output": "During the early iterations of a design cycles, the design data might be incomplete or incorrect.\n This can prevent you from implementing the design for exploration purposes and so on.\n The Fusion Compiler Early Data Check Manager allows you to specify how the tool should handle the data checks it performs during the following implementation tasks:  Design planning  Multivoltage design implementation  Optimization  Placement  Legalization  Hierarchical implementation of designs with physical hierarchy The following table shows the Early Data Check Manager commands available for controlling how the tool handles the data checks and for generating related information.\n Table 18 Early Data Check Manager Commands To do this Use this command  set_early_data_check_policy  report_early_data_checks   get_early_data_check_records  write_early_data_check_config   remove_early_data_check_records You should specify the policies for handling the data checks before you begin implementation.\n The following example specifies that the tool should be strict when checking for the SCANDEF information.\n Therefore, the tool does not proceed with the create_placement command because the SCANDEF information is missing.\n       fc_shell> set_early_data_check_policy -policy strict \\ -check place.coarse.missing_scan_def fc_shell> create_placement Information: Policy for early data check 'place.coarse.missing_scan_def' is 'error'.\n (EDC-001) Error: No valid scan def found.\n (PLACE-042) Information: Ending 'create_placement' (FLW-8001) fc_shell> report_early_data_checks -check place.coarse.missing_scan_def Check Policy Strategy Fail Count  place.coarse.missing_scan_def error 1  The following example specifies that the tool should be lenient with regards to all data checks related to coarse placement.\n Therefore, the tool proceeds with the create_placement command without the SCANDEF information.\n fc_shell> set_early_data_check_policy -policy lenient \\ -check place.coarse.* fc_shell> create_placement Information: Policy for early data check 'place.coarse.missing_scan_def' is 'tolerate'.\n (EDC-001) Continuing without valid scan def.\n Start transferring placement data.\n Creating placement from scratch.\n...\n...\n For more information about the Early Data Check Manager, see the Fusion Compiler Data Model User Guide"}
{"instruction": "How do I Reporting the Routing Results", "input": "", "output": "To perform layout-versus-schematic (LVS) checking, which checks for inconsistencies in the physical layout, use the check_lvs command.\n By default, the check_lvs command performs the following checks for all signal, clock, and PG nets:  Shorted nets A shorted net occurs when a net shapes from different nets touch or intersect.\n By default, the command  Checks for shorts between net shapes in the top-level design, including shapes in top-level blockages To disable checking for shapes in top-level blockages, use the -check_top_level_blockages false option.\n  Does not check for shorts between net shapes in the top-level design and net- shapes in child cells To enable this checking, use the -check_child_cells true option.\n To exclude certain types of child cells from checking, use the -exclude_child_cell_types       option to specify one or more of the following cell types: abstract analog black_box corner cover diode end_cap fill filler flip_chip_driver flip_chip_pad lib_cell macro module pad pad_spacer physical_only and well_tap  Does not check for shorts with zero-spacing blockages To enable this checking, use the -check_zero_spacing_blockages true option.\n Note: The check_lvs command supports only default routing blockages that apply to all net types and have a blockage group ID of 0.\n If the blockage applies only to specific net types or has a nonzero blockage group ID, the command ignores the blockage.\n In addition, the command ignores corridor routing blockages (which are created when you use the -reserve_for_top_level_routing option with the create_routing_blockage command) and boundary routing blockages (which are created when you use the -boundary_internal or -boundary_external options with the create_routing_blockage command).\n  Open nets An open net occurs when the pins of a net are not connected by its net shapes.\n By default, the command  Treats power and ground terminals as unconnected voltage sources To treat power and ground terminals as connected, use the -treat_terminal_as_voltage_source true option.\n For example, assume your block contains the layout shown in.\n By default, the check_lvs command reports two opens for this layout.\n If you use the -treat_terminal_as_voltage_source true option, no opens are reported.\n        Layout With Power and Ground Terminals  Reports open nets as the bounding box of the open net and does not report floating pins To report detailed open locations, use the -open_reporting detailed option.\n Note that using this option might increase the runtime.\n To report the floating pins, use the -report_floating_pins true option.\n  Floating net shapes A floating net shape occurs when a net shape is not physically connecting to a pin of its net.\n To perform a subset of these checks, use the -checks option to specify one or more of the following checks: short (shorted nets), open (open nets), and floating_routes (floating net shapes).\n To check only specific nets, use the -nets option to specify the nets of interest.\n By default, the check_lvs command reports a maximum of 20 violations for each type of error.\n To change this limit, use the -max_errors option.\n To report all violations, specify the maximum number of violations as zero ( -max_errors 0 ).\n You can view the violations reported by the check_lvs command in the GUI by using the error browser.\n For information about using the error browser, see the Fusion Compiler Graphical User Interface User Guide To reduce the runtime required by the check_lvs command, enable multithreading by using the set_host_options command, as described in Enabling Multicore Processing"}
{"instruction": "How do I Single-Loop Matching", "input": "", "output": "The following Custom Router shielding application options allow you to control shielding: Application Option Description custom.route.shield_connect_mesh_overlap        custom.route.shield_connect_route      custom.route.shield_connect_to_supply         Application Option Description custom.route.shield_min_open_loop      custom.route.shield_min_shield_seg   custom.route.shield_min_signal_seg   custom.route.shield_net     custom.route.shield_second_net"}
{"instruction": "How do I Performing Test Insertion and Scan Synthesis", "input": "", "output": "To transfer the clock latencies and balance points from the front-end tool to the back-end tool, use the write_ascii_files command.\n The command writes the design.MCMM directory, which contains timing contexts in Tcl files for.\n For example, The design.MCMM/scenario_ scenario.tcl file contains the following clock latency settings: set_clock_latency -clock [get_clocks {CLK}] -0.3 [get_pins {d_reg[4]/CP}] set_clock_latency -clock [get_clocks {CLK}] 0.1 [get_pins {d_reg[7]/CP}] The design.MCMM/mode_ mode.tcl file contains the following balance points: set_clock_balance_points -clock [get_clocks {CLK}] \\ -balance_points [get_pins {d_reg[4]/CP}] \\ -consider_for_balancing true \\ -rise -delay 0.3 -corners [get_corners {default}] set_clock_balance_points -clock [get_clocks {CLK}] \\ -balance_points [get_pins {d_reg[7]/CP}] \\ -consider_for_balancing true -rise -delay -0.1 -corners [get_corners {default}]"}
{"instruction": "How do I Setting Don't Touch Settings", "input": "", "output": "You can restrict the optimization on portions of the clock network by specifying  Don't touch settings These settings identify parts of the clock tree on which clock tree synthesis and optimization is not performed.\n You can use don't touch settings to prevent the modification of a clock network beyond a specific pin, the buffering of specific nets, or the sizing of specific cells.\n For information about defining don't touch settings, see Setting Don't Touch Settings  Size-only settings These settings identify clock tree cells for which the tool can perform only cell sizing.\n Note that these cells can be moved, unless they have a placement status of fixed.\n For information about defining size-only settings, see Setting Size-Only Settings"}
{"instruction": "How do I Querying Relative Placement Groups", "input": "", "output": "After you run placement and optimization, use the report_rp_groups command to identify placement issues and relative placement violations.\n You must either specify which relative placement groups to analyze or specify the -all option to analyze all relative placement groups.\n By default, the command reports the following types of relative placement groups:  Placed groups that do not have constraint violations  Placed groups that have constraint violations that are not critical  Failed groups that have constraint violations that are critical  Groups that have not yet been placed You can modify the default behavior by using the options described in Table 61 Table 61 The report_rp_groups Command Options To do this Use this option   -critical   -non_critical   -unplaced       Table 61 The report_rp_groups Command Options (Continued) To do this Use this option  -verbose You can also run this command before you run placement and optimization and identify the unplaced groups in the design."}
{"instruction": "How do I Analyzing and Fixing Antenna Violations", "input": "", "output": "In general, the antenna properties for standard cells and hard macros are defined in their frame views in the reference libraries.\n You can set default values for the antenna properties, which apply to cells that do not have antenna properties defined in the reference libraries.\n  route.detail.default_diode_protection Specifies the diode protection value used for standard cell output pins during antenna analysis if the diode protection value is not specified in the e view of the cell.\n  route.detail.default_gate_size Specifies the gate size used for standard cell input pins during antenna analysis if the gate size is not specified in the e view of the cell.\n  route.detail.default_port_external_antenna_area Specifies the antenna area used for ports (top-level pins) during antenna analysis if the antenna area is not specified in the e view of the cell.\n  route.detail.default_port_external_gate_size Specifies the gate size used for ports (top-level pins) during antenna analysis if the gate size is not specified in the e view of the cell.\n  route.detail.macro_pin_antenna_mode Specifies how macro cell pins are treated for antenna considerations.\n        route.detail.port_antenna_mode Specifies how the ports (top-level pins) are treated for antenna considerations.\n If you are using a hierarchical flow and create a block abstraction for a block, you must use the derive_hier_antenna_property command to extract the antenna information from the block to use at the next level of hierarchy.\n Note: If the hierarchical antenna properties are not defined for all layers for a macro, Zroute treats the data as incomplete, skips antenna analysis, and issues a ZRT-311 warning message.\n If you get this error message, see SolvNet article 027178, \"Debugging the ZRT-311 Message.\" See Also  Annotating Antenna Properties on Hard Macro Cells"}
{"instruction": "How do I Specifying the Global Layer Constraints", "input": "", "output": "You can specify the minimum and maximum routing layers both for the block (global layer constraints), for specific nets (net-specific layer constraints), and for unsynthesized clock nets (clock-tree layer constraints).\n If you specify both global layer constraints and net- specific layer constraints, the net-specific constraints override the global constraints.\n In addition to constraining the routing layers, you can also specify a preferred routing direction for each layer.\n The routing layer constraints are used by RC estimation, congestion analysis, and routing.\n Because these constraints affect RC estimation and congestion analysis as well as routing, you should set these constraints before performing placement on the block.\n The following topics describe how to specify routing layer constraints, preferred routing direction, and via ladders :  Specifying the Global Layer Constraints  Specifying Net-Specific Layer Constraints  Specifying Clock-Tree Layer Constraints  Enabling Level-Based Clock Nondefault Routing Rule  Setting the Preferred Routing Direction for Layers"}
{"instruction": "How do I Accessing Libraries", "input": "", "output": "Transparent hierarchy optimization (THO) enables the tool to transparently optimize hierarchical designs.\n THO enables you to optimize the top and subblocks concurrently while honoring the physical block boundaries, and at the same time using all advanced optimization techniques such as CCD, AWP, PrimeTime delay calculation, and so on.\n During THO, the tool optimizes the top-level and specific subblocks (design views) in the context of top-level timing.\n THO does not change the subblock shape and physical hierarchy.\n By default, the tool honors the pin locations, but provides an option to reassign pin locations to improve QoR.\n You can continue your signoff flow with hierarchical blocks because you do not have to flatten the design.\n Signoff extraction and PrimeTime STA can be run hierarchically.\n In addition, any late ECOs can be easier to implement as well.\n THO is available during compile_fusion in the final_place and final_opto stages, as well as during route_opt and hyper_route_opt        Transparent Hierarchy Optimization Flow THO Flow open_block -ref_libs_for_edit (Blocks linked with design view) Enable THO settings compile_fusion -from final_place route_opt OR hyper_route_opt T H O The following example flow shows the usage of THO in the overall implementation flow.\n In this example, the THO operations are run as an additional incremental optimization step on top of the typical hierarchical optimization flow.\n       The THO flow includes the following steps:  Accessing Libraries  Setting up Transparent Hierarchy Optimization  Performing THO During compile_fusion final_place Stage  Performing THO During compile_fusion final_opto Stage  Rebudgeting After compile_fusion  Performing THO During route_opt or hyper_route_opt"}
{"instruction": "How do I Synthesizing, Optimizing, and Routing Clock Trees With the", "input": "", "output": "To perform standalone clock tree synthesis, use the synthesize_clock_trees command, as shown in the following example: fc_shell> synthesize_clock_trees When you use this command, the tool performs the following tasks: 1.\n Clock tree synthesis Before clock tree synthesis, the tool performs virtual routing of the clock nets and uses RC estimation to determine the clock net timing.\n 2.\n Clock tree optimization Before clock tree optimization, the tool performs global routing on the clock nets and uses RC extraction to determine the clock net timing.\n During clock tree optimization, the tool performs incremental global routing on clock nets modified during optimization.\n Note: This command does not detail route the clock trees.\n By default, this command works on all clocks in all active scenarios that are enabled for setup or hold analysis.\n You can specify the clocks to be built by using the -clock option.\n For example, to build only the CLK clock tree, use the following command: fc_shell> synthesize_clock_trees -clocks [get_clocks CLK] After clock tree synthesis, the tool sets the clocks in all modes of all active scenarios as propagated."}
{"instruction": "How do I Defining Area-Based Placement Blockages", "input": "", "output": "You can define an inner keepout margin on a hierarchical cell, but not on a hard macro or a leaf cell.\n When you define an inner keepout margin, you must specify the keepout distance explicitly.\n To explicitly specify an inner keepout margin, use the -inner option to specify the margin distance for each side.\n You specify the left, bottom, right, and top margins using the following format: lx by rx ty.\n A value of 0 results in no keepout margin for that side.\n For example, to create a hard inner keepout margin with a margin of 10 on each side for a hierarchical cell named my_hcell, use the following command: fc_shell> create_keepout_margin -inner {10 10 10 10} my_hcell"}
{"instruction": "How do I Inserting Redundant Vias on Clock Nets", "input": "", "output": "Redundant via insertion is an important design-for-manufacturing (DFM) feature that is supported by Zroute throughout the routing flow.\n In each routing stage, Zroute concurrently optimizes via count as well as wire length.\n The redundant via result is measured by the redundant via conversion rate, which is defined as the percentage of single vias converted into redundant vias.\n You should also pay attention to the number of unoptimized single vias.\n If a block has fewer unoptimized single vias, it is usually better for DFM.\n The following topics describe how to insert redundant vias:  Inserting Redundant Vias on Clock Nets  Inserting Redundant Vias on Signal Nets"}
{"instruction": "How do I Shielding the Nets", "input": "", "output": "Before performing custom routing on your block, you need to define routing constraints by using the commands described in Defining Routing Constraints, and setting the appropriate application options.\n To perform custom routing with Custom Router in the Fusion Compiler environment, run the route_custom command.\n By default, the tool routes all nets in the block.\n To route specific nets, use the -nets option.\n By default, the tool removes the configuration data when the command run is complete.\n To keep the data in the cache for running other Custom Router commands in the current session, set the -keep_session option to true.\n The default is false The following example specifies the bus routing style and then performs bus routing.\n fc_shell> create_bundle -name Bus1 {net0 net1} fc_shell> create_bus_routing_style -for {Bus1} \\ -valid_layers {M5 M6} \\       -layer_widths {M5 0.4 M6 0.44} -force bus1 fc_shell> create_net_shielding -for {Bus1} -shield_net VSS \\ -layer_gaps 0.21 -layer_widths 0.2 -sharing true -force sh1 fc_shell> set_app_options -name custom.route.bus_corner_type \\ -value river fc_shell> set_app_options \\ -name custom.route.bus_intra_shield_placement -value interleave fc_shell> route_custom -nets {net0 net1} The following example creates a constraint group and then performs routing on the nets with the same length.\n fc_shell> set matchNets {net1 net2 net3 net4} fc_shell> create_bundle -name MatchL1 $matchNets fc_shell> create_wire_matching -for MatchL1s \\ -match_type length -tolerance 1  -force matchL1_con fc_shell> route_custom -nets {net1 net2 net3 net4} The following example creates a differential group and then performs custom routing.\n fc_shell> create_differential_group -for {net1 net2} \\ -valid_layers {M3 M4} -layer_widths {M3 0.4 M4 0.4} \\ -layer_spacings {M3 0.4 M4 0.4} \\ -twist_style diagonal -twist_interval 80.0 -force Diff fc_shell> set_app_options\\ -name custom.route.diffpair_twist_jumper_offset -value 10 fc_shell> route_custom -nets {net1 net2}"}
{"instruction": "How do I User-Driven Enable Exclusion", "input": "", "output": "By default, the tool extracts the clock-gating enable condition from one of these sources:  Signals connected directly to the synchronous enable pin of the sequential element, as shown in  Directly Connected Enable Signal  The enable condition on the feedback loops of the input and output data pins of the sequential element, as shown in  Enable Signal in a Feedback Loop When using clock gating, you can specify the signal source of the enable logic.\n The set_clock_gating_objects -enable_source option applies to all objects specified with the -force or -include options in the same command.\n If you use the -enable_source option, you cannot use the -clear -reset, or -exclude options.\n       The -enable_source option can be one of the following:  none  enable_pin_only  feedback_loop_only  both (default)  prefer_enable_pin  prefer_feedback_loop For example, the following command inserts an always-enabled clock-gating cell: set_clock_gating_objects -force [get_cells u_dfx/input_b_reg*] \\ -enable_source none The following command uses the enable signal from the synchronous enable pin, if there is one.\n Otherwise, the tool uses the enable on the feedback loop of the register.\n set_clock_gating_objects -include [get_cells u_dfx/input_c_reg*] \\ -enable_source prefer_enable_pin"}
{"instruction": "How do I Viewing PG Electromigration Analysis Results", "input": "", "output": "Note: PG electromigration analysis is supported in both RedHawk Fusion and RedHawk-SC Fusion analysis flows.\n To analyze the current density on PG nets and identify the segments with potential electromigration issues, run the analyze_rail -electromigration command.\n The analysis results are saved in the working directory.\n To perform electromigration analysis, 1.\n Set the rail.tech_file application option to specify the Apache technology file (*.tech) that defines the layer-by-layer current density limits.\n For example, fc_shell> set_app_options -name rail.tech_file \\ -value design_data/tech/test.tech       2.\n Run the analyze_rail command with the following options:  Use the -voltage_drop and -electromigration options to enable electromigration analysis.\n  Use the -nets option to specify the power and ground supply nets to analyze.\n The tool considers all the switched or internal power nets of the specified power nets in the analysis.\n You do not need to explicitly specify the internal power nets.\n  (Optional) By default, the analyze_rail command automatically generates a GSR file for running RedHawk rail analysis.\n To specify an external GSR file to append to the GSR file generated in the current run, use the -extra_gsr_option_file option.\n  (Optional) To specify an existing RedHawk script file that was generated in an earlier analyze_rail run, use the -redhawk_script_file option.\n When using an existing script file for rail analysis, the tool honors the settings in this script file and ignores all other currently enabled options.\n For example, if you run the analyze_rail -voltage_drop static -redhawk_script_file myscript.tcl command, the tool ignores the -voltage_drop option.\n When the -redhawk_script_file option is not specified, you must specify the nets to analyze by using the -nets option.\n 3.\n Check the analysis results, as described in Viewing PG Electromigration Analysis Results"}
{"instruction": "How do I Checking Relative Placement Groups Before Placement", "input": "", "output": "The following sections explain methods for analyzing your relative placement groups:  Checking Relative Placement Groups Before Placement  Analyzing the Placeability of a Relative Placement Group  Reporting Relative Placement Constraint Violations  Querying Relative Placement Groups  Analyzing Relative Placement in the GUI"}
{"instruction": "How do I Controlling Routing Around the Block Boundary", "input": "", "output": "If your technology file defines metal cut allowed and forbidden preferred grid extension rules and your block already contains boundary cells, you can create routing guides for these rules by using the -add_metal_cut_allowed option with the derive_metal_cut_routing_guides command.\n When you use this option, the derive_metal_cut_routing_guides command creates the following routing guides:  Metal cut allowed routing guides, which cover the area taken up by all the placeable site rows reduced by the vertical shrink factor, which is 50 percent of the smallest site row height  Forbidden preferred grid extension routing guides, which cover the remaining area up to the block boundary To check the inserted routing guides, use the -check_only option with the derive_metal_cut_routing_guides command.\n By default, the checking results are stored in an error data file named block_name.err.\n To specify a different name for the error data file, use the -error_view option."}
{"instruction": "How do I Validating the Taps and Finding Invalid Taps", "input": "", "output": "Taps are used to model the external voltage source environment in which the device under analysis operates; they are not part of the design itself, but can be thought of as virtual models of voltage sources.\n The locations of the ideal voltage sources and the ideal power supplies in the design are required to achieve accurate rail analysis results.\n To create a tap, use the create_taps command.\n You can create multiple taps by running multiple create_taps commands.\n To specify a name for the created tap, use the -name option.\n If the specified name is already present in the design, the tool issues a warning message, and replaces the original tap with the new one.\n When you create multiple taps by a single invocation of the create_taps command, all of the created taps are named by using the tap_name _NNN naming convention, where NNN is a unique integer identifier and tap_name is the string argument to the -name option.\n       You can create taps in any of the following ways:  Treating top-level PG pins as taps Use the -top_pg option when the block does not have physical pad or bump information available.\n Use the -supply_net option to explicitly associate a tap to the supply (either power or ground) net connected to the object from which the tap was defined.\n To implicitly associate a tap with the supply net, run the create_taps command without the -supply_net option.\n The following example creates taps for all the top-level supply pins.\n fc_shell> create_taps -supply_net VDD -top_pg  Creating taps at absolute coordinates Use the -layer and -point options to create taps at the specified coordinates.\n If there is no supply net, supply pin, or via shape at the specified location, which means there is no conductive path to the supply network, the tap has no effect on rail analysis despite being present.\n If you define a tap in a location where a tap already exists, the tool issues a warning message and replaces the original tap with the new one.\n You must specify the supply net with which this tap is associated by using the -supply_net option.\n Note: When you use the -point option with the -of_objects option, the location is relative to the origin of the specified object.\n Otherwise, it is relative to the origin of the current top-level block.\n The following example creates a tap using absolute coordinates.\n The command checks whether the coordinate location touches any layout shape.\n fc_shell> create_taps -supply_net VDD \\ -layer 3 -point {100.0 200.0} Warning: Tap point (100.0, 200.0) on layer 3 for net VDD does not touch any polygon (RAIL-305)  Using existing instances as taps To create taps on specified objects, use the create_taps -of_objects command.\n Each item in the object list can be a cell or a library cell, or a collection of cells and library cells.\n By default, the tool creates the taps on the power and ground pins of the objects.\n If you specify the -point option, the tool creates the taps at the specified physical location relative to the origin of each object.\n       Use existing instances for creating taps when analyzing a top-level design where pad cells are instantiated physically.\n Use the -supply_net option to explicitly associate a tap to the supply (either power or ground) net connected to the object from which the tap was defined.\n To implicitly associate a tap with the supply net, run the create_taps command without the supply_net option.\n The following example creates taps for each instance of the some_pad_cell library cell.\n The tool creates one tap per instance on the metal1 layer at a location of {30.4 16.3} relative to the origin of the cell instance.\n fc_shell> create_taps -of_objects [get_lib_cell */some_pad_cell]\\ -layer metal1 -point {30.4 16.3} The following example creates taps on all power and ground pins of all cell instances that match the *power_pad_cell* pattern.\n fc_shell> create_taps -of_objects [get_cells -hier \\ *power_pad_cell*]  Importing a tap file If you are working with a block that does not yet have pins defined, or if the location or design of the pad cells is not finalized, you can define tap locations, layer numbers, and supply nets in an ASCII file and then import it into the tool by using the create_taps -import command.\n The supported tap file format is as follows: net_name layer_number [X-coord Y-coord] Lines starting with semicolon (;) or pound (#) symbols are treated as comments.\n The x- coordinate and y-coordinate values are specified in microns.\n For example, fc_shell> exec cat tap_file VDD 3 500.000 500.000 VSS 5 500.000 500.000 fc_shell> create_taps -import tap_file  Creating taps with packages A simple package RLC (resistance-inductance-capacitance) model provides a fixed set of electrical characteristics assigned to all power and ground pads.\n To use simple package RLC models for rail analysis, you need to specify the pad location file which defines the simple RLC model by using rail.pad_files application option.\n       For a detailed description about simple package RLC models, see the related section in the RedHawk User Manual Note: Before creating taps with simple package RLC models, you must enable the RedHawk signoff license key using the rail.allow_redhawk_license_checkout application option."}
{"instruction": "How do I Running Interclock Delay Balancing", "input": "", "output": "The Fusion Compiler tool can automatically generate the interclock delay balancing constraints based on the timing relationships between the clocks.\n To automatically generate the interclock delay balancing constraints, use the following command: fc_shell> derive_clock_balance_constraints This command identifies clocks that have interclock timing paths and places them in the same balance group.\n After running this command, use the report_clock_balance_groups command to report the generated interclock delay balancing constraints, as described in Reporting Clock Balance Groups.\n If necessary, you can modify the clock balance groups as described in Defining the Interclock Delay Balancing Constraints By default, the tool considers all timing paths when identifying the timing relationships between the clocks.\n To consider only those timing paths with slack less than a specified value, use the -slack_less_than option with the derive_clock_balance_constraints command.\n For example, to generate interclock balancing constraints only for paths with slack less than -0.2 ns, use the following command: fc_shell> derive_clock_balance_constraints -slack_less_than -0.2 Assume you run this command on a block for which clock A has a timing relationship only with clock B and the worst negative slack (WNS) of this group of timing paths is -0.1 ns and clock C has a timing relationship only to clock D and the WNS of this group of timing paths is -0.3 ns.\n The command considers only those timing paths with slack less than -0.2 ns, so it defines a single balance group that contains clocks C and D.\n Clocks A and B are not constrained because the timing paths between them have slack greater than -0.2 ns."}
{"instruction": "How do I Performing Placement and Legalization of Relative Placement", "input": "", "output": "You can generate a relative placement group for clock sinks and their drivers in a placed design by using the create_clock_rp_groups command.\n When you run placement and optimization, the tool places the clocks sinks and their drivers based on their relative placement constraints it generated.\n This can improve routability and reduce dynamic power.\n You can control the sinks being considered for relative placement groups as follows:  Specify the minimum and maximum of sinks that should be driven by a single driver to be considered by using the -min_sinks and -max_sinks options.\n The default minimum is 2 sinks and the default maximum is 128 sinks.\n  Exclude timing critical sinks by using the -timing_driven option.\n By default, the tool excludes all sinks with a negative slack.\n  Specify the cells to consider by using the -cells option.\n B default, the tool considers all the sinks.\n       You can control the size and shape of the relative placement group by using one of the following methods:  Specify the maximum allowed Manhattan distance between the sinks in one relative placement group by using the -distance option.\n If the Manhattan distance between sinks is more than the specified distance, they are put into separate relative placement groups.\n The default distance is 100 microns.\n  Specify a maximum number of rows by using the -max_rp_rows option.\n The default is 32.\n  Allow the tool to decide the number of rows and columns based on the distribution of the cells by using the -auto_shape option.\n Before you run the create_clock_rp_groups command, the block must be placed.\n After you create the clock relative placement groups, reset the placement of the block by using the reset_placement command, and rerun placement and optimization by using the place_opt command."}
{"instruction": "How do I Excluding Specific Sinks", "input": "", "output": "To exclude a specific scenarios from concurrent clock and data optimization, use the ccd.ignore_scenarios application option and specify the name of the scenarios you want to ignore.\n For example, to ignore the scenario named scnB, use the following command: fc_shell> set_app_options -name ccd.ignore_scenarios \\ -value {scnB}} This setting affects concurrent clock and data optimization performed during the place_opt clock_opt, and route_opt commands."}
{"instruction": "How do I Assigning Nondefault Routing Rules to Nets", "input": "", "output": "To define a nondefault routing rule, use the create_routing_rule command.\n When you define a nondefault routing rule, you must specify a name for the nondefault routing rule.\n You use the name to assign the nondefault routing rule to nets or clocks.\n The following topics describe how to create nondefault routing rules for various purposes:  Defining Minimum Wire Width Rules  Defining Minimum Wire Spacing Rules  Defining Minimum Via Spacing Rules  Specifying Nondefault Vias  Specifying Mask Constraints  Defining Shielding Rules  Reporting Nondefault Routing Rule Definitions  Removing Nondefault Routing Rules  Modifying Nondefault Routing Rules You can create a single routing rule that serves multiple purposes.\n In addition, you can assign multiple nondefault routing rules to a net.\n Defining Minimum Wire Width Rules You can define nondefault minimum wire width rules that are stricter than the minimum width rules defined in the technology file.\n Nondefault minimum width rules are hard constraints, which must be met during routing.\n Note: If you specify a nondefault width that violates the signalRouteMaxWidth setting in the technology file, the tool ignores the nondefault width.\n       The minimum width defined in a nondefault routing rule applies to all metal segments, including via enclosures.\n To avoid DRC violations, ensure that the enclosures for nondefault vias meet the minimum width rule.\n To define a minimum wire width rule, use the create_routing_rule command.\n You can specify the minimum width by specifying a multiplier that is applied to the default width for each layer, by specifying the minimum width in microns for each layer, or both.\n  To use a multiplier to specify the minimum width, use the following syntax: create_routing_rule rule_name [-default_reference_rule | -reference_rule_name ref_rule -multiplier_width multiplier The multiplier value must be between 0.001 and 2000.\n The default width for each layer is determined from the reference rule, which is either the default routing rule or the reference rule specified in the -reference_rule_name option.\n For example, to define a nondefault routing rule named new_width_rule that uses the default routing rule as the reference rule and defines the nondefault width as two times the default width, use the following command: fc_shell> create_routing_rule new_width_rule -multiplier_width 2.0  To specify the minimum width values for each layer, use the following syntax: create_routing_rule rule_name [-default_reference_rule | -reference_rule_name ref_rule -widths { layer width layer width layer width Specify the routing layers by using the layer names from the technology file.\n You can specify a single width value per layer.\n Zroute uses the wire width from the reference rule, which is either the default routing rule or the reference rule specified in the -reference_rule_name option, for any layers not specified in the -widths option.\n For example, to define a nondefault routing rule named new_width_rule2 that uses the default routing rule as the reference rule and defines nondefault width rules for the M1 and M4 layers, use the following command: fc_shell> create_routing_rule new_width_rule2 \\ -widths {M1 0.8 M4 0.9}  If you specify both with -multiplier_width option and the -widths option, the tool uses the -widths option to determine the base width, and then applies the multiplier to that value to determine the minimum width requirement.\n For example, to define a nondefault routing rule named new_width_rule3 that uses the default routing rule as the reference rule and defines the nondefault width as 0.8 for the       M1 layer, 0.9 for the M4 layer, and two times the default width for all other layers, use the following command: fc_shell> create_routing_rule new_width_rule3 \\ -multiplier_width 2.0 -widths {M1 0.4 M4 0.45} Defining Minimum Wire Spacing Rules You can define nondefault minimum wire spacing rules that are stricter than the rules defined in the technology file.\n Nondefault wire spacing rules can be defined as hard constraints, which must be met, or as soft constraints, which Zroute tries to meet.\n Note: The spacing rules defined in the technology file are always considered hard constraints.\n By default, Zroute checks the nondefault spacing rules between signal nets and other signal nets, PG nets, and blockages, but not between shapes of the same signal net or between signal nets and shield wires for PG nets.\n For information about modifying these checks, see Configuring Nondefault Spacing Checks To define a minimum wire spacing rule, use the create_routing_rule command.\n You can specify the minimum spacing by specifying a multiplier that is applied to the default spacing for each layer, by specifying the minimum spacings in microns for each layer, or both.\n  To use a multiplier to specify the minimum spacing, use the following syntax: create_routing_rule rule_name [-default_reference_rule | -reference_rule_name ref_rule -multiplier_spacing multiplier The multiplier value must be between 0.001 and 2000.\n The default wire spacing for each layer is determined from the reference rule, which is either the default routing rule or the reference rule specified in the -reference_rule_name option.\n For example, to define a nondefault routing rule named new_spacing_rule that uses the default routing rule as the reference rule and defines the nondefault spacing as two times the default spacing, use the following command: fc_shell> create_routing_rule new_spacing_rule \\ -multiplier_spacing 2.0  To specify the minimum spacing values for each layer, use the following syntax: create_routing_rule rule_name -spacings { layer spacing 11 spacing 12...\n spacing 1n layer spacing 21 spacing 22...\n spacing 2n...\n layer spacing n1 spacing n2...spacing nn } } -spacing_weight_levels { layer weight 11 weight 12...\n weight 1n       layer weight 21 weight 22...\n weight 2n...\n layer weight n1 weight n2...\n weight nn } } Specify the routing layers by using the layer names from the technology file.\n You can define multiple spacing values per layer.\n Zroute uses the spacing values from the reference rule, which is either the default routing rule or the reference rule specified in the -reference_rule_name option, for any layers not specified in the -spacings option.\n If you specify more than one spacing value per layer, you must assign a weight to each spacing value by using the -spacing_weight_levels option.\n The valid weight values are low medium high, and hard.\n When you assign a weight level other than hard, the spacing rule is a soft spacing rule.\n By default, Zroute does not fix soft routing rule violations.\n To fix soft routing rules, you must map the weight levels to routing effort levels, as described in Specifying the Routing Effort for Soft Spacing Violations For example, to define a nondefault routing rule named new_spacing_rule2 that uses the default routing rule as the reference rule and defines nondefault spacing rules for the M1 and M4 layers, use the following command: fc_shell> create_routing_rule new_spacing_rule2 \\ -spacings { M1 {0.12 0.24} M4 {0.14 0.28} } \\ -spacing_weight_levels { M1 {hard medium} M4 {hard medium} }  If you specify both with -multiplier_spacing option and the -spacings option, the tool uses the -spacings option to determine the base spacing, and then applies the multiplier to that value to determine the minimum wire spacing requirement.\n To limit spacing to one side of the net, use the -single_side_spacing option with the create_routing_rule command.\n For example, the following command creates single-side spacing rules on the M2, M3, and M4 metal layers METAL2, METAL3, and METAL4: fc_shell> create_routing_rule new_spacing_rule3 \\ -spacings {M2 1.300 M3 1.400 M4 1.500 } \\ -single_side_spacing \\ -widths {M1 0.230 M2 0.280 M3 0.280 M4 0.280 M5 0.280 M6 0.440}       Configuring Nondefault Spacing Checks You can configure the checking of nondefault spacing rules by enabling or disabling checks between signal nets and other objects.\n In addition, you can ignore violations of nondefault spacing rules for short parallel distances.\n  To honor nondefault spacing rules between shapes of the same signal net, set the route.detail.var_spacing_to_same_net application option to true  To honor nondefault spacing rules between signal nets and shield wires for PG nets  For all signal nets, set the route.common.ignore_var_spacing_to_shield application option to false"}
{"instruction": "How do I Defining Routing Corridors", "input": "", "output": "To route one or more nets within a specific region, 1.\n Define a routing corridor by using the create_routing_corridor command, as described in Defining Routing Corridors 2.\n Assign the nets and supernets to the routing corridor by using the add_to_routing_corridor command, as described in Assigning Nets to a Routing Corridor 3.\n Verify that the routing corridors are valid by using the check_routing_corridors command, as described in Verifying Routing Corridors If necessary, modify the routing corridors, as described in Modifying Routing Corridors 4.\n Route the nets by using the route_group command, as described in Routing Critical Nets 5.\n Remove the routing corridors, as described in Removing Routing Corridors See Also  Reporting Routing Corridors"}
{"instruction": "How do I Using Variables", "input": "", "output": "The Fusion Compiler tool uses application options to control the tool behavior.\n Application options use the following naming convention: category [.\n subcategory ].\n option_name where category is the name of the engine affected by the application option.\n Some application option categories have subcategories to further refine the area affected by the application option.\n Application options have either a global scope or a block scope.\n  Block-scoped application options apply only to the block on which they are set.\n They are saved in the design library and are persistent across tool sessions.\n  Global-scoped application options apply to all blocks, but only within the current session.\n They are not saved in the design library; you must specify them in each fc_shell session.\n You might want to consider adding the global settings to your.synopsys_fc.setup file.\n To get a list of available application options, use the get_app_options command.\n By default, this command lists all application options.\n To restrict the reported application options, provide a pattern string as an argument to the command.\n For example, to list all available application options, use the following command: fc_shell> get_app_options To list all available timer application options, use the following command: fc_shell> get_app_options timer.* To generate a report of application options, use the report_app_options command.\n For detailed information about application options, see Application Options in the Fusion Compiler Data Model User Guide See Also  Using Setup Files"}
{"instruction": "How do I Reading Via Definitions from a LEF File", "input": "", "output": "The router supports the following types of via definitions:  Simple vias and simple via arrays A simple via is a single-cut via.\n It is specified by a cut layer and the height and width of the rectangular shapes on its cut and metal layers.\n A simple via array is a multiple-cut simple via.\n Simple vias and simple via arrays can be used for the following purposes:  Clock or signal routing using nondefault routing rules  Redundant via insertion  Power and ground routing with advanced via rules  Custom vias A custom via is a multiple-cut, odd-shaped via that is created from an arbitrary collection of Manhattan polygons.\n Custom vias can be used only for redundant via insertion.\n Via definitions can come from the following sources:  The ContactCode sections of the technology file associated with the design library The technology file can contain definitions for simple vias, simple via arrays, and custom vias.\n For information about defining vias in the technology file, see the Synopsys Technology File and Routing Rules Reference Manual  Via rule GENERATE statements in a LEF file The via rule GENERATE statements define simple via arrays.\n For information about defining vias in a LEF file, see Reading Via Definitions from a LEF File  User-defined via definitions You can define simple vias, simple via arrays, and custom vias.\n For information about creating user-defined via definitions, see Creating a Via Definition If you want Zroute to use a via definition for signal routing, ensure that is has the following attribute settings:  is_default attribute is true  is_excluded_for_signal_routing attribute is false In addition, Zroute uses nondefault vias that are explicitly specified in a fat via table in the technology file or in a nondefault routing rule defined by the create_routing_rule command."}
{"instruction": "How do I Setting Up for Voltage-Drop-Aware Placement", "input": "", "output": "To set up for dynamic power shaping, perform the following steps: 1.\n Ensure UPF settings are specified for multiple-power nets.\n 2.\n Ensure both setup and hold scenarios are created and are active.\n 3.\n (Optional) Control the power analysis and optimization performed during dynamic power shaping as follows:  Specify the scenarios to focus on by setting the ccd.dps.focus_power_scenario application option.\n By default, the tool focuses on all active scenarios.\n When the design has many active scenarios, specifying one or two scenarios, as shown in this example, can reduce the runtime.\n fc_shell> set_app_options -name \\ -name ccd.dps.focus_power_scenario -value {{S1 S2}}  Control the type of internal power analysis performed by setting the ccd.dps.use_case_type application option.\n By default, this application option is set to autovector.\n With this setting, the tool runs internal power analysis with a 20 percent probability of toggling a flip-flop and all flip-flops are effectively clocked every cycle.\n However, the clock gating is not modeled.\n If you set this application option to activity, the tool chooses probability of toggling a flip-flop and the clock-gating model based on the activity.\n  Define the overall dynamic power optimization goal by setting the ccd.dps.optimize_power_target_modes application option.\n  To reduce the global peak current, where the optimization goal is to reduce the total peak current, set this application option to global_peak  To reduce the local peak current in windows stepped across the design, where the optimization goal is to reduce voltage drop violations, set this application option to stepped_psgs, which is the default.\n  Specify the tradeoff between power and setup timing during dynamic power shaping by setting the ccd.dps.optimize_setup_tradeoff_level application option to low medium (default), or high  Specify the tradeoff between power and hold timing during dynamic power shaping by setting the ccd.dps.optimize_hold_tradeoff_level application option to low medium (default), or high       You can further control the power analysis and optimization performed during dynamic power shaping by using application options such as ccd.dps.use_cases ccd.dps.auto_targets, and ccd.dps.auto_target_constraint_parameters which are more complex to use.\n For more information on these application options, see the corresponding man pages."}
{"instruction": "How do I Performing Postroute Clock Tree Optimization", "input": "", "output": "If you run the synthesize_clock_trees or clock_opt command, after clock tree synthesis, the tool removes the ideal setting from all the synthesized clocks of all active scenarios and sets all the corresponding register clock pins as propagated.\n If your block has scenarios that were not set as active before clock tree synthesis, set these scenarios as active by using the set_scenario_status -active true command and mark all clocks as propagated by using the synthesize_clock_trees -propagate_only command, as shown in the following example: fc_shell> set_scenario_status -active true [all_scenarios] fc_shell> synthesize_clock_trees -propagate_only The synthesize_clock_trees -propagate_only command removes the ideal setting from the clocks; it does not perform clock tree synthesis."}
{"instruction": "How do I Controlling the Selection of Self-Gating Objects", "input": "", "output": "To specify self-gating settings, use the set_self_gating_options command with the following options:  -min_bitwidth bitwidth Controls the minimum size for self-gating banks.\n The default is 4.\n  -max_bitwidth bitwidth Controls the maximum size for self-gating banks.\n The default is 8.\n  -interaction_with_clock_gating interaction_type Registers gated by user-instantiated clock-gating cells are candidates for self-gating.\n Valid arguments are none (skip registers gated by tool-inserted clock gates), insert (the default; insert self-gates on gated registers), and collapse (collapse tool-inserted clock gates if they are in the same hierarchy).\n  -objects object_list Specifies that the options should be applied to the listed objects.\n Supported objects are hierarchical instances, power domains, and designs.\n By default, the settings apply to the entire design.\n  -tree_type combinational_logic_type Uses the specified logic type to build the gated enable tree.\n Valid arguments are xor nand or, and auto (the default).\n       To report self-gating options, use the report_self_gating_options command."}
{"instruction": "How do I Inserting Isolation Cells", "input": "", "output": "Level-shifter cells function as the interface between power domains that operate at different voltage levels.\n These cells ensure that the output transition of a driver can cause the receiving cell to switch even though the receiver is operating at a different voltage level.\n To insert level shifters in the current block, use the create_mv_cells command.\n The tool inserts the level shifters using the cell mapping and strategy defined in the UPF specification.\n When the tool inserts a level shifter, it sets a atributesize_onlysize_only atribute size_only attribute on the level shifter and a atributesdont_touchdont_touch atribute dont_touch attribute on the port-to-level-shifter net.\n By default, the command inserts cells only from the generic library.\n If you use the -mapped option with the create_mv_cells command, the command inserts cells only from the user-provided logic library and issues an error if a suitable cell is not available.\n If the tool does not insert a level-shifter cell, you can use the analyze_mv_design -level_shifter command to obtain more information.\n If you specify a net or pin with the -through option, the report lists the power domains and related supplies for the driver and load sides of the net or pin, along with error messages that indicate why a level-shifter cell was not inserted.\n The report includes errors about the specific insertion point as well as errors that are applicable to the entire path through the net or pin specified with the -through option."}
{"instruction": "How do I Running RedHawk Fusion Commands in the Background", "input": "", "output": "When you have specified the necessary input and design data, you can perform voltage drop and PG electromigration analyses on the design.\n  illustrates the steps in a basic static rail analysis flow.\n        Required Steps for a Basic Rail Analysis Using RedHawk and RedHawk-SC Fusion Run rail analysis analyze_rail -voltage_drop Run PG electromigration analysis Check current violations Check rail analysis results Set application options set_app_options Read design data open_lib open_block Specify ideal voltage sources create_taps analyze_rail -electromigration"}
{"instruction": "How do I Generating Geometry-Based Analysis Reports", "input": "", "output": "Use the get_instance_result command to write the cell instances with rail data to an output text file, in the units defined with the report_units command.\n       To specify which analysis or checking types to report, use the -type option.\n The supported analysis types are: effective_voltage_drop current effective_resistance min_path_resistance, and power Here are commonly used options of the get_instance_result command: Option Description -net   -touching  {{ x1 y1 } { x2 y2 }} -top value   -threshold    -percentage   voltage_drop   -histogram   -collection   Example The following example reports instance voltage drop results within the specified area: fc_shell> get_instance_result -net VDD -type voltage_drop \\ -touching {{1610 1968} {1620 1969}} Isw2/Isw2_pktpro/U179070 -0.053533 Isw2/Isw2_pktpro/U64076 -0.0535949 Isw2/Isw2_pktpro/U83035 -0.0535949 Isw2/Isw2_pktpro/U181017 -0.0535949 Isw2/Isw2_pktpro/U103335 -0.05366 Isw2/Isw2_pktpro/U181049 -0.05366 Isw2/Isw2_pktpro/U98988 -0.05366 Isw2/Isw2_pktpro/U201986 -0.053713 Isw2/Isw2_pktpro/U181064 -0.053719 Isw2/Isw2_pktpro/U79791 -0.053809 Isw2/Isw2_pktpro/U179052 -0.054007 Isw2/Isw2_pktpro/U4120 -0.054073 Isw2/Isw2_pktpro/U20408 -0.054287       See Also  Voltage Hotspot Analysis"}
{"instruction": "How do I Identification of Shift Registers", "input": "", "output": "When the RTL contains synchronous enable logic, the compile_fusion command maps the enable logic to library cells with a synchronous enable pin.\n When the library cells with a synchronous enable pin are not available, the compile_fusion command maps the pushed-out synchronous logic to a MUX, as shown in this figure.\n       To enable the mapping to MUXs, set the map_sync_enable_to_mux attribute on the SEQGEN cells before compile.\n For example, fc_shell> set_attribute \"A1/B/reg[30] mid/sub1/Q_reg[0]\" \\ -name map_sync_enable_to_mux -value true fc_shell> set_attribute \"mid3/apbif_reg/Q_reg[4]\" \\ -name map_sync_enable_to_mux -value true fc_shell> compile_fusion"}
{"instruction": "How do I Fusion Compiler", "input": "", "output": "shows the basic place and route design flow using the Fusion Compiler tool.\n        Fusion Compiler Place and Route Flow To run the Fusion Compiler place and route flow, 1.\n Set up the library data, preparingtiming librarieslogic libraries libraries and prepare the design data, as described in Preparing the Design 2.\n Perform design planning and power planning.\n When you perform design planning and power planning, you create a floorplan to determine the size of the design, create the boundary and core area, create site rows for the placement of standard cells, set up the I/O pads, and create a power plan.\n For more information about design planning and power planning, see the Fusion Compiler Design Planning User Guide 3.\n Perform placement and optimization.\n To perform placement and optimization, use the place_opt command.\n The place_opt command placement and optimization addresses and resolves timing closure for your design.\n This iterative process uses enhanced placement and synthesis technologies to generate       legalized placement for leaf cells and an optimized design.\n You can supplement this functionality by optimizing for power, recovering area for placement, minimizing congestion, and minimizing timing and design rule violations.\n 4.\n Perform clock tree synthesis and optimization.\n To perform clock tree synthesis and optimization, use the clock_opt command.\n Fusion Compiler clock tree synthesis and embedded optimization solve complicated clock tree synthesis problems, such as blockage avoidance and the correlation between preroute and postroute data.\n Clock tree optimization improves both clock skew and clock insertion delay by performing buffer sizing, buffer relocation, gate sizing, gate relocation, level adjustment, reconfiguration, delay insertion, dummy load insertion, and balancing of interclock delays.\n For more information about clock tree synthesis and optimization, see Clock Tree Synthesis 5.\n Perform routing and postroute optimization, as described in Routing and Postroute Optimization The Fusion Compiler tool uses Zroute to perform global routing, track assignment, detail routing, topological optimization, and engineering change order (ECO) routing.\n To perform postroute optimization, use the route_opt command.\n For most designs, the default postroute optimization setup produces optimal results.\n If necessary, you can supplement this functionality by optimizing routing patterns and reducing crosstalk or by customizing the routing and postroute optimization functions for special needs.\n 6.\n Perform chip finishing and design for manufacturing tasks, as described in Chip Finishing and Design for Manufacturing The Fusion Compiler tool provides chip finishing and design for manufacturing and design for yield capabilities that you can apply throughout the various stages of the design flow to address process design issues encountered during chip manufacturing.\n 7.\n Save the design."}
{"instruction": "How do I Optimizing Datapaths", "input": "", "output": "All the extracted datapath elements and the synthetic operators are implemented by the DesignWare datapath generator in Fusion Compiler.\n The DesignWare datapath generator uses the smart generation technology to perform the following tasks:  Implement datapath blocks using context-driven optimizations  Revisit high-level optimization decisions for a given timing context  Consider logic library characteristics Use these commands and application option for datapath implementation:  set_datapath_architecture_options This command controls the strategies used to generate the datapath cells for arithmetic and shift operators.\n The following example specifies to use Booth-encoded architectures to generate multiplexers: fc_shell> set_datapath_architecture_options -booth_encoding true  set_implementation This command specifies the implementation to use for dividers and synthetic library cell instances with a specified name.\n The following example specifies to use the cla3 implementation for the instantiated DesignWare cell U1.\n fc_shell> set_implementation cla3 U1  set_synlib_dont_use       This command prevents using specific implementation.\n The following example specifies not to use the rpl implementation.\n fc_shell> set_synlib_dont_use \\ {dw_foundation/DW_div/cla standard/DW_div/rpl}  compile.datapath.ungroup By default, all DesignWare cells and datapath blocks are ungrouped during timing optimization.\n To disable the ungrouping, set the compile.datapath.ungroup application option to false, changing from its default of true.\n For example, fc_shell> set_app_options \\ -name compile.datapath.ungroup -value false"}
{"instruction": "How do I Inserting Clock Drivers for Designs With Multiple Levels of", "input": "", "output": "Before you insert clock drivers, ensure that all clock tree synthesis settings, including clock routing rules, are specified.\n To perform clock tree synthesis and detail routing to build an H-tree style global clock tree, use the synthesize_multisource_global_clock_trees command.\n When building the H-tree, the tool tries to minimize the skew between the endpoints, which is essential for multisource clock tree synthesis.\n When you use this command to synthesize and detail route a global clock tree, you must specify  The clock net to synthesize by using the -nets option.\n  The library cells to use by using the -lib_cells option.\n You can specify both single- rail and dual-rail library cells.\n By default, the tool uses the Custom Router to route the H-tree structure and connect to the pins of the clock tree cells.\n If the Custom Router is unable to resolve all routing DRC violation when making pin connections, use the Zroute to make the pin connections by using the -use_zroute_for_pin_connections options.\n To stop the routes at the highest available metal layer close to the pin shape, use the -skip_pin_connections option.\n The following example synthesizes and detail routes a global clock tree for the clock net named clkA.\n fc_shell> synthesize_multisource_global_clock_trees \\ -nets [get_nets clkA] -lib_cells [get_lib_cells my_lib/CKBUF8X] \\ -use_zroute_for_pin_connections You can also use the synthesize_multisource_global_clock_trees command to only perform clock detail routing for an H-tree style clock structure that has already been synthesized.\n       When doing so, you must specify  The startpoint of the global clock structure, by using the -roots option.\n  The endpoints of the global clock structure by using the -leaves option.\n The following example detail routes an existing H-tree style clock structure.\n The startpoint is the port named clk1 and the endpoints are the inputs of a group of mesh drivers.\n fc_shell> synthesize_multisource_global_clock_trees \\ -roots [get_ports clk1] -leaves [get_pins mesh_buf*/A] \\ -use_zroute_for_pin_connections When routing the H-trees, the tool uses the highest routing layers available, based on the routing rules specified for the clock nets.\n These same layers can contain prerouted nets such as power and ground nets.\n To prevent placing clock drivers under these preroutes, which can cause pin accessibility issues, the synthesize_multisource_global_clock_trees command creates temporary placement blockages for the preroutes on the same layers it uses for routing.\n After it completes routing, it removes these temporary placement blockages, as shown in the following example output.\n...\n Net: clk; Rule: htree_ndr; Min Layer: M8; Max Layer: M9...\n Information: Using routing H/V layer pair 'M9'/'M8' for net 'clk'.\n (CTS-659)...\n Converting metal shapes in horizontal layer M9 and vertical layer M8 into placement blockages.\n In total 703 placement blockages created.\n...\n Successfully deleted all temporary placement blockages and the cell map.\n If you specify multirow-height cells with the -lib_cell option of the synthesize_multisource_global_clock_trees command, the tool might not be able to place them due to the temporary placement blockages it creates for the preroutes.\n If so, you can prevent the tool from creating the temporary placement blockages by setting the cts.multisource.enable_pin_accessibility_for_global_clock_trees application option to false To remove a clock structure created by the synthesize_multisource_global_clock_trees command, use the remove_multisource_global_clock_trees command."}
{"instruction": "How do I Power Intent Concepts", "input": "", "output": "This topic introduces the following concepts used in the Fusion Compiler tool:  Power Intent Concepts  UPF Flows  Multiple-Patterning Concepts"}
{"instruction": "How do I Performing Voltage Drop Analysis", "input": "", "output": "You can create multiple rail scenarios and associate them with a design scenario in the current design.\n The created rail scenarios are saved in the design library.\n To run rail analysis on multiple rail scenarios: 1.\n Specify the following application options to enable multi-rail-scenario rail analysis: fc_shell> set_app_options -list { rail.enable_new_rail_scenario true rail.enable_parallel_run_rail_scenario true        2.\n Run the set_scenario_status -ir_drop command to enable rail analysis for the specified design scenario.\n fc_shell> set_scenario_status func_cbest -ir_drop true 3.\n Create a rail scenario and associate it with an existing design scenario.\n Specify rail attributes for the created rail scenario with the set_rail_scenario command.\n You can create multiple rail scenarios for different purposes and associate them with an existing design scenario.\n fc_shell> create_rail_scenario -name T_low -scenario func_cbest fc_shell> set_rail_scenario -name T_low -voltage_drop dynamic -extra_gsr_option_file extra.gsr \\ -nets {VDD VSS} fc_shell> create_rail_scenario -name T_high -scenario func_cbest fc_shell> set_rail_scenario -name T_high -voltage_drop dynamic -extra_gsr_option_file extra2.gsr \\ -nets {VDD VSS} 4.\n Specify the multicore processing setting with the set_host_options command.\n For example, to run rail analysis on a local host, use the following command: fc_shell> set_host_options -submit_command {local} To run analysis on a grid system, use the following command: fc_shell> set_host_options -submit_protocol sge \\ -submit_command {qsub -V -b y -cwd -P bnormal -l mfree=16G} 5.\n Run rail analysis with the analyze_rail command.\n fc_shell> analyze_rail -rail_scenario {T_high T_low} \\ -submit_to_other_machines Note: To use the multi-rail-scenario rail analysis capability for IR-driven placement or IR-driven concurrent clock and data optimization, you must set the following application option before the clock_opt -from final_opto or route_opt command: fc_shell> set_app_options \\ -name opt.common.power_integrity -value true For more information about power integrity analysis, see the Enabling the Power Integrity Features section.\n       Examples The following example runs rail analysis on multiple rail scenarios.\n # Specify options for multi-rail-scenario rail analysis set_app_options -list { rail.enable_new_rail_scenario true rail.enable_parallel_run_rail_scenario true   set_host_options -submit_protocol sge \\ -submit_command {qsub -V -notify -b y -cwd -j y \\ -P bnormal -l mfree=16G}  # Custom RedHawk-SC Fusion python script to specify user-defined # toggle rate for scenario T_custom create_rail_scenario -name T_custom -scenario func_cbest set_rail_scenario -name T_custom -custom_script_file custom_sc.py  # Default RedHawk-SC Fusion toggle rate for scenario T_def create_rail_scenario -name T_def -scenario func_cbest set_rail_scenario -name T_def -voltage_drop dynamic -nets {VDD VSS}  # Run RedHawk-SC Fusion rail analysis on the specified rail scenario analyze_rail -rail_scenario {T_custom T_def} -submit_to_other_machines  # (Optional) Open and examine the generated rail results open_rail_result {T_custom T_def} The following example runs multi-rail-scenario rail analysis in the power integrity flow.\n ## Specify RedHawk/RedHawk-SC based options set_app_options -name rail.enable_redhawk -value true set_app_options -name rail.enable_redhawk_sc -value false set_app_options -name rail.redhawk_path -value /test/bin set_app_options -name rail.tech_file -value design_data/tech/rh.tech set_app_options -name rail.lib_files -value { inputs/test/CCSP_test.lib  set_host_options -submit_protocol sge -submit_command {qsub -V \\ -notify -b y -cwd -j y -P bnormal -l mfree=16G}  ## Specify options for running optimization on multiple rail scenarios set_app_options -list { rail.enable_new_rail_scenario true rail.enable_parallel_run_rail_scenario true  ## Enable IR drop flag for a given scenario set_scenario_status [current_scenario] -ir_drop true ## Create a rail scenario and specify associated options create_rail_scenario -name T_low -scenario func_cbest set_rail_scenario -name T_low -voltage_drop dynamic \\       -extra_gsr_option_file extra.gsr -nets {VDD VSS}  create_rail_scenario -name T_high -scenario func_cbest set_rail_scenario -name T_high -voltage_drop dynamic \\ -extra_gsr_option_file extra2.gsr -nets {VDD VSS}  ## Enable power integrity flow set_app_options -name opt.common.power_integrity -value true"}
{"instruction": "How do I Defining Matching Wire Lengths", "input": "", "output": "To define different minimum wire lengths for each object in a group of nets or pin-to- pin connections, use the create_length_limit command.\n Use this feature to perform length-limit routing on the nets with a single driver and multiple receivers, and run point-to- point routing between the drivers, steiners, and receivers.\n The Custom Router generates the steiner nodes in the background when the create_length_limit command is       executed.\n Each of the receivers has either a matched length constraint or an independent length constraint from the driver.\n  shows an example of a single driver with multiple receivers, where each receiver has an individual length constraint.\n  Receivers With Individual Wire Length Constraints From Driver D Syntax: fc_shell> create_length_limit - for objects -min_value float -exclude pins or ports -driver pin or port -force intent_name Command or Options Description create_length_limit  -for   -min_value  -exclude   -driver  -force   intent_name    -force        In Example 31, the commands create five wire-length constraints, one for each of the receivers shown in Example 31 Five Wire-Length Constraints for the Five Receivers Connected to Driver D fc_shell> create_length_limit -for [get_nets -physical_context $net] \\ -driver [get_pins -physical_context D/o1] fc_shell> create_length_limit -for [get_pins -physical_context R1/a] \\ -min_value 38.0 fc_shell> create_length_limit -for [get_pins -physical_context R2/a] \\ -min_value 40.0 fc_shell> create_length_limit -for [get_pins -physical_context R3/a] \\ -min_value 45.0 fc_shell> create_length_limit -for [get_pins -physical_context R4/a] \\ -min_value 42.0 fc_shell> create_length_limit -for [get_pins -physical_context R5/a] \\ -min_value 41.0 In Example 32, the command creates a constraint group named abc for the nets and bundles that match the pattern pr*.\n The minimum wire length is set to 10, so the length for each of the wires in the group is at least 10 microns.\n See Example 32 Minimum Wire Length Constraint for pr* Nets fc_shell> create_length_limit abc -for [get_nets -physical_context pr*] \\ -min_value 10 {abc}  Minimum Wire Length Constraint for pr* Nets In Example 33, the command creates a constraint group named gcr_length_limit_a2 for the a2/A pin.\n The minimum wire length is set to 2000.\n See Example 33 Minimum Wire Length Constraint for the a2/A Pin fc_shell> create_length_limit -for [get_pins -physical_context a2/A] \\ -min_value 2000.0 -force gcr_length_limit_a2        Minimum Wire Length Constraint for the a2/A Pin"}
{"instruction": "How do I Creating Block Contexts", "input": "", "output": "To consider current and power distribution inside the memory blocks when analyzing hierarchical designs, use the exec_gds2rh command to generate RedHawk macro models for memory blocks.\n The command invokes the RedHawk gds2rh utility under the hood and extracts power grids and current sources from the memory blocks with varying levels of abstraction\u2015from fully detailed models for accurate block-level analysis to abstract models for full-chip dynamic analysis.\n Note: To create RedHawk macro models using the RedHawk gds2rh utility, you must specify the path to the RedHawk executable with the rail.redhawk_path application option before running the exec_gds2rh command.\n The generated macro models are saved in the current working directory or the one you specify in the configuration file.\n To run rail analysis with the generated macro models, set the rail.macro_models -value { cell1 path1 application option before running the analyze_rail command.\n For more information about how to run the RedHawk gds2rh utility, see the RedHawk User Manual Required Configuration File To generate a RedHawk macro model using the gds2rh utility, you must provide a configuration file that includes the following information:  GDS II files  Top design name  Layer mapping file  Power and ground net names       Specify one configuration file per memory.\n Make sure the configuration file contains the correct syntax for macro model generation.\n Otherwise, the tool issues an error message and terminates the model generation process.\n The following is an example of the configuration file: LEF_FILE {./lef/saed32sram.lef  GDS_FILE./gds/SRAMLP2RW128x16.gds GDS_MAP_FILE./redhawk.gdslayermap OUTPUT_DIRECTORY SRAMLP2RW128x16_output  USE_LEF_PINS_FOR_TRACING 1  VDD_NETS { VDD  GND_NETS { VSS  TOP_CELL SRAMLP2RW128x16 Examples The following example generates RedHawk macro models for the SRAMLP2RW128x16, SRAMLP2RW32x4, SRAMLP2RW64x32 and SRAMLP2RW64x8 memories: ## Specify the path to the RedHawk executable ## set_app_options -name rail.redhawk_path -value $env(REDHAWK_IMAGE)/bin  ## Specify one configuration file per memory ## exec_gds2rh -config_file SRAMLP2RW128x16.config exec_gds2rh -config_file SRAMLP2RW32x4.config exec_gds2rh -config_file SRAMLP2RW64x32.config exec_gds2rh -config_file SRAMLP2RW64x8.config The following example runs top-level rail analysis with the macro models that are generated in the previous example: ## Run rail analysis at top level ##  open_lib open_block top link_block  ## Specify rail analysis setting ## create_taps set_app_options -name rail.tech_file -value test.tech set_app_options -name rail.lib_files -value test.lib  ## Set macro model file ## set_app_options -name rail.macro_models \\       -value { SRAMLP2RW128x16./data_ SRAMLP2RW128x16 SRAMLP2RW32x4./data_SRAMLP2RW32x4 SRAMLP2RW64x32./data_SRAMLP2RW64x32 SRAMLP2RW64x8./data_ SRAMLP2RW64x8   ## Run rail analysis ## analyze_rail -voltage_drop static -all_nets  ## Check rail results ## open_rail_result gui_start"}
{"instruction": "How do I Tracing Minimum Path Resistance Using the Mouse Tool", "input": "", "output": "When the analysis is complete, run the report_rail_result -type minimum_path_resistance command to display a minimum path resistance map, write the results to an output file, or query the attributes on the cell instances or pins.\n The following example writes the calculated minimum path resistances for the VDD net to an output file called minres.rpt.\n fc_shell> open_rail_result fc_shell> report_rail_result -type minimum_path_resistance \\ -supply_nets { vdd } minres.rpt fc_shell> sh cat minres.rpt FEED0_14/vbp 518.668 INV61/vdd 158.268 ARCR0_1/vbp 145.582 FEED0_5/vdd 156.228 INV11/vbp 518.669 The output minres.rpt report includes the entire resistance information.\n fc_shell> sh cat minres.rpt FEED0_14/vbp 518.668 INV61/vdd 158.268 ARCR0_1/vbp 145.582 FEED0_5/vdd 156.228 INV11/vbp 518.669       To write out a detailed minimum resistance path report for a cell instance, run the report_rail_minimum_path command and specify the net and the cell to report.\n To see the segment-by-segment resistance of that minimum resistance path shown in the GUI, use the -error_cell option of the report_rail_minimum_path command to save the geometries on the path of the minimum path resistance to an error cell.\n You can then open the error cell in an error browser and check the minimum resistance path from the pin of the cell to the nearest tap and trace each go-through geometries.\n In the following example, all geometries on the minimum resistance path are traced and written to the error cell named test.err.\n fc_shell> open_rail_result fc_shell> report_rail_minimum_path -cell U19 -net VDD -error_cell Shown in, each geometry is classified by layer in the error browser.\n Click a geometry to check for detailed information, such as IR drop, resistance, bounding box, and so on.\n In the GUI, the traced path is highlighted in yellow in the design layout (see ).\n  Displaying Error Cells in the Error Browser        Traced Segment is Highlighted in the Design Layout See Also  Displaying Maps in the GUI  Writing Analysis and Checking Reports  Querying Attributes"}
{"instruction": "How do I Performing Standalone Clock Trees Synthesis", "input": "", "output": "Before you perform clock tree synthesis, you should save the block.\n This allows you to refine the clock tree synthesis goals and rerun clock tree synthesis with the same starting point, if necessary.\n The following topics describe the different methods available for implementing clock trees:  Performing Standalone Clock Trees Synthesis  Synthesizing, Optimizing, and Routing Clock Trees With the clock_opt Command  Controlling Concurrent Clock and Data Optimization        Splitting Clock Cells  Balancing Skew Between Different Clock Trees  Performing Global-Route-Based Optimization Using Machine Learning Data  Routing Clock Trees  Inserting Via Ladders During Clock Tree Synthesis, Optimization, and Clock Routing  Marking Clocks as Propagated After Clock Tree Synthesis  Performing Postroute Clock Tree Optimization  Performing Voltage Optimization  Marking Clock Trees as Synthesized  Removing Clock Trees"}
{"instruction": "How do I Using Pin Access Checker Utility", "input": "", "output": "You can restrict the mapping and optimization of sequential cells and instantiated combinational cells to a subset of reference libraries and library cells.\n To specify one or more library subset restrictions, use the define_libcell_subset command followed by the set_libcell_subset command.\n       Note: The library subset restrictions apply to leaf cells, but not to hierarchical cells.\n To set library subset restrictions on leaf cells, 1.\n Define a subset of library cells by using the define_libcell_subset command.\n After the library cells are grouped into a subset, they are not available for general mapping during optimization, but for sequential cells and instantiated combinational cells only.\n 2.\n Restrict the optimization of sequential and instantiated combinational cells by using the set_libcell_subset command.\n During optimization, the tool uses the library subset defined in step 1 to optimize sequential cells (both mapped and unmapped) and mapped combinational cells.\n In the following example, the define_libcell_subset command groups the SDFLOP1 and SDFLOP2 library cells into a subset called special_flops, and then the set_libcell_subset command restricts the mapping of the LEAF1 leaf cell to the special_flops library subset.\n fc_shell> define_libcell_subset \\ -libcell_list \"SDFLOP1 SDFLOP2\" -family_name special_flops fc_shell> set_libcell_subset \\ -object_list \"HIER1/LEAF1\" -family_name special_flops Reporting Library Subset Restrictions To report library subsets specified for sequential cells and instantiated combinational cells, use the report_libcell_subset command.\n Removing Library Subset Restrictions To remove library subsets, use the remove_libcell_subset command."}
{"instruction": "How do I Performing Dynamic-Power Optimization", "input": "", "output": "To set up for conventional leakage-power optimization during the preroute optimization stage, perform the following: 1.\n Ensure that at least one scenario is enabled for leakage-power optimization by using the set_scenario_status -leakage_power true command and the set_qor_strategy -metric leakage_power | total_power command for deciding the target of power flow.\n 2.\n (Optional) Change the effort level for power optimization by setting the set_qor_strategy -mode command to balanced or extreme_power.\n The default is balanced.\n This effort control for the power flow.\n When you use these settings, the tool performs conventional leakage-power optimization during the clock_opt command.\n However, it does not perform any dynamic-power optimization."}
{"instruction": "How do I Summary of Relative Placement Commands", "input": "", "output": "The relative placement information is automatically saved in the design library database when you save the design by using the save_lib command.\n You can also save the relative placement information to a file that contains Tcl commands that re-creates the relative placement groups, their objects, and their settings.\n To do so, use the write_rp_groups -file_name command.\n You must either specify which relative placement groups to write commands for or specify the -all option to write commands for all relative placement groups.\n By default, the write_rp_groups command writes out commands for creating the specified relative placement groups and to add leaf cells, hierarchical groups, and blockages to these groups.\n The commands for generating subgroups within hierarchical groups are not written.\n You can modify the default behavior by using the options described in Table 62 Table 62 The write_rp_groups Command Options To do this Use this option    -hierarchical create_rp_group  -create add_to_rp_group -cells  -cell add_to_rp_group -rp_group  -rp_group add_to_rp_group -blockage  -blockage"}
{"instruction": "How do I Inserting Diodes After Detail Routing", "input": "", "output": "One way to protect gates from antenna effects is to provide a discharge path for the accumulated charge to leave the net.\n However, the discharge path should not allow current to flow during normal chip operation.\n Discharging can be accomplished by inserting a reverse-biased diode on the net close to the gate that is being protected.\n To enable diode insertion during detail routing, set the route.detail.insert_diodes_during_routing application option to true To control diode insertion, set the following application options:  To specify a preference for fixing antenna violations by using diode insertion rather than layer hopping, set the route.detail.antenna_fixing_preference application option to use_diodes  To require fixing of antenna violations by using diode insertion, disable layer hopping by setting the route.detail.hop_layers_to_fix_antenna application option to false  By default, when you enable diode insertion, Zroute can fix an antenna violation either by adding a new diode or by using an existing spare diode.\n Zroute determines which method to use, based on which is closest to the required location: an empty location for a new diode or an existing spare diode.\n  To specify a preference for using a new diode or using an existing spare diode, set the route.detail.diode_preference application option to new or spare respectively.\n To reset the diode preference to the default behavior, set the route.detail.diode_preference application option to none  If you want Zroute to use only one of these methods, set the route.detail.diode_insertion_mode application option to new to force the insertion of new diodes or to spare to force the use of existing spare diodes.\n To reset the diode insertion method to the default behavior, set the route.detail.diode_insertion_mode application option to new_and_spare Note: To take advantage of spare diodes for antenna violation fixing, you need to add the spare diodes either before or after standard-cell placement and before routing the areas where antenna violations might occur.\n  When inserting new diodes, Zroute selects the diodes from the reference libraries and inserts them into existing open spaces.\n To control which diodes are used, set the route.detail.diode_libcell_names application option.\n When you specify the diode library cells, use only the cell names.\n If you include the library name, the tool does not recognize the diode cells.\n        By default, Zroute reuses existing filler cell locations for diode insertion.\n To prevent Zroute from reusing these locations, set the route.detail.reuse_filler_locations_for_diodes application option to false Zroute considers voltage areas when inserting diode cells and also observes the logic hierarchy assignments for diode cells.\n  If a pin has an antenna violation, the diode cells are inserted at the same level of logic hierarchy as the violating pin.\n  If a top-level port has an antenna violation, by default, the diode cells are inserted at the top level.\n However, if the port belongs to a voltage area, you can insert the diode cells in the logic hierarchy associated with the voltage area by setting the route.detail.use_lower_hierarchy_for_port_diodes application option to true To remove the redundant diodes during routing, set the route.detail.delete_redundant_diodes_during_routing application option to true"}
{"instruction": "How do I Reporting Multibit Registers", "input": "", "output": "Fusion Compiler allows you to customize the naming styles for multibit registers.\n You can use the application options described in this topic to modify the default naming styles to match the requirements of your design flow or a third-party tool.\n       Specifying a Name Separator When creating the name for a multibit register, the tool concatenates the names of the single-bit registers and uses the _ character as the name separator.\n For example, when the tool combines the single-bit registers named reg1 and reg2, the multibit register is named reg1 reg2.\n To change the default name separator, use the multibit.naming.multiple_name_separator_style application option.\n Specifying a Name Prefix To specify a prefix for the multibit register name, use the multibit.naming.name_prefix application option.\n Compacting Hierarchical Names When combining single-bit registers with hierarchical names, the tool uses the full hierarchical names of the single-bit registers for the concatenated multibit register name.\n For example, when the tool combines the single-bit registers named A/B/P/reg1 and A/B/ Q/reg2, the multibit register is named A/B/P/reg1_A/B/Q/reg2.\n To make the multibit register name more compact, specify that common hierarchical names should not be repeated by setting the multibit.naming.compact_hierarchical_name application option to true.\n If you do so, when the tool combines the single-bit registers named A/B/P/reg1 and A/B/Q/reg2, the multibit register is named A/B/P/reg1_Q/reg2.\n Specifying a Range Separator for Contiguous Bits of a Bused Register When combining contiguous bits of a bused register, the tool uses the : character as the range separator.\n For example, when the tool combines single-bit bused registers named regA[0], regA[1], and regA[2], the multibit register is named regA[2:0].\n To change the range separator, use the multibit.naming.range_separator_style application option.\n Specifying an Element Separator for Noncontiguous Bits of a Bused Register When combining noncontiguous bits of a bused register, the tool uses the, character as the element separator.\n For example, when the tool combines the single-bit bused registers named regA[1] and regA[3], the multibit register is named regA[3,1].\n To change the element separator, use the multibit.naming.element_separator_style application option.\n       Specifying the Expanded Naming Styles When Combining Elements of a Multidimensional Register Array To specify the expanded naming styles for the multibit registers created by combining elements of a multidimensional register array, use the multibit.naming.expanded_name_style application option.\n For more information on this application option, see the man page."}
{"instruction": "How do I Modifying the Stacking Order", "input": "", "output": "If voltage area shapes from two or more voltage areas overlap, either completely or partially, the tool uses the stacking order of the shapes to determine the effective shapes of the voltage areas.\n By default, the stacking order is the order in which you define the shapes, with the last shape defined on top.\n The tool assigns the overlapped region to the voltage area associated with the top shape.\n Unlike merging shapes within a voltage area, when the tool resolves overlapping shapes from different voltage areas, it does not remove any shapes; only the interpretation of the shapes changes.\n       For example, assume you want to create nested voltage areas, as shown in  Nested Voltage Areas To generate these effective voltage areas, you must specify the outer voltage area first, followed by the inner voltage area, so that the voltage area shape for the inner voltage area is on top: fc_shell> create_voltage_area -power_domains PD1 \\ -region { {0 0} {30 30} } {PD1} fc_shell> create_voltage_area -power_domains PD2 \\ -region { {10 10} {20 20} } {PD2} fc_shell> get_attribute -objects [get_voltage_areas PD2] \\ -name effective_shapes {{10.0000 10.0000} {20.0000 20.0000}} fc_shell> get_attribute -objects [get_voltage_areas PD1] \\ -name effective_shapes {{0.0000 0.0000} {10.0000 0.0000} {10.0000 20.0000} {20.0000 20.0000} {20.0000 10.0000} {10.0000 10.0000} {10.0000 0.0000} {30.0000 0.0000} {30.0000 30.0000} {0.0000 30.0000}} If you specify the inner voltage area first, the shape for the outer voltage area is on top and it masks the inner voltage area, so it is ignored by the tool, as shown in the following example: fc_shell> create_voltage_area -power_domains PD2 \\ -region { {10 10} {20 20} } {PD2} fc_shell> create_voltage_area -power_domains PD1 \\ -region { {0 0} {30 30} } {PD1} fc_shell> get_attribute -objects [get_voltage_areas PD2] \\ -name effective_shapes fc_shell> get_attribute -objects [get_voltage_areas PD1] \\       -name effective_shapes {{0.0000 0.0000} {30.0000 30.0000}} To report the stacking order of the voltage area shapes, use the report_voltage_areas -verbose command.\n To modify the stacking order of the voltage area shapes, use the set_voltage_area_shape command, as described in Modifying the Stacking Order"}
{"instruction": "How do I Controlling Datapath Optimization", "input": "", "output": "You can control boundary optimization across user-specified logical boundaries and match the interface boundary of a physical block with the RTL module interface.\n Boundary optimization allows you to use RTL floorplanning DEF files during block implementation.\n The following figure shows how constants, equal-opposite logic, logic phase, and unloaded ports are propagated through the logical boundaries without and with boundary optimization.\n        Constant propagation  Phase inversion Unloaded propagation Equal-opposite logic propagation Disabled Enabled Disabled Disabled Disabled Enabled Enabled Enabled Disabled: without boundary optimization Enabled: with boundary optimization Global Control By default, all types of boundary optimization are enabled during compile.\n To control boundary optimization globally, use the compile.flow.boundary_optimization application option.\n When you set the compile.flow.boundary_optimization application option to false  The tool disables propagation of equal-opposite logic and phase inversion.\n  Constants and unloaded ports continue to propagate across hierarchies.\n To disable constant propagation and unloaded propagation globally, set the compile.flow.constant_and_unloaded_propagation_with_no_boundary_opt application option to false.\n The default is true.\n The setting does not affect object-level specifications.\n Controlling Specific Types of Boundary Optimization To control the level of boundary optimization for hierarchical cell pins, hierarchical cells, or blocks, use the set_boundary_optimization command.\n The tool uses the following order of precedence to determine the setting to use during optimization: 1.\n Setting applied on pins of hierarchical cell instances 2.\n Setting applied on hierarchical cell instances, which applies to its pins as well 3.\n Setting applied on modules, which applies to all its cells and pins as well       The following table shows the level of boundary optimization you can specify with the set_boundary_optimization command and the types of boundary optimization that are enabled for each level.\n Table 20 Levels of Boundary Optimization and the Corresponding Types of Boundary Optimization Performed Level of boundary optimization Constant propagation Unloaded propagation Equal-opposite logic propagation Phase inversion all     auto     none     You can control the specific types of boundary optimization allowed for an object by using the -constant_propagation -unloaded_propagation -equal_opposite_propagation, and -phase_inversion options.\n These options are mutually exclusive with the boundary optimization level specified by using all auto, or none and cannot be used on the same object in a single command invocation.\n If you use the set_boundary_optimization command multiple times on the same object, the effect is additive.\n Based on the types of boundary optimization you enable or disable, the tool sets the constant_propagation unloaded_propagation equal_opposite_propagation, and phase_inversion read-only attributes of the specified object to true or false.\n If you  Enable all four types, the tool sets the boundary_optimization read-only attribute to true  Disable at least one type, the tool sets the boundary_optimization read-only attribute to false To report boundary optimization settings, use the report_boundary_optimization command.\n To remove them, use the remove_boundary_optimization command.\n The following example  Disables all boundary optimization for the U1 cell instance  Disables equal-opposite logic propagation and phase inversion for the U2 cell instance  Disables constant propagation and phase inversion for the U3/in1 cell pin fc_shell> set_boundary_optimization [get_cells U1] none fc_shell> set_boundary_optimization [get_cells U2] auto fc_shell> set_boundary_optimization [get_pins U3/in1] \\ -constant_propagation false       fc_shell> set_boundary_optimization [get_pins U3/in1] \\ -phase_inversion false Controlling Phase Inversion To prevent boundary optimization from moving inverters across hierarchical boundaries, set the compile.optimization.enable_hierarchical_inverter application option to false.\n By default, the application option is set to true to allow phase inversion.\n This application option is only effective when boundary optimization is enabled."}
{"instruction": "How do I Adding Buffers on Nets", "input": "", "output": "To revert the ECO changes made with the size_cell command, use the revert_cell_sizing command.\n To include the sized cells that are adjacent to the specified cell in the same row, use the -include_adjacent_sized_cells option.\n When you use this option, by default, the tool also reverts the cells that abut the specified cell on either side, if they are also sized cells.\n However, if you use the -adjacent_cell_distance option, the tool recursively reverts the sized cells that are adjacent to sized cells within the specified distance in the same row."}
{"instruction": "How do I Defining Routing Guides", "input": "", "output": "To remove routing blockages from the current block, use the remove_routing_blockages command.\n  To remove specific routing blockages, specify the routing blockages, either as a list or collection, such as that returned by the get_routing_blockages command.\n  To remove all routing corridors, specify the -all option."}
{"instruction": "How do I Library Configuration", "input": "", "output": "You can specify a reference library list for a design library when you create the design library by using the -ref_libs option of the create_lib command.\n You can also change the reference library list at any time by using the set_ref_libs command.\n Use the following commands to specify, rebind, and report reference libraries:  create_lib -ref_libs You can specify a relationship between a new design library and its lower-level reference libraries by using the create_lib command.\n For example, fc_shell> create_lib lib_B \\ -ref_libs {../LIBS/lib_c../STND/stdhvt.ndm}...\n {lib_B}  set_ref_libs -ref_libs For an existing design library, open the library and then use the set_ref_libs command to specify the reference libraries.\n For example, fc_shell> current_lib {lib_B} fc_shell> set_ref_libs \\ -ref_libs {../LIBS/lib_C../STND/stdhvt.ndm}../LIBS/lib_C../STND/stdhvt.ndm  report_ref_libs To report the reference libraries of a design library, use the report_ref_libs command.\n       For example, fc_shell> create_lib lib_A -ref_libs \\ {../libs/SCLL.ndm../libs/SCHH.ndm../BLOCKS/MACROS} {lib_A} fc_shell> report_ref_libs...\n Name Path Location  *+ SCLL../libs/SCLL.ndm /remote/project/libs/SCLL.ndm *+ SCHH../libs/SCHH.ndm /remote/project/libs/SCHH.ndm * MACROS../BLOCKS/MACROS /remote/project/BLOCKS/MACROS \"*\" = Library currently open \"+\" = Library has technology information  set_ref_libs -rebind When you make a change that invalidates the reference library list, such as moving a reference library to a new location, you need to rebind the reference libraries.\n To do so, use the -rebind option, which rebinds each reference library path specified by the search_path variable to libraries that are currently loaded in memory.\n For example, fc_shell> current_lib {lib_A} fc_shell> set_app_var search_path {.\n../REFLIBS../CLIBS}.\n../LIBS../BLOCKs fc_shell> set_ref_libs -rebind../REFLIBS/lib_C../REFLIBS/lib_D../CLIBS/stdhvt.ndm} Rebinding a library does not affect the bindings of blocks already existing in the design library.\n To rebind these blocks using an updated reference library list, use the -rebind option with the link_block command."}
{"instruction": "How do I Physical Synthesis", "input": "", "output": "To apply tool settings required for improving specific QoR metrics, use the set_qor_strategy -stage synthesis command.\n To specify the QoR metrics to improve, use the -metric option.\n To optimize the block for the best  Timing, use the timing setting  Leakage power and timing, use the leakage_power setting  Total power and timing, use the total_power setting.\n To further improve total power, at the expense of runtime, you can use the -mode extreme_power option, with the -metric total_power option.\n To specify a target mode for the design flow, use the -mode option.\n The supported values for this option are:  balanced, which is the default mode that optimizes the design for the target metrics specified with the -metric option.\n  early_design, which is suitable for the early stages of the design flow, when the emphasis is on short turnaround time for prototyping purposes.\n With this mode, you can reduce runtime at the expense of QoR.\n  extreme_power, which is recommended for further improvement of total power at the expense of runtime.\n This mode should only be specified along with the -metric leakage_power or -metric total_power option.\n If you specify this mode with the -metric timing option, the tool ignores the specified mode setting and uses the balanced mode.\n       When you use this command, the tool applies application option settings that affect synthesis, optimization, placement, legalization, clock tree synthesis, routing, extraction, and timing analysis.\n To review the required setting, use one of the following methods:  Generate a Tcl script that contains the settings by using the -output option  Report the settings by using the -report_only option  Report only the settings that are not set to their required values by using the -diff_only option"}
{"instruction": "How do I Specifying the Editability of Blocks From the Top-Level", "input": "", "output": "When saving a block at different stages of an implementation flow, you can use the -as blockName labelName or -label labelName option with the save_block command to specify a unique label for each version of the block you save.\n By default, during linking, the top-level block links to lower-level blocks with the same label.\n If a subblock has been saved with multiple labels, you can specify which label the top level should link to by using the set_label_switch_list command.\n The command specifies a precedence-ordered list of labels to use during linking.\n By default, the command applies to all subblocks; to specify a different block or blocks, use the -reference option.\n   The following example specifies that the top level should link to the PostCompile label of the BLOCK1 and BLOCK2 subblocks: fc_shell> set_label_switch_list \\ -reference {BLOCK1 BLOCK2} PostCompile The following example specifies that the PostCompile label has a higher priority than the PreCompile label when linking any subblock at the top level.\n fc_shell> set_label_switch_list {PostCompile PreCompile}       You can use the set_label_switch_list command again to update the block's label switch list and then relink the block using the link_block -rebind command.\n Note the following when relinking a block:  If none of the labels in the switch list are available in a subblock, or if you specify an empty switch list, the tool links the subblock to the label it was previously linked to  If you have removed all the labels for a subblock, the tool links the subblock to the label of its parent block"}
{"instruction": "How do I Tap Attributes", "input": "", "output": "When creating taps, by default the create_taps command verifies that the created taps are inserted in valid locations.\n The tool issues a warning message if the tap does not touch any layout shape.\n To place taps outside a shape without issuing a warning message, disable the checking by using the -nocheck option.\n The following command creates a tap with the -nocheck option enabled.\n No warning message is issued.\n fc_shell> create_taps -supply_net VDD -layer 3 \\ -point {100.0 200.0} -nocheck"}
{"instruction": "How do I Aligning by the Right Edges", "input": "", "output": "By default, the Fusion Compiler tool aligns the leaf cells by aligning the left edges.\n To explicitly specify this alignment method, use the -alignment left option of the set_rp_group_options command.\n  shows cells that are left aligned.\n  Bottom-Left-Aligned Relative Placement Group U4 U3 U2 U1 row 3 row 2 row 1 row 0 col 0"}
{"instruction": "How do I Placing ECO Cells", "input": "", "output": "You can identify if ECO changes made with the size_cell add_buffer, and add_buffer_on_route commands can cause large displacements during ECO placement and legalization by using the report_eco_physical_changes command.\n       You can revert the nonoptimal ECO changes made by the size_cell add_buffer, and add_buffer_on_route commands by using the revert_eco_changes command.\n To specify the cells for which to revert the ECO changes, use the -cells option."}
{"instruction": "How do I Performing Design Rule Checking in an External Tool", "input": "", "output": "Signoff design rule checking runs the IC Validator tool within the Fusion Compiler tool to check the routing design rules defined in the foundry runset.\n To perform signoff design rule checking, run the signoff_check_drc command, as described in Performing Signoff Design Rule Checking.\n In addition, you can use the signoff_fix_drc command to automatically fix the DRC violations detected by the signoff_check_drc command.\n For more information, see Automatically Fixing Signoff DRC Violations Note: An IC Validator license is required to run the signoff_check_drc and signoff_fix_drc commands."}
{"instruction": "How do I Viewing Density Heat Maps in the GUI", "input": "", "output": "To report the metal density information, run the signoff_report_metal_density command.\n       To calculate the metal density information for a metal layer, the tool requires the following information:  The density rule for the layer The tool determines the density rule for each layer by using the following settings, in order of priority: 1.\n The setting of the _min_density parameter in the IC Validator parameter file specified by the signoff.report_metal_density.user_defined_options application option 2.\n The setting of the signoff.report_metal_density.min_density application option 3.\n The setting of the minDensity attribute in the DensityRule section of the technology file 4.\n 10 percent  The grid sizes for the layer The tool determines the grid sizes for each layer by using the following settings, in order of priority: 1.\n The setting of the _min_density_window or _density_gradient_window parameter in the IC Validator parameter file specified by the signoff.report_metal_density.user_defined_options application option 2.\n The setting of the signoff.report_metal_density.density_window or signoff.report_metal_density.gradient_window_size application option 3.\n The setting of the windowSize attribute in the DensityRule section of the technology file 4.\n 50 microns For information about defining the metal density and density gradient rules in the technology file, including the windowSize attribute, see the Synopsys Technology File and Routing Rules Reference Manual By default, the signoff_report_metal_density command  Uses the fill data for the metal density calculation  To use the design view if its timestamp is newer than the fill data, set the signoff.report_metal_density.fill_view_data application option to read_if_uptodate  To use the design view regardless of its timestamp, set the signoff.report_metal_density.fill_view_data application option to discard        Creates the density and gradient windows starting from the lower-left corner of the chip boundary using a step size of half the window size  To change the starting point for creating the windows, use the -starting_point option with the signoff_report_metal_density command.\n  To change the step size, set the signoff.report_metal_density.density_window_step application option or the _window_step_size parameter in the IC Validator parameter file specified by the signoff.report_metal_density.user_defined_options application option.\n If you specify both, the IC Validator parameter overrides the application option.\n  Computes the metal density for the entire block To compute the metal density for specific regions, use the -coordinates option with the signoff_report_metal_density command.\n When you use this option, the command combines the specified regions when reporting the metal density; it does not report the metal density individually for each specified region.\n To specify the coordinates, use the following format: {{x } {x }...\n {x }} Note that there must be a space between each set of coordinates.\n  Computes the metal density for each metal layer from the minimum routing layer to the maximum routing layer To compute the metal density for specific layers, use the -select_layers option with the signoff_report_metal_density command.\n  Does not generate heat maps To generate heat maps to enable viewing of the metal density information in the GUI, set the signoff.report_metal_density.create_heat_maps application option to true.\n For information about viewing the heat maps, see Viewing Density Heat Maps in the GUI  Writes the report files to a directory named signoff_report_metal_density_run under the current working directory To change the name of the IC Validator working directory, set the signoff.report_metal_density.run_dir application option.\n By default, the report files are named report_metal_density.txt and gradient_density_report.txt.\n To change the file names, use the -output option with the signoff_report_metal_density command.\n When you use this option, the density report has the specified file name, while the gradient density report adds a.gradient file extension.\n For example, if you specify - output density.rpt, the density report is named density.rpt and the gradient density report is named density.rpt.gradient.\n       The signoff_report_metal_density command uses the options that you specify to generate the command line to invoke the IC Validator tool.\n You can specify additional options for the IC Validator command line by setting the signoff.report_metal_density.user_defined_options application option.\n The string that you specify in this option is added to the command line used to invoke metal density reporting in the IC Validator tool.\n The Fusion Compiler tool does not perform any checking on the specified string.\n See Also  Using an IC Validator Parameter File"}
{"instruction": "How do I Adding Blockages", "input": "", "output": "By default, construction for relative placement aligns cells from their bottom-left corner.\n Compression removes empty space in rows to create a more compact structure.\n The columns are no longer aligned, and utilization is higher in the area of the compressed cells.\n  shows the same cells aligned with and without compression.\n To create the compressed structure shown in this example, first create three relative placement groups, rp1, rp2, and rp3, that contains a row of leaf cells.\n Then create a group, rp4, that contains all these groups.\n Example 45 shows the commands used to build the compressed structure.\n  Bottom-Left Alignment Construction and Compression rpA rpB rpC Without compression With compression U3 U6 U2 U5 U1 U4 U1 U2 U3 U6 U5 U4 rp3 rp2 rp1 rp4 Example 45 Compression With Hierarchical Relative Placement create_rp_group -name rp1 -columns 2 -rows 1 add_to_rp_group rp1 -cells U1 -column 0 -row 0 add_to_rp_group rp1 -cells U4 -column 1 -row 0  create_rp_group -name rp2 -columns 2 -rows 1 add_to_rp_group rp2 -cells U2 -column 0 -row 0 add_to_rp_group rp2 -cells U5 -column 1 -row 0  create_rp_group -name rp3 -columns 2 -rows 1 add_to_rp_group rp3 -cells U3 -column 0 -row 0 add_to_rp_group rp3 -cells U6 -column 1 -row 0        create_rp_group -name rp4 -columns 1 -rows 3 add_to_rp_group rp4 -rp_group rp1 -column 0 -row 0 add_to_rp_group rp4 -rp_group rp2 -column 0 -row 1 add_to_rp_group rp4 -rp_group rp3 -column 0 -row 2 Alternatively, you can apply compression in the horizontal direction by using the -tiling_type option with the set_rp_group_options command, as described in Controlling the Tiling Within Relative Placement Groups"}
{"instruction": "How do I Handling Fixed Cells During Relative Placement", "input": "", "output": "To prevent other relative placement groups from being placed close to a specific relative placement group, you can specify a keepout margin that applies only to other relative placement groups.\n To do so, use the -rp_only_keepout_margin option with the set_rp_group_options command.\n You can specify a different margin for the left, bottom, right, and top sides of the group.\n The following command applies a margin of 15 on the left and right and a margin of 10 on the top and bottom of the relative placement group named rp1: fc_shell> set_rp_group_options rp1 \\ -rp_only_keepout_margin {15 10 15 10}"}
{"instruction": "How do I Reporting the Wire Length", "input": "", "output": "To report statistics about the routing results, use the report_design -routing command.\n This command reports the following information:  Final wiring statistics, including the  Number of signal net shapes for each metal layer  Signal wire length for each metal layer  Number of PG net shapes for each metal layer  PG wire length for each metal layer  Horizontal and vertical wire distribution for each metal layer  Total number of signal net shapes for the block  Total signal wire length for the block  Final via statistics, including the  Simple vias used in each via layer, including the number of instances of each via  Double via conversion rate for each via layer  Double via conversion rate for the block  Custom vias used in the block, including the number of instances of each user- defined via By default, this command reports the information only for the top-level block.\n To report the information for the entire physical hierarchy, use the -hierarchical option.\n See Also  Defining Vias"}
{"instruction": "How do I Comparing Libraries", "input": "", "output": "To identify redundancies, outliers, equivalences, and gaps in cell families, perform the following steps: 1.\n Create a collection of the libraries you want to analyze, which is known as a libset, by using the create_libset command, as shown in the following example: fc_shell> create_libset -name myLibset -libs {abcd1 abcd2} When you perform library analysis, you can specify the libset name, instead of specifying a list of libraries.\n In addition, the tool uses the libset name, instead of using a list of the library names, when it generates reports, plots, and so on during analysis.\n You can include a cell library in more then one libset.\n 2.\n Set up the library analysis environment by using the create_environment command.\n       The create_environment command associates a predefined libset with a process label, process number, voltage, and temperature, which is then used for analyzing the libraries.\n You can specify the process label, process number, voltage, and temperature of the analysis environment by using one of the following methods:  Specify the process label, process number, voltage, and temperature values by using the -ppvt option, as shown in the following example: fc_shell> create_environment -name env1 -libset myLibset \\ -ppvt {{SS1p, 1.0, 0.6, 125}}  Specify a predefined operating condition from the reference library by using the -op_conds option, as shown in the following example: fc_shell> create_environment -name env2 -libset myLibset \\ -op_conds SS1p6125  Specify corners from which to infer the process label, process number, voltage, and temperature values by using the -corners option, as shown in the following example: fc_shell> create_environment -name env3 -libset myLibset \\ -corners nworst  Specify that the tool infers the process label, process number, voltage, and temperature values from the scenario and UPF associated with the current design by using the -auto_infer_design option, as shown in the following example: fc_shell> create_environment -name env4 -libset myLibset \\ -auto_infer_design 3.\n (Optional) Specify a naming convention for library cells and perform lexical classification by using lexical attributes.\n A library cell naming convention can help the tool identify cell families in the subsequent steps of the flow.\n For example, assume a cell in your library is named AND2D4WSULVT.\n This name is the concatenation of the AND2, D4, WS, and ULVT string, which represent cell characteristics that are associated to a lexical attribute, as shown in the following table: Table 2 Lexical Attributes for the Naming Convention of an Example Library Column Lexical Attribute Allowed Pattern Values for Example Cell  lexical_cell_prefix_name         Column Lexical Attribute Allowed Pattern Values for Example Cell  lexical_drive1_name    lexical_well_substrate_bias_arch itecure    lexical_device_threshold_voltage   You can specify the naming convention for this library by using the set_lib_cell_naming_convention command, as shown in the following example: fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 1 -pattern {[a-zA-Z0-9]*} -attribute \"lexical_cell_prefix_name\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 2 -pattern {D[0-9]*} -attribute \"lexical_drive1_name\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 3 -pattern {WS} -attribute \"lexical_well_substrate_bias_architecure\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 4 -pattern {ULVT|SVT|LVT|MVT} -attribute \"lexical_device_threshold_voltage\" Note: For a list of all the predefined lexical attributes, see the man page for the set_lib_cell_naming_convention command.\n After you specify the naming convention, perform lexical classification by using the classify_lib_cell_attributes command, as shown in the following example: fc_shell> classify_lib_cell_attributes -libset myLibset To report the lexical classification results, use the report_lib_cell_classification_attributes command.\n To report the library cells which are not lexical classified under the given naming convention, use the report_lexical_unclassified_lib_cells command.\n If there are lexically unclassified cells, specify additional naming conventions and rerun lexical classification.\n 4.\n (Optional) Exclude specific cells from library analysis by using the set_use_for_library_analysis command.\n       By default, the tool excludes physical-only cells during library analysis.\n However, you can exclude additional cells, such as library cells with a dont_use setting, as shown in the following example: fc_shell> set exclude_cells [filter_collection [get_lib_cells $libs/*]  \"dont_use == true\"] fc_shell> set_use_for_library_analysis $exclude_cells false To report the cells that are excluded, use the report_lexical_ignored_lib_cells command.\n 5.\n Identify all library cell families by using the identify_lib_cell_families command.\n To report the tool-identified library cell families, use the report_lib_cell_families -all_initial command.\n To manually define library cell families, use the define_lib_cell_family command.\n To report these user-defined cell families, use the report_lib_cell_families -all_userdef command.\n 6.\n Analyze the libraries by using the run_library_analysis command, as shown in the following example: fc_shell> run_library_analysis -name myLibset_run1 -environment env1 The name of the environment you specify with the -environment option must correspond to an environment you previously created by using the create_environment command.\n 7.\n Identify redundancies in library cell families by using the find_library_redundancies command, as shown in the following example: fc_shell> find_library_redundancies -analysis myLibset_run1 The name of the analysis you specify with the -analysis option must correspond to the name of an analysis you previously performed by using the run_library_analysis command.\n In addition to redundant cells, the find_library_redundancies command also identifies outlier and equivalent cells in library families.\n 8.\n Identify gaps in library cell families by using the find_library_gaps command, as shown in the following example: fc_shell> find_library_gaps -analysis myLibset_run1 The name of the analysis you specify with the -analysis option must correspond to the name of an analysis you previously performed by using the run_library_analysis command.\n       9.\n Report the library cell gaps, redundancies, outliers, and equivalence identified during library analysis by using the report_library_analysis command, as shown in the following example: fc_shell> report_library_analysis -analysis myLibset_run1 \\ -analysis_type redundancy -detail fc_shell> report_library_analysis -analysis myLibset_run1 \\ -analysis_type outlier -detail fc_shell> report_library_analysis -analysis myLibset_run1 \\ -analysis_type equivalent -detail fc_shell> report_library_analysis -analysis myLibset_run1 \\ -analysis_type gap -detail 10.\n Generate plots by using the gui_plot_lib_cells_attributes command, as shown in the following example: fc_shell> gui_plot_lib_cells_attributes -libset myLibset \\ -lib_cell_type all -x_attrib_name leakage -y_attrib_name avgsl_fall_delay \\ -chart_type scatter"}
{"instruction": "How do I Associating Power Strategies With Existing Multivoltage Cells", "input": "", "output": "Isolation cells are used to selectively shut off the input side of the voltage interface of a power domain; they do not shift the voltage.\n Isolation cells should be instantiated at the RTL level to prevent formal verification errors.\n However, you can also insert them using the Fusion Compiler tool.\n To insert isolation cells in the current block, use the create_mv_cells command.\n The tool inserts the isolation shifters using the cell mapping and strategy defined in the UPF specification.\n When the tool inserts an isolation cell, it sets a atributesize_onlysize_only atribute size_only attribute on the level shifter and a atributesdont_touchdont_touch atribute dont_touch attribute on the port-to-level-shifter net.\n By default, the command inserts cells only from the generic library.\n If you use the -mapped option with the create_mv_cells command, the command inserts cells only from the user-provided logic library and issues an error message if a suitable cell is not available."}
{"instruction": "How do I Creating Abstracts With Power Information", "input": "", "output": "In the Fusion Compiler tool, you can create an abstract view for a block at various stages of the design flow, such as, after synthesis, placement, optimization, clock tree synthesis, routing, and so on.\n Before you create the abstract view, ensure that the scenarios needed at the top level have been created and are active.\n To create an abstract view for top-level closure, use the create_abstract command.\n  To control the amount of timing information in the abstract view, use the -timing_level option with the following settings:  To create an abstract that contains only the boundary cells (one level of logic) connected to each boundary port, use the boundary setting.\n Such an abstract also contains feedthrough data paths, feedthrough combinational clock paths, and internal clock logic driving output ports.\n You can use this type of abstract to fix DRC violations at the top level.\n  To create a compact abstract that contains the timing information for only the critical setup and hold timings paths of the interface logic, use the compact setting.\n This is the default setting, and is preferred for top-level design closure.\n  To create an abstract with timing information for all the interface logic, use the full_interface setting.\n A full_interface abstract might be required for multiply-instantiated blocks (MIBs) when the constraints for MIB instances are different at the top-level.\n  To create the abstracts for lower-level blocks of the current block, use one of the following methods:  Create abstracts for specific lower-level blocks by using the -blocks option and specify the names of the blocks.\n  Create abstracts for all lower-level blocks by using the -all_blocks option.\n When you use the -blocks or -all_blocks option, if a specified block has an abstract view, by default, the tool does not re-create it.\n To force the tool to re-create the abstract, use the -force_recreate option.\n  To include specific nets, ports, hierarchical pins, or leaf pins in a placement or timing abstract, use the -include_objects option.\n        To create a read-only abstract, use the -read_only option.\n By default, the tool creates an editable abstract that you can modify in the context of a parent block.\n However, the tool makes the design view of the block read-only, preventing any changes from being made to the block.\n When you create the abstract view as read-only, the design view of the block is editable, and you can make changes to it.\n  To flatten the physical hierarchy and retain only the relevant logic of the lower-level blocks, use the -preserve_block_instances false option.\n  To specify if the abstract is going to be used for design planning or top-level implementation, use the -target_use planning or -target_use implementation option.\n Based on the intended usage, the tool applies appropriate internal settings to the abstract.\n  To specify host options for distributed processing, use -host_options option and specify the appropriate settings.\n You can use this option to reduce the runtime when you create more than one abstract view by using the -blocks or -all_blocks option."}
{"instruction": "How do I Linking to Abstract Views at the Top-Level", "input": "", "output": "To perform top-level routing, including virtual routing, every abstract view must have a corresponding frame view.\n To create a frame view, use the create_frame command, which extracts the blockage, pin, and via information from the design view.\n After you create the frame view, save the design library by using the save_lib command."}
{"instruction": "How do I Track Assignment", "input": "", "output": "By default, the global router ignores existing global routes.\n To perform incremental global routing by reusing the existing global routes, use the -reuse_existing_global_route true option when you run global routing.\n Note that this option affects only the global router and not the net delay calculation that occurs before timing-driven or crosstalk-driven global routing."}
{"instruction": "How do I Managing Constraint Groups", "input": "", "output": "To define a group of nets or pin-to-pin connections that must have the same wire length, use the create_wire_matching command.\n By matching wire lengths, you can minimize clock skews and timing violations.\n Two types of length-matching methods are available:  Pin-based: Wire lengths between pins must match.\n  and show examples of pin-based wire length matching.\n In both examples, all the receivers have a matched length constraint from driver D.\n  Net-based: Total length of wires of specified nets must match.\n  shows an example of net-based wire length matching.\n In the example, the total wire length of nets pr1 and pr2 are within the specified 10-um tolerance.\n  Receivers With Matched Wire Length From Driver D        Receivers With Matched Wire Length From Driver D'  Net-Based Wire Length Matching Syntax: fc_shell> create_wire_matching -for objects -tolerance float -match_type type -exclude objects -relative -driver pin or port -force intent_name Command or Options Description create_wire_matching  -for    -tolerance   -relative  -relative -tolerance  the -tolerance        Command or Options Description -match_type  length   length_per_layer -exclude   -driver  -force   intent_name      -force  After routing, a matched-length summary report is generated.\n You can use the report to troubleshoot and resolve the lengths that do not meet the set tolerance.\n The content of the report depends on the -for option that you specified:  If the -for option specifies pin-based length matching, a Length Constraint Report is generated.\n See  If the -for option specifies net-based length matching, a Routed Length report is generated.\n See  Pin-Based Length Matching Report        Net-Based Length Matching Report In Example 34, the commands create wire matching constraints for the scenario shown in.\n All receivers (R) have a matched length constraint from the balanced driver (D).\n Example 34 Wire Matching Constraints for All Receivers to Driver D fc_shell> create_wire_matching -for [get_nets -physical_context $net] \\ -relative -tolerance 0.01 -driver [get_pins -physical_context D/o1]  fc_shell> create_wire_matching -for [get_pins \\ -physical_context {R1/a R2/a R3/a}] -tolerance 0.01 -relative In Example 35, the commands create wire matching constraints for the example in  Example 35 Wire Matching Constraints for Receivers Connected to Driver D' fc_shell> create_wire_matching -for [get_nets -physical_context $net] \\ -tolerance 0.01 -relative -driver $driver_full_name  fc_shell> create_wire_matching \\ -for [get_pins -physical_context -of_objects $net] \\ -exclude $driver_full_name -tolerance 0.01 -relative In Example 36, the command creates a constraint group named abc for the nets and bundles that match the pattern pr*.\n An absolute tolerance of 10 um is set to match the length of each object in the constraint group.\n See Example 36 Wire Matching Constraints for pr* Nets fc_shell> create_wire_matching abc \\ -for [get_nets -physical_context pr*] \\ -tolerance 10 {abc} In Example 37, the command creates a constraint group named def for the nets and bundles that match the pattern pr*.\n A tolerance of 10 percent is set to match the length of each object in the constraint group.\n       Example 37 Wire Matching Constraints for pr* Nets With Percent Tolerance fc_shell> create_wire_matching def \\ -for [get_nets -physical_context pr*] \\ -tolerance 0.10 -relative {def} In Example 38, the command creates a wire matching constraint named clk_match for the clk net.\n Each of the endpoints that connect to the clk net has an individual matching constraint.\n Example 38 Wire Matching Constraints for the clk Net fc_shell> create_wire_matching -for [get_pins -physical_context \\ -of_objects [get_nets -physical_context {clk}]] \\ -tolerance 0.01 -match_type length_per_layer -relative \\ -force clk_match In Example 39, the command creates a wire matching constraint named clk_match2 for all the nets in the block, except the pins whose names match the keyword u1/a.\n Example 39 Wire Matching Constraints With Exclusions fc_shell> create_wire_matching -for $nets -tolerance 0.05 \\ -exclude [get_pins -physical_context u1/a] -force clk_match2 In Example 40, the script creates a net-based wire matching constraint for the scenario shown in.\n The cell instance has an o1 terminal that servers as an inverter pin, driving signals on net1 and net1p to the pins.\n The I2 and I3 pins are excluded from the length matching constraint.\n Example 40 Net-Based Wire Matching Constraints With Pin Exclusions # set nets to match length set nets [get_nets -physical_context {net1 net1p}] # set allowed layers set_routing_rule -min_routing_layer m7 -max_routing_layer m8 \\ -min_layer_mode allow_pin_connection -max_layer_mode hard $nets # setup Custom Router wire match constraint set exclude_pin_names [get_pins -physical_context {i2/a i3/a}] create_wire_matching -for $nets -tolerance 0.01 -relative \\ -exclude $exclude_pin_names # run Custom Router route_custom -nets $nets        Net-Based Wire Matching Constraints With Pin Exclusions"}
{"instruction": "How do I Specifying UPF Constraints for Physical-Only Cells", "input": "", "output": "The Early Data Check Manager allows you to check designs for power and multivoltage issues early in the design cycle.\n You can configure different error conditions in different ways.\n The tool provides comprehensive reports about the data checks.\n The general flow is as follows: 1.\n Use the set_early_data_check_policy command to define the global violation handling policy for data checks.\n 2.\n Proceed with your tool flow.\n The tool detects and responds to violations throughout the flow.\n 3.\n Use the report_early_data_checks command to obtain a report about all violations or specific data checks.\n 4.\n Use the get_early_data_check_records command to get a Tcl collection of violations that can be used in other commands.\n 5.\n Use the write_early_data_check_config command to save the settings in a file for future use.\n 6.\n Use the remove_early_data_check_records command to clear the data in preparation for another iteration.\n For more information about the Early Data Check Manager, see the Fusion Compiler Data Model User Guide For a list of power and multivoltage data checks, see the Fusion Compiler Multivoltage User Guide"}
{"instruction": "How do I Troubleshooting", "input": "", "output": "Use the add_group_repeaters and place_group_repeaters commands to display the summary report for variant cell swapping in command cell collections."}
{"instruction": "How do I Configuring Checkpoints", "input": "", "output": "To define a checkpoint, insert the eval_checkpoint command in your script.\n Specify a unique name for the checkpoint and wrap the checkpoint around a block of code.\n Note the following:  If you define a checkpoint with the same name as an existing checkpoint, the tool automatically adds a unique suffix to the name of the newly-defined checkpoint when it encounters the checkpoint in a run.\n For example, if you duplicate a checkpoint named placement, the tool renames the duplicate to placement2 when it encounters the checkpoint in a run.\n  Do not nest checkpoints within other checkpoints.\n Although the tool does not prevent you from defining nested checkpoints, the tool ignores nested checkpoints during a run, executing only the outermost checkpoints (note that the Tcl code inside nested checkpoints still runs, but runs as though the checkpoints do not exist).\n Suppose you have a simple place_opt script: open_block./design:init_design source./scripts/pre_place_opt_settings.tcl remove_buffer_trees -all place_opt -from initial_place -to initial_drc create_placement -incremental -timing_driven -congestion place_opt -from initial_drc -to -initial_opto place_opt -from final_place -to final_opto The following example wraps a checkpoint around each major step in your golden flow.\n The checkpoints are named remove_buffers, place_opt_to_initial_drc, incr_placement, place_opt_to_initial_opto, and place_opt_to_final_opto.\n open_block./design.nlib:init_design source./scripts/pre_place_opt_settings.tcl        eval_checkpoint remove_buffers { remove_buffer_trees -all   eval_checkpoint place_opt_to_initial_drc { place_opt -from initial_place -to initial_drc   eval_checkpoint incr_placement { create_placement -incremental -timing_driven -congestion   eval_checkpoint place_opt_to_initial_opto { place_opt -from initial_drc -to initial_opto   eval_checkpoint place_opt_to_final_opto { place_opt -from final_place -to final_opto  By default, the checkpoints run the Tcl code they enwrap.\n That is, simply inserting checkpoints with the eval_checkpoint command does not change your flow.\n However, you can associate these checkpoints with flow changes or reports that you want to run before, after, or in place of the code the checkpoints enwrap, as described in Configuring Checkpoints See Also  Configuring Checkpoints  Querying Checkpoints and Checkpoint Behaviors"}
{"instruction": "How do I Defining the Interclock Delay Balancing Constraints", "input": "", "output": "The Fusion Compiler tool can automatically balance the skew between a group of clocks.\n The set of clocks considered during delay balancing is referred to as a clock balance group.\n You can define multiple clock balance groups.\n For each clock balance group, you can define a delay offset between the clocks.\n Together, the clock balance groups and their delay offset settings are referred to as interclock delay balancing constraints Note: The tool cannot balance skew between a generated clock and other clocks.\n To balance multiple clocks, perform the following steps: 1.\n Generate the interclock delay balancing constraints either by Manually defining the clock balance groups and their delay offsets, as described in Defining the Interclock Delay Balancing Constraints Having the tool generate them as described in Generating Interclock Delay Balancing Constraints Automatically 2.\n Balance the interclock delays as described in Running Interclock Delay Balancing"}
{"instruction": "How do I Reporting Hotspots", "input": "", "output": "To generate voltage drop hotspots for a power or ground net, use the generate_hot_spots command.\n This command divides the whole chip area into multiple grids in terms of rows and columns, such as 100x100.\n The grid boxes are sorted by the maximum effective voltage drop value in each grid box and are indexed with an integer number starting from 0.\n To create rows based on standard cell site rows, and columns by using vertical PG straps on the lowest metal layers, use the -site_row option.\n Alternatively, you can specify the number of rows and columns for dividing the whole region with the -row and -column options.\n The command creates error cells for the grids with voltage drop values greater than the specified percentage of the ideal voltage drop.\n The default is 0.1, meaning that error cells are created for the grids with voltage drop values greater than 10% of the ideal supply voltage.\n The error cell type is effective voltage drop.\n To examine the content of the generated error cells, open the error cells in the error browser.\n To examine the content of the grid box, use the report_hot_spots command.\n To remove the generated voltage hotspot data from memory, use the remove_hot_spots command.\n       Option Description -net   -percentage   -site_row integer     -site_row  -site_row    -row  -column  See Also  Reporting Hotspots  Removing Hotspots  Voltage Hotspot Analysis Examples"}
{"instruction": "How do I Abutting Standard Cells With Specific Filler Cells", "input": "", "output": "a single constrained cell group, use the -constraint_fillers option.\n To specify the replacement cells, use the -non_constraint_fillers option.\n Note: If you use both methods, the command uses only the information specified in the -refill_table option.\n For example, the following command identifies and fixes maximum horizontal length violations where the horizontal length exceeds 70 um: fc_shell> replace_fillers_by_rules \\ -replacement_rule od_horiztonal_distance -max_constraint_length 70 \\ -refill_table { {{VT1_FILL1} {VT1_DECAP4 VT1_DECAP2}} {{VT2_FILL1} {VT2_DECAP4 VT2_DECAP2}} }  shows the original row, which violates this rule, and the resulting row, which uses unconstrained filler cells to fix the violation.\n The constrained cells are shown in red, while the unconstrained cells are shown in green.\n        Fixing a Maximum Vertical Edge Length Violation Maximum Stacking for Small Filler Cells Rule The maximum stacking for small filler cells rule restricts the stacking height of small filler cells.\n To select this rule, use the -replacement_rule small_filler_stacking option with the replace_fillers_by_rules command.\n When you select this rule, you must specify  The maximum stacking height To specify the maximum stacking height, use the -max_constraint_length option.\n  The cells considered for the check To specify the small filler cells to consider for the check, use the -small_fillers option.\n  The filler cells that can be used to replace the violating filler cells To specify the filler cells that can be used to replace the violating filler cells, use the -replacement_fillers option.\n If the vertical stacking of the small filler cells specified in the -small_fillers option exceeds the height specified in the -max_constraint_length option, the command replaces two or three consecutive filler cells adjacent to the violating cell with one of the large filler cells specified in the -replacement_fillers option.\n If it is not possible to replace the filler cells, the command reorders the filler cells to fix the violation.\n The command maintains the threshold voltage at the location of the violating filler cell; however, the threshold voltage at the location of non-violating filler cells might change.\n The command does not check for threshold voltage violations caused by these changes.\n       For example, the following command identifies and fixes maximum stacking violations where the stacking height exceeds 10 um: fc_shell> replace_fillers_by_rules \\ -replacement_rule small_filler_stacking -max_constraint_length 10 \\ -small_fillers {FILL1 FILL2} -replacement_fillers {FILL3 FILL4} \\ The following figures illustrate the replacement and reordering techniques used to fix maximum stacking violations.\n In these figures, the small filler cells are shown in red, while the large filler cells are shown in green.\n  shows the original stack, which violates this rule, and the resulting stack, which uses a replacement filler cell to fix the violation.\n  Fixing Maximum Stacking Violation by Replacement FILL1  vt1 FILL3 vt1 FILL1  vt1 FILL2  vt1 FILL1  vt1 FILL1  vt1 FILL1  vt1 10um FILL2  vt1  shows the original stack, which violates this rule, and the resulting stack, which uses reordering to fix the violation.\n Note that the threshold voltage of the FILL3 cell is changed so that the vt2 threshold voltage is maintained in the original location.\n  Fixing Maximum Stacking Violation by Reordering FILL1  vt1 FILL1  vt1 FILL2  vt1 10um FILL2  vt1 FILL3 vt1 FILL3 vt2 FILL2  vt2 FILL2  vt2       Maximum Vertical Edge Length Rule The maximum vertical edge length rule restricts the vertical length of a stack of certain filler cells and standard cells.\n To select this rule, use the -replacement_rule max_vertical_constraint option with the replace_fillers_by_rules command.\n When you select this rule, you must specify  The maximum vertical length To specify the maximum vertical length, use the -max_constraint_length option.\n  The cells considered for the check These cells are called constrained cells.\n To specify the constrained filler cells, use the -constraint_fillers option.\n By default, all standard cells are constrained.\n To exclude specific standard cells from consideration, use the -exception_cells option.\n To ignore the length violation when a constrained cell abuts an exception cell, use the -no_violation_along_exception_cells option.\n  The filler cells that can be used to replace the violating filler cells These filler cells are called unconstrained cells ; they are not considered when measuring the stack height.\n  To specify the filler cells that can be used to break a continuous edge of constrained cells, use the -non_constraint_fillers option.\n  To specify the filler cells whose left side can be used to break a continuous edge of constrained cells, use the -non_constraint_left_fillers option.\n  To specify the filler cells whose right side can be used to break a continuous edge of constrained cells, use the -non_constraint_right_fillers option.\n Note: Any filler cells that are not identified as constrained, unconstrained, or exception cells are considered constrained cells.\n For example, the following command identifies and fixes maximum vertical edge length violations where the vertical length exceeds 180 um: fc_shell> replace_fillers_by_rules \\ -replacement_rule max_vertical_constraint -max_constraint_length 180 \\ -constraint_fillers {DECAP8 DECAP4} -non_constraint_fillers {FILL1} \\ -exception_cells {BUF1 TAP1}  shows the original stack, which violates this rule, and the resulting stack, which uses unconstrained filler cells to fix the violation.\n The constrained cells are shown in red, while the unconstrained cells are shown in green.\n        Fixing a Maximum Vertical Edge Length Violation Tap Cell Spacing Rule The tap cell spacing rule restricts the distance between a target tap cell's oxide diffusion (OD) layer and the OD layers of its neighboring cells on the left and right sides.\n To select this rule, use the -replacement_rule od_tap_distance option with the replace_fillers_by_rules command.\n When you select this rule, you must specify  The disallowed spacing range The spacing distance is specified as the number of sites.\n To specify the disallowed spacing range, use the -tap_distance_range option.\n  The tap cells considered for the check These cells are called target cells.\n To specify the target tap cells, use the -tap_cells option.\n  The tap cells used to replace the violating tap cells These cells are called replacement cells  To specify the replacement tap cell for left-side violations, use the -left_violation_tap option.\n  To specify the replacement tap cell for right-side violations, use the -right_violation_tap option.\n  To specify the replacement tap cell for both-side violations, use the -both_violation_tap option.\n Note: The target and replacement tap cells must all have the same size.\n       By default, the command measures the distance to the cells that abut the target tap cell.\n If the tap cells abut library cells that do not have an OD layer, use the -adjacent_non_od_cells option to specify these library cells.\n The command skips the specified non-OD cells and measures to distance from the target tap cell to the first neighbor with an OD layer.\n For example, the following command replaces tap cells named TAP that have a separation distance of three sites: fc_shell> replace_fillers_by_rules -replacement_rule od_tap_distance \\ -tap_cells {TAP} -tap_distance_range {3 3} \\ -left_violation_tap {TAPl} -right_violation_tap {TAPr} \\ -both_violation_tap {TAPb}  shows the original row, which violates this rule, and the resulting row, which uses the replacement filler cells to fix the violations.\n The target filler cells are shown in red, while the replacement filler cells are shown in green.\n  Fixing a Tap Cell Spacing Violation Random Filler Cell Replacement To randomly replace existing filler cells, use the -replacement_rule random option with the replace_fillers_by_rules command.\n When you select this rule, you must specify  The filler cells to be replaced and their replacements The filler cells to be replaced are called target cells.\n To specify the target filler cells and their replacements, use the following syntax with the -random_replace option: { { target_cell refill_cell 1a refill_cell 1b...}}...\n } To ignore filler cells that have a physical_status attribute of fixed or locked"}
{"instruction": "How do I Controlling Via Ladder Connections", "input": "", "output": "By default, the wire is tapered to the default routing width, which is the routing width that is defined for the metal layer in the technology file.\n  To taper the wire to the pin width rather than the default routing width, change the route.detail.pin_taper_mode application option to pin_width from its default of default_width before you perform detail routing.\n  To enable pin tapering only when required to avoid DRC violations, set the route.detail.use_wide_wire_effort_level application option to either low or high This setting improves the nondefault via rate at a cost of longer runtime.\n You should use this option only when the majority of pins on the nets being routed are accessible with nondefault vias.\n  To disable tapering for certain types of pins, set one or more of the following application options to true route.detail.use_wide_wire_to_input_pin route.detail.use_wide_wire_to_output_pin route.detail.use_wide_wire_to_macro_pin route.detail.use_wide_wire_to_pad_pin, and route.detail.use_wide_wire_to_port  To disable tapering for all pins, set the -taper_distance option to 0 when you create the nondefault routing rule with the create_routing_rule command."}
{"instruction": "How do I Comparing QoR Data", "input": "", "output": "To measure the quality of results (QoR) of the design in the current state and store the quality information in a set of report files, use the create_qor_snapshot command.\n You can capture the QoR information using different optimization strategies or at different stages of the design and compare the quality of results.\n The create_qor_snapshot command measures and reports the quality of the design in terms of timing, design rules, area, power, congestion, and so on.\n It stores the quality information into a set of snapshot files in a directory named snapshot under the current working directory.\n To store the quality information in a different directory, use the time.snapshot_storage_location application option.\n The following example generates a QoR snapshot named r1 using zero wire load for timing paths in the snapshot directory: fc_shell> create_qor_snapshot -name r1 -zero_wire_load...\n No.\n of scenario = 4 s1 = function s2 = function_min s3 = test s4 = test_min         WNS of each timing group: s1 s2 s3 s4  REGIN -0.34 - - - fref_clk0 2.34 3.25 - - pld_clk 0.65 1.65 - - pll_fixed_clk_central 0.61 1.65 - - REGOUT 0.39 0.44 1.18 1.72...\n  Setup WNS: -0.34 0.44 1.02 1.10 -0.34 Setup TNS: -130.82 0.00 0.00 0.00 -130.82 Number of setup violations: 1152 0 0 0 1152 Hold WNS: -0.55 -0.78 -0.30 -1.28 -1.28...\n  Area: 1200539.606 Cell count: 562504 Buf/inv cell count: 145920 Std cell utilization: 0.24...\n Querying QoR Snapshots To retrieve and report the snapshots created by the create_qor_snapshot command, use the query_qor_snapshot command.\n The query_qor_snapshot command analyzes the QoR timing reports previously generated and displays the results in HTML format according to the specified filters.\n In this example, the query_qor_snapshot command analyzes the placeopt snapshot that is stored in the snapshot directory and then reports the timing paths with worst negative slack values between -2.0 ns and -1.0 ns: fc_shell> create_qor_snapshot -name placeopt fc_shell> query_qor_snapshot -name placeopt -filters \"-wns -2.0,-1.0\" Reporting QoR Snapshots To display QoR information and statistics for the current design, use the report_qor command.\n For example, the following command displays the r1 QoR snapshot that is stored in the snapshot directory: fc_shell> report_qor_snapshot -name r1 -display Removing QoR Snapshots To remove an existing QoR snapshot from the directory specified by the time.snapshot_storage_location application option, use the remove_qor_snapshot       command.\n For example, the following command removes the preroute QoR snapshot that is stored in the snapshot directory: fc_shell> remove_qor_snapshot -name preroute"}
{"instruction": "How do I Setting the Clock-Gating Style", "input": "", "output": "The clock gating feature only supports latch-based integrated clock-gating cells.\n Tool- inserted clock-gating cells do not have a wrapper hierarchy.\n       Clock-gating optimization uses the following defaults:  The maximum fanout is infinite.\n  The minimum bitwidth is three.\n  A latch-based precontrol cell for both positive-edge and negative-edge triggered flip- flops is used as a clock-gating cell.\n If only positive-edge or negative-edge clock-gating cells are available, the tool inserts inverters on both sides of the clock-gating cell.\n You must load a design and libraries before you can set up clock-gating constraints.\n To set up clock gating, follow these steps: 1.\n Load the design and libraries 2.\n Set the style by using the set_clock_gate_style command 3.\n Set the options by using the set_clock_gating_options command 4.\n Specify objects to be included or excluded from clock gating by using the set_clock_gating_objects command 5.\n Allow other clock-gating cell types for optimization by using the following command: fc_shell> set_lib_cell_purpose -include cts [get_lib_cells...] For more information, see the following topics:  Setting the Clock-Gating Style  Setting Clock-Gating Options  Enabling or Disabling Clock Gating on Design Objects  Clock-Gating Enable Source Selection  User-Driven Enable Exclusion  Report Clock Gating Options"}
{"instruction": "How do I Controlling Tool-Inserted Clock-Gate Levels", "input": "", "output": "The default of the compile.clockgate.max_number_of_levels application option is 0, which allows an infinite number of levels.\n You can set the compile.clockgate.max_number_of_levels application option without enabling clock-gate level expansion.\n In this case, the tool collapses clock-gate levels if the specified limit is less than the number of clock-gate levels that exist after the initial_map stage.\n The tool collapses clock-gate levels as follows:  The tool first tries to collapse clock gates at levels higher than the specified maximum level, starting from the clock gates at the highest level (closest to the registers).\n  If a specific clock gate cannot be collapsed, the tool tries to collapse upstream clock gates until it reaches the lowest level (closest to the source).\n The collapse can occur across hierarchies.\n The tool tries to preserve the parent (upstream) clock gate.\n  The tool removes all remaining clock gates that do not meet the maximum number of levels."}
{"instruction": "How do I Performing Postroute Optimization Using the hyper_route_opt", "input": "", "output": "Before performing postroute optimization, ensure that the block is fully routed and legalized and does not have excessive logical or routing DRC violations.\n  To verify the legality of the design, use the check_legality command.\n  To verify the routing, use the check_routes command.\n To perform postroute logic optimization, 1.\n Update the clock latency by using the compute_clock_latency command.\n 2.\n Run the route_opt command two times.\n The legalization and ECO routing performed by the first route_opt run might impact the block timing, which is then optimized by the second route_opt run.\n As the number of changes during postroute logic optimization decreases, the timing improvement from subsequent route_opt runs decreases.\n The route_opt command performs the following tasks: 1.\n Performs extraction and updates the timing By default, the route_opt command uses the PrimeTime delay calculation engine, which requires that you use cell libraries generated by O-2018.06 or later versions of the Library Manager tool.\n Before using PrimeTime delay calculation, use the check_consistency_settings command to verify that the Fusion Compiler and PrimeTime settings are consistent, as described in Checking for Consistency in Timing Analysis and Extraction Settings in the Fusion Compiler Timing Analysis User Guide The route_opt command supports both graph-based analysis (GBA), the default, and path-based analysis (PBA).\n To enable path-based optimization, use the time.pba_optimization_mode application option.\n  To use path-based timing for the worst path of each endpoint, which is identified by using graph-based analysis, set this application option to path This is the recommended setting because it provides the best tradeoff between runtime and QoR.\n  To use exhaustive path-based search algorithms for each endpoint, set this application option to exhaustive Using this setting increases the runtime.\n       2.\n Performs the enabled optimizations By default, the route_opt command optimizes for setup, hold, area, and logical DRC violations for the data paths in the block.\n To enable additional optimizations, set the application options shown in Table 33 before running the route_opt command: Table 33 Application Options to Enable route_opt Optimizations Optimization Application option settings    route_opt.flow.enable_ccd true  route_opt.flow.enable_cto true  route_opt.flow.enable_clock_power_recovery area  route_opt.flow.enable_clock_power_recovery power  route_opt.flow.enable_power true   route_opt.flow.enable_ml_opto true time.pba_optimization_mode path   set_app_options -name route_opt.flow.enable_irdrivenopt -value true set_app_options -name opt.common.ir_drop_threshold -value 10 set_app_options-name opt.common.power_integrity-value true   set_app_options -name ccd.ccd_opt_voltage_drop_enable_ml_predictor -value true Note: The route_opt command honors only the route_opt application options; it does not honor the settings of the opt application options, except the opt.common.allow_physical_feedthrough application option.\n 3.\n Legalizes the block 4.\n Performs ECO routing 6.\n For information about setting application options to control concurrent clock and data optimization, see Controlling Concurrent Clock and Data Optimization 7.\n This optimization is supported only when concurrent clock and data optimization is not enabled.\n 8.\n The type of power optimization performed depends on scenario configuration.\n Total power optimization is performed if there is an active scenario with both leakage power and dynamic power enabled.\n Leakage power optimization is performed if there is an active scenario with only leakage power enabled.\n       By default, the route_opt command performs five detail routing iterations during the ECO routing phase.\n To change the number of iterations, use the route.detail.eco_max_number_of_iterations application option.\n To perform track assignment instead, set the route_opt.eco_route.mode application option to track If the PrimeTime tool reports setup or hold violations during signoff timing analysis, you can use the following process to fix these violations: 1.\n Specify the target endpoints and their PrimeTime timing information by using the set_route_opt_target_endpoints command with the -setup_timing and -hold_timing options.\n 2.\n Optimize the endpoints by using the route_opt command.\n 3.\n Remove the adjusted timing information by using the set_route_opt_target_endpoints -reset command.\n You can also use this process to perform incremental optimization on specific endpoints.\n In this case, use the -setup_endpoints -hold_endpoints, and -ldrc_objects options to specify the endpoints.\n To close the final setup, hold, or logical DRC violations with minimal disturbance to the block, use size-only mode when you run the route_opt command.\n To enable size- only mode, set the route_opt.flow.size_only_mode application option to one of the following values:  true_footprint, which allows resizing a cell only to a library cell that is an exact physical match, as determined by the tool after analysis This ensures that legalization or ECO routing is not required after resizing  footprint, which allows resizing a cell only to a library cell that has the same footprint attribute in the library  equal, which allows resizing a cell only to a library cell that is equal in size  equal_or_smaller, which allows resizing a cell only to a library cell that is equal or smaller in size To disable size-only mode, set the route_opt.flow.size_only_mode application option to none or \"\", which is the default."}
{"instruction": "How do I Checking Signoff Design Rules Interactively in the GUI", "input": "", "output": "The signoff_fix_drc command writes a summary report file, result_summary.rpt, to the current working directory.\n Example 30 shows an example of this report.\n Example 30 result_summary.rpt Example Input DRC Error Database............\n./inputs/signoff_check_drc_run Limit to Cell.......................\n my_design Maximum Errors/Command..............\n 1000        SIGNOFF AUTOFIX DRC SUMMARY: : : : : REMAINING : : : TOTAL : PROCESSED : TARGETED : TARGETED : FIX : DRC NAME : DRC : DRC : DRC : DRC : RATE : COMMENT M0.W.3 : Maximum width : 1 : 1 : 0 : 0 :  : 1 ignored: <#1> M4.L.3 : Edge length wi : 4 : 4 : 0 : 0 :  : 4 ignor ed: <#2> M4.L.5:2 : Edge length : 2 : 2 : 0 : 0 :  : 2 ignored: <#2> M4.W.3 : Maximum width : 1 : 1 : 0 : 0 :  : 1 ignored: <#1> VIA1.S.10.3 : Space of : 10 : 10 : 2 : 0 : 100.0% : 8 ignored: <#2> TOTAL : : : 2 : 0 : 100.0% :  NOTE: Reasons : Detail Info #1 : size of error shapes greater than threshold #2 : shapes associated with error are not Zroute modifiable The following table describes each of the columns in the summary report: Table 45 Definitions of Columns in result_summary.rpt Column heading Description     signoff_check_drc"}
{"instruction": "How do I ECO-Aware Removal of PG Augmentation Shapes", "input": "", "output": "To remove PG augmentation shapes, use the -mode remove option with the signoff_create_pg_augmentation command.\n When you use this command, the IC Validator tool removes all PG augmentation shapes from the current block."}
{"instruction": "How do I Deriving Metal Cut Routing Guides", "input": "", "output": "When you reuse hard macros, such as IP blocks, at a smaller technology node, it can cause routability issues to the macro pins.\n You can use routing guides and blockages to enable access to the hard macro pins.\n To create these routing guides, which are referred to as pin access routing guides, and the associated blockages, use the derive_pin_access_routing_guides command.\n You must specify the following information:  The hard macros around which to create the routing guides and routing blockages (the -cells option)  The metal layers for which to create the routing guides and routing blockages (the -layers option)  The width of the routing guides and blockages in the x- and y-directions (the -x_width and -y_width options) For each specified hard macro, the command creates routing guides to enable routing to the pins on the specified layers.\n In addition, the command creates metal blockages that surround the macro on the specified layers, with cutouts to allow pin access, and via blockages that cover the macro on the via layers adjacent to the specified metal layers.\n Vertical routing guides and blockages have the specified x-width.\n Horizontal routing guides and blockages have the specified y-width.\n By default, the command creates pin cutouts only in the preferred routing direction.\n To also create pin cutouts in the nonpreferred direction, use the -nonpreferred_direction option.\n To extend the pin cutouts beyond the metal blockages, use the -pin_extension option to specify the extension distance in microns.\n To enable proper routing to pins of different sizes, the command sets the is_rectangle_only_rule_waived attribute on the macro pins to true to waive the rectangle-only rule.\n For example, the following command creates the routing guides and blockages shown in  fc_shell> derive_pin_access_routing_guides -cells myMacro \\ -layers {M2 M3} -x_width 0.6 -y_width 0.7 In the figure, the two red pins are on M2 and the green pin is on M3.\n The figure shows the routing guides created on both the M2 and M3 layers, the metal blockages created on the M3 layer, and the via blockage created on the V2 and V3 layers.\n The tool also creates metal blockages on the M2 layer, which have cutouts for the M2 pins, and via blockages on the V1 and V2 layers.\n        Pin Access Routing Guides and Blockages If a hard macro already has pin access routing guides and their associated blockages, the command removes these existing objects and creates new routing guides and blockages.\n If you move a hard macro after deriving the pin access routing guides, you must regenerate the pin access routing guides.\n You cannot manually modify pin access routing guides."}
{"instruction": "How do I Defining Via Ladder Constraints", "input": "", "output": "To use the constraint-based method to insert via ladders, 1.\n Ensure that the via ladder rules are defined, as described in Defining Via Ladder Rules 2.\n Define the via ladder constraints by using the set_via_ladder_rules command, as described in Defining Via Ladder Constraints 3.\n Insert the via ladders by using the insert_via_ladders command, as described in Inserting Via Ladders 4.\n Verify the via ladders by using the verify_via_ladders command, as described in Verifying Via Ladders"}
{"instruction": "How do I Generating Verification Checkpoints During Compilation", "input": "", "output": "Fusion Compiler can perform compile prechecks to prevent compile failures and allow the compile_fusion command to quickly exit with error messages when incomplete (or inconsistent) setup or constraint issues are encountered.\n This capability is on by default, but these checks can also be enabled by the compile_fusion -check_only command.\n The compile_fusion command and the compile_fusion -check_only command perform the prechecks listed in the following table and issue error messages accordingly.\n A check mark (X) indicates the precheck is performed.\n Table 19 Prechecks Performed During the Compile Flow Code Message Default compile prechecks compile_fusion -check_only      Cell     Cell      Cell         hierarchy                     Macro     port     Pad port              Layer         Table 19 Prechecks Performed During the Compile Flow (Continued) Code Message Default compile prechecks compile_fusion -check_only           check_duplicates    Compile Precheck Examples The following two examples show the compile logs with and without compile prechecks.\n When the -check_only option is set, the compile_fusion command issues error messages for any failed precheck condition.\n Example 14 Compile Log With Compile Prechecks Begin compile flow (FLW-3001) ->Compile in progress: 1.0% (FLW-3778) Error: Layer M1 does not have a preferred direction (OPT-1008)...\n Error: Layer MRDL does not have a preferred direction (OPT-1008)...\n Warning: Technology layer 'M1' setting 'routing-direction' is not valid (NEX-001)...\n Warning: Technology layer 'MRDL' setting 'routing-direction' is not valid (NEX-001) Example 15 Compile Log Without Compile Prechecks End logic optimization (FLW-3001) ->Compile in progress: 27.0% (FLW-3778) ->Compile in progress: 28.0% (FLW-3778) ->Compile in progress: 28.5% (FLW-3778) Information: Corner max_corner: no PVT mismatches.\n (PVT-032) Warning: Technology layer 'M1' setting 'routing-direction' is not valid (NEX-001)...\n Warning: Technology layer 'MRDL' setting 'routing-direction' is not valid (NEX-001) Information: Design Average RC for design top (NEX-011)"}
{"instruction": "How do I Defining an Inner Keepout Margin", "input": "", "output": "You can define an outer keepout margin on a hard macro, a hierarchical cell, or a leaf cell.\n When you define an outer keepout margin, you can either specify the keepout distance explicitly or, for hard macros, you can have the tool derive the keepout distance based on the macro pin count.\n To explicitly specify an outer keepout margin, use the -outer option to specify the margin distance for each side.\n You specify the left, bottom, right, and top margins using the following format: lx by rx ty.\n A value of 0 results in no keepout margin for that side.\n For example, to create a hard outer keepout margin with a margin of 10 on each side for a macro named my_macro, use the following command: fc_shell> create_keepout_margin -outer {10 10 10 10} my_macro To have the tool derive the outer keepout distance for a hard macro based on its pin count, use the -tracks_per_macro_pin option to specify the track-to-pin ratio.\n When you use this option, the tool calculates the keepout margin from the track width, the number of macro pins, and the specified track-to-pin ratio, which is typically set to a value near 0.5.\n A larger value results in larger keepout margins.\n The derived keepout margin is always hard; the -type setting is ignored.\n The derived margins are subject the minimum and maximum       values specified by the -min_padding_per_macro and -max_padding_per_macro options.\n For example, to have the tool derive the outer keepout margin for a macro named my_macro by using a track-to-pin ratio of 0.6 with a minimum keepout distance of 0.1 and a maximum keepout distance of 0.2, use the following command: fc_shell> create_keepout_margin -tracks_per_macro_pin 0.6 \\ -min_padding_per_macro 0.1 -max_padding_per_macro 0.2 my_macro"}
{"instruction": "How do I Fixing Multivoltage Violations", "input": "", "output": "add_group_repeaters ECO-276 Warning: Unable to find free area for repeater on net (%s) at location (%s) within range {%s}.\n      set_repeater_group ECO-277 Warning: You are trying to override cells of repeater group %d.\n Use -override if you have to override cells.\n    -override  create_repeater_gro ups ECO-278 Error: Get 0 transparent cells from all supernets.\n     create_repeater_gro ups ECO-279 Error: %s has multiple(%lu) inputs."}
{"instruction": "How do I Removing Hotspots", "input": "", "output": "After you generate hotspots on the block using the generate_hot_spots command, run the report_hot_spots command to report cells with voltage-related cell attributes for a hotspot grid box.\n The voltage-related cell attributes are: current overlap_current effective_resistance timing_ window slack output_load and static_power Use these voltage-related cell attributes to identify root causes of voltage violations.\n For example, you can  Use the current and output loading capacitance information to determine if a cell's high peak is caused by too much load.\n If this is the case, splitting the output might be an effective approach to reduce high voltage drops.\n  Check and compare the timing windows in the current hotspot to those of the neighboring grids to determine if a cell can be moved to other grids (targets).\n Doing so reduces the voltage drop of the original grid without increasing the voltage drop of the target grid.\n        Compare slew and load capacitance among all cells in the region to determine if high peak current is caused by sharp slew or large load capacitance.\n  Find cells that have overlapping timing windows to identify cells that might switch at the same time during dynamic rail analysis.\n Here are commonly used options of the report_hot_spots command: Option Description -index  -summary     -object   -verbose  Listing Voltage Grid Boxes by Index By default, the tool searches three rows up and down and three columns left and right to determine the neighboring grids of the current one.\n Use the -index option to specify the index used to report voltage hotspot grids.\n The grid box with the smallest index number has the highest effective voltage drop values.\n This allows you to determine the size of the hotspot area.\n For example, if all the surrounding grids have bigger index numbers and only a few central grids have much smaller numbers, this voltage drop hotspot is assumed to be an island.\n Accumulating Current For an instance in a grid box, the overlap current is estimated by accumulating the timing- window-based current across its neighboring cells.\n The command scans all timing windows from smallest to biggest, and accumulates current on the way as time passes by.\n Use the information to determine how many cells are switching simultaneously and how much time shift to apply when shifting timing windows for voltage drop reduction.\n       Assume the design has three instances with current as follows: As shown in the following report, the accumulated current on point 2.0 is from inst2 and inst3, and does not include the current from inst1.\n inst1 [0.0 2.0] current 0.11 inst2 [1.0 2.5] current 0.22 inst3 [2.0 3.0] current 0.33 See Also  Generating Hotspots  Removing Hotspots  Voltage Hotspot Analysis Examples"}
{"instruction": "How do I Enabling Enhanced Delay Optimization to Improve Total Negative", "input": "", "output": "In most cases, you can achieve better timing QoR by enabling high-effort timing mode for compile.\n To enable this mode, set the compile.flow.high_effort_timing application option to Running high-effort timing mode during compile can increase runtime because the tool performs multiple passes of placement and optimization to obtain optimal timing results."}
{"instruction": "How do I Verifying Routing Corridors", "input": "", "output": "To assign nets and supernets to a routing corridor, use the add_to_routing_corridor command.\n You must specify the routing corridor and the nets or supernets to add to it.\n You can assign a net or supernet to only one routing corridor and that routing corridor must cover all pins connected to the associated nets.\n In addition, the supernets must belong to the same block as the routing corridor.\n For example, to define a routing corridor named corridor_a and assign the nets named n1 and n2 to this routing corridor, use the following commands: fc_shell> create_routing_corridor -name corridor_a \\ -boundary { {10 10} {20 35} } \\ -min_layer_name M2 -max_layer_name M4 fc_shell> create_routing_corridor_shape -routing_corridor corridor_a \\ -boundary { {20 25} {40 35} } \\ -min_layer_name M2 -max_layer_name M4 fc_shell> create_routing_corridor_shape -routing_corridor corridor_a \\ -boundary { {40 10} {50 35} } \\ -min_layer_name M2 -max_layer_name M4 fc_shell> add_to_routing_corridor corridor_a [get_nets {n1 n2}]       Note: You can also assign nets or supernets to the routing corridor by using the -object option when you use the create_routing_corridor command to create the routing corridor."}
{"instruction": "How do I Fixing Missing Vias", "input": "", "output": "RedHawk or RedHawk-SC Fusion saves the checking error data file, named missing_via.\n supplyNetName.err, in the working directory.\n The tool generates one error data file for each supply net.\n Before you quit the current session, be sure to save the block to keep the generated error data in the block.\n To open an error data file in the error browser, choose File > Open...\n from the Error Browser dialog box, and then select the error data file to open in the Open Error Data dialog box (see ).\n For more information about the error browser, see \"Using the Error Browser\" in the Fusion Compiler Graphical User Interface User Guide        Opening Error Data from the Error Browser See Also  Specifying RedHawk and RedHawk-SC Working Directories"}
{"instruction": "How do I Foundry Fill and Track Fill Combined for Different Flows of", "input": "", "output": "If you have already used the signoff_create_metal_fill command to performed metal fill insertion on a block, you can use incremental metal fill insertion to perform the following tasks:  Insert additional metal and via fill to specific layers or regions, as described in Adding to Existing Metal and Via Fill  Replacing existing metal and via fill in specific locations, as described in Replacing Existing Metal and Via Fill  Replacing existing metal and via fill in modified regions, as described in Performing Metal Fill Insertion Only in Modified Regions Adding to Existing Metal and Via Fill To insert metal and via fill without first removing the existing metal fill, use the -mode add option with the signoff_create_metal_fill command.\n When you use this mode, you can control where the metal fill insertion occurs by using one or more of the following options:  -select_layers, which restricts the layers on which to perform metal fill insertion, as described in Specifying the Layers for Metal Fill Insertion  -coordinates or -excluded_coordinates, which restrict the regions on which to perform metal fill insertion, as described in Specifying the Regions for Metal Fill Insertion When you use these options with the -mode add option, the tool does not fix density errors regardless of the setting of the signoff.create_metal_fill.fix_density_errors application option.\n See Also  Pattern-Based Metal Fill Insertion  Track-Based Metal Fill Insertion Replacing Existing Metal and Via Fill To remove and insert metal and via fill only for the specified locations, use the -mode replace option with the signoff_create_metal_fill command.\n You would typically       use this mode after ECO changes.\n When you use this mode, you must also specify one or more of the following options:  -select_layers, which restricts the layers on which to perform metal fill insertion For example, to remove all metal fill on the M1 and M3 layers and refill those two layers without affecting the metal fill on other layers, use the following command: fc_shell> signoff_create_metal_fill -mode replace \\ -select_layers {M1 M3} For more information about this option, see Specifying the Layers for Metal Fill Insertion  -coordinates or -excluded_coordinates, which restrict the regions on which to perform metal fill insertion For more information about these options, see Specifying the Regions for Metal Fill Insertion  -nets or -timing_preserve_setup_slack_threshold, which restricts the nets on which to perform metal fill insertion For more information about these options, see Timing-Driven Metal Fill Insertion See Also  Pattern-Based Metal Fill Insertion  Track-Based Metal Fill Insertion  Timing-Driven Metal Fill Insertion Performing Metal Fill Insertion Only in Modified Regions If you have previously run the signoff_create_metal_fill command on a block, you can use the -auto_eco true option to restrict the metal fill insertion to those regions of the block that have been modified since the last run.\n Note: You can use this option only if the percentage of change to the block since the previous signoff_create_metal_fill run is less than the change threshold.\n The default change threshold is 20 percent; to modify the change threshold, set the signoff.create_metal_fill.auto_eco_threshold_value application option.\n When you use the -auto_eco true option, the tool automatically determines the modified regions and layers, and then removes the existing metal fill and redoes metal fill insertion only in those locations; all other existing metal fill is retained.\n       By default, the tool compares the current block to the version used for the previous signoff_create_metal_fill run.\n To compare the current block to a different block, use the -pre_eco_design option to specify the comparison block.\n Note: A change on a metal layer triggers metal fill removal and insertion on the adjacent via layers to ensure that no floating or hanging vias remain when the metal shapes are removed from the ECO area.\n If a cell instance changes, the tool takes a conservative approach and considers changes on all layers.\n For example, to remove the metal fill from the regions modified since the last time you ran the signoff_create_metal_fill command and then fill only those regions using the pattern-based mode, use the following command: fc_shell> signoff_create_metal_fill -auto_eco true You can use the -select_layers option to specify the layers on which to perform metal fill insertion; however, the tool performs metal fill insertion on a specified layer only if it is one of the automatically detected changed layers.\n You cannot use the -coordinates or -excluded_coordinates options to restrict the metal fill insertion regions.\n When you use the -auto_eco true option,  You cannot use the -mode -all_runset_layers, and -report_density options  The tool does not insert dense fill on the double-patterning layers regardless of the setting of the -fill_all_tracks option  The tool does not fix density errors regardless of the setting of the signoff.create_metal_fill.fix_density_errors application option See Also  Pattern-Based Metal Fill Insertion  Track-Based Metal Fill Insertion  Timing-Driven Metal Fill Insertion"}
{"instruction": "How do I Adding Leaf Cells", "input": "", "output": "After you create a relative placement group by using the create_rp_group command, you can add the following types of objects to it by using the add_to_rp_group command:  Leaf cells, as described in Adding Leaf Cells  Leaf cells, as described in Adding Hard Macro Cells  Relative placement groups, as described in Adding Relative Placement Groups  Blockages, as described in Adding Blockages When you add an object to a relative placement group,  The relative placement group to which you are adding the object must exist.\n  The object must be added to an empty location in the relative placement group.\n  Only one object can be added in one location of relative placement group.\n To remove objects from a relative placement group, use the remove_from_rp_group command.\n You can remove leaf cells ( -cells ), relative placement groups ( -rp_group ), and blockages ( -blockage ).\n When you remove objects from a group, the space previously occupied by the removed objects is left unoccupied."}
{"instruction": "How do I Specifying Naming Styles for Multibit Registers", "input": "", "output": "Before you run the compile_fusion command on an RTL design, you can create a multibit bus consisting of specific unmapped single-bit registers by using the create_multibit command.\n Then, when you run the compile_fusion command,       during the initial_map stage, the tool maps the single-bit cells of the bus created by the create_multibit command into a multibit register bank.\n The single-bit registers you specify with the create_multibit -sort command must  Be unmapped  Be of the same type  Be in the same logical hierarchy  Not be part of an existing bus In the following example, the single-bit cells are combined in alphanumeric descending order, which is reg4, reg3, reg2, and reg1: fc_shell> set_app_option -name compile.flow.enable_multibit -value true fc_shell> create_multibit {reg2 reg4 reg1 reg3} -sort descending fc_shell> compile_fusion In the following example the single-bit cells are combined in alphanumeric ascending order, which is reg1, reg2, reg3, and reg4: fc_shell> set_app_option -name compile.flow.enable_multibit -value true fc_shell> create_multibit {reg2 reg4 reg1 reg3} -sort ascending fc_shell> compile_fusion In the following example, the single-bit cells are combined in the specified order, which is reg2, reg4, reg1, and reg3: fc_shell> set_app_option -name compile.flow.enable_multibit -value true fc_shell> create_multibit {reg2 reg4 reg1 reg3} -sort none fc_shell> compile_fusion Note: You must set the compile.flow.enable_multibit application option to true before you run the create_multibit command on unmapped single bit registers.\n If not, the tool issues the following error message: Error: create_multibit command failed: multibit flow is disabled.\n (MBIT-068)"}
{"instruction": "How do I Improving the Correlation Between the SMSCTS and CCD", "input": "", "output": "In a structural multisource clock tree, the subtrees are driven directly by the clock mesh or through predefined drivers.\n These drivers are, in turn, driven by a global clock distribution structure such as a clock mesh or an H-tree.\n By using the compile_fusion command to synthesize and optimize the local subtrees of a structural multisource clock tree, you can improve the timing QoR of the design.\n       To implement a structural multisource clock tree using the integrated subtree synthesis capabilities, perform the following steps: 1.\n Specify your clock tree constraints and settings.\n 2.\n For multivoltage designs,  Enable multivoltage support by using the following application option setting: fc_shell> set_app_options \\ -name cts.multisource.enable_full_mv_support -value true  Enable the addition of physical feedthrough cells, which belong to a voltage area physically but not logically, by using the following application option setting: fc_shell> set_app_options \\ -name opt.common.allow_physical_feedthrough -value true  Run the check_mv_design command and fix all multivoltage issues 3.\n Enable multicorner optimization by setting the cts.multisource.enable_multi_corner_support application option to true.\n The default is false By default, the tool uses the worst corner associated with the mode in which the subtree options are defined.\n When you enable this feature, the tool considers all corners associated with the mode in which the subtree options are defined.\n 4.\n Enable cell electromigration fixing by setting the cts.multisource.cell_em_aware application option to true 5.\n Create the clock mesh by using the create_clock_straps command, as described in Creating Clock Straps 6.\n Insert the mesh drivers by using the create_clock_drivers command, as described in Inserting Clock Drivers 7.\n Build the global clock tree structure by using the synthesize_multisource_global_clock_trees command, as described in Synthesizing the Global Clock Trees 8.\n Model the delays of the mesh nets by using the set_annotated_delay and set_annotated_transition commands.\n 9.\n Specify settings for local subtree synthesis by using the set_multisource_clock_subtree_options command.\n       You must specify  The clock for synthesizing the subtrees by using the -clock option  The drivers of the subtrees to be synthesized by using the -driver_objects option Optionally you can  Prevent the tool from merging specific clock tree cells by using the -dont_merge_cells option  Balance the levels of the local subtree by using the -balance_levels true option and specify a target number of levels, which applies to all sinks of the local subtree, by using the -target_level option Optionally, you can apply a different target number of levels to specific sink pins and clock balance points by using the set_multisource_clock_subtree_constraints command with the -pins and -target_level options.\n  Specify a maximum total wire delay from any subtree driver to any of its sinks by using the -max_total_wire_delay option and the corner it applies to by using the -corner option  Enable the reordering of clock-gating cells by using the -enable_icg_reordering true option When you enable this feature, the tool swaps the position of a clock-gating cell with the buffer that is driving the clock-gating cell.\n Then, the buffer is gated by the clock- gating cell, thereby reducing the dynamic power consumption.\n To perform reordering, you must  Enable the scenarios used for structural multisource clock tree synthesis  Annotate the switching activity for the enable pins of the clock-gating cells You can prevent specific clock-gating cells from being reordered by using the set_multisource_clock_subtree_constraints command with the -cells and -ignore_for_icg_reordering options.\n To report the settings you specified, use the report_multisource_clock_subtree_options command.\n To remove the settings you specified, use the remove_multisource_clock_subtree_options command.\n 10.\n Enable integrated structural multisource clock tree synthesis by setting the compile.flow.enable_multisource_clock_trees application option to true and build the local subtree by running the compile_fusion -from initial_place command.\n       fc_shell> set_app_options \\ -name compile_flow.flow.enable_multisource_clock_trees -value true fc_shell> compile_fusion -from initial_place 11.\n Route the connections between the clock mesh and the local subtrees by using the route_clock_straps command, as described in Routing to Clock Straps 12.\n Analyze the clock mesh by using the analyze_subcircuit command, as described in Analyzing the Clock Mesh 13.\n Enable integrated structural multisource clock tree synthesis by setting the clock_opt.flow.enable_multisource_clock_trees application option to true and synthesize the entire clock tree from the clock root by using the clock_opt -from build_clock -to route_clock command.\n fc_shell> set_app_options \\ -name clock_opt.flow.enable_multisource_clock_trees -value true fc_shell> clock_opt -from build_clock -to route_clock During this step, the tool incrementally optimizes the existing subtrees.\n 14.\n Analyze the clock tree results as described in Analyzing the Clock Tree Results"}
{"instruction": "How do I Anchoring Relative Placement Groups", "input": "", "output": "To specify properties of relative placement groups, use the set_rp_group_options command as described in Table 57 Table 57 Specifying Relative Placement Group Properties To do this Use this option   -x_offset -y_offset -x_offset  -y_offset   -anchor_corner   -alignment   -pin_name   -tiling_type   -group_orientation  -utilization       Table 57 Specifying Relative Placement Group Properties (Continued) To do this Use this option   -place_around_fixed_cells    -optimization_restriction    -move_effort To remove relative placement group option settings, use the remove_rp_group_options command.\n You must specify the group name and at least one option; otherwise, this command has no effect."}
{"instruction": "How do I Querying Routing Guides", "input": "", "output": "River routing is a special routing topology that tries to minimize the space taken by the router by compressing the routes to follow a particular contour.\n River routing is useful if there are tight routing channels or you need to minimize the space taken by routing.\n You can use a routing guide to encourage river routing within the routing guide boundary.\n To create this type of routing guide, use the -river_routing option with the create_routing_guide command.\n You must also use the -layers option to specify the affected layers.\n       For example, to encourage river routing on the M2 layer within the rectangle with its lower- left corner at (0, 0) and its upper-right corner at (100, 100), use the following command: fc_shell> create_routing_guide -boundary {{0.0 0.0} {100.0 100.0}} \\ -river_routing -layers {M2}"}
{"instruction": "How do I Specifying Synchronous Input Types for Library Cell Pins", "input": "", "output": "Fusion Compiler does not infer synchronous reset or preset flip-flops by default.\n To infer registers with synchronous reset or preset pins, use the sync_set_reset Synopsys compiler directive in the RTL.\n HDL Compiler then connects these signals to the synch_clear and synch_preset pins on the SEQGEN WVGTECH cell to communicate to the mapper that these are the synchronous control signals and they should be kept as close to the register as possible.\n To enable the tool to infer synchronous reset or preset flip-flops or preserve the synchronous reset or preset logic close to the flip-flop, use either of the following two methods:  The sync_set_reset directive Specify the //synopsys sync_set_reset directive in the RTL.\n  The hdlin.elaborate.ff_infer_sync_set_reset application option When the RTL does not contain the directive, set the hdlin.elaborate.ff_infer_sync_set_reset application option to true.\n The default is false       During compile, the tool performs the following mapping based on the registers in your library:  The library containing registers with synchronous reset (or preset) pins The tool connects the reset (or preset) net to the reset (or preset) pin of a register with a dedicated reset pin.\n  The library containing no registers with synchronous reset (or preset) pins The tool adds extra logic to the data input to generate the reset (or preset) condition on a register without a reset (or preset) pin.\n The tool also attempts to map the logic as close as possible to the data pin.\n This figure shows examples of mapping to registers with and without a synchronous reset pin.\n   ___ RST  Using a reset pin   ___ RST  Using a gate on the data pin If your library has registers with synchronous reset (or preset) pins, you must still use the sync_set_reset directive in the RTL so that the tool can distinguish the reset (or preset) signals from other data signals and connect the reset signal as close to the register as possible.\n To connect the reset signal close to the register, set the compile.seqmap.honor_sync_set_reset application option to true Note: Synchronous reset and preset signals are not inferred for level-sensitive latches.\n A synchronous reset or preset coding style on a latch always results in combinational logic on the data signal even if the library has latches with synchronous reset or preset pins.\n Mapping of Asynchronous Reset or Preset Registers To enable the tool to map to registers with asynchronous reset or preset signals, you must  Describe the registers correctly in the RTL  Ensure the library contain the corresponding cells  Ensure the hdlin.elaborate.ff_infer_async_set_reset application option is set to true (the default)       If the library does not contain this type of registers, the tool leaves the cells unmapped."}
{"instruction": "How do I Displaying ECO Shapes in the GUI", "input": "", "output": "When analysis is complete, the tool saves the design data and analysis results (*.result) in the in-design.redhawk/ design_name.result directory under the RedHawk working directory.\n You must run the open_rail_result command to load the analysis results before displaying a map in the Fusion Compiler GUI.\n Table 70 lists the map types that are supported in the RedHawk Fusion analysis flow.\n Table 70 Analysis Maps Supported in RedHawk Fusion Flow Map Type Description       Note:                                Table 70 Analysis Maps Supported in RedHawk Fusion Flow (Continued) Map Type Description                      To display maps in the Fusion Compiler GUI, 1.\n To load the rail analysis result using the GUI, choose Task > Rail Analysis > 2D Rail Analysis.\n The Block Level 2D Rail Analysis window appears.\n In the window, click the open_rail_result link.\n Alternatively, run the open_rail_result command to load the design data and analysis results (*.result) saved in the design_name directory under the RedHawk working directory when analysis is complete.\n 2.\n In the GUI layout window, choose View > Map and select the map to display.\n In the map, problem areas are highlighted in different colors.\n Move the pointer over an instance to view more information in the InfoTip.\n By default, the tool displays information for all instances in the block.\n To examine information of one specific instance, deselect all the instances and then select the instance to display from the list.\n Use the options in the map panel to change map display:  Value: Sets the type of map to display  Bins: Changes the number of unique bins used to display the map  From: and To: Sets the range of density values displayed in the power density map  Text: Displays the calculated values in the layout; if the values are not visible, zoom in to view them       Examples The following examples show how to display various types of maps in the GUI:  shows how to display a rail instance effective voltage drop map and check for hotspots in the instance effective voltage drop map.\n  shows how to display a rail map for the switch cells in the block and examine the voltage information of the selected switch cell.\n  shows how to display a power density map for the block.\n        Displaying a Rail Instance Effective Voltage Drop Map Hover over the red spot to see the voltage drop number on the instance Select the instance to display Choose Rail Instance Effective Voltage Drop Click \"Zoom to Critical\" to examine the critical area Histogram for the range of map values (in volts) Options for map display control        Displaying Maps for Switch Cells Choose Rail Instance Switch Cell.\n Select the instance to Choose the type of map to display The tool shows the switch cells display on the selected power domain in the map.\n In the map, hover over a switch cell to examine its voltage information        Displaying a Power Density Map Choose Rail Power Choose Density to display power density values See Also  Specifying RedHawk and RedHawk-SC Working Directories"}
{"instruction": "How do I Inserting Level Shifters", "input": "", "output": "A multivoltage design requires special multivoltage cells, such as level shifters and isolation cells, at the interface between power domains.\n Level shifters are required between power domains that operate at different voltage levels, while isolation cells are required between power domains that are in different states (powered-down versus always-on or powered-up)."}
{"instruction": "How do I Skewing Latch and Discrete Clock Gates", "input": "", "output": "To copy concurrent clock and data (CCD) offsets to the new scenarios for consistent timing or QoR reports, use the copy_useful_skew command.\n During CCD, when a pin's CTS balance point offsets are derived in only a subset of the active scenarios, use this command to copy the offsets to the in-active scenarios and active scenarios that do not have the CCD offsets.\n The copy_useful_skew command also ensures that the corresponding constraints of the set_clock_latency command are set on all the pins and scenarios, where a new offset value is computed.\n The average value of the offset is computed across all scenarios, where a balance point offset is defined.\n The average value is then scaled back in each active scenario using the scaling factor of the scenario corner."}
{"instruction": "How do I Performing THO During compile_fusion", "input": "", "output": "During the final_place stage of compile_fusion, placement can be performed transparently across the top and blocks.\n Placement occurs within the boundaries of the       respective blocks, but it is global at the same time, ignoring pin locations to get straight paths.\n Placement can be wire length, timing, and congestion driven.\n THO provides an option to refine pin locations based on global placement.\n Pin locations can be refined for the specified set of blocks during coarse placement steps of the THO.\n 1.\n To refine pin locations and enable pin movement, use the following command: fc_shell> set_hierarchy_options -enable_pin_movement true -blocks {list of blocks} 2.\n To report the settings applied with the set_hierarchy_options command, use the report_hierarchy_options command.\n 3.\n To run the final_place stage with THO, use the following commands: fc_shell> init_hier_optimization -flow pre_route fc_shell> compile_fusion -from final_place -to final_place fc_shell> commit_hier_optimization The init_hier_optimization command issues messages similar to the following message to confirm which blocks are available for optimization: Information: Sub-block u1/a/i1 (Reference: ablock.design) is treated as modifiable.\n (HOPT-001) After global placement, THO calls the place_pins command to create the new pin placement."}
{"instruction": "How do I Running IC Validator Using a Job Scheduler", "input": "", "output": "To enable IC Validator multicore processing on one or more specific hosts, use the set_host_options command to specify the following information:  The host names If you do not specify a host name or submit protocol, the IC Validator tool runs on the host on which the Fusion Compiler tool is currently running.\n  The number of cores Use the -num_processes or -max_cores options to specify the number of cores.\n If you use both options, the IC Validator tool uses the product of the settings as the number of cores.\n The specified number of cores applies to each specified host.\n For example, to enable the IC Validator tool to use 4 cores on the current host, use the following command: fc_shell> set_host_options -target ICV -num_processes 4 To enable the IC Validator tool to use 16 cores (8 cores each on machineA and machineB), use the following command: fc_shell> set_host_options -target ICV \\ -num_processes 2 -max_cores 4 {machineA machineB}"}
{"instruction": "How do I Defining Checkpoints", "input": "", "output": "When running experiments on a block, you might might modify your golden flow scripts in two ways:  By making flow changes, such as setting application options, modifying constraints, or making other changes to improve the quality of results  By generating reports (either additional reports or reports at new places in the flow) to help debug an issue In most flows, you apply changes through a small number of static, modifiable files.\n Here is a typical example to run the place_opt flow, where you source a pre_place_opt_settings.tcl file to add flow changes and a generate_reports.tcl file to run all your reporting at the end of the script: open_block./design:init_design source./scripts/pre_place_opt_settings.tcl remove_buffer_trees -all place_opt -from initial_place -to initial_drc create_placement -incremental -timing_driven -congestion place_opt -from initial_drc -to -initial_opto place_opt -from final_place -to final_opto source./scripts/generate_reports.tcl In some cases, complex changes require modifying your golden flow script directly, which might be discouraged or difficult to do in many design environments.\n These changes can be time consuming to implement, especially if you are implementing them at multiple stages in the design flow, across multiple blocks, or for several different flow experiments.\n The checkpoint system streamlines this process and allows you to run experiments without modifying your golden flow scripts.\n When you use the checkpoint system, you define checkpoints for important steps in your golden flow.\n These checkpoints enwrap the commands associated with the given steps.\n By default, the checkpoints run only the code they enwrap.\n That is, simply inserting checkpoints in your script does not change your flow.\n However, you can associate these checkpoints with flow changes or reports that you want to run before, after, or in place of the code the checkpoints enwrap.\n By defining these       associations in a portable configuration file, you isolate your golden flow scripts from the changes you want to apply for a particular run.\n The following figure shows how you can use checkpoints to instantly apply a set of flow changes or reports to multiple designs, without modifying each design's golden flow script.\n In this case, the configuration file has been copied to each design's run directory.\n In addition, when multiple users are working on a project, each user can have an individual checkpoint.config.tcl file with his or her preferred settings: The checkpoint system also allows you to  Insert flow changes and reports with more precision than you could using a typical non- checkpointed flow script  Reference a single source of truth where you can find all changes that have been applied to your default golden flow  Report a history of the checkpoints that were run, including the flow changes or reports that were run at those checkpoints        Report runtime and memory information for each checkpoint that was run  Generate unique and descriptive report names The general process for using the checkpoint system is as follows: 1.\n Insert checkpoints in your script 2.\n Configure your checkpoints by  Defining the flow changes or reports you want to associate with the checkpoints  Associating the flow changes or reports you defined with your checkpoints 3.\n Run your script."}
{"instruction": "How do I Performing Automated Tap Insertion and H-Tree Synthesis", "input": "", "output": "To reduce skew variation, clock mesh structures require higher timing accuracy than traditional clock structures.\n Therefore, to analyze a clock mesh structures, use the analyze_subcircuit command, which performs transistor level circuit simulation for the clock mesh and back-annotates accurate timing information.\n Before you run the analyze_subcircuit command, you must  Detail route the clock mesh net.\n  Have a circuit-level model for each of the gates in your clock tree and a transistor model for each of the transistors in the circuit-level models.\n  Have access to a SPICE simulator such, as NanoSim, FineSim, or HSPICE.\n       For the analyze_subcircuit command, you must specify  The clock mesh net you want to simulate by using the -net option.\n Alternatively, you can specify the sinks of the mesh net by using the -to option.\n If multiple clocks reach the net or the sinks you specify, use the -clock option to distinguish the clock you want to analyze.\n  A name by using the -name option.\n This name is used to construct the circuit elements.\n It is also used in the name of the output files and the directory where the output is stored.\n Optionally you can specify an annotated transition on the clock port or the start point of your clock mesh.\n If you do so, the clock must be specified as propagated by using the set_propagated_clock command.\n When you run the analyze_subcircuit command, the tool performs the following steps: 1.\n Performs RC extraction and generates parasitic files.\n You can run extraction as a standalone step by using the -extraction option.\n 2.\n Generates SPICE files for simulating the clock mesh, using the parasitic files from the previous step as input.\n You can generate the SPICE files as a standalone step by using the -create_spice_deck option.\n When you do so, you can use parasitic files generated by a different extraction tool.\n If these parasitic files have a different naming convention, you can specify the appropriate file suffix by using the -spef_input_file_suffix and -rc_include_file_suffix options.\n 3.\n Runs SPICE simulation, using the SPICE files generated in the previous step as input.\n By default, the tool uses the NanoSim simulator.\n You can use the FineSim or HSPICE simulators by using the finesim or hspice setting with the -simulator option.\n You must specify the location of the circuit-level and transistor-level models by using the -driver_subckt_files and -spice_header_files options.\n You customize these settings by specifying different files for the maximum and minimum conditions within each scenario by using the -configuration option.\n You can run simulation as a standalone step by using the -run_simulation option.\n 4.\n Generates timing annotation files containing set_disable_timing set_annotated_delay, and set_annotated_transition commands, using the simulation results as input.\n       For clock mesh nets, which have multiple drivers, the set_disable_timing command is used to disable all except one of the drivers, which is called the anchor driver.\n The annotated net delay arcs are defined from the anchor driver.\n You can generate the timing annotation files as a standalone step by using the -write_annotation option.\n 5.\n Applies the annotation files generated in the previous step.\n You can apply the annotation files as a standalone step by using the -apply_annotation option.\n If you want to run some of the steps of the mesh analysis flow, such as extraction or SPICE simulation, using other tools, you can do so and run the rest of the steps of the flow using the standalone options of the analyze_subcircuit command, following the same sequence.\n The following example analyzes the clock mesh net named clk_mesh using the HSPICE simulator.\n It customizes the simulation by using different files for the minimum and maximum conditions of each scenario.\n fc_shell> analyze_subcircuit -net clk_mesh \\ -driver_subckt_files max_spice_model \\ -spice_header_files header_file \\ -configuration { \\ {-scenario_name scenario1 \\ -max_driver_subckt_files max_file1 \\ -max_spice_header_files header_max1 \\ -min_driver_subckt_files min_file1 \\ -min_spice_header_files header_min1} \\ {-scenario_name scenario2 \\ -max_driver_subckt_files max_file2 \\ -max_spice_header_files header_max2 \\ -min_driver_subckt_files min_file2 \\ -min_spice_header_files header_min2}} -simulator hspice \\ -name clk_mesh_analysis"}
{"instruction": "How do I Inserting Redundant Vias", "input": "", "output": "You can fix antenna violations after detail routing by explicitly specifying which violations to fix and providing constraints for fixing them.\n Based on the specified constraints and the setting of the route.detail.diode_insertion_mode application option, Zroute either inserts new diodes or reuses existing spare diodes to fix the specified violations.\n To insert diodes after detail routing, use the create_diodes command.\n When you use this command, you must use the -options option to specify the location of each antenna violation to fix by specifying the port and cell instance, the reference cell for the diode, the number of diodes to insert, the highest allowed routing layer used for connecting the diode, and the maximum distance from the specified pin that the diode can be inserted.\n If a diode cannot be inserted or reused within the specified distance, the tool does not insert a diode for that violation.\n Use the following format to specify these values: port_name instance_name diode_reference number_of_diodes max_routing_layer max_routing_distance Note: You can use this command to insert diodes for top-level ports by specifying the name of the top-level block for the cell instance.\n       The create_diodes command uses the following application options to control diode insertion:  To control whether Zroute inserts new diodes or reuses spare diodes, set the route.detail.diode_insertion_mode application option.\n  To control whether Zroute can reuse filler cell locations, set the route.detail.reuse_filler_locations_for_diodes application option.\n  To specify the logic hierarchy in which to insert the diodes for top-level ports, set the route.detail.use_lower_hierarchy_for_port_diodes application option.\n For example, to insert 2 Adiode diode cells to fix an antenna violation on the A port of the CI cell instance, where the diodes must be inserted using no higher layer than M5 and within 2.5 microns of the port, enter the following command: fc_shell> create_diodes {{A CI Adiode 2 M5 2.5}}"}
{"instruction": "How do I Marking the Clock Networks", "input": "", "output": "You can preserve networks, such as clock trees, during optimization by using the set_dont_touch_network command.\n The command places the dont_touch attribute on the clocks, pins, or ports to prevent the cells and nets in the transitive fanout of the network from being modified or replaced.\n By default, the command preserves both clock paths and clock as data paths and propagates the dont_touch attribute throughout the hierarchy of the network.\n To preserve clock paths only, specify the -clock_only option.\n To remove the dont_touch attribute from the network, specify the -clear option.\n This example sets the dont_touch attribute on the CLK clock network.\n fc_shell> set_dont_touch_network [get_clocks CLK] This example sets the dont_touch attribute on the CLK clock paths only.\n fc_shell> set_dont_touch_network [get_clocks CLK] -clock_only This example removes the dont_touch attribute from the CLK clock network.\n fc_shell> set_dont_touch_network [get_clocks CLK] -clear"}
{"instruction": "How do I Copying Clock Tree Exceptions Across Modes", "input": "", "output": "To set a size-only setting on a cell, use the set_size_only command.\n The following command sets a size-only setting on a cell: fc_shell> set_size_only [get_cells cell_name ] true To report all the size-only settings set on the current block, use the report_size_only command.\n fc_shell> report_size_only -all"}
{"instruction": "How do I Reporting Clock Tree Power", "input": "", "output": "To generate clock tree QoR reports, use the report_clock_qor command.\n The report_clock_qor command can generate the following types of reports:  Summary By default, this command reports a summary of the clock tree QoR, which includes the latency, skew, DRC violations, area, and buffer count.\n  Latency       To report the longest and shortest path for each clock, use the -type latency option.\n  DRC violators To report the maximum transition and capacitance constraint violators, use the -type drc_violators option.\n  Robustness To report the robustness of each sink, use the -type robustness option.\n The robustness of a sink is the ratio between its latency for the corner for which the report is being generated and its latency for the corner specified by the -robustness_corner option.\n The mode use for both latency values is the mode for which the report is being generated.\n When you use the -type robustness option, you must specify a corresponding robustness corner by using the -robustness_corner option.\n  Clock-balance groups QoR To report a clock QoR summary for each clock-balance group, use the -type balance_groups option.\n It also reports the clock latency offset values specified with the -offset_latencies option of the create_clock_balance_group command and the actual latency offset values.\n  Local skew To report the QoR summary and the worst local skew for each clock or skew group, use the -type local_skew option.\n It also reports the five largest and five smallest local skew values and the corresponding endpoints.\n  Clock tree power To report a summary of the leakage, internal, sink, net switching, dynamic, and total power per clock per scenario, use the -type power option.\n If the power.clock_network_include_clock_sink_pin_power application option is set to off, the sink power is not reported.\n When you use the -type option, you can generate the output as a comma separated values (CSV) file by using the -csv option and specifying the output file name using the -output option.\n The report_clock_qor command can also generate the following types of histograms:  Latency histogram To report the latency of each sink in a histogram format, use the -histogram_type latency option.\n  Transition histogram       To report the transition time of each sink in a histogram format, use the -histogram_type transition option.\n  Capacitance histogram To report the capacitance of each sink in a histogram format, use the -histogram_type capacitance option.\n  Local skew histogram To report the local skew in a histogram, use the -histogram_type local_skew option.\n  Robustness histogram To report the robustness of each sink, with respect to a robustness corner specified by using the -robustness_corner option, in a histogram format, use the -histogram_type robustness option.\n  Wire delay fraction histogram To report the ratio between the wire delay and the total delay for each logic stage of the clock tree in a histogram format, use the -histogram_type wire_delay_fraction option.\n By default, the tool generates a report for all clock trees in all active modes and corners in all active scenarios.\n You can limit the report to specific  Portions of a clock network by using the -from -to, and - through options  Clock trees by using the -clock option  Skew groups by using the -skew_group option  Modes by using the -mode option  Corners by using the -corner option  Scenarios by using the -scenario option If you use the -scenario option, you cannot use the -mode and -corner options."}
{"instruction": "How do I Removing Placement Blockages", "input": "", "output": "To return a collection of placement blockages in the current block that match certain criteria, use the get_placement_blockages comandcomandsget_placement_blockages get_placement_blockages command."}
{"instruction": "How do I Specifying Placement Settings", "input": "", "output": "To remove spacing rules, use the remove_al_spacing_rules comandcomandsremove_al_spacing_rules remove_placement_spacing_rules command.\n You must specify which rules to remove.\n  To remove all spacing rules, use the -all option.\n  To remove a specific label and all rules associated with that label, use the -label option to specify the label.\n  To remove a rule between labels, use the -rule option to specify two labels associated with the rule."}
{"instruction": "How do I Removing Constraints for a Group of Repeaters", "input": "", "output": "To report the constraints that were assigned to a group of repeaters using the place_group_repeaters add_group_repeaters or set_repeater_group_constraints commands, use the report_repeater_group_constraints command.\n For example: fc_shell> report_repeater_group_constraints"}
{"instruction": "How do I Defining Group Bounds", "input": "", "output": "A move bound is a fixed region within which to place a set of cells.\n It comprises one or more rectangular or rectilinear shapes, which can be abutted, disjoint, or overlapping.\n To define the boundaries of these shapes, use the -boundary option with the create_bound command (if you are creating a new move bound) or the create_bound_shape command       (if you are adding shapes to an existing move bound).\n Note that you can specify one or more shapes when using the create_bound command, but only a single shape in each create_bound_shape command.\n  To specify the boundary of a rectangle shape, use the following format to specify its lower-left and upper-right corners: {{ llx lly } { urx ury }}  To specify the boundary of a rectilinear shape, use the following format to specify the coordinates of its vertices: {{ x1 y1 } { x2 y2 } { x3 y3 } { x4 y4 }...} Move bounds can be hard, soft, or exclusive.\n  To define a soft move bound, use the following syntax: create_bound -name name [-type soft] [-effort effort_level ] \\ -boundary { coordinates } [ bound_objects The default effort is ultra ; you can also specify low medium, or high For example, to define a rectangular soft move bound for the INST_1 cell instance with its lower-left corner at (100, 100) and its upper-right corner at (200, 200), use the following command: fc_shell> create_bound -name b1 -boundary {100 100 200 200} INST_1  To define a hard move bound, use the following syntax: create_bound -name name -type hard -boundary { coordinates }\\ bound_objects For example, to define a rectangular hard move bound for the INST_1 cell instance with its lower-left corner at (100, 100) and its upper-right corner at (200, 200), use the following command: fc_shell> create_bound -name b2 -type hard \\ -boundary {100 100 200 200} INST_1  To define an exclusive move bound, use the following syntax: create_bound -name name -exclusive -boundary { coordinates } \\ bound_objects For example, to define a rectangular exclusive move bound for the INST_1 cell instance with its lower-left corner at (100, 100) and its upper-right corner at (200, 200), use the following command: fc_shell> create_bound -name b3 -exclusive \\ -boundary {100 100 200 200} INST_1       To add shapes to an existing move bound, use the create_bound_shape command.\n To remove shapes from an existing move bound, use the remove_bound_shapes command."}
{"instruction": "How do I Creating Collections of Clock Network Pins", "input": "", "output": "To report the power of a clock tree, use the report_clock_power command.\n By default, it reports the power for all clock trees in all modes and corners of all active scenarios.\n To report the power  Only for specific clocks, use the -clocks option  Only for specific modes, use the -modes option  Only for specific corners, use the -corners option  Only for specific scenarios, use the -scenario option  On a per-segment or per-subtree basis, use the -type per_segment or -type per_subtree option  A segment is the clock buffer tree from one integrated-clock-gating (ICG) cell to the next ICG cells or sinks in its fanout For example, in the following figure, the buffer tree from the output of the ICG1 cell to the inputs of the ICG2, ICG3, and ff2 cells is considered a segment.\n  A subtree is a clock tree from an ICG cell all the way to the sinks in its fanout.\n For example, in the following figure, the clock tree from the output of the ICG1 cell to the inputs of the ff1, ff2, and ff3 cells is considered a subtree.\n  Segments and Subtrees of a Clock Tree"}
{"instruction": "How do I Abutting Standard Cells With Specific Filler Cells", "input": "", "output": "Some mask design rules are not handled during filler cell insertion because they have a low probability of occurrence and they require extensive computation time if handled during insertion.\n After filler cell insertion, use the replace_fillers_by_rules command to fix these design rule violations.\n You must specify the rule to fix by using the -replacement_rule option.\n Table 41 lists the design rules supported by this command and the keywords used to specify these rules.\n Table 41 Design Rules Supported by the replace_fillers_by_rules Command Design rule Description -replacement_rule keyword      end_orientation       half_row_adjacency       Table 41 Design Rules Supported by the replace_fillers_by_rules Command (Continued) Design rule Description -replacement_rule keyword     illegal_abutment       od_horiztonal_distance       horizontal_edges_distance        small_filler_stacking       max_vertical_constraint      od_tap_distance     random End Orientation Rule The end orientation rule searches for a continuous horizontal sequence of cells and modifies the orientation of the leftmost or rightmost filler cells if they are in the specified list of target filler cells.\n To select this rule, use the -replacement_rule end_orientation option with the replace_fillers_by_rules command.\n When you select this rule, you must specify  The target filler cells To specify the target filler cells, use the -target_fillers option.\n        The orientation modification  To change the orientation for the leftmost filler cells, use the -left_end option.\n  To change the orientation for the rightmost filler cells, use the -right_end option.\n  To change the orientation for both the leftmost and rightmost filler cells, use both options.\n The valid values for these options are  inverse, which flips the cell from its existing orientation  R0_or_MX, which changes an MY orientation to R0 or an R180 orientation to MX  MY_or_R180, which changes an R0 orientation to MY or an MX orientation to R180 To report the cells whose orientation would be changed, without actually changing the orientations, use the -check_only option.\n This option is valid only for the end orientation rule.\n Half Row Adjacency Rule The half row adjacency rule searches for half-height filler cells and replaces them based on the types of their neighboring standard cells.\n The standard cells are classified as one of the following types:  Inbound The top or bottom boundary of an inbound standard cell is at the half-height location of the cell rows.\n When a half-height filler cell abuts an inbound cell, the abutting side of the half-height filler cell must have inbound-cell interaction polygons (ICIP).\n  Regular The top and bottom boundaries of a regular standard cell are at the full-height location of the cell rows.\n When a half-height filler cell abuts a regular cell, the abutting side of the half-height filler cell must not have inbound-cell interaction polygons (ICIP).\n The replacement filler cells have the same height, width, and threshold voltage as the original filler cells.\n The cell libraries include both p-type and n-type replacement filler cells with inbound-cell interaction polygons (ICIP).\n For p-type half-height filler cells, the inbound- cell interaction polygons (ICIP) are at the bottom corners.\n For n-type half-height filler cells, the inbound-cell interaction polygons (ICIP) are at the top corners.\n shows the replacement of the half-height filler cells using the half row adjacency rule.\n        Half-Height Filler Cell Replacement With Half Row Adjacency Rule Regular height Inbound height Inbound height FILL2_P FILL4_N Regular height Inbound height Inbound height FILL2_PR FILL4_NR Inbound-cell interaction polygons To select this rule, use the -replacement_rule half_row_adjacency option with the replace_fillers_by_rules command.\n The following table lists the options used to specify the replacement half-height filler cells.\n Each list can contain only one cell of each size.\n Table 42 Half-Height Replacement Filler Cells Abutting inbound standard cells Option to specify replacement filler cells  -p_none -n_none  -p_left -n_left  -p_right -n_right  -p_left_right -n_left_right   -p_left_center_right -n_left_center_right       Illegal Abutment Rule The illegal abutment rule restricts the filler cells that can abut certain other filler cells.\n To select this rule, use the -replacement_rule illegal_abutment option with the replace_fillers_by_rules command.\n When you select this rule, you must specify  The filler cells considered for the check and their replacements The filler cells considered for the check are called target cells.\n To specify the target filler cells and their replacements, use the following syntax with the -replace_abutment option: { { target_cell refill_cell }...\n } To ignore filler cells that have a physical_status attribute of fixed or locked, use the -skip_fixed_cells option with the replace_fillers_by_rules command.\n  The filler cells that cannot abut the target filler cells These filler cells are called illegal cells.\n To specify the illegal cells, use the -illegal_abutment option.\n Note that the target filler cells specified in the -replace_abutment option are also considered illegal cells.\n For example, the following command fixes illegal abutment violations where the FILL1a and FILL1b filler cells cannot abut the FILLx and FILLy cells: fc_shell> replace_fillers_by_rules -replacement_rule illegal_abutment \\ -replace_abutment { {FILL1a CFILL1a} {FILL1b CFILL1b} } \\ -illegal_abutment {FILLx FILLy}  shows the original row, which violates this rule, and the resulting row, which uses the replacement filler cells to fix the violations.\n The target filler cells are shown in red, while the replacement filler cells are shown in green.\n  Fixing an Illegal Abutment Violation       Maximum Horizontal Edge Length Rule The maximum horizontal edge length rule restricts the horizontal length of a sequence of abutted objects on a specific layer.\n To select this rule, use the -replacement_rule horizontal_edges_distance option with the replace_fillers_by_rules command.\n When you select this rule, you must specify  The maximum horizontal length To specify the maximum length, use the -max_constraint_length option.\n  The layer to check To specify the layer, use the -layer option.\n  The cells considered for the check and the filler cells that can be used to replace the violating filler cells The filler cells considered for the check are called constrained cells.\n The filler cells considered to replace violating cells are called unconstrained cells  To specify multiple constrained cell groups and their replacement cells, use the following syntax with the -refill_table option: { {{ refill_cell constrained_cells }}...\n } You can use this syntax to fix violations for threshold-voltage-based filler cells.\n  To specify a single constrained cell group, use the -constraint_fillers option.\n To specify the replacement cells, use the -non_constraint_fillers option.\n Note: If you use both methods, the command uses only the information specified in the -refill_table option.\n For example, the following command identifies and fixes maximum horizontal edge length violations where the horizontal length on the M2 layer exceeds 30 um: fc_shell> replace_fillers_by_rules \\ -replacement_rule horizontal_edges_distance \\ -max_constraint_length 30 -layer M2 \\ -refill_table { {{VT1_FILL1} {VT1_DECAP4 VT1_DECAP2}} {{VT2_FILL1} {VT2_DECAP4 VT2_DECAP2}} }       Maximum Horizontal Length Rule The maximum horizontal length rule restricts the horizontal length of a sequence of abutted filler cells.\n To select this rule, use the -replacement_rule od_horiztonal_distance option with the replace_fillers_by_rules command.\n When you select this rule, you must specify  The maximum horizontal length To specify the maximum length, use the -max_constraint_length option.\n  The cells considered for the check and the filler cells that can be used to replace the violating filler cells The filler cells considered for the check are called constrained cells.\n The filler cells considered to replace violating cells are called unconstrained cells  To specify multiple constrained cell groups and their replacement cells, use the following syntax with the -refill_table option: { {{ refill_cell constrained_cells }}...\n } You can use this syntax to fix violations for threshold-voltage-based filler cells.\n  To specify"}
{"instruction": "How do I Defining a Customized Via Mapping Table", "input": "", "output": "By default, Zroute reads the default contact codes from the technology file and generates an optimized via mapping table.\n To see the default mapping table, use the add_redundant_vias -list_only true command.\n In most cases you achieve better results if you use a customized mapping table rather than the default mapping table.\n For information about defining a customized mapping table, see Defining a Customized Via Mapping Table If you have not previously defined a customized mapping table for the block, you can see the default mapping table by using the add_redundant_vias -list_only true command.\n Note: After you have created a customized mapping table by using the method described in Defining a Customized Via Mapping Table, this command shows the customized mapping table.\n       Example 27 shows an example of a default via mapping table.\n Example 27 Default Via Mapping Table fc_shell> add_redundant_vias -list_only...\n Redundant via optimization will attempt to replace the following vias:  VIA12SQ_C -> VIA12SQ_C_2x1 VIA12SQ_C_2x1(r) VIA12SQ_C_1x2 VIA12SQ_ C_1x2(r) VIA12SQ_C(r) -> VIA12SQ_C_2x1 VIA12SQ_C_2x1(r) VIA12SQ_C_1x2 VIA12SQ_ C_1x2(r)...\n VIA89(r) -> VIA89_C_1x2(r) VIA89_1x2(r) VIA89_1x2 VIA89_C_1x2 VIA89_C_2x1(r) VIA89_2x1(r) VIA89_2x1 VIA89_C_2x1 VIA9RDL -> VIA9RDL_2x1 VIA9RDL_1x2"}
{"instruction": "How do I Routing Nets in the GUI", "input": "", "output": "Whenever you modify the nets in your block, you need to run engineering change order (ECO) routing to reconnect the routing.\n To run ECO routing, use the route_eco command.\n The route_eco command sequentially performs global routing, track assignment, and detail routing to reconnect the routing.\n By default, the route_eco command  Considers timing and crosstalk during routing, which means that the tool performs extraction and updates the timing before performing the routing.\n The extraction and timing update can be time consuming and might not be necessary for your block.\n To prevent the extraction and timing update, use the following commands to disable the timing-driven and crosstalk-driven modes: fc_shell> set_app_options \\ -name route.global.crosstalk_driven -value false fc_shell> set_app_options \\ -name route.global.timing_driven -value false fc_shell> set_app_options \\ -name route.track.crosstalk_driven -value false fc_shell> set_app_options \\ -name route.track.timing_driven -value false fc_shell> set_app_options \\ -name route.detail.timing_driven -value false        Works on all open nets in the block To perform ECO routing only on specific nets, use the -nets option to specify the nets.\n  Ignores existing global routes To honor the existing global routes and perform incremental global routing, set the -reuse_existing_global_route option to true  Relieves congestion by reusing dangling wires instead of rerouting detail routed wires To disable the reuse of dangling wires, set the -utilize_dangling_wires option to false  Performs a maximum of 40 detail routing iterations To change the maximum number of detail routing iterations, use the -max_detail_route_iterations option.\n  Fixes hard DRC violations on the entire block by rerouting any wires, whether modified or not  To fix DRC violations only in the neighborhood of the open nets, set the -open_net_driven option to true Note: When you use the -nets option to perform ECO routing on specific nets, Zroute fixes DRC violations only within the bounding box of the specified nets.\n In this case, Zroute ignores the setting of the -open_net_driven option.\n  To change the scope of rerouting performed by the ECO router, use the -reroute option.\n  To limit rerouting to modified wires, set this option to modified_nets_only  To first attempt to fix DRC violations by rerouting modified nets and then reroute other nets if necessary, set this option to modified_nets_first_then_others A common use for this option is to route the clock nets affected by an ECO in a fully routed block.\n  To enable the fixing of soft DRC violations, such as bridge rule violations, after the final detail routing iteration, set the route.common.post_eco_route_fix_soft_violations application option to true Zroute generates a DRC violations summary at the end of each detail routing iteration.\n Before reporting the final DRC violations, Zroute merges redundant violations.\n For more       information about the DRC violations reported by Zroute, see Performing Design Rule Checking Using Zroute Zroute also reports the nets changed during ECO routing.\n By default, it reports the first 100 changed nets.\n You can use the -max_reported_nets option to set a different limit on the reported nets.\n To report all changed nets, set the -max_reported_nets option to -1.\n See Also  Routing Nets in the GUI"}
{"instruction": "How do I Restricting Optimization on the Clock Network", "input": "", "output": "To identify pins on the clock network, the tool checks if the is_clock_is_used_as_clock pin attribute is true during clock tree synthesis.\n If you use the set_clock_balance_points command to specify a clock tree exception on a pin for which the is_clock_is_used_as_clock attribute is false, the tool accepts the exception, but ignores it during clock tree synthesis.\n       Consider the clock network in the following figure.\n  Specifying Exceptions on Pins Beyond the Clock Network CLK   U2  FF3  n5 By default, the tool sets  The is_clock_is_used_as_clock attribute of the U2/A, U2/Y, and FF3/D pins to false and considers the network beyond the U2/A pin as part of the data network  The U2/A pin as an ignore pin and excludes the network beyond this from clock tree synthesis During clock tree synthesis, the tool adds a guide buffer and isolates the U2/A pin from the rest of the clock network.\n During subsequent data path optimization, the tool can fix any existing DRC violations beyond the guide buffer.\n Assume you want clock tree synthesis to fix DRC violations up to the FF3/D pin using clock tree constraints.\n To do so, you must 1.\n Specify the FF3/D pin as an explicit ignore pin as follows: fc_shell> set_clock_balance_points -clock [get_clocks CLK] \\ -consider_for_balancing false -balance_points [get_pins FF3/D]       2.\n Force clock tree synthesis to consider the FF3/D pin as a pin on the clock network by using the set_sense -clock_leaf command as follows: fc_shell> set_sense -clock_leaf [get_pins FF3/D] The set_sense -clock_leaf command sets  The is_clock_used_as_clock attribute to true for all the pins of the clock branches that fanin to the pin specified as the clock leaf  The is_clock_used_as_clock attribute to false for all the pins in the fanout from the pin specified as the clock leaf, and the tool does not propagate the clock beyond this pin To check the value of the is_clock_used_as_clock pin attribute, use the get_attribute command, as shown in the following example: fc_shell> get_attribute [get_pins FF3/D] is_clock_used_as_clock"}
{"instruction": "How do I Removing PG Augmentation Shapes", "input": "", "output": "Guided power grid augmentation inserts PG augmentation shapes only near cells with IR violations.\n For each candidate cell, guided PG augmentation inserts PG augmentation shapes in the region from the cell to its tap with the minimum path resistance.\n Because the shapes are added only in specific regions, guided PG augmentation inserts fewer metal shapes and uses less routing area, which decreases the coupling capacitance impacts.\n To perform guided power grid augmentation, 1.\n Ensure that the block is fully routed with very few or no DRC violations.\n 2.\n Run voltage drop and minimum path resistance analysis using RedHawk Fusion, as described in Performing Voltage Drop Analysis and Performing Minimum Path Resistance Analysis The IC Validator tool uses the results of these analyses to drive the power grid augmentation.\n 3.\n Set up the IC Validator environment as described in Setting Up the IC Validator Environment 4.\n (Optional) Enable distributed processing by using the set_host_options command, as described in Enabling IC Validator Multicore Processing 5.\n Save the block to disk.\n When you run the fix_pg_wire command, the IC Validator tool uses the on- disk information for the block, not the information in memory.\n To ensure accurate information, use the save_block command to save the current state of the block before running the fix_pg_wire command.\n       6.\n Perform guided power grid augmentation by using the fix_pg_wire command with one or both of the following options:  -number_pins This option specifies the number of worst IR-drop violators for which to perform PG augmentation.\n  -voltage_drop_threshold This option enables the tool to automatically determine the cells to process based on a voltage drop threshold.\n In addition, you must specify  The power and ground nets on which to perform PG augmentation by using the -supply_nets option  The technology node for which to perform the augmentation by using the -node option For information about the fix_pg_wire command, see the man page.\n 7.\n Run voltage drop analysis using RedHawk Fusion, as described in Performing Voltage Drop Analysis This second voltage drop analysis run measures the voltage drop after power grid augmentation.\n See Also  Standard Power Grid Augmentation  Timing-Driven Power Grid Augmentation  Removing PG Augmentation Shapes"}
{"instruction": "How do I Creating Placement Blockages During Boundary Cell Insertion", "input": "", "output": "If your technology file defines metal cut allowed and forbidden preferred grid extension rules, you can create routing guides for these rules during boundary cell insertion by using the -add_metal_cut_allowed option with the set_boundary_cell_rules command.\n When you use this option, the compile_boundary_cells command creates the following routing guides:  Metal cut allowed routing guides, which cover the area taken up by all the placeable site rows reduced by the vertical shrink factor, which is 50 percent of the smallest site row height  Forbidden preferred grid extension routing guides, which cover the remaining area up to the block boundary Note: This feature is supported only by the compile_boundary_cells command; it is not supported by the compile_targeted_boundary_cells command."}
{"instruction": "How do I Specifying Clock-Tree Layer Constraints", "input": "", "output": "To remove net-specific routing layer constraints, use the set_routing_rule -clear command.\n Note that when you use this command, it also removes any nondefault routing rules assigned to the specified nets."}
{"instruction": "How do I Specifying Settings for Preroute Optimization", "input": "", "output": "You can specify a name prefix for the cells added on the data nets during optimization by using the opt.common.user_instance_name_prefix application option.\n The following example specifies a name prefix of  CF_ for the cells added on the data nets during the compile_fusion command  CO_ for the cells added on the data nets during the clock_opt command  RO_ for the cells added on the data nets during the route_opt command fc_shell> set_app_options \\ -name opt.common.user_instance_name_prefix -value \"CF_\" fc_shell> compile_fusion  fc_shell> set_app_options \\ -name opt.common.user_instance_name_prefix -value \"CO_\" fc_shell> clock_opt fc_shell> set_app_options \\ -name opt.common.user_instance_name_prefix -value \"PO_\" fc_shell> route_opt To specify a name prefix for the cells added on the clock network during clock tree synthesis, use the cts.common.user_instance_name_prefix application option, as described in Defining a Name Prefix for Clock Cells"}
{"instruction": "How do I Providing Block-Level Test Models", "input": "", "output": "To check the readiness for the hierarchical synthesis flow, use the following commands:  check_design -checks block_ready_for_top Run this command after block synthesis to check whether the synthesized block is ready for the top-level synthesis.\n  check_design -checks hier_pre_compile Run this command before top-level synthesis to check whether the subblocks and top- level design are ready for the hierarchical flow."}
{"instruction": "How do I Defining the Layer Mapping for IC Validator In-Design Commands", "input": "", "output": "If you are using LSF, you can specify both specific hosts and an LSF configuration for adding additional hosts as they become available.\n To use this method,  Specify the configuration for the specific hosts as described in Running IC Validator on Specific Hosts  Specify the LSF configuration as described in Running IC Validator Using a Job Scheduler, but use the -add_hosts option in addition to the other command options.\n This configuration represents both optional resources and maximum resources \u2013 the resources can be given to the IC Validator tool at the beginning, middle, or end of the run, or not given at all.\n For example, to use four cores on the current host and potentially acquire up to four more hosts from LSF, use the following commands: fc_shell> set_host_options -target ICV -max_cores 4 fc_shell> set_host_options -target ICV -add_hosts \\ -submit_protocol lsf -num_processes 4"}
{"instruction": "How do I Removing Net-Specific Routing Layer Constraints", "input": "", "output": "To specify net-specific layer constraints, use the set_routing_rule command.\n To specify the minimum and maximum routing layers, use the -min_routing_layer and -max_routing_layer options.\n Specify the routing layers by using the layer names from the technology file.\n For example, to use layers M2 through M7 when routing the n1 net, use the following command: fc_shell> set_routing_rule [get_nets n1] \\ -min_routing_layer M2 -max_routing_layer M7       By default, net-specific minimum layer constraints are soft constraints, while net-specific maximum layer constraints are hard constraints.\n You can change the default behavior as follows:  To change the default constraint strength for all net-specific layer constraints, set the route.common.net_min_layer_mode and route.common.net_max_layer_mode application options.\n Set these application options to soft to specify a soft constraint, allow_pin_connection to allow the use of lower layers only for pin connections, or hard to specify a hard constraint.\n  To change the constraint strength for a single net-specific layer constraint, use the -min_layer_mode and -max_layer_mode options when you define the constraint with the set_routing_rule command.\n Set these options to soft to specify a soft constraint, allow_pin_connection to allow the use of lower layers only for pin connections, or hard to specify a hard constraint.\n  To set the cost of violating soft constraints for all net-specific layer constraints, set the route.common.net_min_layer_mode_soft_cost and route.common.net_max_layer_mode_soft_cost application options.\n Set these application options to low medium (the default), or high.\n The cost setting controls the effort expended by the router to avoid violations.\n The high setting can reduce violations at a cost of increased runtime.\n The low setting can reduce runtime at a cost of increased violations.\n  To set the cost of violating soft constraints for a single net-specific layer constraint, use the -min_layer_mode_soft_cost and -max_layer_mode_soft_cost options when you define the constraint with the set_routing_rule command.\n Set these options to low medium (the default), or high.\n The cost setting controls the effort expended by the router to avoid violations.\n The high setting can reduce violations at a cost of increased runtime.\n The low setting can reduce runtime at a cost of increased violations."}
{"instruction": "How do I Fixing Site Name Mismatches", "input": "", "output": "To read the floorplan information from a DEF file, use the read_def command.\n fc_shell> read_def block.def Note: When possible, use DEF v5.8 or later, as this version supports more types of physical objects and obstructions than previous versions.\n By default, the read_def command  Annotates the floorplan information onto the current block To annotate the information onto a different block, use the -design option to specify the block name.\n  Preserves the existing floorplan information In incremental mode,  The placement area is imported based on the current core area and site rows in the DEF files  Physical constraints that can have only one value are overwritten by the value from the latest DEF file; for example, port location and macro location are overwritten.\n  Physical constraints that can have accumulated values are recomputed; that is, core area can be recomputed based on the existing value and the site row definitions in the latest DEF file.\n Placement keepouts from different DEF files are accumulated and the final keepout geometry is computed internally during synthesis.\n To remove the existing floorplan information before annotating the floorplan information from the DEF file, use the -no_incremental option.\n In this mode, the placement area is imported based on the site rows in the DEF files.\n  Uses rule-based name matching for macros and ports       Rule-based name matching automatically resolves name differences by using the tool's intelligent name matching capability.\n By default, when rule-based name matching is enabled, the following characters are considered equivalent:  Hierarchical separators { / _.\n } For example, a cell named a.b_c/d_e is automatically matched with the string a/b_c.d/e in the DEF file.\n  Bus notations { [ ] __ ( ) } For example, a cell named a [4] [5] is automatically matched with the string a_4__5_ in the DEF file.\n To disable rule-based name matching and require exact name matches between the DEF file and the block, set the file.def.rule_based_name_matching application option to false For more information, see \"Rule-Based Name Matching\" in the Fusion Compiler Data Model User Guide  Ignores any objects in the DEF file that do not exist in the block, except for PG objects To allow new non-PG objects to be created from the DEF file, use the -add_def_only_objects option to specify the types of objects to create.\n Specify one or more of the following keywords:  cells The tool creates the cells that exist only in the DEF file and connects their power and ground pins as defined in the DEF file; it does not connect the signal, clock, or tie pins even if these connections are defined in the DEF file.\n The tool also does not create new hierarchical cells; any hierarchy specified in the DEF file must already exist in the block.\n  nets The tool creates the signal, clock, and tie nets that exist only in the DEF file and connects them to the ports specified in the DEF PINS section; it does not connect the nets to any other ports or pins in the netlist even if these connections       are defined in the DEF file.\n The tool does not create new hierarchical nets; any hierarchy specified in the DEF file must already exist in the block.\n  ports The tool creates the signal, clock, and tie ports that exist only in the DEF file and connects them to the nets specified in the DEF PINS section.\n  all The tool creates the non-PG cells, nets, and ports that exist only in the DEF file, as if you had specified cells nets, and ports"}
{"instruction": "How do I Reading Blocks for Signoff Design Rule Checking", "input": "", "output": "Before you run the signoff_check_drc command, configure the run by setting the application options shown in Table 43.\n To set the application options, use the set_app_options command.\n To see the current settings, use the report_app_options command.\n Table 43 Application Options for Signoff Design Rule Checking Application option Default Description signoff.check_drc.runset     signoff.check_drc.\n auto_eco_threshold_value    -auto_eco true  signoff.check_drc.\n excluded_cell_types    lib_cell  macro pad  filler  signoff.check_drc.\n fill_view_data read_if_uptodate      read  discard signoff.check_drc.\n ignore_blockages_in_cells true   true false signoff.check_drc.\n ignore_child_cell_errors false   false true signoff.check_drc.\n max_errors_per_rule    signoff.check_drc.\n read_design_views             Table 43 Application Options for Signoff Design Rule Checking (Continued) Application option Default Description signoff.check_drc.\n read_layout_views       signoff.check_drc.\n run_dir    signoff_check_drc      signoff.check_drc.\n user_defined_options         signoff.physical.\n layer_map_file            signoff.physical.\n merge_exclude_libraries     signoff.physical.\n merge_stream_files  signoff.physical.\n merge_stream_files"}
{"instruction": "How do I Updating Activity for Improved Power Optimization", "input": "", "output": "The Fusion Compiler tool can perform percentage-LVT-based optimization.\n By limiting the percentage of LVT cells in a block, you can improve yield of your design.\n You can also use this feature during the early design phases to get an idea of the quality of the RTL.\n To set up for percentage-LVT-based optimization, 1.\n Specify the LVT library cell group as shown in the following example: fc_shell> remove_attributes -quiet \\ [get_lib_cells -quiet */*] threshold_voltage_group fc_shell> set_attribute [get_lib_cells -quiet *lvt*/*] \\ threshold_voltage_group LVT       fc_shell> set_threshold_voltage_group_type \\ -type low_vt LVT Ensure that the LVT library cells you specify  Have a valid purpose defined by using the set_lib_cell_purpose command and are included in the appropriate target-library subsets defined by using the set_target_library_subset command  Do not have a dont_touch or dont_use attribute setting 2.\n Specify the percentage limit for LVT cells by using the set_multi_vth_constraint -low_vt_percentage command.\n To specify the limit as a percentage of the  Cell count, use the -cost cell_count option  Cell area, use the -cost area option To reset this constraint, use the reset_multi_vth_constraint command, and to report it, use the report_multi_vth_constraint command.\n After you set up percentage-LVT-based optimization, when you run the compile_fusion clock_opt, or route_opt command, the tool uses the specified LVT cell percentage constraint for the data path cells in the block.\n However, to minimize QoR disturbance, the tool limits the percentage-LVT-based optimization performed during the route_opt command.\n Therefore, it is important to enable this feature earlier in the design flow.\n Note: LVT cells have a smaller cell delay, but higher leakage power dissipation.\n Therefore, limiting the number of LVT cells reduces leakage-power dissipation.\n However, to further reduce leakage power, enable leakage power optimization for the different stages of the design flow."}
{"instruction": "How do I ECO-Aware Removal of Track Based Metal Fill Shapes", "input": "", "output": "By default, when you use the signoff_create_metal_fill -mode remove command, the IC Validator tool removes all metal and via fill from the current block, including the typical critical dimension (TCD) structures.\n  To remove the metal fill only from specific regions, use the -coordinates option, as described in Specifying the Regions for Metal Fill Insertion  To remove the metal fill only from specific layers, use the -select_layers option, as described in Specifying the Layers for Metal Fill Insertion By default, when you use this option, the IC Validator tool does not remove the TCD structures.\n To remove the TCD structures in addition to the metal and via fill, set the signoff.create_metal_fill.tcd_fill application option to true before running the signoff_create_metal_fill command.\n  To remove the metal fill only over critical nets, use one or both of the -nets and -timing_preserve_setup_slack_threshold options to identify the critical nets.\n Note: The existing fill is not considered when determining the critical nets for pattern-based metal fill, but not for track-based metal fill.\n  To honor certain rules when removing the metal fill, use the -remove_by_rule option.\n You can specify one or more of the following rules:  Nondefault routing rules ( ndr This rule applies to both pattern-based and track-based metal fill.\n  Maximum density rules ( max_density_threshold This rule applies only to track-based metal fill.\n When you enable this rule, metal fill removal honors the maximum density threshold set by the signoff.create_metal_fill.max_density_threshold application option.\n For example, to honor nondefault routing rules during fill removal, use the following command: fc_shell> signoff_create_metal_fill -mode remove \\ -remove_by_rule {ndr} See Also  Specifying the Layers for Metal Fill Insertion  Specifying the Regions for Metal Fill Insertion"}
{"instruction": "How do I Balancing Skew Between Different Clock Trees", "input": "", "output": "You can manually split clock cells that have DRC violations by using the split_clock_cells -cells command, as shown in the following example: fc_shell> split_clock_cells -cells [get_cells U1/ICG*] The tool does not split the specified cells if  They do not have DRC violations  They have don't-touch, size-only, or fixed-placement attribute settings  It is necessary to punch ports on the boundaries of power domains or blocks that have been identified with the set_freeze_ports command After splitting a cell, the tool  Names the new cells using the <original_cell_name>_split_<integer> naming convention  Copies all the settings and constraints from the original cell to the newly created cells       Instead of specifying the cells to split, you can specify one or more collection of loads that are driven by the same driver by using the -loads option, as shown in the following example: fc_shell> set loads1 [get_pins I1/reg*/CK] fc_shell> set loads2 [get_pins I2/reg*/CK] fc_shell> split_clock_cells -loads [list $load1 $load2] After splitting, each set of loads is driven by a newly created driver."}
{"instruction": "How do I RedHawk Fusion and RedHawk-SC Fusion Data Flow", "input": "", "output": "Both RedHawk Fusion and RedHawk-SC Fusion support gate-level rail analysis capabilities, including static and dynamic rail analysis.\n You can use RedHawk Fusion and RedHawk-SC Fusion to perform the following types of checking and analysis in the Fusion Compiler environment:  Missing via and unconnected pin shape checking To check for missing vias or unconnected pin shapes in the design, you first configure the checking-related settings by using the set_missing_via_check_options command, and then perform the check by using the analyze_rail -check_missing_via command.\n For more information, see Missing Via and Unconnected Pin Checking  Voltage drop analysis To perform voltage drop analysis, use the analyze_3d_rail -voltage_drop -electromigration command.\n To perform voltage drop analysis, use the analyze_rail -voltage_drop command or choose Rail > Analyze Rail and select \"Voltage drop analysis\" in the GUI.\n Set the -voltage_drop option to static dynamic dynamic_vcd, or dynamic_vectorless to choose the type of analysis you want to perform.\n For more information, see Performing Voltage Drop Analysis  PG electromigration analysis To perform PG electromigration analysis, use the analyze_rail -electromigration command.\n For more information, see Performing PG Electromigration Analysis        Minimum path resistance analysis To perform minimum path resistance analysis, use the analyze_rail -min_path_resistance command.\n In the RedHawk-SC analysis flow, you must use the -voltage_drop option together with the -min_path_resistance option for minimum path resistance analysis.\n For more information, see Performing Minimum Path Resistance Analysis Depending on the type of analysis you run, the tool generates visual displays (maps) of the results that you can view in the Fusion Compiler GUI, as well as error data that you can display in the Fusion Compiler error browser.\n These maps and error data help you discover problem areas and determine corrective action without leaving the Fusion Compiler environment.\n Note: RedHawk Fusion in the Fusion Compiler tool does not support RedHawk signoff analysis features, such as hierarchical analysis or dynamic analysis with lumped or SPICE packages.\n For more information, see Preparing Design and Input Data for Rail Analysis By default, RedHawk Fusion analyzes only the current design scenario when analyzing a multicorner-multimode design.\n For more information about how to create and specify a rail scenario for rail analysis, see Running Rail Analysis with Multiple Rail Scenarios  shows where you would use these analysis capabilities in a typical design flow.\n  Using RedHawk Fusion in the Fusion Compiler Design Flow Find missing vias Design planning place_opt Initial placement Static voltage clock_opt Static voltage Filler cell insertion drop analysis drop analysis Power-switch cell insertion Dynamic voltage drop analysis route_opt PG electromigration analysis and fixing"}
{"instruction": "How do I Viewing Voltage Drop Analysis Results", "input": "", "output": "Note: Before running the analyze_rail command to calculate voltage drops and current violations, ensure you have provided the input files required by each analysis mode.\n For more information about preparing input files, see Preparing Design and Input Data for Rail Analysis Voltage drop analysis is supported in both RedHawk Fusion and RedHawk-SC Fusion analysis flows.\n To invoke the RedHawk/RedHawk-SC tool for calculating voltage drops of the specified power and ground networks, use the analyze_rail command with the following options:  Use the -voltage_drop option to specify the analysis mode.\n  static : Performs static voltage drop analysis.\n  dynamic : Performs dynamic voltage drop analysis.\n  dynamic_vectorless : Performs dynamic voltage drop analysis without Verilog Change Dump (VCD) inputs.\n  dynamic_vcd : Performs dynamic voltage drop analysis with VCD inputs.\n When enabled, use the -switching_activity option to specify a switching activity file.\n  Use the -nets option to specify the power and ground supply nets to analyze.\n The tool considers all the switched or internal power nets of the specified power nets in the analysis.\n You do not need to explicitly specify the internal power nets.\n  (Optional) Use the -switching_activity option to specify a switching activity file.\n The supported file formats are: VCD SAIF, and ICC_ACTIVITY.\n When not specified, by default the tool uses the toggle rate for rail analysis.\n       Note: If you are using the dynamic vector-free mode, you do not need to provide switching activity information.\n To specify a switching activity file, use the -switching_activity option by using the following syntax: -switching_activity { type file_name strip_path ] [ start_time end_time ]}  VCD : Specifies a VCD file that is generated from a gate-level simulation.\n By default, rail analysis reads and uses all time values in the VCD file.\n To specify the time window to read from the VCD file, use the optional start_time and end_time arguments with the -switching_activity option.\n When specified, the tool reads all time values from the VCD file, but uses only the specified time window for rail analysis.\n  SAIF : Specifies one or more SAIF files generated from a gate-level simulation.\n  icc_activity : Runs the Fusion Compiler write_saif command to generate a SAIF file that contains the user-annotated and simulated switching activity (including state-dependent and path-dependent information) for the complete design without running the propagation engine.\n The strip_path argument removes the specified string from the beginning of the object names.\n Using this option modifies the object names in the VCD or SAIF file to make them compatible with the object names in the block.\n The strip_path argument is case- sensitive.\n  (Optional) By default, the tool uses the RedHawk/RedHawk-SC power analysis feature to calculate the power consumption of the specified power and ground network.\n If you prefer having the Fusion Compiler tool generate the necessary power data for rail analysis, set the -power_analysis option to icc2 For a detailed description about how to run power analysis using the Fusion Compiler report_power command.\n  (Optional) By default, the analyze_rail command automatically generates a GSR file for running RedHawk rail analysis.\n To specify an external GSR file to append to the GSR file generated in the current run, use the -extra_gsr_option_file option.\n  (Optional) Use the -redhawk_script_file option to specify an existing RedHawk script file generated in an earlier analyze_rail run.\n When using an existing script file for rail analysis, the tool honors the settings in this file and ignores all other currently enabled options.\n For example, if you run the       analyze_rail -voltage_drop static -redhawk_script_file myscript.tcl command, the tool ignores the -voltage_drop option setting.\n When the -redhawk_script_file option is not specified, you must specify the nets to analyze by using the -nets option.\n The following example performs static voltage drop analysis on the VDD and VSS nets with the optional settings defined in a GSR configuration file.\n fc_shell> analyze_rail -voltage_drop static -nets {VDD VSS} \\ -extra_gsr_option_file add_opt.gsr"}
{"instruction": "How do I Considering the Dynamic Power QoR During Placement", "input": "", "output": "To improve the DFT logic placement by enhancing the tool's awareness of DFT connectivity, while still optimizing for functional placement, set the place.coarse.enable_dft_modeling application option to auto.\n The default is false"}
{"instruction": "How do I Inserting Via Ladders During Clock Tree Synthesis, Optimization,", "input": "", "output": "After synthesizing and optimizing the clocks, you can detail route the clock nets by using the route_group command, as shown in the following example: fc_shell> route_group -all_clock_nets -reuse_existing_global_route true When you set the -reuse_existing_global_route option of the route_group command to true, the detail router uses the existing clock global routes, which ensures better correlation.\n Alternatively, you can detail route the clock nets by using the clock_opt command, as shown in the following example: fc_shell> clock_opt -from route_clock -to route_clock"}
{"instruction": "How do I Using Custom Routing Application Options", "input": "", "output": "You can report or remove the constraint groups that you create when specifying the routing constraints.\n To report the created constraint groups, use the report_constraint_groups command and specify the constraint types to report by using the -type option.\n By default, all constraint types are reported.\n To specify the number of objects to report, use the -count option.\n The supported constraint types are:  bus_style, which is specified by the create_bus_routing_style command  differential_group, which is specified by the create_differential_group command  differential_pair, which is specified by the create_differential_group command  matched_wire, which is specified by the create_wire_matching command  net_priority, which is specified by the create_net_priority command  shielding, which is specified by the create_net_shielding command  wire_length_limit, which is specified by the create_length_limit command       The following example shows the default output of the report_constraint_groups command: fc_shell> report_constraint_groups **************************************** Report : report_constraint_groups Design : top_new Version: Date : ****************************************   Name Type Objects  NDR_bus bus_style bus1 NDR_diff differential_pair clk0 clk1 shield_diff shielding clk0 clk1 matchL_con matched_wire match1 To remove the created constraint groups, use the remove_constraint_groups command and specify the constraint groups to remove.\n To remove all the constraint groups, use the -all option.\n The command disassociates the constituent objects from the removed constraint groups, and reports the number of the groups that are removed.\n For example, fc_shell> report_constraint_groups {shield_1} {shield_2} fc_shell> remove_constraint_groups $a"}
{"instruction": "How do I Displaying Maps in the GUI", "input": "", "output": "Use the get_geometry_result command to write the geometry information with rail results to an output file, in the units defined with the report_units command.\n You can specify one net at a time.\n When more than one net is specified, an error message is displayed.\n To specify which analysis or checking types to report, use the -type option.\n The supported analysis types are: voltage_drop and min_path_resistance The report is in the following format: geometry bbox_value Here are commonly used options of the get_geometry_result command: Option Description -layer   -touching {{ x1 y1 x2 y2 }}   -top value   -threshold   -percentage      -histogram   Example The following example writes geometry data with voltage drop values that are within the specified area: fc_shell> get_geometry_result -net VDD -type voltage_drop \\ -touching {{1610 1968} {1620 1969}}       { { 1616.475 1967.000 } { 1617.525 1969.800 } } metal5 0.00448 { { 1608.810 1946.000 } { 1611.190 1974.000 } } metal7 0.00446 { { 1608.000 1946.000 } { 1612.000 1974.000 } } metal9 0.00446 See Also  Voltage Hotspot Analysis"}
{"instruction": "How do I Deriving Routing Guides", "input": "", "output": "To remove routing guides from the current block, use the remove_routing_guides command.\n  To remove specific routing guides, specify the routing rules, either as a list or collection, such as that returned by the get_routing_guides command.\n  To remove all routing guides, specify the -all option.\n For example, to remove the new_width_rule routing rule, use the following command: fc_shell> remove_routing_rules new_width_rule"}
{"instruction": "How do I Performing Concurrent Clock and Data Optimization During the", "input": "", "output": "To improve timing QoR, the tool can use nondefault routing rules on timing critical nets during preroute optimization.\n In addition, the tool can guide the router to honor these nondefault rule assignments as soft constraints.\n To enable this capability for the clock_opt command, set the clock_opt.flow.optimize_ndr application option to true fc_shell> set_app_options -name clock_opt.flow.optimize_ndr \\ -value true"}
{"instruction": "How do I Performing Hierarchical Synthesis Using Abstracts", "input": "", "output": "The following topics describe how to perform synthesis, placement, optimization, clock tree synthesis, routing, and postroute optimization on hierarchical designs.\n  Performing Hierarchical Synthesis Using Abstracts  Performing Hierarchical Synthesis Using ETMs  Performing Prechecks for Hierarchical Synthesis Flows  Providing Block-Level Test Models  Specifying Block-Level Power Intent  Overview of Abstract Views  Creating Abstract Views  Reporting Abstract Inclusion Reasons  Making Changes to a Block After Creating an Abstract  Creating a Frame View  Linking to Abstract Views at the Top-Level  Changing the Block From Abstract to Design View  Linking to Subblocks With Multiple Labels  Specifying the Editability of Blocks From the Top-Level  Preparing for Top-Level Closure With Abstracts  Checking Designs With Abstracts for Top-Level-Closure Issues  Performing Top-Level Closure With Abstract Views  Creating ETMs and ETM Cell Libraries  Linking to ETMs at the Top Level  Performing Top-Level Closure With ETMs  Transparent Hierarchy Optimization"}
{"instruction": "How do I Running the Hierarchical Synthesis Feasibility Analysis Flow", "input": "", "output": "During the hierarchical flow, block designs are represented as a macro with a frame view, and the implemented blocks are modeled as extracted timing models (ETMs) for the top- level implementation of the design.\n An extracted timing model (ETM) is a Liberty model representation of a design that is generated by the extract_model command in the PrimeTime tool.\n The command captures the timing and power information of the design using relevant Liberty attributes, as shown in the following example.\n library(\"block1\") {...\n comment : \"PrimeTime Extracted Model.\" ; voltage_map(\"nom_voltage\", 1.08); voltage_map(\"VDD\", 1.08);...\n cell( block1 ) { timing_model_type : \"extracted\";...\n pg_pin(\"VDD\") { voltage_name : \"VDD\"; pg_type : primary_power; ...\n Topics in this section  Running the Hierarchical Synthesis Feasibility Analysis Flow Using ETMs  Running the Hierarchical Synthesis Flow Using ETMs  Example Script of Hierarchical Synthesis Using ETMs See Also  SolvNet article 2284429, \"Top-level Closure Flow With Extracted Timing Models (ETMs)\"  Creating ETMs and ETM Cell Libraries  Linking to ETMs at the Top Level"}
{"instruction": "How do I Generating Interclock Delay Balancing Constraints Automatically", "input": "", "output": "To define interclock delay balancing constraints, use the create_clock_balance_group command.\n At a minimum, you must specify a name for the clock balance group and the clocks in the group.\n  To specify a name for the clock balance group, use the -name option.\n  To specify the clock trees in the group, use the -objects option with the get_clocks command.\n By default, the clock balance group is defined for the clocks in the current mode.\n To define a clock balance group for the clocks of a specific mode, use the -objects option with the get_clocks -mode command.\n       For example, to define a clock balance group named group1 that contains the clocks named clk1 and clk2 in the current mode, use the following command: fc_shell> create_clock_balance_group -name group1 \\ -objects [get_clocks {clk1 clk2}] To define a clock balance group named group2 that contains all the clocks in the mode named m2, use the following command: fc_shell> create_clock_balance_group -name group2 \\ -objects [get_clocks -mode m2] By default, the tool has a goal of zero delay offset between clocks.\n All clocks are balanced with the same insertion delay which usually is the longest insertion delay among the clocks.\n To specify a delay offset between clocks in the group, use the -offset_latencies option.\n By default, the delay offset applies to the current corner.\n To apply the delay offset to a specific corner, use the -corner option.\n For example, assume the design has three clocks named clk1, clk2, and clk3.\n If you want the insertion delay of clk1 and clk2 to be the same and the insertion delay of clk3 to be 100 less than that of clk1 and clk2, use the following command: fc_shell> create_clock_balance_group -name group3 \\ -objects [get_clocks {clk1 clk2 clk3}] \\ -offset_latencies {0 0 -100} Reporting Clock Balance Groups To report clock balance groups, use the report_clock_balance_groups command.\n The report lists the clock balance groups for all modes in all active scenarios.\n Removing Clock Balance Groups To remove clock balance groups, use the remove_clock_balance_groups command.\n You can either remove specific clock balance groups by specifying the clock balance groups or all clock balance groups by using the -all option.\n If you specify the clock balance groups to remove, they are removed from the current mode.\n However, if a specified group does not exist in the current mode, but does exist in another mode, it is removed from that mode.\n If you use the -all option, the clock balance groups are removed from all modes.\n For example, to remove a previously defined clock balance group named group1 from the current mode, use the following command: fc_shell> remove_clock_balance_groups group1 To remove all clock balance groups from all modes, use the following command: fc_shell> remove_clock_balance_groups -all"}
{"instruction": "How do I Setting Skew and Latency Targets", "input": "", "output": "You can restrict the libraries used during clock tree synthesis for the top level or a lower level of the logical hierarchy of a design by using the set_target_library_subset -clock command.\n To enable the use of the target library subset, you must set the opt.common.enable_target_library_subset_opt application option to The following example specifies the buf1 and buf2 cells from the HVT_lib and LVT_lib libraries as clock tree references.\n However, it restricts the lower-level block named TSK_BLK to use only the cells from the LVT_lib library for its clock nets.\n Therefore, only buf1 and buf2 cells from the LVT_lib library are used as clock references for that block.\n fc_shell> set_lib_cell_purpose -include cts \\ {HVT_lib/buf1 HVT_lib/buf2 LVT_lib/buf1 LVT_lib/buf2} fc_shell> set_target_library_subset -clock {LVT_lib} \\ -objects [TOP/TSK_BLK] fc_shell> set_app_options \\ -name opt.common.enable_target_library_subset_opt -value 1"}
{"instruction": "How do I Setting the Routing Constraints", "input": "", "output": "Before you use Zroute to perform secondary power and ground pin routing, you must verify that the cell libraries have the correct attributes on the secondary power and ground pins of the standard cell frame views.\n The following attributes are required for secondary power and ground pins:  is_secondary_pg This attribute must have a setting of true  port_type This attribute must have a setting of power or ground To verify that these attributes are correctly set on the library pins in the standard cell frame view, use the following command: fc_shell> report_attributes -application \\ [get_lib_pins -of_objects reflib cell /frame -all \\ -filter \"name == attr_name \"] where reflib is the cell library name, cell is the name of the library cell you want to check (or * if you want to check all library cells), and attr_name is either is_secondary_pg or port_type       See Also  Identifying Secondary PG Pins"}
{"instruction": "How do I Defining Simple Vias", "input": "", "output": "To create a via definition, use the create_via_def command.\n After it is created, the via definition can be used anywhere in the design, similar to a ContactCode definition in the technology file.\n The user-specified via definition is stored in the design library, so it can be used only in that design.\n The create_via_def command can define simple vias, simple via arrays, and custom vias.\n The following topics describe how to define these types of vias.\n  Defining Simple Vias  Defining Custom Vias"}
{"instruction": "How do I Clock Gating", "input": "", "output": "To query specific information, warning, or error message IDs that occurred during the previous command step, you can use the check_error command.\n Follow these steps to set up and query the message IDs: 1.\n Specify the message IDs to query by using the global scope application option, shell.common.check_error_list fc_shell> set_app_options -name shell.common.check_error_list \\ -value {DES-001 CMD-005} shell.common.check_error_list {DES-001 CMD-005} 2.\n Use the link command as an example for the previous command step.\n fc_shell> link Error: Current block is not defined.\n (DES-001) 3.\n Query the specified message IDs that were issued during the link command step by using the check_error command.\n fc_shell> check_error -verbose {DES-001}  The result shows that the DES-001 message ID was issued during the link command step.\n If none of the specified message IDs was issued, the check_error command returns 0."}
{"instruction": "How do I Mitigating Design Mismatches", "input": "", "output": "To embed Tcl command in RTL code, use the fc_tcl_script_begin and fc_tcl_script_end directives.\n The following Tcl commands can be embedded in an RTL design:  set_attribute  set_ungroup  set_size_only  set_dont_touch  set_dont_retime  set_implementation  set_optimize_registers        get_modules  get_cells  get_pins  get_nets  get_ports The following RTL example contains an embedded Tcl command that applies a size_only setting on all the registers with the iPort1.dout_reg name prefix: module bot ( myIntf1.datIn iPort1 ); always @(posedge iPort1.clk or negedge iPort1.rst) begin if (iPort1.rst == 1'b0) begin iPort1.dout <= '0; end else begin iPort1.dout <= iPort1.iData1 + iPort1.iData2; end end // synopsys fc_tcl_script_begin // set_size_only [get_cells iPort1.dout_reg*] // synopsys fc_tcl_script_end endmodule"}
{"instruction": "How do I Performing Effective Resistance Analysis", "input": "", "output": "Use the minimum path resistance (MPR) mouse tool to interactively trace the path with the least resistance from the specified power or ground net to the boundary nodes in the design.\n To trace the calculated minimum path resistance with the MPR interactive tracing tool, 1.\n In the GUI, choose Task > Rail and then select Analysis Tools from the task list to open the Task Assistant window (see ).\n 2.\n In the Task Assistant - Rail window that opens, click MPR Mouse Tool.\n 3.\n Specify the nets to analyze.\n Click OK.\n       The MPR interactive mouse tool, which is shown in, provides the following features:  Zoom in and out to review the traces  Provide two operation modes: the Add mode to trace multiple paths in the map, and the Replace mode to trace one path at a time in the map  Use the standard query tool to show tracing information  Select a region to trace multiple cells at a time  Document the interactive actions by scripts  Invoking the MPR Mouse Tool        Tracing the Path With the Least Resistance Click to hide all objects from showing in the map Then select Cell"}
{"instruction": "How do I Specifying Library Cells as Repeaters", "input": "", "output": "Use one of the following mutually exclusive options to control spacing for repeater groups:  -horizontal_repeater_spacing specifies the horizontal spacing between repeaters by cell site width.\n  -vertical_repeater_spacing specifies the vertical spacing between repeaters by site row height."}
{"instruction": "How do I Routing the Secondary Power and Ground Pins", "input": "", "output": "You can set routing constraints for secondary power and ground pin routing in the same way as for regular signal routing.\n For example, you can set constraints by  Defining the minimum and maximum routing layers by using the set_routing_rule command For more information about using the set_routing_rule command, see Specifying Net-Specific Layer Constraints  Specifying the preferred pin connections by setting the route.common.single_connection_to_pins and route.common.connect_within_pins_by_layer_name application options For example, to require a single connection to the secondary power and ground pins and require that the M1 connections use vias contained within the pin shapes, use the following command: fc_shell> set_app_options \\ -name route.common.single_connection_to_pins \\ -value standard_cell_pins fc_shell> set_app_options \\ -name route.common.connect_within_pins_by_layer_name \\ -value {{M1 via_standard_cell_pins}}  Defining the maximum number of power or ground pins in a cluster by setting the route.common.number_of_secondary_pg_pin_connections application option A cluster is a set of connected secondary power or ground pins that has one connection to a PG strap or ring.\n By default, the value of this option is 0, which means that there is no limit on the number of secondary power or ground pins in a cluster.\n For example, to connect all secondary power and ground pins directly to a PG strap or ring, use the following command: fc_shell> set_app_options \\ -name route.common.number_of_secondary_pg_pin_connections \\ -value 1        Defining a nondefault routing rule for secondary power and ground pin routing For example, to define a nondefault routing rule named wideSVDD for wide M1 and M2 and set the nondefault routing rule on the VDD2 net, to which the secondary power and ground pins are connected, use the following commands: fc_shell> create_routing_rule wideSVDD -widths { M1 0.3 M2 0.3 } fc_shell> set_routing_rule -rule wideSVDD {VDD2} For more information about using nondefault routing rules, see Using Nondefault Routing Rules By default, the constraints apply to both the secondary power and ground connections and the tie-off connections.\n To separate these connections so that you can set constraints only for the secondary power and ground connections, set the route.common.separate_tie_off_from_secondary_pg application option to true"}
{"instruction": "How do I Setting Options for Signoff Metal Fill Insertion", "input": "", "output": "After routing, you can fill the empty spaces in the block with fill shapes to meet the metal density rules required by most fabrication processes.\n Before inserting metal and via fill, the block should be close to meeting timing and have only a very few or no DRC violations.\n To insert metal fill, 1.\n Set up the IC Validator environment as described in Setting Up the IC Validator Environment 2.\n (Optional) Enable distributed processing by using the set_host_options command, as described in Enabling IC Validator Multicore Processing 3.\n Set the application options for metal fill insertion.\n For information about the metal fill insertion options, see Setting Options for Signoff Metal Fill Insertion 4.\n Save the block to disk.\n When you run the signoff_create_metal_fill command, the IC Validator tool uses the on-disk information for the block, not the information in memory.\n To ensure accurate information, use the save_block command to save the current state of the block before running the signoff_create_metal_fill command.\n 5.\n Perform metal fill insertion by using the signoff_create_metal_fill command as described in Performing Metal Fill Insertion The signoff_create_metal_fill command respects routing blockages defined by the create_routing_blockage command and does not insert metal fill in the blockages; however, it does not respect routing guides created by the create_routing_guide command.\n Note: If you are performing pattern-based metal fill insertion and the blockage layer numbers differ between the technology file and the foundry runset file, you must provide a layer mapping file, as described in Defining the Layer Mapping for IC Validator In-Design Commands When the IC Validator tool performs metal fill insertion, it creates an internal subdesign named block.FILL in the design view of the block and inserts the fill cells in this internal       subdesign.\n The fill cells are named FILL_INST_#.\n To query the fill cells, use the get_fill_cells command.\n For information about the result files generated by the signoff_create_metal_fill command, see Signoff Metal Fill Result Files After you insert metal fill, you can  Display the added metal fill in the layout view in the GUI, as described in Viewing Metal Fill in the GUI  Modify the metal fill, as described in Modifying Metal Fill  Perform extraction for timing analysis using the real metal fill, as described in Performing Real Metal Fill Extraction"}
{"instruction": "How do I Inserting Shields on the Nets", "input": "", "output": "To create a differential group or a differential pair for the nets, bundles, or topology edges, use the create_differential_group command.\n Custom Router pairs the closest pins, one from each net, and routes from a common gather point between the pins.\n The tool routes the net pair the closest to each other with the most similar routing patterns.\n You need to specify the following information:  The name of the constraint group If you do not specify a name, the command names the constraint group differential_pair_ or differential_group_, where is a unique integer.\n By default, the command returns an error if the assigned name already exists.\n To delete the existing constraint group, use the -force option.\n  The nets in the differential group To specify the nets included in the created group, use the -for option.\n If you specify two nets, the nets form a differential pair.\n If you specify three nets, the nets form a differential group.\n  The constraints for the differential group To define the constraints that are applied per layer to the trunks and override any settings on the individual bits, use the options listed in Table 54 Table 54 Differential Group Constraints Option Constraint -twist_style  diagonal none  orthogonal -twist_interval  -twist_offset   -valid_layers  -layer_spacings  -layer_widths        Option Constraint -shield_placement    double_interleave half_interleave  interleave  outside (default):    -gap   The following example creates a differential pair for the prn and prp nets.\n The command names the differential pair differential_pair_0, because there is no name assigned to the different pair.\n No twist style is specified and the shield placement is set to default fc_shell> create_differential_group \\ -for [get_nets -physical_context {prn prp}] \\ -shield_placement default {differential_pair_0}"}
{"instruction": "How do I Performing Wire Spreading", "input": "", "output": "A critical area is a region of the block where, if the center of a random particle defect falls there, the defect causes circuit failure, thereby reducing yield.\n A conductive defect causes a short fault, and a nonconductive defect causes an open fault.\n       The following topics describe how to  Reduce critical area short faults by performing wire spreading  Reduce critical area open faults by performing wire widening"}
{"instruction": "How do I Differential-Pair Options", "input": "", "output": "To specify whether the route and corresponding parallel shields, jumpers for parallel shields, and bus shields will snap to the wire tracks or the routing grid, use the custom.route.layer_grid_mode application option.\n Specify the application option with a list of paired values.\n The syntax for each pair of values is as follows: {{ layerName mode }} where layerName is the name of the layer.\n To include all layers, specify All mode is one of the following:  on : Snaps routes and shields to tracks.\n If the layer does not have tracks, the routes and shields will snap to the routing grid.\n  off : Does not snap routes and shields to tracks or grids.\n Choose this option for gridless routing.\n  off_cost <costValue> : Snaps routes and shields to tracks when possible.\n The value you specify determines the adherence of routes to tracks.\n The higher the value, the more likely the route will remain on the track or grid.\n If tracks and routing grids are not defined and you specify on, the Custom Router uses an internally determined grid.\n The grid pitch is route width + minSpacing between shapes.\n The grid offset is 0 or pitch/2, whichever permits more pins on the grid.\n Example 41 Layer and Cost Specification for Track Adherence {{M2 off} {M3 off_cost 4}}"}
{"instruction": "How do I ECO Fusion Power Integrity Flow", "input": "", "output": "The Fusion Compiler ECO Fusion flow allows you to use the PrimeTime physically aware ECO capabilities and the StarRC extraction capabilities (In-Design signoff extraction) within the Fusion Compiler tool.\n You can use this flow to fix timing and DRC violations and improve area and power QoR at the very late stages of the implementation flow.\n The Fusion Compiler ECO Fusion flow consists of the following steps: 1.\n Specify the settings required to run the PrimeTime ECO capabilities within the Fusion Compiler tool by using the set_pt_options command.\n The following example specifies the path to the PrimeTime executable and the settings for distributed processing: fc_shell> set_host_options -name pteco_host_option \\ -submit_command \"/lsf/bin/bsub -R \\\"rusage\\[mem=$MEM\\]\\\"\" fc_shell> set_pt_options -pt_exec /snps_tools/PT/pt_shell \\ -host_option pteco_host_option If there is a.synopsys_pt.setup file in the current working directory, the Fusion Compiler tool uses the information in this file.\n However, you can specify additional PrimeTime settings by using the -pre_link_script and -post_link_script options of the set_pt_options command.\n For more information, see the man page for the set_pt_options command.\n 2.\n Set up StarRC extraction as follows: a.\n Ensure that StarRC extraction is enabled.\n StarRC extraction is controlled by the extract.starrc_mode application option.\n The default is fusion_adv.\n You can also set the value to in-design or none To use the native extractor, set the value to none b.\n Specify a configuration file for running StarRC extraction by using the set_starrc_options -config command.\n       3.\n Perform optimization by using the eco_opt command.\n By default, the eco_opt command fixes timing (setup and hold) and DRC violations, removes redundant buffers, and improves the total power QoR by using PrimeTime ECO capabilities.\n It then incorporates the ECO changes by using the Fusion Compiler ECO place and route capabilities.\n However, you can control the type of optimization by using the -type option.\n The following example uses the eco_opt command to fix only timing and DRC violations using exhaustive path-based analysis: fc_shell> eco_opt -pba_mode exhaustive -types {timing drc} The following example uses the eco_opt command to improve the leakage power QoR.\n However, it does not incorporate the ECO changes.\n Instead, it generates a file containing the ECO changes.\n fc_shell> eco_opt -pba_mode exhaustive -types {timing drc} \\ -write_change_file_only For more information, see the man page for the eco_opt command.\n 4.\n Analyze the PrimeTime QoR by using the check_pt_qor command.\n For more information, see the man page for the check_pt_qor command.\n Note: When you run the eco_opt command, the Fusion Compiler tool specifies the PrimeTime settings based on the Fusion Compiler timing analysis settings, such as derating factors, on-chip-variation settings, and so on.\n Therefore, for optimal convergence, ensure that the Fusion Compiler timing analysis settings at the postroute stage are consistent with the PrimeTime timing analysis settings.\n You can identify the differences in the Fusion Compiler and PrimeTime settings by using the check_consistency_settings command."}
{"instruction": "How do I Identifying the Clock Endpoints", "input": "", "output": "The Fusion Compiler tool uses the clock sources defined by the create_clock command, which can be either input ports or internal hierarchical pins, as the clock roots.\n For nested clock trees with generated clocks, which are defined by the create_generated_clock command, the tool considers the master-clock source to be the clock root, and the clock endpoints of the nested clock tree are considered endpoints of the master-clock source.\n For example, for the netlist shown in, the tool considers the CLK port to be the clock root for the genclk1 generated clock.\n        Nested Clock Tree With a Generated Clock   CLK QN clk1 genclk1 If the block contains generated clocks, ensure that the master-clock sources are correctly defined, as incorrect definitions can result in poor skew and timing QoR.\n In particular,  If the tool cannot trace back to the master-clock source, it cannot balance the sink pins of the generated clock with the sink pins of its source.\n  If the master-clock source is not a clock source defined by the create_clock or create_generated_clock command, the tool cannot synthesize a clock tree for the generated clock or its source.\n Use the check_clock_trees command to verify that your master-clock sources are correctly defined, as described in Verifying the Clock Trees Specifying the Clock Root Timing Characteristics To get realistic clock tree synthesis results, you must ensure that the timing characteristics of the clock roots are correctly modeled.\n  If the clock root is an input port without an I/O pad cell, you must accurately specify the driving cell of the input port.\n If you specify a weak driving cell, the tool might insert extra buffers to try to meet the clock tree design rule constraints, such as maximum transition time and maximum capacitance.\n If you do not specify a driving cell (or drive strength), the tool assumes that the port has infinite drive strength.\n For example, if the CLK1 port is the root of the CLK1 clock tree, use the following command to set its driving cell as the CLKBUF cell in the mylib cell library: fc_shell> set_driving_cell -lib_cell mylib/CLKBUF [get_ports CLK1]  If the clock root is an input port with an I/O pad cell, you must accurately specify the input transition time of the input port.\n       For example, if the CLK1 port is the root of the CLK1 clock tree and the I/O pad cell has already been inserted, use the following commands to set its input transition time to 0.3 for rising delays and 0.2 for falling delays: fc_shell> set_input_transition -rise 0.3 [get_ports CLK1] fc_shell> set_input_transition -fall 0.2 [get_ports CLK1]"}
{"instruction": "How do I Creating ETMs in the PrimeTime Tool", "input": "", "output": "Rather than creating an ETM for each mode and corner combination using the extract_model command in the PrimeTime tool and then using the Library Manager tool to combine the ETMs with the corresponding physical information to create a cell library, you can perform both steps together by using the extract_model command directly in the Fusion Compiler tool.\n The following Fusion Compiler script creates an ETM for every mode and corner of the design and then combines them with the corresponding physical information and creates the corresponding cell library: # Open the design, create a frame view, and set the PrimeTime options open block.nlib:block.design create_frame <options> ;# extract_model needs Frame for library preparation set_pt_options -pt_exec_path <> -work_dir ETM_work_dir \\ -post_link_script <tcl script with extract_model* variables> \\  # To use StarRC for Parasitic extraction set_app_options -name extract.starrc_mode -value true set_starrc_options -config <starrc_config_file>  # Create the ETM and generate the ETM cell library extract_model By default, the Fusion Compiler tool writes out the generated ETM library into  [pwd]/ETM_Lib_work_dir/block_name/label_name/ for labelled blocks  [pwd]/ETM_Lib_work_dir/block_name/ for non-labelled blocks"}
{"instruction": "How do I Resolving Overlapping Voltage Areas", "input": "", "output": "To merge the voltage area shapes into a minimum set of disjoint shapes, use the -merge_regions option with the create_voltage_area create_voltage_area_shape or set_voltage_area command.\n  When you use the create_voltage_area -merge_regions command, the merges the shapes specified with the -region option.\n  When you use the create_voltage_area_shape -merge_regions command, the merges the shape specified with the -region option and the existing shapes of the specified voltage area.\n  When you use the set_voltage_area -merge_regions command, the tool merges all existing shapes of the specified voltage area.\n The tool merges the voltage area shapes based on their stacking order.\n By default, the stacking order is the order in which you define the shapes, with the last shape defined on top.\n The merged shape replaces the top shape of a set of abutting or overlapping shapes; the other shapes in the set are removed and are no longer associated with the voltage area.\n       For example, assume that you use the following command to create a voltage area comprising three rectangle shapes, as shown on the left side of fc_shell> create_voltage_area -power_domains {PD1} \\ -region { {{0 0} {10 10}} {{10 0} {30 10}} {{15 5} {35 15}} } {PD1} fc_shell> get_voltage_area_shapes -of_objects PD1 {VOLTAGE_AREA_SHAPE_1 VOLTAGE_AREA_SHAPE_2 VOLTAGE_AREA_SHAPE_3} After you use the -merge_regions option to merge these shapes, the voltage area consists of a single rectilinear shape, as shown on the right side of.\n The merged voltage area shape is named VOLTAGE_AREA_SHAPE_3, which was the last voltage area shape defined when the voltage area was created.\n fc_shell> set_voltage_area PD1 -merge_regions Information: Merging abutted and overlapping shapes in voltage_area 'PD1'.\n (NDMUI-154)  fc_shell> get_voltage_area_shapes -of_objects PD1 {VOLTAGE_AREA_SHAPE_3}  Merging Voltage Area Shapes To report the stacking order of the voltage area shapes, use the report_voltage_areas -verbose command.\n To modify the stacking order of the voltage area shapes, use the set_voltage_area_shape command, as described in Modifying the Stacking Order"}
{"instruction": "How do I Clock-Gate Collapsing", "input": "", "output": "It is difficult to determine the optimum number of clock-gate levels manually.\n The Fusion Compiler tool can analyze all clock gates and insert additional clock gates to take advantage of shared enable signals.\n Inserting additional clock gates might improve power, area, or other metrics, depending on the design.\n If six or more clock gates share an enable signal, the tool inserts an additional level of clock gates.\n The new clock gates are then considered for further level expansion.\n As a result, the tool might insert multiple levels of clock gates during this operation.\n To allow the tool to expand the number of clock-gate levels, set the compile.clockgate.enable_level_expansion application option to true (the default is false ) and set the compile.clockgate.max_number_of_levels application option to a nonzero value (the default is 0).\n In this case, the tool adds clock gates until either of the following conditions is met:  All common enable signals are identified and used for multilevel clock-gate expansion.\n  The user-specified maximum number of clock-gate levels is reached.\n The tool names a newly-inserted clock gate as follows, based on the status of the set of clock gates whose enable functions are modified by the new clock gate:  If at least one of the clock gates is a tool-inserted clock gate, the base name is clock_gate_ml.\n The name is appended with the register base name, similar to the naming convention for other tool-inserted clock gates.\n  If all of the clock gates are preexisting clock gates, the tool uses the name of the preexisting clock gate with the largest fanout and appends _ml_pe The compile.clockgate.max_number_of_levels application option specifies the maximum number of allowed clock-gate levels.\n The default is 0, which allows an infinite number of levels.\n However, to use the clock-gate level expansion capability, you must set this application option to a nonzero value (in addition to setting the compile.clockgate.enable_level_expansion application option to true ).\n If you specify clock-gate level expansion, but the maximum number of allowed levels is low, the expansion operation cannot take full advantage of the tool's ability to analyze common enable signals.\n To enable or disable clock-gate level expansion for specific clock gates, use the set_clock_gate_transformations -expand_levels command.\n       The following usage notes apply:  New clock gates inherit the constraints set by the set_clock_gate_transformations set_clock_gate_style set_dont_touch set_size_only set_lib_cell_purpose and set_boundary_optimization commands.\n  Clock-gate latency estimations are updated after the insertion operation.\n  When the insertion operation modifies the enable signal of a clock gate, the gate retains its original designation of being either preexisting or tool-inserted.\n  Self-gating is restricted from adding another clock-gate level if the maximum number of levels is already reached.\n  Integrated clock gates inside multibit cells are counted as part of the clock-gate level count.\n However, they are not candidates for clock-gate level expansion.\n  New clock gates do not inherit timing exceptions."}
{"instruction": "How do I Modifying Voltage Areas", "input": "", "output": "You can query the following information about voltage areas:  The voltage areas in the current block To create a collection of voltage areas in the current block, including the default voltage area, use the get_voltage_areas command.\n  Detailed information about the voltage areas       To display detailed information about voltage areas, including the default voltage area, use the report_voltage_areas command.\n To include information about the voltage area shapes that comprise each voltage area, use the -verbose option with the report_voltage_areas command.\n  Effective placement area of a voltage area To display the effective placement area of a voltage area, query the effective_shapes attribute of the voltage area.\n  Effective guard bands of a voltage area To display the effective guard bands of a voltage area, query the effective_guard_band_boundaries attribute of the voltage area.\n Note that you can also query this attribute for individual voltage area shapes."}
{"instruction": "How do I Benefits of Relative Placement", "input": "", "output": "Relative placement is usually applied to datapaths and registers, but you can apply it to any cell in your design, controlling the exact relative placement topology of gate-level logic groups and defining the circuit layout.\n You can use relative placement to explore QoR benefits, such as shorter wire lengths, reduced congestion, better timing, skew control, fewer vias, better yield, and lower dynamic and leakage power.\n       The relative placement constraints that you create and annotate implicitly generate a matrix structure of the instances and control the placement of the instances.\n You use the resulting annotated netlist for physical optimization, during which the tool preserves the structure and places it as a single entity or group, as shown in  Relative Placement in a Floorplan Relative placement groups can be floating or fixed.\n Obstructions Relative placement cells Macro Relative placement groups Standard cells"}
{"instruction": "How do I Guided Power Grid Augmentation", "input": "", "output": "Timing-driven power grid augmentation inserts PG augmentation shapes in the specified regions of the block, except around timing-critical nets.\n By default, the tool does not perform timing-driven PG augmentation.\n To perform timing-driven power grid augmentation, 1.\n Ensure that the block is fully routed with very few or no DRC violations.\n 2.\n Run voltage drop analysis using RedHawk Fusion, as described in Performing Voltage Drop Analysis The IC Validator tool uses the results of this voltage drop analysis to drive the power grid augmentation.\n 3.\n Set up the IC Validator environment as described in Setting Up the IC Validator Environment 4.\n (Optional) Enable distributed processing by using the set_host_options command, as described in Enabling IC Validator Multicore Processing 5.\n Set the application options for performing power grid augmentation, as described in Setting Options for Power Grid Augmentation At a minimum, you must define the power and ground nets on which to perform PG augmentation.\n  To specify the power net, set the signoff.create_pg_augmentation.power_net_name application option.\n  To specify the ground net, set the signoff.create_pg_augmentation.ground_net_name application option.\n If your design contains more than one power net, you must perform PG augmentation once for each power net.\n       6.\n Save the block to disk.\n When you run the signoff_create_pg_augmentation command, the IC Validator tool uses the on-disk information for the block, not the information in memory.\n To ensure accurate information, use the save_block command to save the current state of the block before running the signoff_create_pg_augmentation command.\n 7.\n Perform timing-driven power grid augmentation by using the signoff_create_pg_augmentation command with one or both of the following options:  -nets This option explicitly specifies the critical nets.\n  -timing_preserve_setup_slack_threshold This option enables the tool to automatically determine the critical nets based on a setup slack threshold.\n The unit for the slack threshold setting is the library time unit.\n You must also specify the technology node for which to perform the augmentation by using the -node option.\n If you must perform incremental PG augmentation because your design contains more than one power net, use the -mode add command on subsequent runs to append the PG augmentation shapes to the existing shapes.\n 8.\n Run voltage drop analysis using RedHawk Fusion, as described in Performing Voltage Drop Analysis This second voltage drop analysis run measures the voltage drop after power grid augmentation.\n During timing-driven PG augmentation, the signoff_create_pg_augmentation command 1.\n Performs timing analysis, including multicorner-multimode analysis, to minimize timing impact 2.\n Identifies timing-critical nets based on the options you specify 3.\n Removes existing PG augmentation shapes By default, the command removes the PG augmentation shapes from the entire block.\n To keep the existing PG augmentation shapes, use the -mode add option.\n 4.\n Inserts PG augmentation shapes for the specified regions, except in the areas around the critical nets       5.\n Saves the updated block to disk 6.\n Generates a summary report See Also  Standard Power Grid Augmentation  Guided Power Grid Augmentation  Removing PG Augmentation Shapes"}
{"instruction": "How do I Setting Up for Dynamic Power Shaping", "input": "", "output": "The Fusion Compiler tool provides the following techniques for ensuring power integrity of the design:  Dynamic power shaping (DPS), which reduces transient power by utilizing concurrent clock and data optimization (useful skew) techniques.\n  Voltage-drop-aware placement, which use the voltage (IR) drop values of cells to identify areas of high power density and spread the cells with high voltage drop values to reduce the power density of such areas.\n       To set up for the recommended power integrity flow, perform the following steps: 1.\n Enable the power integrity flow by setting the opt.common.power_integrity application option to true When you do so, the Fusion Compiler uses the following techniques during the different stages of the design flow: a.\n Dynamic power shaping (DPS) during the final_opto stage of the compile_fusion command b.\n Voltage-drop-aware placement during the clock_opt command 2.\n (Optional) Specify that the tool should not reduce the voltage drop at the expense of the timing QoR by reducing the power integrity effort level by setting the opt.common.power_integrity_effort application option to low The default is high, and by default the tool tries to reduce the voltage drop to less than eight percent of the supply voltage at the expense of the timing QoR.\n 3.\n (Optional) Specify a maximum voltage drop threshold by using the opt.common.ir_drop_threshold application option.\n The default is eight percent of the supply voltage.\n If you change the default effort level by setting the opt.common.power.integrity_effort application option to low the tool ignores the threshold specified with the opt.common.ir_drop_threshold application option.\n 4.\n Specify settings for dynamic power shaping as described in Setting Up for Dynamic Power Shaping 5.\n Specify settings for voltage-drop-aware placement as described in Setting Up for Voltage-Drop-Aware Placement 6.\n (Optional) Enable IR-driven sizing, which uses the RedHawk dynamic voltage drop analysis results to identify cells involved in voltage drop violations, and then tries to replace those cells with cells having smaller leakage current.\n To enable this feature, set the clock_opt.flow.enable_irdrivenopt application option to true, in addition to setting the opt.common.power_integrity application option to true Instead of using the recommended power integrity flow setting, you can manually enable dynamic power shaping or voltage-drop-aware placement for the compile_fusion or clock_opt commands as described in Manually Enabling Dynamic Power Shaping and Voltage-Drop-Aware Placement"}
{"instruction": "How do I Standard Power Grid Augmentation", "input": "", "output": "Power grid augmentation is a technique used to improve the instance voltage drop.\n When you augment the power grid, the added metal shapes act as parallel resistors, which reduces the resistance of the power grid as seen by the standard cells.\n shows an example of power grid augmentation and how it lowers the power grid resistance.\n  Power Grid Augmentation The Fusion Compiler tool supports the following methods of power grid augmentation:  Standard power grid augmentation, which inserts as many PG augmentation shapes as possible to lower the instance voltage drop  Timing-driven power grid augmentation, which inserts PG augmentation shapes in the specified regions of the block, except around timing-critical nets  Guided power grid augmentation, which inserts PG augmentation shapes only from cells with IR violations to their tap with the minimum path resistance Each method uses RedHawk Fusion analysis to drive the power grid augmentation and uses the IC Validator tool to insert the PG augmentation shapes.\n For detailed information about performing power grid augmentation, see the topic associated with the method you want to use.\n See Also  RedHawk and RedHawk-SC Fusion"}
{"instruction": "How do I Querying Via Ladders", "input": "", "output": "To manually insert a via ladder, 1.\n Create the shapes that compose the via ladder by using the create_shape and create_via commands.\n 2.\n Create the via ladder by using the create_via_ladder command.\n At a minimum, you must specify the shapes that compose the via ladder by using the -shapes option.\n The command creates a via ladder named VIA_LADDER_, where is a unique integer.\n Use the following options with the create_via_ladder command to specify attributes of the created via ladder:  -via_rule This option specifies the via ladder rule associated with the created via ladder.\n It sets the via_rule_name attribute of the created via ladder.\n  -electromigration This option specifies that the via ladder is an electromigration via ladder.\n It sets the is_electromigration attribute of the created via ladder to true  -high_performance This option specifies that the via ladder is a performance via ladder.\n It sets the is_high_performance attribute of the created via ladder to true  -pattern_must_join This option specifies that the via ladder is a pattern-must-join via ladder.\n It sets the is_pattern_must_join attribute of the created via ladder to true  -pin This option specifies the physical pin connected to the created via ladder.\n It sets the pin attribute of the created via ladder.\n See Also  Querying Via Ladders  Removing Via Ladders"}
{"instruction": "How do I Finding and Fixing Antenna Violations", "input": "", "output": "After inserting the boundary cells with the compile_boundary_cells command, verify the placement by using the check_boundary_cells command.\n To create an error data file to view the errors in the error browser, use the -error_view option.\n For information about using the error browser, see the Fusion Compiler Graphical User Interface User Guide This command checks the boundary cell placement for the following issues:  Missing boundary or corner cells This check verifies that there are boundary and corner cells around the entire boundary, with no gaps.\n shows a valid placement, as well as errors caused by missing cells.\n        Check for Missing Boundary or Corner Cells Note: For established nodes, boundary cells are inserted only on the left and right sides.\n In this case, the command verifies only that there are no gaps on these sides.\n  Incorrect boundary or corner cells This check verifies that the library cells used for the boundary and corner cells match the cells specified by the set_boundary_cell_rules command.\n  Incorrect orientation of boundary cells This check verifies that the orientation of each boundary cell matches the allowed orientations specified by the set_boundary_cell_rules command.\n  Short rows and edges This check verifies that each row of boundary cells is wider than the value specified by the set_boundary_cell_rules -min_row_width option and that the horizontal edges of each blockage are greater than the value specified by the set_boundary_cell_rules -min_horizontal_jog option."}
{"instruction": "How do I Restricting Library Cell Usage", "input": "", "output": "Library configuration allows you to specify which vendor libraries to use as reference libraries for the current design.\n You specify the technology file, physical libraries, and logic libraries by using the search_path and link_library variables, and then you use the create_lib or set_ref_libs command to assemble the cell libraries.\n       During library configuration,  The Fusion Compiler tool automatically calls the Library Manager tool without user intervention to generate cell libraries, as shown in the following figure:.frame files ASCII technology file.db files Cell libraries Synthesis  The tool saves the generated cell libraries to disk and adds them to the reference library list of the design library.\n  These cell libraries are the same as when the cell libraries are created during library preparation in the Library Manager tool.\n For more information, see the Configuring Cell Libraries topic in the Fusion Compiler Data Model User Guide"}
{"instruction": "How do I Handling Design Data Using the Early Data Check Manager", "input": "", "output": "The Fusion Compiler tool requires that the preferred routing direction is specified for the routing layers defined in the technology file.\n Typically this information is defined in the cell library.\n To set or change the preferred routing direction for a layer, use the following syntax to set its routing_direction attribute: set_attribute -objects layers -name routing_direction -value vertical | horizontal Specify the routing layers by using the layer names from the technology file.\n The layer direction set with this attribute applies only to the current block.\n For example, to set the preferred routing direction to vertical for the M5 and M7 layers, use the following command: fc_shell> set_attribute -objects [get_layers {M5 M7}] \\ -name routing_direction -value vertical Note: Settings made with the create_routing_guide -switch_preferred_direction command, which changes the preferred direction within the area that is covered by the routing guide, override the routing_direction attribute settings.\n To report the user-defined preferred routing direction for one or more routing layers, use the get_attribute command.\n To remove the user-defined preferred routing direction for one or more routing layers, use the remove_attributes command.\n       See Also  Preparing Routing Layers"}
{"instruction": "How do I Performing Shielding Checks", "input": "", "output": "To report the shielding statistics, use the report_shields command.\n Note: In some cases there is a slight difference in the shielding ratios reported by the create_shields and report_shields commands.\n This is due to graph connectivity differences between the two commands.\n When the reported values differ, use the values reported by the report_shields command.\n The difference in reported values is typically less than three percent, but can be up to five percent.\n By default, the power and ground structure is not included in the shielding ratio calculation.\n To include the power and ground structure within a threshold distance in the shielding ratio calculation, set the route.common.pg_shield_distance_threshold application option, which specifies the distance threshold in microns.\n fc_shell> set_app_options \\ -name route.common.pg_shield_distance_threshold -value distance The default report generated by the report_shields command provides overall statistics, as shown in Reporting Shielding Statistics Example 23 Default Shielding Report fc_shell> report_shields...\n Shielded 82% side-wall of (reset) Shielded 96% side-wall of (clk) Shielded 2 nets with average ratio as follows.\n 1) 89.00% (total shield ratio/number of shielded nets) 2) 87.92% (total shield length/total shielded net length) You can output the statistics for each layer by using the -per_layer true option with the report_shields command, as shown in Example 24 Example 24 Layer-Based Shielding Report fc_shell> report_shields -per_layer true...\n Shielded 82% side-wall of (reset) Layer: M1 ratio: 0% Layer: M2 ratio: 40%       Layer: M3 ratio: 85% Layer: M4 ratio: 80% Shielded 96% side-wall of (clk) Layer: M3 ratio: 0% Layer: M4 ratio: 96% Layer: M5 ratio: 100% Shielded 2 nets with average ratio as follows.\n 1) 89.00% (total shield ratio/number of shielded nets) 2) 87.92% (total shield length/total shielded net length)"}
{"instruction": "How do I Enabling Clock Tree Power Reduction Techniques", "input": "", "output": "When you perform clock tree synthesis, by default, the tool removes all preexisting clock buffers and inverters.\n You can prevent this by setting the cts.compile.remove_existing_clock_trees application option to false To preserve specific preexisting clock buffers or inverters, apply a don't touch or size-only exception on them."}
{"instruction": "How do I Displaying Block-Level Rail Results", "input": "", "output": "When the checking or analysis is complete, run the report_rail_result command to write the analysis or checking results to a text file.\n The power unit in the report file is watts, the current unit is amperes, and the voltage unit is volts.\n To display block-level rail results from top-level RAIL_DATABASE for debugging purposes, use the -top_design and -block_instance options.\n For more information, see Displaying Block-Level Rail Results To write rail results for the cells or geometries in the hotspot area only, use the get_instance_result and get_geometry_result commands.\n For more information, see Generating Instance-Based Analysis Reports and Generating Geometry-Based Analysis Reports       To specify the data or error types to report, use the -type option.\n Table 69 lists the supported data and error types.\n To limit the number of the elements to report, use the -limit option.\n Set the value to zero to write all elements to the output file in descending order.\n To filter the data to write to the report file, use the -threshold option.\n The tool writes the data greater than the specified threshold to the output file.\n The option is valid only for the following data types:  pg_pin_power  voltage_drop_or_rise  effective_voltage_drop To restrict the report to the specified supply nets, use the -supply_nets option.\n This option is valid only for the following data types:  effective_voltage_drop  instance_minimum_path_resistance  minimum_path_resistance  pg_pin_power  voltage_drop_or_rise Table 69 Supported Data and Error Types for report_rail_result Type Description effective_voltage_drop  effective_resistance  instance_minimum_path_resistance  instance_power  minimum_path_resistance   missing_vias   pg_pin_power  unconnected_instances         Table 69 Supported Data and Error Types for report_rail_result (Continued) Type Description voltage_drop_or_rise  In the output report file, the tool lists the data in different formats.\n For example:  For the effective_voltage_drop type,  When reporting the static analysis results, the format is cell_instance_pg_pin_name mapped_pg_pin_name supply_net_name effective_voltage_drop  When reporting the dynamic analysis results, the format is cell_instance_pg_pin_name mapped_pg_pin_name supply_net_name average_effective_voltage_drop_in_tw max_effective_voltage_drop_in_tw min_effective_voltage_drop_in_tw max_effective_voltage_drop  For the instance_minimum_path_resistance type, the result is sorted by the Total_R value in the following format: Supply_net Total_R(ohm) R_to_power(ohm) R_to_ground(ohm) Location Pin_name Instance_name  For the minimum_path_resistance type, the format is full_path_cell_instance_name/pg_pin_name resistance  For the missing_vias type, the format is net_name via_location_x_y top_metal_layer bottom_via_layer delta_voltage  For the pg_pin_power type, the format is full_path_cell_instance_name pg_pin_name power  For the voltage_drop_or_rise type, the format is full_path_cell_instance_name/pg_pin_name voltage  For the unconnected_instances error type, the format depends on the error condition.\n  When either or both of power and ground nets are physically disconnected from ideal voltage sources, the format is unconnected_type {net_names} instance_name instance bbox        When the power or ground nets are either floating or logically floating but physically connected to ideal voltage sources, the format is unconnected_type {net_names} instance_name:pin_name instance bbox The following example writes a file containing the top five PG pin power values to an output file called power.rpt.\n fc_shell> open_rail_result fc_shell> report_rail_result -type pg_pin_power -limit 5 \\ -supply_nets { VSS } power.rpt fc_shell> sh cat power.rpt FI2/vss 4.81004e-06 FI6/vss 4.80959e-06 FI3/vss 4.79702e-06 FI4/vss 4.79684e-06 FI1/vss 4.79594e-06...\n The following example writes the calculated effective voltage drop values for the VDD and VSS nets to an output file called inst_effvd.rpt.\n fc_shell> open_rail_result fc_shell> report_rail_result -type effective_voltage_drop \\ -supply_nets { VDD VSS } inst_effvd.rpt fc_shell> sh cat inst_effvd.rpt  #cell_instance_pg_pin_name mapped_pg_pin_name supply_net_name #average_effective_voltage_drop_in_tw #max_effective_voltage_drop_in_tw min_effective_voltage_drop_in_tw #max_effective_voltage_drop  S32_reg_14_/VDD VSS VDD -7.750034332e-03 -1.017999649e-02 -5.850076675e-03 -1.029992104e-02  S32_reg_15_/VDD VSS VDD -7.709980011e-03 -1.013994217e-02 -5.810022354e-03 -1.026010513e-02  S46_reg_8_/VDD VSS VDD -7.179975510e-03 -9.490013123e-03 -5.330085754e-03 -1.021003723e-02...\n The following example writes the minimum path resistance values of cell instances to an output file called inst_minres.rpt.\n fc_shell> open_rail_result fc_shell> report_rail_result -type instance_minimum_path_resistance \\ -supply_nets { VDD VSS } inst_minres.rpt fc_shell> sh cat inst_minres.rpt  Rmax(ohm) = 162.738       Rmin(ohm) = 4.418 ====================================================================== Supply_net Total_R(ohm) R_to_power(ohm) R_to_ground(ohm) Location Pin_name Instance_name ====================================================================== VDD 162.738 152.686 10.052 586.000, 496.235 VDDL dataout_44_u VSS 156.266 137.963 18.303 544.000, 613.235 VSS GPRs/U2317 VDD 156.266 137.963 18.303 544.000, 613.235 TVDD GPRs/U2317 VDD 152.690 135.223 17.467 602.400, 467.435 VDDL dataout_33_u"}
{"instruction": "How do I Routing Clock Nets", "input": "", "output": "The Fusion Compiler tool provides application options that affect the individual routing engines (global routing, track assignment, and detail routing), as well as application options that affect all three routing engines.\n  For information about the application options that affect all three routing engines, see the route.common_options man page.\n  For information about the application options that affect global routing, see the route.global_options man page.\n  For information about the application options that affect track assignment, see the route.track_options man page.\n  For information about the application options that affect detail routing, see the route.detail_options man page.\n Zroute uses these option settings whenever you perform routing functions.\n When you run a routing command, Zroute writes the settings for any routing options that you have set (or that the tool has set for you) in the routing log.\n To display the settings for all routing options, not only those that have been set, set the route.common.verbose_level application option to 1.\n fc_shell> set_app_options \\ -name route.common.verbose_level -value 1"}
{"instruction": "How do I Using Setup Files", "input": "", "output": "To view your checkpoint history, navigate to the checkpoint directory.\n This directory contains a checkpoint_history.rpt file that captures the history of the checkpoints the tool encounters during each run, along with the runtime and memory usage for each checkpoint.\n The checkpoint directory also contains any checkpoint reports you have written to it.\n In some cases, you might want to clear the full or partial contents of your checkpoint history.\n A typical example is when a checkpointed run fails and you want to clear some or all of the data associated with that run, depending on whether you plan to rerun a portion of the flow or the entire flow.\n  To clear your full checkpoint history, use the reset_checkpoints command.\n fc_shell> reset_checkpoints The reset_checkpoints command clears the full contents of the checkpoint directory, including any reports you have written to it.\n However, before clearing the checkpoint directory, the reset_checkpoints command saves a timestamped copy of the directory in your run directory.\n        To clear a portion of the data in your checkpoint_history.rpt file, use the -from option to specify the name of a checkpoint.\n This option removes all checkpoints including and following the specified checkpoint from your checkpoint_history.rpt file; it does not remove any checkpoint reports you have written to the checkpoint directory.\n For example, suppose your checkpoint_history.rpt file contains the following data after your run a full checkpointed flow: Checkpoints Memory StartTime...\n remove_buffers 32290.86 2020-03-23_14:34:17...\n place_opt_to_initial_drc 36389.15 2020-03-23_17:21:56...\n incr_placement 37002.09 2020-03-23_19:13:02...\n place_opt_to_initial_opto 45655.59 2020-03-23_23:44:36...\n place_opt_to_final_opto 46322.34 2020-03-23_30:12:55...\n Suppose you want to rerun your flow beginning with incremental placement.\n To clear the last three checkpoints from your checkpoint_history.rpt file before rerunning that portion of your flow, use the following command: fc_shell> reset_checkpoints -from incr_placement After clearing the checkpoints, your checkpoint_history.rpt file lists only the first two checkpoints in your flow: Checkpoints Memory StartTime...\n remove_buffers 32290.86 2020-03-23_14:34:17...\n place_opt_to_initial_drc 36389.15 2020-03-23_17:21:56...\n Similarly, the get_checkpoint_data -list_names command returns only the remove_buffers and place_opt_to_initial_drc checkpoints: fc_shell> get_checkpoint_data -list_names remove_buffers place_opt_to_initial_drc"}
{"instruction": "How do I Generating Clock Tree QoR Reports", "input": "", "output": "After synthesizing the clock trees, analyze the results to verify that they meet your requirements.\n Typically the analysis process consists of the following tasks:  Analyzing the clock tree QoR (as described in Generating Clock Tree QoR Reports  Analyzing the clock tree timing (as described in Analyzing Clock Timing  Verifying the placement of the clock instances, using the GUI (as described in Analyzing Clock Trees in the GUI"}
{"instruction": "How do I Detail Routing", "input": "", "output": "Before you run track assignment,  Define the common routing application options For information about the common routing application options, see the route.common_options man page.\n  Define the global routing application options For information about the track assignment application options, see the route.track_options man page.\n  Complete global routing To perform standalone track assignment, run the route_track command.\n       The main task of track assignment is to assign routing tracks for each global route.\n During track assignment, Zroute performs the following tasks:  Assigns tracks in horizontal partitions.\n  Assigns tracks in vertical partitions.\n  Reroutes overlapping wires.\n After track assignment finishes, all nets are routed but not very carefully.\n There are many violations, particularly where the routing connects to pins.\n Detail routing works to correct those violations.\n Note: Because track assignment replaces the global routes with actual metal shapes, the block no longer contains global routes after track assignment completes.\n By default, the route_track command is not timing-driven or crosstalk-driven.\n  To enable timing-driven mode, set the route.track.timing_driven application option.\n  To enable crosstalk-driven mode, set the route.track.crosstalk_driven and time.si_enable_analysis application options to true At the end of track assignment, Zroute reports a summary of the wire length and via count.\n Example 20 shows a track assignment report.\n Example 20 Track Assignment Report Wire length and via report:  Number of M1 wires: 215327 : 0 Number of M2 wires: 1124740 VIA12SQ_C: 1067462...\n Total number of wires: 2508734 vias: 2769482  Total M1 wire length: 924480.9 Total M2 wire length: 4147032.0...\n Total wire length: 21281278.0  Longest M1 wire length: 1541.7 Longest M2 wire length: 926.0..."}
{"instruction": "How do I Global Routing During Design Planning", "input": "", "output": "Before you run global routing,  Define the common routing application options For information about the common routing application options, see the route.common_options man page.\n  Define the global routing application options For information about the global routing application options, see the route.global_options man page.\n To perform standalone global routing, use the route_global command.\n By default, global routing is not timing-driven.\n For information about enabling timing-driven global routing, see Timing-Driven Global Routing The global router divides a block into global routing cells.\n By default, the width of a global routing cell is the same as the height of a standard cell and is aligned with the standard cell rows.\n For each global routing cell, the routing capacity is calculated according to the blockages, pins, and routing tracks inside the cell.\n Although the nets are not assigned to the actual wire tracks during global routing, the number of nets assigned to each global routing cell is noted.\n The tool calculates the demand for wire tracks in each global routing cell and reports the overflows, which are the number of wire tracks that are still needed after the tool assigns nets to the available wire tracks in a global routing cell.\n For advanced node designs, via density can be a concern.\n To enable via density modeling, set the route.global.via_cut_modeling application option to true.\n The tool then calculates the number of vias in each global routing cell and reports via overflows.\n Global routing is done in two phases:  The initial routing phase (phase 0), in which the tool routes the unconnected nets and calculates the overflow for each global routing cell  The rerouting phases, in which the tool tries to reduce congestion by ripping up and rerouting nets around global routing cells with overflows The tool might perform several rerouting phases.\n At the end of each rerouting phase, the tool recalculates the overflows.\n You should see a reduction in the total number of global routing cells with overflow and in the total overflow numbers.\n The global router stops and exits from the rerouting phase when the congestion is solved or cannot be solved further or after the maximum number of phases has occurred, as defined by the -effort_level option.\n You can force the global router to perform the maximum number of phases based on the specified effort level by setting the route.global.force_full_effort application option to true.\n By default, the tool       uses medium effort and performs a maximum of three rerouting phases.\n You can perform up to six rerouting phases by specifying ultra effort.\n There are five global routing effort levels: minimum low medium high, and ultra  Minimum ( -effort_level minimum The minimum effort level uses two times larger global routing cells relative to the other effort levels.\n It also has a much lower congestion cost and runs only one rerouting phase.\n It should only be used for prototype routing or for an initial congestion evaluation, not for detail routing.\n  Low ( -effort_level low Low effort runs a maximum of two rerouting phases with very similar congestion cost.\n It is faster in comparison to medium effort and has reasonable QoR.\n If your block is not very congested, you can use the low effort level.\n  Medium ( -effort_level medium Medium effort is the default effort level and runs a maximum of three rerouting phases.\n Global routing stops after the third phase or when the overflow is resolved, whichever occurs first.\n  High ( -effort_level high High effort runs up to four rerouting phases.\n If your block is congested, use the high effort level.\n  Ultra ( -effort_level ultra Ultra effort runs up to six rerouting phases.\n If your block is very congested, use the ultra effort level.\n At the end of global routing, the following information is stored in the design library:  The g-links and g-vias on each routed net This information is used for the next routing steps.\n After Zroute performs track assignment and detail routing, it removes these g-links and g-vias from the design library.\n  The congestion data This information is used to generate a congestion map.\n By default, only the hard congestion data is saved in the design library.\n To also save the soft congestion data, set the route.global.export_soft_congestion_maps application option to true before performing global routing.\n The soft congestion data includes demand from soft nondefault spacing rules, as well as tool-generated soft rules.\n       The global router reports block statistics and congestion data after the initial routing phase and after each rerouting phase.\n When global routing is complete, the global router reports a summary of the wire length and via count.\n Example 19 shows a global routing report.\n In the congestion report, the Overflow value is the total number of wires in the block that do not have a corresponding track available.\n The Max value corresponds to the highest number of overutilized wires in a single global routing cell.\n The GRCs value is the total number of overcongested global routing cells in the block.\n Example 19 Global Routing Report Start Global Route...\n...\n Design statistics: Design Bounding Box (0.00,0.00,3180.00,1154.00) Number of routing layers = 10 layer M1, dir Hor, min width = 0.05, min space = 0.05 pitch = 0.15 layer M2, dir Ver, min width = 0.06, min space = 0.06 pitch = 0.15...\n Net statistics: Total number of nets = 255165 Number of nets to route = 248716 Number of single or zero port nets = 1721 4728 nets are fully connected, of which 4728 are detail routed and 0 are global routed.\n 1648 nets have non-default rule clock_spacing...\n phase3.\n Routing result: phase3.\n Both Dirs: Overflow = 3320 Max = 3 GRCs = 4405 (0.08%) phase3.\n H routing: Overflow = 1759 Max = 2 (GRCs = 1) GRCs = 2756 (0.10%) phase3.\n V routing: Overflow = 1560 Max = 3 (GRCs = 20) GRCs = 1649 (0.06%) phase3.\n M1 Overflow = 1475 Max = 2 (GRCs = 1) GRCs = 2426 (0.09%) phase3.\n M2 Overflow = 1265 Max = 3 (GRCs = 20) GRCs = 1343 (0.05%)...\n Overflow over macro areas  phase3.\n Both Dirs: Overflow = 293 Max = 2 GRCs = 300 (0.04%) phase3.\n H routing: Overflow = 133 Max = 1 (GRCs = 129) GRCs = 139 (0.03%) phase3.\n V routing: Overflow = 160 Max = 2 (GRCs = 2) GRCs = 161 (0.04%) phase3.\n M1 Overflow = 0 Max = 0 (GRCs = 0) GRCs = 0 (0.00%) phase3.\n M2 Overflow = 0 Max = 0 (GRCs = 0) GRCs = 0 (0.00%)...\n Density distribution: Layer 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 > 1.2 M1 86.6 10.5 0.37 1.46 0.02 0.44 0.18 0.14 0.07 0.03 0.10 0.00 0.00 0.02 M2 68.2 14.4 7.71 4.54 2.07 1.55 0.66 0.28 0.13 0.03 0.33 0.00 0.00 0.02...\n phase3.\n Total Wire Length = 21154552.81 phase3.\n Layer M1 wire length = 947324.93 phase3.\n Layer M2 wire length = 3959478.25...\n phase3.\n Total Number of Contacts = 2530044 phase3.\n Via VIA12SQ_C count = 1050582 phase3.\n Via VIA23SQ_C count = 856311...\n phase3.\n completed.\n       Before proceeding to detail routing, display the congestion map in the GUI and check the overflow distribution.\n The congestion report and map help you to identify congested areas.\n For more information about the congestion report, see Generating a Congestion Report For more information about the congestion map, see Generating a Congestion Map"}
{"instruction": "How do I Automatically Adding Spare Cells", "input": "", "output": "You can manually instantiate spare cells in a block by using one of the following methods:  Instantiating them in the Verilog netlist  Adding them by using netlist editing commands such as create_cell connect_pin s, and so on When manually instantiating spare cells, you should  Evenly distribute the spare cells through the logical hierarchy to better handle ECO changes anywhere in the block  Tie their inputs to power or ground, as appropriate, to prevent the spare cells from creating noise and consuming power       If a cell meets the following criteria, the tool automatically identifies it as a spare cell:  It is not a physical-only cell  All inputs except for the clock pin are unconnected or tied to a logic constant The clock pin of the spare cell, if any, can be connected to the clock network.\n  All outputs are unconnected If you instantiate the spare cells before you perform physical synthesis on a block, the tool places and legalizes the spare cells during the subsequent physical synthesis steps.\n However, if you instantiate the spare cells in a block that is optimized, placed and legalized, you must place and legalize the spare cells by using the following steps: 1.\n Spread the spare cells by using the spread_spare_cells command.\n By default, this command distributes and places all the spare cells evenly throughout the core area.\n You can specify the spare cells to place by using one of the following two methods:  To place specific spare cells, use the -cells option.\n  To place all the spare cells that belong to specific voltage areas, use the -voltage_areas option.\n You can control the placement of the spare cells as follows:  Specify an area within which to place the spare cells by using the -boundary option.\n  Ignore specific types of placement blockages by using the -ignore_blockage_types option.\n By default, the add_spare_cells command honors all placement blockage types.\n  Ignore the current cell density and place the spare cells randomly by using the -random_distribution option.\n  Specify a percentage of spare cells to be placed based on the cell density distribution by using the -density_aware_ratio option.\n The rest of the spare cells are placed randomly throughout the design.\n By default, all the spare cells are placed based on the cell density distribution.\n For example, if you specify a setting of -density_aware_ratio 80, the tool places 80 percent of the spare cells based on the cell density distribution and 20 percent randomly across the design.\n 2.\n Legalize the spare cells by using the place_eco_cells -legalize_only -cells command."}
{"instruction": "How do I Modifying Metal Fill", "input": "", "output": "To automatically remove track based metal fill shapes that are causing DRC violations after ECO, use the -remove_by_rule drc_auto option with the signoff_create_metal_fill command.\n This action removes DRC violating track based metal fill shapes or vias from design, including any floating connections created due to removal of DRC violating shape.\n For example, fc_shell> signoff_create_metal_fill -remove_by_rule drc_auto"}
{"instruction": "How do I Specifying Timing Constraints and Settings", "input": "", "output": "To ensure that the design does not have any multivoltage design violations, use the check_mv_design command.\n You can restrict the types of rules to check with the check_mv_design command.\n For example, the -isolation option specifies to check isolation strategy and isolation cell rules, while the -pg_pin option specifies to check rules associated with PG pins.\n To report multivoltage information for your design, use the commands shown in the following table.\n       Table 6 Commands for Reporting Multivoltage Information To do this Use this command  report_mv_design  report_mv_cells  report_mv_lib_cells   report_mv_path  report_power_domains   check_equivalent_power_domains  get_equivalent_power_domains  report_voltage_areas"}
{"instruction": "How do I Improving Yield By Limiting the Percentage of", "input": "", "output": "Total power optimization considers the combined leakage- and dynamic-power cost during optimization.\n To setup for total-power optimization, 1.\n Setup for leakage-power optimization by performing the following steps: a.\n Ensure that at least one scenario is enabled for leakage-power optimization by using the set_scenario_status -leakage_power true command.\n 2.\n Setup for dynamic-power optimization by performing the following steps: a.\n Annotate switching activity on the design, as described in Annotating the Switching Activity b.\n Ensure that at least one scenario is enabled for dynamic-power optimization by using the set_scenario_status -dynamic_power true command.\n When you use this setting, the tool performs total-power optimization during the clock_opt command."}
{"instruction": "How do I Defining Custom Vias", "input": "", "output": "To define a simple via or via array, use the following create_via_def syntax: create_via_def -cut_layer layer -cut_size { width height -upper_enclosure { width height -lower_enclosure { width height [-min_rows number_of_rows [-min_columns number_of_columns [-min_cut_spacing distance [-cut_pattern cut_pattern [-is_default] [-force] via_def_name You do not need to specify the enclosure layers; the tool determines them from the technology file by getting the metal layers adjacent to the specified via layer.\n       By default, when you define a simple via, it is a single-cut via.\n To define a via array, specify the minimum number of rows and columns for the array ( -min_rows and -min_columns options), as well as the minimum cut spacing ( -min_cut_spacing option).\n By default, the cut pattern is a full array of cuts.\n To modify the cut pattern, use the -cut_pattern option.\n To overwrite an existing via definition, use the -force option; otherwise, the command fails if the specified via definition already exists.\n For example, to create a single-cut via definition named design_via1_HV for the VIA12 via layer, use the following command: fc_shell> create_via_def design_via1_HV \\ -cut_layer VIA12 -cut_size {0.05 0.05} \\ -lower_enclosure {0.02 0.0} -upper_enclosure {0.0 0.2} \\ To create a via definition with an alternating 2x2 cut pattern in which the lower-left cut is omitted, use the following command: fc_shell> create_via_def design_via12 -cut_pattern \"01 10\" To report information about the user-defined via definitions, use the report_via_defs command."}
{"instruction": "How do I Performing Auto Grouping Flow", "input": "", "output": "The group repeater query commands provide improved ease of use for interconnect repeater while planning.\n The group repeater query  Provides capability to query group repeater path info  Supports cross-session query operation  Supports query operation before group repeater placement To query the group repeater during planning, you must be familiar with the following key information  Cells of a repeater group  Cutline of a repeater group  Group virtual connection If a group has either driver group or path drivers, then it is categorized as group virtual connection.\n Cell connection between groups depend on cell order in a group.\n The following topics describe the steps in this flow  Performing Auto Grouping Flow  Performing Manual Grouping Flow  Cell Input Mode"}
{"instruction": "How do I Adding Buffers at a Specified Interval", "input": "", "output": "To add buffers at specific locations of a net, specify  A list of one or more library cells to select from by using the -lib_cell option  The exact locations by using the -location option With the -location option, you must specify the x- and y-coordinates and a layer with an existing routing shape at that location by using the {x1 y1 layer1....} format.\n Instead of specifying the routing layer, you can have the tool automatically detect the closest routing shape to that location by using the -detect_layer option.\n With this method, you can add cells only on one net at a time.\n The following example adds the BUF1 library cell on the net1 net at location (100, 200) and connects it to an existing route shape on the M4 layer.\n fc_shell> add_buffer_on_route net1 -lib_cell BUF1 \\ -location {100 200 M4}       By default, the tool uses eco_cell and eco_net as the name prefix of all new cells and nets.\n To specify a name prefix for the new cells and nets, use the -cell_prefix and -net_prefix options, respectively."}
{"instruction": "How do I Removing Routing Corridors", "input": "", "output": "To report the routing corridors in your block, use the report_routing_corridors command.\n By default, the command reports all routing corridors; to report specific routing corridors, specify the routing corridors to report.\n By default, the command reports the following information for each routing corridor: its name; the shapes associated with the routing corridor, including their names, minimum routing layer, maximum routing layer, and boundary; the connectivity of the routing corridor shapes; and the nets and supernets associated with the routing corridor.\n To output a Tcl script that re-creates the routing corridors, use the -output option.\n To report the routing corridor for a specific net or supernet, use the get_routing_corridors command.\n When you run this command, you must use the -of_objects option to specify the nets of interest."}
{"instruction": "How do I Using a DDR Routing Flow", "input": "", "output": "In the hybrid flow, Custom Router is used to perform trunk routing without completing some pin connections, and then Zroute is used to complete the pin connections.\n This flow enables the use of Custom Router for prerouting while avoiding or reducing inconsistency and DRC violations associated with differences in the pin connection behavior between the two tools.\n To enable the hybrid routing flow, set the custom.route.skip_connect_pin_type application option to one or more of the following values, depending on your routing requirements.\n By default, this application option is set to none, which disables the hybrid routing flow.\n  auto Custom Router identifies the routability of the pins in the block, and then completes the pin connections for the pins that can be connected.\n  all Custom Router does not complete the pin connections for all pins in the block.\n  stdcell Custom Router completes the pin connections for all pins in the block, except for standard cell pins.\n  io Custom Router completes the pin connections for all pins in the block, except for I/O cell pins.\n  macro Custom Router completes the pin connections for all pins in the block, except for macro pins.\n By default, when the hybrid flow is enabled, Custom Router determines how much space to leave between a route and an unconnected pin.\n To specify the maximum distance,       use the custom.route.distance_to_net_pin application option.\n Set the value in the following format: net_name distance The following script uses Custom Router for trunk routing and Zroute for all pin connections.\n set_app_options -name custom.route.skip_connect_pin_type -value all route_custom -nets {net1 net2} route_eco -nets {net1 net2} remove_redundant_shapes -nets {net1 net2} route_detail -incremental true The following script uses Custom Router for trunk routing and pin connections except standard cell pins, and then uses Zroute for standard cell pin connections.\n set_routing_rule -min_routing_layer m3 \\ -max_routing_layer m9 \\ -min_layer_mode allow_pin_connection \\ -max_layer_mode hard {net1 net2} set_app_options -name custom.route.skip_connect_pin_type \\ -value stdcell set_app_options -name custom.route.distance_to_net_pin \\ -value {{{net1 5.0} {net2 3.0}}} route_custom -nets {net1 net2} route_eco -nets {net1 net2} remove_redundant_shapes -nets {net1 net2} route_detail -incremental true"}
{"instruction": "How do I Specifying Settings for Multivoltage Designs", "input": "", "output": "By default, the add_buffer_on_route command  Adds buffers over all placement blockages, soft macros, and hard macros.\n To prevent buffers from being placed over specific macro cells, use the -dont_allow_insertion_over_cell option.\n Alternatively, you can prevent the buffers from being placed over all blockages and macro cells by using the -respect_blockages option.\n When you use this option, you can specify a list of macro cells over which buffers are allowed by using the -allow_insertion_over_cell option.\n  Adds buffers on both global routed and detail routed nets.\n To add buffers only on nets that are global routed only, without assigned tracks or detail routing, use the -only_global_routed_nets option.\n  Adds buffers at the lowest common level of hierarchy of the pins being driven by the buffers.\n To add the buffers on the highest possible level of hierarchy, use the -on_top_hierarchy option.\n  Does not add buffers on a route segment if it is necessary to create new ports because of a difference in the logical and physical topology of the net.\n To add buffers on such route segments by creating new ports, use the -punch_port option."}
{"instruction": "How do I Loading and Applying UPF Information", "input": "", "output": "To learn about applying the multivoltage power intent, see  Loading and Applying UPF Information  Using Incomplete UPF Information  Specifying UPF Constraints for Physical-Only Cells  Saving UPF Information"}
{"instruction": "How do I Defining Move Bounds", "input": "", "output": "A placement bound is a constraint that controls the placement of groups of cells.\n It allows you to group cells to minimize wire length and place the cells at the most appropriate locations.\n For example, you might want to define a bound for clock-gating cells or extremely timing-critical groups of cells that you want to guarantee will not be disrupted for placement by other logic.\n Table 8 lists the types of placement bounds supported by the Fusion Compiler tool.\n Table 8 Types of Placement Bounds Bound type Description                  Table 8 Types of Placement Bounds (Continued) Bound type Description       opt.buffering.exclusive_hard_bound_buffering_mode                To define a placement bound, use the create_bounds comandcomandscreate_bounds create_bound command.\n When you define a bound, you must use the -name option to specify its name.\n In general, you also specify the cells and ports to be included in the bound.\n If a hierarchical cell is included, all cells in the subdesign belong to the bound.\n However, you can create an empty bound and specify the contents of the bound later by using the add_to_bound command.\n You can remove objects from a bound by using the remove_from_bound command.\n You must also specify the options required for the specific type of bound you want to create.\n The following topics describe how to create the various types of move bounds:  Defining Move Bounds  Defining Group Bounds  Querying Placement Bounds  Removing Placement Bounds"}
{"instruction": "How do I Creating New Relative Placement Groups in a Placed Design", "input": "", "output": "You can improve the placement of relative placement groups in a placed design by legalizing only the relative placement groups.\n To legalize the placement of only the relative placement groups, but not nonrelative placement cells, use the legalize_rp_groups command.\n You can also specify a list of relative placement groups to be legalized.\n To perform a fast legalization of the relative placement groups, use the -prototype option.\n When you use this option, the tool does not ensure that all legalization constraints are met.\n Therefore, use it during the prototyping stages of the design flow when you are developing relative placement groups.\n To specify that relative placement groups can overlap with each other, use the -legalize_over_rp option.\n For example, the following command legalizes the RP3 relative placement group over the RP1 and RP2 relative placement groups.\n fc_shell> legalize_rp_groups -legalize_over_rp RP3  shows the placement before and after running the command.\n        Legalizing the RP3 Relative Placement Group Over Other Groups RP1 RP2 RP1 RP2 RP3 After you legalize one or more relative placement groups by using the legalize_rp_groups command, there might be overlaps with cells in other relative placement groups or cells that are not in relative placement groups.\n Use the check_legality command to identify any cell overlaps and use the legalize_placement command to resolve any remaining cell overlaps."}
{"instruction": "How do I Setting Options for Fixing Isolated Vias", "input": "", "output": "An isolated via is a via that does not have neighboring vias close enough to meet the requirements of the technology.\n To check for and fix isolated vias, 1.\n Set up the IC Validator environment as described in Setting Up the IC Validator Environment 2.\n (Optional) Enable distributed processing by using the set_host_options command, as described in Enabling IC Validator Multicore Processing 3.\n Set the application options for fixing isolated vias.\n At a minimum, you must define the maximum distance within which a neighboring via must exist so that a via is not considered an isolated via.\n To define this information for each via layer, set the signoff.fix_isolated_via.isolated_via_max_range application option, which has the following syntax: { { via_layer distance }...\n { via_layer distance } } where the via_layer argument uses the mask names, such as via1, and the distance argument is in microns.\n For information about the options available for fixing isolated vias, see Setting Options for Fixing Isolated Vias 4.\n Run isolated via checking and fixing by using the signoff_fix_isolated_via command as described in Running the signoff_fix_isolated_via Command"}
{"instruction": "How do I Setting up Transparent Hierarchy Optimization", "input": "", "output": "Before enabling the THO settings, you must ensure that:  All the subblock libraries are opened in mode.\n You must open the parent library using the -ref_libs_for_edit option with the open_lib command.\n  The design view of the blocks is editable."}
{"instruction": "How do I Specifying Tap Assignment Options and Settings", "input": "", "output": "To perform this task, you must 1.\n Specify the required settings by using the set_regular_multisource_clock_tree_options command as follows:  The clock name by using the -clock option  The topology for the global clock tree by using the -topology htree_only option        The number of rows and columns of tap cells to insert by using the -tap_boxes option  A list of library cells to use as tap drivers by using the -tap_lib_cells option The library cells you specify must be enabled for clock tree synthesis by using the set_lib_cell_purpose -include cts command.\n  A list of library cells to use for synthesizing the global clock tree (H-tree) by using the -htree_lib_cells option The library cells you specify must be enabled for clock tree synthesis by using the set_lib_cell_purpose -include cts command.\n By default, the tool uses  The net connected to the clock root to insert the tap drivers and build the clock structure You can specify a different net of the clock network by using the -net option.\n  The locations of the sinks to determine the boundary within which to insert the tap cells You can control this boundary by using the -tap_boundary option.\n You can also specify keepout areas within which tap cells should not be inserted by using the -keepouts option.\n  The layers and routing rules specified by the clock tree synthesis settings to route the H-tree You can specify the layers and routing rules for the H-tree by using the -htree_layers and -htree_routing_rule options.\n You can report the options you specified by using the report_regular_multisource_clock_tree_options command, and remove them by using the remove_regular_multisource_clock_tree_options command.\n 2.\n (Optional) Enable flexible H-tree synthesis by setting the cts.multisource.flexible_htree_synthesis application option to true When you enable this feature, the tool identifies the best count, configuration, and placement of tap drivers for minimum clock tree insertion delay.\n To specify that the tool derives a symmetric configuration of tap drivers, set the cts.multisource.tap_selection application option to symmetric.\n The default is user, and by default the tool uses the configuration specified by the -tap_boxes option of the set_regular_multisource_clock_tree_options command for the tap drivers.\n       3.\n Insert the tap drivers and build the H-tree by using the synthesize_regular_multisource_clock_trees command, which consists of the following two stages:  tap_synthesis, during which the tool inserts the tap drivers  htree_synthesis, during which the tool builds the H-tree that drives the tap drivers By default, the tool performs both stages.\n You can perform only one of these stages by using the -to or -from option.\n The following example script inserts tap drivers in four columns and two rows and builds the H-tree: ## Include only tap driver and H-tree library cell for CTS set cts_references [get_lib_cells -filter valid_purposes=~*cts*] set_lib_cell_purpose -exclude cts [get_lib_cells $cts_references] set_lib_cell_purpose -include cts { LIB1/BUF1 LIB1/BUF2}  ## Set up and insert tap drivers and build H-tree set_regular_multisource_clock_tree_options \\ -clock clk -topology htree_only -tap_boxes {4 2} \\ -tap_lib_cells [get_lib_cells */CKBUF*] \\ -htree_lib_cells [get_lib_cells */CKINV*] \\ -htree_layers \"m9 m10\" -htree_routing_rule \"htree_ndr\" synthesize_regular_multisource_clock_trees  ## Reapply CTS library cell list as required for the subsequent steps set_lib_cell_purpose -include cts [get_lib_cells $cts_references]"}
{"instruction": "How do I Specifying Automatic Via Ladder Insertion Settings for Preroute", "input": "", "output": "To improve correlation with the postroute stage of the design flow, the tool can perform global routing, perform extraction based on the global routes, and use this parasitic information when performing optimization.\n If the tool detects that the technology used by the design is an advanced technology node that is less than 16 nm, the tool enables this feature by default.\n To enable it for any technology, set the opt.common.enable_rde application option to true When enabled, the tool performs route-driven parasitic estimation during the final_opt stage of the compile_fusion and clock_opt commands.\n The parasitic information is       stored in the design library and used during subsequent optimization steps.\n If you enable this feature, you should enable it for all subsequent preroute optimization steps in the design flow.\n During route-driven estimation, the tool honors the capacitance and resistance scaling factors specified with the -early_cap_scale -late_cap_scale -early_res_scale and -late_res_scale options of the set_extraction_options command.\n When route-driven estimation is enabled, the tool ignores the setting of the opt.common.use_route_aware_estimation application option, which enables global- route-layer-based (GRLB) RC estimation."}
{"instruction": "How do I Adding Changes to a Script With Checkpoints", "input": "", "output": "You can use Tcl scripts to accomplish routine, repetitive, or complex tasks.\n You create a command script file by placing a sequence of Fusion Compiler commands in a text file.\n Any Fusion Compiler command can be executed within a script file.\n In Tcl, a pound sign (#) at the beginning of a line denotes a comment.\n For example, # This is a comment For more information about writing scripts and script files, see the Using Tcl With Synopsys Tools manual.\n Use one of the following methods to run a Tcl script:  Use the -file option with the fc_shell command when you start the Fusion Compiler tool.\n  Use the source command from the fc_shell command line.\n  Choose File > Execute Script in the GUI.\n If an error occurs when running a command, the Fusion Compiler tool raises the TCL_ERROR condition, which immediately stops the script execution.\n To tolerate errors and allow the script to continue executing, either  Check for TCL_ERROR error conditions with the Tcl catch command on the commands that might generate errors.\n  Set the sh_continue_on_error variable to true in the script file.\n       See Also  Starting the CLI  Adding Changes to a Script With Checkpoints"}
{"instruction": "How do I Displaying Objects for Design Rule Checking", "input": "", "output": "The Live DRC feature performs interactive signoff design rule checking in the Fusion Compiler GUI.\n Note: This feature requires version P-2019.06 or later of the IC Validator tool.\n To perform interactive signoff design rule checking using Live DRC, 1.\n Set up the IC Validator environment as described in Setting Up the IC Validator Environment 2.\n Set the application options for interactive signoff design rule checking.\n At a minimum, you must specify the following information:  The foundry runset to use for design rule checking by setting the signoff.check_drc_live.runset application option  The layer mapping file that maps the technology file layers to the runset layers by setting the signoff.physical.layer_map_file application option For information about the options for interactive signoff design rule checking, see Setting Options for Interactive Design Rule Checking 3.\n Ensure that the layout window displays the objects on which you want to perform design rule checking.\n By default, interactive design rule checking uses the frame view of each cell.\n If you want more detail than what is provided in the frame view, use the GDSII or OASIS view for specific cells by specifying the files containing the cells in the signoff.physical.merge_stream_files application option.\n For example, fc_shell> set_app_options \\ -name signoff.physical.merge_stream_files \\ -value {stdcell.gds macro.oas} For information about controlling the objects displayed for design rule checking, see Displaying Objects for Design Rule Checking 4.\n Display the DRC toolbar by right-clicking in the GUI menu bar and selecting DRC Tools.\n For details about the icons in the DRC toolbar, see DRC Toolbar 5.\n Configure the rules to use for design rule checking by choosing Edit > Options > ICV- Live or by clicking the gear icon ( ) in the DRC toolbar.\n       To exclude all density or connectivity rules from design rule checking, set the signoff.check_drc_live.exclude_command_class application option.\n 6.\n Run design rule checking by choosing Edit > ICV > Run ICV-Live on Current View or by clicking the ICV Run button ( ) in the DRC toolbar.\n The first time that you run Live DRC, the tool performs the following initialization steps, which can take a couple of minutes:  Processes the IC Validator runset to get the list of available rules and caches the runset  Processes the GDSII or OASIS files specified in the signoff.physical.merge_stream_files application option Unless the layers, rules, or stream files change, subsequent runs do not perform this caching and therefore are much quicker.\n To reduce the initial runtime, you can perform this initialization before running Live DRC by using the signoff_init_live_drc command.\n The tool displays the design rule checking results in the error browser.\n You can select errors in the error browser and then fix them interactively.\n For information about using the error browser, see Using the Error Browser See Also  Running the signoff_check_drc Command"}
{"instruction": "How do I Fixing Diodes", "input": "", "output": "The -diode option of the fix_mv_design command fixes mismatches between power domains and voltage areas for diodes throughout the design.\n The fix_mv_design -diode command examines all diodes, including diodes inserted by the create_cell create_diodes, and add_port_protection_diodes commands, as well as several routing commands that insert diodes to fix antenna violations.\n If a power domain and voltage area mismatch for a diode is found, the tool uses the voltage area to explicitly assign a corresponding power domain.\n If multiple power domains exist in a single voltage area, the tool picks one of the available power domains.\n Any of the available power domains is acceptable because all diode cells are single-rail cells and all power domains in the same voltage area have the same primary supply.\n       To obtain a report of the changes that the tool recommends without making any netlist changes, use the -report_only option.\n To obtain detailed information in the report, use the -verbose option.\n These are the only two options that you can use with the -diode option."}
{"instruction": "How do I Performing Unified Physical Synthesis", "input": "", "output": "The Fusion Compiler tool synthesizes the RTL descriptions into optimized gate-level designs, optimizes them to achieve the best quality of results (QoR) in speed, area, and power, and allows early design exploration and prototyping.\n Topics in this section include  Performing Unified Physical Synthesis  Controlling Mapping and Optimization  Performing Multibit Optimization  Running Concurrent Clock and Data Optimization  Performing Test Insertion and Scan Synthesis  Specifying Settings for Performance, Power, and Area Improvement  Performing Design Analysis"}
{"instruction": "How do I Reporting Concurrent Clock and Data Timing", "input": "", "output": "To improve QoR by removing buffers or pairs of inverters when concurrent clock and data optimization is performed during route_opt command, set the ccd.post_route_buffer_removal application option to true, as shown in the following example: fc_shell> set_app_options -name route_opt.flow.enable_ccd \\ -value true fc_shell> set_app_options -name ccd.post_route_buffer_removal \\ -value true fc_shell> route_opt"}
{"instruction": "How do I Clock-Gating Prerequisite Conditions", "input": "", "output": "The tool uses the following definitions for clock-gate levels and stages in clock-gating commands and options:  The clock-gate level is related to the fanin of a clock gate.\n  Level 0 refers to a clock source.\n  Level 1 refers to the first clock gate downstream from the clock source.\n  Level 2 refers to a clock gate driven by a clock gate of level 1.\n  Level N refers to a clock gate driven by a clock gate of level N-1.\n  The clock-gate stage is related to the fanout of a clock gate.\n  Stage 0 refers to a register driven by a clock gate.\n  Stage 1 refers to a clock gate that only drives registers directly or across buffers, inverters, or hierarchical transitions.\n In addition, stage 1 is assigned to clock gates whose stage cannot be determined, such as unloaded clock gates.\n  Stage N refers to a clock gate that drives at least one clock gate of stage N-1, either directly or across buffers, inverters, or hierarchical transitions.\n The tool keeps a separate count of levels and stages for all clock gates and for tool- inserted clock gates.\n  shows a simple circuit with two preexisting clock gates (P1 and P2) and three tool-inserted clock gates (T1, T2, and T3).\n Table 23 lists the levels and stages for each of the clock gates.\n        Design With Multiple Clock Gates Table 23 Clock-Gate Levels and Stages Object Type Level (all clock gates) Level (tool-inserted clock gates) Stage (all clock gates) Stage (tool-inserted clock gates)"}
{"instruction": "How do I Chip Finishing and Design for Manufacturing", "input": "", "output": "Cut metal shapes are used in double-patterning designs to reduce the line-end minimum spacing.\n A cut metal layer and the dimensions of the cut metal shapes inserted on that layer are defined in a Layer section in the technology file.\n The width of the cut metal shapes is defined by the cutMetalWidth attribute.\n The height of the cut metal shapes is defined by either the cutMetalHeight or cutMetalExtension attribute.\n A cut metal layer corresponds to the metal layer with the same mask number.\n For example, cutMetal1 corresponds to metal1.\n For information about the technology file, see the Synopsys Technology File and Routing Rules Reference Manual After the routing is finalized, you add cut metal shapes by using the create_cut_metals command.\n This command inserts a cut metal shape between metal shapes when all of the following conditions are met:  The spacing between the metal shapes in the preferred direction is cutMetalWidth  There is no existing cut metal shape in the location.\n  The metal shapes are on a metal layer that corresponds to a cut metal layer defined in the technology file.\n The command inserts cut metal shapes between net shapes, a net shape and a PG shape, a PG shape and an obstruction, and metal shapes within a child cell.\n After inserting       the cut metal shapes, the command propagates the color of the metal shapes to the cut metal shapes.\n When you write a DEF file for a block with cut metal shapes, you must use the -version 5.8 option with the write_def command.\n When you write a GDSII or OASIS file for a block with cut metal shapes, you must use the -connect_below_cut_metal option with the write_gds or write_oasis command.\n If the routing changes after inserting cut metal shapes, you must remove the existing cut metal shapes and reinsert them.\n To remove all cut metal shapes from a block, use the remove_cut_metals command."}
{"instruction": "How do I Creating ETMs and ETM Cell Libraries", "input": "", "output": "You can perform top-level closure by implementing the blocks first, and then implementing the top level, as shown in the following flow diagram:  Top-Level Implemented After Blocks are Completed Abstract and frame views for each block Clock Tree Synthesis Routing Postroute Optimization Postroute Optimization Synthesis, Placement and Optimization Synthesis, Placement and Optimization Clock Tree Synthesis Routing Block-level flow Top-level flow       Alternatively, you can implement the blocks and top-level in parallel, as shown in the following flow diagram:  Top and Block Levels Implemented in Parallel Clock Tree Synthesis Routing Postroute Optimization Postroute Optimization Synthesis, Placement and Optimization Synthesis, Placement and Optimization Clock Tree Synthesis Routing Block-level flow Top-level flow Abstract and frame views for each block When using abstracts at the top-level, you can perform top-level synthesis, placement, optimization, clock tree synthesis, routing, and postroute optimization using the commands supported at the block level.\n Currently the tool does not make changes within the abstracts during top-level closure.\n Therefore, you can create and use read-only abstracts."}
{"instruction": "How do I Defining Via Ladder Rules", "input": "", "output": "via ladder, which is also referred to as a via pillar, is a stacked via that starts from the pin layer and extends into an upper layer where the router connects to it.\n Via       ladders reduce the via resistance, which can improve performance and electromigration robustness.\n To use via ladders,  Via ladder rules must be defined in the technology data for the design For information about defining via ladder rules, see Defining Via Ladder Rules  You must insert the via ladders before you perform global routing The Fusion Compiler tool provides the following methods for inserting via ladders:  Automatic insertion during preroute optimization, as described in Specifying Automatic Via Ladder Insertion Settings for Preroute Optimization  Constraint-based insertion by using the insert_via_ladders command, as described in Constraint-Based Via Ladder Insertion  Manual insertion by using the create_via_ladder command, as described in Manual Via Ladder Insertion See Also  Querying Via Ladders  Removing Via Ladders  Controlling Via Ladder Connections"}
{"instruction": "How do I Enabling Variant-Aware Legalization", "input": "", "output": "The Library Compiler tool supports creating variant-ready libraries from the Liberty source files.\n The physical_variant_cells attribute in the Liberty source defines the variants of a cell.\n When the physical_variant_cells attribute exists in the logic libraries, the compiled library is variant ready.\n If you logic library is not variant ready, you can define the equivalent cell variants during the library preparation stage in the Library Manager tool.\n To do so, create equivalent cell groups by using the create_cell_groups command.\n The following example adds equivalent cell groups to an existing reference library in the Library Manager tool: lm_shell> create_workspace -flow LIB.ndm lm_shell> create_cell_group -name A [get_lib_cells MY_LIB/A_*] lm_shell> create_cell_group -name B [get_lib_cells MY_LIB/B_*]       lm_shell> check_workspace lm_shell> commit_workspace You can report the equivalent cell groups in a library by using the report_cell_groups command in either the Library Manager tool or the Fusion Compiler tool.\n This command reports equivalent cell groups that are defined in Liberty source files of variant-ready logic libraries as well as those defined by using the create_cell_groups command in the Library Manager tool."}
{"instruction": "How do I Excluding Specific Scenarios", "input": "", "output": "To exclude a specific path group from concurrent clock and data optimization, use the ccd.skip_path_groups application option and specify the name of the path group you want ignored.\n You can ignore a path group for all scenarios or a specific scenario.\n For example, to ignore the path group named CLK1 for all scenarios and CLK2 for the scenario named scnA, use the following command: fc_shell> set_app_options -name ccd.skip_path_groups \\ -value {CLK1 {CLK2 scnA}} This setting affects concurrent clock and data optimization performed during the place_opt clock_opt, and route_opt commands."}
{"instruction": "How do I Highlighting Violations From the Error Browser Onto an ICV Heat", "input": "", "output": "You can select or unselect violation types from the heat map, as shown in figure  and  Violation Type        Violation Type in ICV Heat Map To control the opacity of the heat map squares, adjust the Alpha percentage value.\n Set this percentage to 100% to create a completely filled block.\n The lower you set the percentage, the more transparent the block becomes, as shown in and  Completely Filled Block        You can adjust the range of colored squares by modifying the Bins value, as shown in  and  Bins Value Set to 5        Bins Value Set to 8 You can select or unselect layers from the heat map, as shown in and   Selecting Layers        Selecting Layers in the ICV Heatmap You can modify the squares of the heat map by right-clicking the colored squares and selecting Set Style.\n See and        Set Style  Select Style"}
{"instruction": "How do I Specifying Antenna Properties", "input": "", "output": "The tool supports several ways to calculate the antenna ratio (antenna-area/gate-area).\n You control this calculation by specifying the following information:  How the antenna area is calculated (the antenna area mode)  Which metal segments are considered for the calculation (the antenna recognition mode) To specify these modes, use the -mode option with the define_antenna_rule and define_antenna_layer_rule commands, as described in Setting the Antenna Mode This is a required option for both of these commands.\n       Setting the Antenna Mode The antenna mode controls the antenna ratio calculation by determining  How the antenna area is calculated (the antenna area mode) The tool supports the following area calculation modes:  Surface area, which is calculated as W x L  Sidewall area, which is calculated as (W + L) x 2 x thickness Note: If you use sidewall area calculation, you must define the metal thickness by specifying the unitMinThickness unitNomThickness, and unitMaxThickness attributes in each Layer section of the technology file.\n  Which metal segments are considered for the calculation (the antenna recognition mode) To tool supports the following antenna recognition modes:  Single-layer mode In single-layer mode, the tool considers only the metal segments on the current layer; the metal segments on all lower layers are ignored.\n This mode allows the best routability.\n In this mode, the antenna ratio is calculated as antenna_ratio = connected metal area of the layer / total gate area  Accumulated-ratio mode In accumulated-ratio mode, the tool considers the metal segments on the current layer and the lower-layer segments connected to the input pins.\n In this mode, the antenna ratio is calculated as antenna_ratio = accumulation of the single-layer mode ratios of the current layer and layers below  Accumulated-area mode In accumulated-area mode, the tool considers the metal segments on the current and lower layers.\n In this mode, the antenna ratio is calculated as antenna_ratio = connected metal area of the current and lower layers/ total gate area For a detailed example of the antenna recognition modes, see Antenna Recognition Mode Example       The Fusion Compiler tool supports six antenna modes, which are described in Table 39 To specify the antenna mode, use the -mode option with the define_antenna_rule and define_antenna_layer_rule commands.\n This is a required option for both commands.\n Table 39 Antenna Mode Settings Antenna mode value Area calculation mode Antenna recognition mode                   Antenna Recognition Mode Example  shows a layout example with a lateral view, which is used to explain the antenna recognition modes.\n Table 40 shows the antenna ratios for each antenna recognition mode for this layout example.\n  Layout Example for Antenna Recognition Modes       Table 40 Antenna Recognition Modes and Ratios Considered segments Antenna ratio                                           Table 40 Antenna Recognition Modes and Ratios (Continued) Considered segments Antenna ratio"}
{"instruction": "How do I Using a Hybrid Routing Flow", "input": "", "output": "After running Custom Router to perform custom routing on the nets, you can check the routing results by reporting the routing results, generating a congestion map, running DRC checks, and so on.\n For more information about the tasks to check the routing results, see Analyzing the Routing Results"}
{"instruction": "How do I Specifying Clock Gating Constraints", "input": "", "output": "Minimum capacitance, maximum capacitance, and maximum transition are logical design rule constraints that your design must meet to function as intended.\n They are technology- specific restrictions that are specified in the logic libraries.\n However, you can specify more restrictive design rule constraints by using the constraint commands given in Table 7 During optimization, the Fusion Compiler tries to meet the design rule constraints, even if it means violating optimization constraints such as timing, power, and area goals; these design rule constraints have a higher priority.\n After optimization, you can use the reporting commands given in Table 7 to identify design rule constraint violations in a block.\n Table 7 Design Rule Commands To do this Use this command   set_min_capacitance   set_max_capacitance   set_max_transition  remove_min_capacitance       Table 7 Design Rule Commands (Continued) To do this Use this command  remove_max_capacitance  remove_max_transition  report_constraints -min_capacitance  report_constraints -max_capacitance  report_constraints -max_transition    set_connection_class  set_attribute \\ -name connection_class Note: The maximum fanout design rule constraint is not honored by the Fusion Compiler tool.\n However, you can specify a maximum fanout for the data paths in a block by using the opt.common.max_fanout application option.\n This is a soft optimization constraint.\n During optimization the tool tries to ensure that data path cells do not drive more than the specified maximum fanout."}
{"instruction": "How do I Removing Placement Bounds", "input": "", "output": "To report the placement bounds in a block, use the report_bounds comandcomandsreport_boundsmove boundreporting report_bounds command.\n To return a collection of placement bounds in the current block that match certain criteria, use the get_bounds comandcomandsget_bounds get_bounds command.\n       To return a collection of bound shapes associated with one or more move bounds, use the get_bound_shapes command."}
{"instruction": "How do I Verifying the Secondary Power and Ground Pin Attributes", "input": "", "output": "To use Zroute to perform secondary power and ground pin routing, 1.\n Verify that the secondary power and ground pins have the appropriate attributes in the standard cell frame views.\n 2.\n Set the routing constraints for secondary power and ground pin routing.\n 3.\n Perform secondary power and ground pin routing by using the route_group command.\n The following topics describe these steps."}
{"instruction": "How do I Enabling Global-Route-Layer-Based (GRLB) Preroute", "input": "", "output": "The following topics describe how you can control parasitic estimation during the preroute stage of the design flow:  Enabling Global-Route-Layer-Based (GRLB) Preroute Optimization  Enabling Route-Driven Estimation (RDE) for Preroute Optimization"}
{"instruction": "How do I Using an IC Validator Parameter File", "input": "", "output": "Track-based metal fill insertion inserts metal and via fill by using a runset derived from the attributes and rules in the technology file to create fill shapes aligned to tracks.\n It does not use the runset specified by the signoff.create_metal_fill.runset application option.\n Track-based metal fill insertion offers the following benefits as compared to pattern-based metal fill insertion:  Higher density  Better control of density  Well-balanced mask distribution for double-patterning layers To perform track-based metal fill insertion, set the -track_fill option of the signoff_create_metal_fill command to a value other than off.\n Track-based metal fill insertion supports three modes:  Sparse This is the default mode.\n In this mode, track-based metal fill insertion skips one track between signal shapes and fill shapes.\n  Dense To enable this mode, use the -fill_all_tracks true option.\n In this mode, track- based metal fill insertion does not skip tracks between signal shapes and fill shapes.\n Note: If your block uses double-patterning technology, using this option increases runtime if the block is not precolored.\n  Mixed To use sparse mode for some layers and dense mode for other layers, you must set the appropriate parameters in a parameter file, as described in Using an IC Validator Parameter File During track-based metal fill insertion, the signoff_create_metal_fill command performs the following tasks: 1.\n Loads the block into the IC Validator tool, as described in Reading Blocks for Signoff Metal Fill Insertion 2.\n Removes existing metal and via fill By default, the command removes the metal and via fill from the entire block.\n For information about performing incremental metal fill insertion, see Incremental Metal Fill Insertion       3.\n Inserts metal and via fill in the empty regions By default, the command uses the design rules defined in the technology file to insert fill shapes on-track on all metal and via layers for the entire block.\n The fill shapes have a length of 5 microns and a width equal to the defaultWidth attribute defined for the layer in the technology file.\n You can modify the default behavior by using  Application options set before running the signoff_create_metal_fill command:  signoff.create_metal_fill.apply_nondefault_rules To enable the use of nondefault spacing rules in addition to the spacing rules defined in the technology file, set this application option to true  Command options used with the signoff_create_metal_fill command:  -track_fill foundry_node To enable foundry-specific design rules, use the appropriate foundry keyword with the -track_fill option.\n To see the list of supported keywords, use the following command: fc_shell> signoff_create_metal_fill -track_fill list  -select_layers To restrict the layers on which to insert metal fill, use this option, as described in Specifying the Layers for Metal Fill Insertion  -coordinates and -excluded_coordinates To restrict the regions on which to insert metal fill, use these options, as described in Specifying the Regions for Metal Fill Insertion  Parameters set in the IC Validator parameter file:  _fill_width _min_fill_length, and _max_fill_length To change the size of the fill shapes, set these parameters, as described in Using an IC Validator Parameter File 4.\n Removes floating via fill from the specified via layers and from the via layers associated with the specified metal layers Floating via fill is via fill that does not have both an upper and lower metal enclosure.\n       5.\n (Optional) Trims the metal fill for each layer to try to meet the maximum density threshold defined by the signoff.create_metal_fill.max_density_threshold application option The metal fill is trimmed only for those layers specified in the application option.\n If you do not set this option, the metal fill is not trimmed.\n 6.\n Saves the fill data in the design view By default,  The IC Validator tool assigns a data type of 0 to the fill shapes To use different data type values, set the _fill_datatype and via _fill_datatype parameters in the IC Validator parameter file.\n  The IC Validator tool does not set mask constraints on the fill shapes If your block uses double-patterning technology, use the -output_colored_fill true option to set mask constraints on the fill shapes.\n If your block uses double-patterning technology, use the -output_colored_fill option to set mask constraints on the fill shapes.\n When you use this option, the IC Validator tool assigns a data type of 235 for fill shapes with mask_one mask constraint and 236 for fill shapes with a mask_two mask constraint.\n To assign different data types for the colored fill, set the following parameters in the IC Validator parameter file: _fill_datatype_color1 via _fill_datatype_color1 _fill_datatype_color2, and via _fill_datatype_color2  The IC Validator tool does not write exclude layers If your foundry uses exclude layers, define the data types for the exclude layers by setting the _exclude_layer_datatype parameters in the IC Validator parameter file.\n The IC Validator tool outputs the exclude layers that have defined data types.\n For information about using an IC Validator parameter file, see Using an IC Validator Parameter File 7.\n Writes the result files to the run directory In addition to the standard output files generated by signoff metal fill insertion, when you perform track-based metal fill insertion, you can output detailed density and density gradient reports by using the -report_density option.\n When you enable this option, the signoff_create_metal_fill command writes the following report files:  prefix _color_balance_and_density_report.txt  prefix _fill_density_gradient_report.txt       You can specify either on (in which case the tool uses \"track_fill\" as the prefix) or the prefix string.\n For example, to perform track-based metal fill insertion and use a prefix of my_prefix for the detailed density and density gradient reports, use the following command: fc_shell> signoff_create_metal_fill -track_fill true \\ -report_density my_prefix For information about the generated result files, see Signoff Metal Fill Result Files See Also  Timing-Driven Metal Fill Insertion"}
{"instruction": "How do I Defining an Outer Keepout Margin", "input": "", "output": "A keepout margin is a region (the shaded portions in ) around the boundary of fixed cells in a block in which no other cells are placed.\n        Placement Keepout Margins left bottom right top  Fixed cell left bottom right top   An outer keepout margin is a region outside the cell boundary, while an inner keepout margin is a region inside the cell boundary.\n The width of the keepout margin on each side of the fixed cell can be the same or different, depending on how you define the keepout margin.\n In addition, keepout margins can be defined as hard or soft.\n Keeping the placement of cells out of such regions avoids congestion and net detouring and produces better QoR.\n kepout marginscel-specific, seting To define a keepout margin, use the set_kepout_margin comandcomandset_kepout_margin create_keepout_margin command.\n By default, the command creates a hard keepout margin.\n To create a soft keepout margin, use the -type soft option."}
{"instruction": "How do I Hierarchical Synthesis Flow When Floorplans are not Available", "input": "", "output": "The hierarchical synthesis flow using abstracts includes the following two cases:  Floorplans of subblock and top-level design are not available  Floorplans of subblock and top-level design are available"}
{"instruction": "How do I Enabling Buffer Removal at the Postroute Stage", "input": "", "output": "At the final stages of your design flow, you can use the route_opt command to perform concurrent clock and data optimization on specific critical path groups or endpoints by using the following steps: 1.\n Enable targeted concurrent clock and data optimization for the route_opt command by setting the route_opt.flow.enable_targeted_ccd_wns_optimization application option to true 2.\n Specify the optimization target by using one or both of the following settings:  Specify the path groups to optimize by using the ccd.targeted_ccd_path_groups application option Note: If a path group you specify with this application option is also specified as a path group to skip by using the ccd.skip_path_groups application option, the path group is skipped during targeted concurrent clock and data optimization.\n  Specify the endpoints to optimize by using the ccd.targeted_ccd_end_points_file application option You must specify at least one of these settings.\n If you specify both, the tool optimizes the specified endpoints of the specified paths groups.\n 3.\n (Optional) Specify the type of optimization to perform by using the ccd.targeted_ccd_select_optimization_moves application option.\n The valid values are        auto, the default, which enables all optimization types, including buffering  size_only, which enables sizing only  equal_or_smaller_sizing, which enables sizing only to cells that are the same size or smaller  footprint_sizing, which enables sizing only to cells with the same footprint 4.\n (Optional) Specify an optimization effort by using the ccd.targeted_ccd_wns_optimization_effort application option.\n The valid values are high (default), medium, and low.\n The higher the effort, the more timing QoR improvement you will see in the targeted paths, but at the expense of the timing QoR of other paths.\n 5.\n (Optional) Specify a threshold beyond which the slack for the path groups that are not targeted can be degraded by using the ccd.targeted_ccd_threshold_for_nontargeted_path_groups application option.\n By default, if the path groups that are not targeted have positive slack, the tool can degrade this slack until it reaches zero.\n For the route_opt command, you can enable both regular concurrent clock and data optimization by using the route_opt.flow.enable_ccd application option and targeted concurrent clock and data optimization by using the route_opt.flow.enable_targeted_ccd_wns_optimization application option.\n If you enable both, the tool performs regular concurrent clock and data optimization first, followed by targeted concurrent clock and data optimization.\n The following script performs targeted concurrent clock and data optimization on a block that has already undergone routing and postroute optimization: open_lib design1 open_block blk.post_route  #Disable regular CCD optimization, which was previously performed set_app_options \\ -name route_opt.flow.enable_ccd -value false  #Enable targeted CCD optimization set_app_options \\ -name route_opt.flow.enable_targeted_ccd_wns_optimization \\ -value true  #Specify the path group and endpoints to target set_app_options -name ccd.targeted_ccd_path_groups -value clkA set_app_options -name ccd.targeted_ccd_end_points_file \\ -value clkA_ep.txt        #Specify the optimization type and effort set_app_options \\ -name ccd.targeted_ccd_select_optimization_moves -value size_only set_app_options -name ccd.targeted_ccd_wns_optimization_effort \\ -value medium  # Perform the targeted CCD optimization route_opt"}
{"instruction": "How do I Fixing Buffers", "input": "", "output": "Design changes in the ECO flow sometimes result in isolation or voltage violations.\n The fix_mv_design command can identify and fix violations for buffer trees and diodes.\n You must use either the -buffer or -diode option with the fix_mv_design command.\n For more information, see the following topics:  Fixing Buffers  Fixing Diodes       If the UPF has not already been committed, the fix_mv_design command automatically commits the UPF before performing any operations.\n Netlist changes that the tool makes as a result of the fix_mv_design command are not guaranteed to be placement or timing legal.\n You must check the design after running the fix_mv_design command."}
{"instruction": "How do I Reading Verilog Gate-Level Netlist Files", "input": "", "output": "The analyze command with the VCS command-line options provides compatibility with VCS simulation scripts and makes reading large designs easier.\n When you use the VCS command-line options, the tool automatically resolves references for instantiated designs by searching the referenced designs in the specified libraries and then loading these referenced designs.\n To read designs containing many HDL source files and libraries, specify the -vcs option with the analyze command.\n You must enclose the VCS command-line options in double quotation marks.\n For example, analyze -vcs \"-verilog -y mylibdir1 +libext+.v -v myfile1 \\ +incdir+myincludedir1 -f mycmdfile2\" top.v elaborate ${DESIGN_NAME} set_top_module ${DESIGN_NAME} To read SystemVerilog files with a specified file extension and Verilog files in one analyze command, use the -vcs \"+systemverilogext+ext\" option.\n When you do so, the files must not contain any Verilog 2001 styles.\n For example, the following command analyzes SystemVerilog files with the.sv file extension and Verilog files: analyze -format verilog -vcs \"-f F +systemverilogext+.sv elaborate ${DESIGN_NAME} set_top_module ${DESIGN_NAME}"}
{"instruction": "How do I Reading the Design", "input": "", "output": "in the following table.\n Table 5 Reference and Target Library Lexical Attribute Differences Lexical Attribute Reference Library Target Library lexical_cell_prefix_name   lexical_drive1_name   lexical_device_threshold_voltage   In this situation, you must create a lexical attribute mapping file that contains the following information: lexical_cell_prefix_name ND2 nand2 lexical_drive1_name D1 x1 lexical_device_threshold_voltage SVT svth       Then, when you run the compare_libraries command, you must specify this lexical attribute mapping file by using the -lex_attr_mapping option, as shown in the following example: fc_shell> set_app_options -name libra.complib.match_full_cell_name -value false fc_shell> compare_libraries -ref_lib $rvt_lib -ref_lib_analysis sa \\ -target_lib $lvt_lib -target_lib_analysis sa \\ -lex_attr_mapping attribute_map_file -verbose -plot  The reference and target library naming convention is completely different.\n In this situation, you must create a custom mapping file that contains the cell pairs you want to compare, as shown in the following example: INVD2BWP INVD1BWPHVT INVD2BWP BUFFD4BWPHVT ND2D2BWP ND2D4BWPHVT DFQD4BWP SDFQD4BWPHVT Then, when you run the compare_libraries command, you must specify this mapping file, by using the -custom_pairs option, as shown in the following example: fc_shell> set_app_options -name libra.complib.match_full_cell_name -value false fc_shell> compare_libraries -ref_lib $rvt_lib -ref_lib_analysis sa \\ -target_lib $lvt_lib -target_lib_analysis sa \\ -custom_pairs custom_map_file -verbose -plot"}
{"instruction": "How do I Allowing Nonrelative Placement Cells", "input": "", "output": "To specify how to handle fixed cells in the floorplan during legalization of relative placement groups, use the -place_around_fixed_cells option with the set_rp_group_options command.\n Table 59 shows the different settings you can specify for the -place_around_fixed_cells option.\n Table 59 Settings for the -place_around_fixed_cells Option To do this Use this setting   standard   physical_only   all  none For hierarchical relative placement groups, you can use the set_rp_group_options -place_around_fixed_cells command and specify different settings for the top-level and the lower-level relative placement groups.\n If you do not specify a setting for the lower- level placement groups, the value of the top-level relative placement group is used for the lower-level relative placement groups.\n       Assume a hierarchical relative placement group named top-rp contains three lower-level relative placement groups named rp1, rp2, and rp3.\n The following example specifies a less restrictive setting for the top level and a more restrictive setting for the lower-level group named rp1.\n It also specifies that the setting for the rp1 lower-level group overrides the top- level setting when legalizing the cells in the rp1 group.\n fc_shell> set_rp_group_options top-rp \\ -place_around_fixed_cells physical_only fc_shell> set_rp_group_options rp1 \\ -place_around_fixed_cells none fc_shell> set_app_options -name set_rp_group_options -value true"}
{"instruction": "How do I Controlling the Placement Density", "input": "", "output": "The tool can automatically generate group bounds for integrated clock gating cells and the sequential cells they drive.\n To do so, use the following application option setting: fc_shell> set_app_options -name place.coarse.icg_auto_bound \\ -value true When you enable this feature, the tool creates the automatic group bounds at the beginning of placement and removes them at the end of placement.\n The tool does not include cells that already belong to another group bound in the automatic group bounds.\n To limit the maximum number of fanouts that can be included in an automatic bound, use the icg_auto_bound_fanout_limit application option setting, as shown in the following example: fc_shell> set_app_options \\ -name place.coarse.icg_auto_bound_fanout_limit -value 30 The default fanout limit is 40."}
{"instruction": "How do I Relative Placement in a Design Containing Obstructions", "input": "", "output": "The following topics provide information related to the placement and legalization of relative placement groups:  Relative Placement in a Design Containing Obstructions  Legalizing Relative Placement Groups in a Placed Design  Creating New Relative Placement Groups in a Placed Design"}
{"instruction": "How do I Annotating the Switching Activity", "input": "", "output": "The following topics describe the tasks you need to perform for setting up a design for power-related features:  Annotating the Switching Activity  Enabling Leakage Power Optimization for the compile_fusion Command  Enabling Power Optimization for the clock_opt Command  Improving Yield By Limiting the Percentage of Low-Threshold-Voltage (LVT) Cells  Updating Activity for Improved Power Optimization  Enabling the Power Integrity Features"}
{"instruction": "How do I Preventing Specific Clocks From Being Synthesized", "input": "", "output": "During initial clock tree synthesis, by default, the tool identifies the corner with the worst clock delays and inserts buffers to balance the clock delays in all modes of this corner.\n To identify a specific corner as the primary corner for initial clock tree synthesis, use the cts.compile.primary_corner application option."}
{"instruction": "How do I Performing Dynamic-Voltage-Drop-Driven Concurrent Clock and", "input": "", "output": "By default, the tool adjusts the latencies of the I/O clocks, which are the clocks that constrain the boundary ports, when it performs concurrent clock and data optimization during the clock_opt command.\n The tool also uses concurrent clock and data optimization techniques to adjust the I/O clock latencies when you subsequently run the compute_clock_latency command.\n The tool does not update the I/O clock latencies  For the entire block when you disable this feature by setting the ccd.adjust_io_clock_latency to false or when you disable concurrent clock and data optimization for boundary paths by setting the ccd.optimize_boundary_timing application option to false  For specific I/O paths when you exclude the corresponding path groups from concurrent clock and data optimization by using the ccd.skip_path_groups application option  For specific I/O clocks when you disable latency adjustment those I/O clocks by using the set_latency_adjustment_options -exclude_clocks command"}
{"instruction": "How do I Define a Blockage of a Predefined Category", "input": "", "output": "To define a partial blockage, specify the boundary, type ( -type partial option), blockage percentage ( -blocked_percentage option), and optionally the name for the placement blockage.\n For example, to define a partial blockage with a maximum allowed cell density of 60 percent (a blocked percentage of 40), enclosed by the rectangle with corners at (10, 20) and (100, 200), use the following command: create_placement_blockage -boundary {10 20 100 200} \\ -type partial -blocked_percentage 40 To allow unlimited usage of a partial blockage area, specify a blockage percentage of zero -blocked_percentage 0 option).\n Note: Partial placement blockages are honored during placement, but not during legalization, optimization, or clock tree synthesis."}
{"instruction": "How do I Creating Floating Logical Supply Nets", "input": "", "output": "After you read in the design, you must ensure that there are logical connections between the power and ground nets and the power, ground, and tie-off pins on the cells in your design.\n If your design does not already have these connections, use the connect_pg_net command to create them.\n This command creates the logical power and ground connections for leaf cells, hierarchical cells, and physical-only cells in both single- voltage and multivoltage designs.\n Before creating the logical power and ground connections, you must resolve any PG conflicts among the netlist, floorplan, and UPF specification.\n  For multivoltage designs, the conflicts are resolved when you commit the power intent, as described in Loading and Applying UPF Information  For single-voltage designs, you must run the resolve_pg_netx command to resolve the conflicts.\n Note that the UPF specification for a single-voltage design is the default power domain generated by the read_verilog command.\n If your design contains unmapped instances, the tool issues an information message to indicate that only mapped instances are connected.\n       The connect_pg_net command operates in two modes:  Automatic In automatic mode, the command derives all power and ground nets, power and ground pins, tie-off pins, and connections from the UPF specification.\n If the supply nets do not exist, the tool creates them.\n To create the logical power and ground connections in automatic mode, use the -automatic option with the connect_pg_net command.\n  Manual In manual mode, the command makes the connections that you specify.\n If a specified pin or port has an existing connection, the tool removes the existing connection and then creates the specified connection.\n The tool verifies that the connections match the power intent.\n If it finds a mismatch, the tool issues a warning but still creates the connection.\n To create logical power and ground connections in manual mode, use the -net option to specify the power or ground net and specify the pins and ports to be connected to that net as an argument to the command.\n For example, to connect the VDD power net to all pins named vdd and the VSS ground net to all pins named vss, use the following commands: fc_shell> connect_pg_net -net VDD [get_pins */vdd] fc_shell> connect_pg_net -net VSS [get_pins */vss] To connect PG nets to power and ground pins only, use the -pg option.\n To connect PG nets to tie-off pins only, use the -tie option.\n By default, if neither option is specified, the tool makes connections to all power, ground, and tie-off pins.\n These options cannot be used with an object list or with the -create_nets_only and -net options.\n Regardless of the command options used, the tool always creates connections required to build a complete PG netlist structure, such as top-level PG ports and intermediate hierarchical PG pins."}
{"instruction": "How do I Placing ECO Cells With Minimal Physical Impact (MPI)", "input": "", "output": "After placing the ECO cells, the place_eco_cells command legalizes each ECO cell to the closest unoccupied site.\n The existing cells are untouched to minimize the impact to their placement.\n You can control the legalization of the ECO cells by using one of the following methods:  To not legalize the cells after ECO placement, use the -no_legalize option.\n  To perform legalization only, use the -legalize_only option.\n  To specify the mode in which to legalize, use the -legalize_mode option with one of the following settings:  free_site_only When you specify this setting, the default, the tool legalizes the ECO cells on free sites without moving preexisting cells.\n An ECO cells can have a large displacement, if a free site is unavailable nearby.\n  allow_move_other_cells When you specify this setting, the tool legalizes the ECO cells to the nearest legal location by moving preexisting cells.\n  minimum_physical_impact When you specify this setting, the tool first legalizes the ECO cells the can be legalized to a free site nearby without moving preexisting cells.\n For the ECO cells that do not have a free site nearby, the tool legalizes them by moving preexisting cells.\n  To specify a displacement threshold for legalization, use the -displacement_threshold option.\n When you use the -displacement_threshold option, you can also use the -max_displacement_threshold option to specify a second displacement threshold that is larger than the value specified with the -displacement_threshold option.\n How the tool uses these displacement thresholds depends on the setting you use for the -legalize_mode option as follows:  free_site_only When you specify this setting, the tool legalizes the ECO cells that have available free sites within the specified displacement threshold.\n       If the displacement of an ECO cell exceeds this threshold, the tool does not legalize the cell.\n It creates a collection named epl_legalizer_rejected_cells that consists of such ECO cells that are not legalized.\n You can subsequently legalize the ECO cells in the epl_legalizer_rejected_cells collection by using the following command: fc_shell> place_eco_cells -legalize_mode allow_move_other_cells \\ -legalize_only -cells $epl_legalizer_rejected_cells This command moves existing cells to find legal locations for the ECO cells in the collection named epl_legalizer_rejected_cells.\n If you also specify the -max_displacement_threshold option with -displacement_threshold option, the tool also creates a collection named epl_max_displacement_cells that consists of ECO cells with a displacement larger than that specified by this option.\n The collection epl_max_displacement_cells is a subset of the collection epl_legalizer_rejected_cells and its cells are also not legalized.\n You can use the -max_displacement_threshold option to identify ECO cells with a very large displacement, for which you want to reject the ECO changes.\n  allow_move_other_cells When you specify this setting, you cannot specify the -displacement_threshold and -max_displacement_threshold options.\n  minimum_physical_impact When you specify this setting, the tool first legalizes the ECO cells that have available free sites within the specified displacement threshold.\n If the displacement of an ECO cell exceeds this threshold, the tool legalizes them by moving preexisting cells.\n If you also specify the -max_displacement_threshold option, the tool does not legalize the cells that exceed this maximum displacement threshold specified with this option and the tool creates a collection named epl_max_displacement_cells that consists of ECO cells with a displacement larger than that specified by this option.\n You can use the -max_displacement_threshold option to identify ECO cells with a very large displacement, for which you want to reject the ECO changes.\n  To specify the types of filler cells that can be removed during legalization, use the -remove_filler_references option.\n       When you use this option, the filler cells are removed if they do not have a fixed placement and they overlap with ECO cells.\n By default, the tool does not remove any filler cells when legalizing ECO cells.\n  To enable Advanced Legalizer and remove the filler cells that have violations, set the place.legalize.enable_advanced_legalizer application option to true When you specify this setting, the filler cells that have violations are removed after legalizing ECO cells.\n Using the -remove_filler_references option with this application option set to true results in the legalizer engine removing only those fillers that are specified by this option.\n The -min_filler_distance option is ignored when you use the place.legalize.enable_advanced_legalizer application option."}
{"instruction": "How do I Performing Manual Grouping Flow", "input": "", "output": "Auto repeater group creation allows you to derive virtual connections for register paths.\n For repeater group input mode, place_group_repeaters applies cutline based placement.\n You must specify cutline for each group.\n Repeaters are placed based on the intersection point between cutline and route.\n Build connectivity based on real connections for target cells.\n 1.\n To group a list of repeaters or repeaters of supernets, use the create_repeater_groups command.\n Also, specify the constraint option.\n The command automatically performs cell grouping and groups virtual connection with the following attributes:  group_repeater_driver  group_repeater_loads  pin_pair  group_id 2.\n Verify the path with the get_attribute report_repeater_group, and get_repeater_path_info commands.\n The get_attribute command reports the attributes of the group_repeater_driver or group_repeater_loads command.\n The command report_repeater_groups reports group information for specified groups or all groups, and reports repeater paths for all groups if -verbose is on.\n The get_repeater_paths_info command returns cells on repeater paths that include input cells.\n 3.\n Specify the cutline for a group manually with the set_repeater_group -group_id- cutline command.\n 4.\n Place interconnect repeaters on route with cutline for input repeater groups."}
{"instruction": "How do I Removing Metal Fill", "input": "", "output": "After running the signoff_report_metal_density command, you can view heat maps in the GUI for both the minimum density and gradient density reports.\n Note: The signoff_report_metal_density command generates the heat maps only when the signoff.report_metal_density.create_heat_maps application option is set to true  To display the minimum density heat map, choose View > Map > ICV Metal Fill Density  shows a minimum density heat map.\n In this heat map, each colored tile corresponds in size to the density window step.\n To customize or standardize the density values in the heat map, set the bin range, which is indicated by a red oval in the figure.\n        Minimum Density Heat Map  To display the density gradient heat map, choose View > Map > ICV Metal Fill Gradient        shows a density gradient heat map.\n In this heat map, each colored bar corresponds in size to the gradient window size.\n  Density Gradient Heat Map"}
{"instruction": "How do I Defining a Partial Placement Blockage", "input": "", "output": "To define a soft blockage, specify the boundary, type ( -type soft option), and optionally the name for the placement blockage.\n For example, to define a soft blockage enclosed by a rectangle with corners at (120, 75) and (230, 200), use the following command: create_placement_blockage -boundary {120 75 230 200} \\ -type soft A soft blockage prevents the initial placement from placing cells within the specified area, but allows legalization, optimization, and clock tree synthesis to do so.\n However, after placement and optimization, during subsequent incremental placement, the tool can move the cells added during legalization, optimization, and clock tree synthesis out of the soft blockage area.\n To prevent this, use the following application option setting: set_app_options -name place.coarse.enable_enhanced_soft_blockages \\ -value true       Note: Soft placement blockages are honored during placement, but not during legalization, optimization, or clock tree synthesis."}
{"instruction": "How do I Running Concurrent Clock and Data Optimization", "input": "", "output": "Combinational multibit cells are library cells, where a single cell implements multiple logic functions.\n The Fusion Compiler tool:  Banks single-bit combinational cells to multibit combinational cells.\n  Debanks multibit combinational cells as needed to improve QoR.\n  Combinational Multibit Library Cells To enable combinational multibit mapping during the initial_opto and final_opto stages, use the opt.common.enable_multibit_combinational_cells application option with the set_app_options command: fc_shell> set_app_options -name opt.common.enable_multibit_combinational_cells -value true       Note: By default, the tool debanks the multibit combinational cells.\n  Combinational Multibit Library Cell Flow To print the combinational multibit cells created for each library reference, use the -combinational option with the report_multibit command, as shown in the following example: fc_shell> report_multibit -combinational  #Outputs Reference Number of instances Single-bit Equivalent  2 combo_mbit_lib_cell1 257 514 4 combo_mbit_lib_cell2 57 228  To check the compatible single-bit equivalent cell of combinational multibit library cells, use the -combinational option with the check_multibit_library command, as shown in the following example: fc_shell> check_multibit_library -combinational  Combinational Compatibility Equivalent Singlebit Multibit LibCell LibCell  combo_mbit_lib_cell1 Compatible combo_sbit_lib_cell1 Don't use Mismatch combo_sbit_lib_cell2       combo_mbit_lib_cell2 PVT mismatch combo_sbit_lib_cell3"}
{"instruction": "How do I Clock Gating Flows", "input": "", "output": "The report_clock_gating_options command reports the clock gating settings used for the current design.\n The command output includes a table that reports minimum bitwidth and maximum fanout for each relevant object.\n A parameter to print options set up only for certain objects is also available.\n The available parameters for the command are as follows:  No split: By default, long names are split in multiple lines for easier human reading.\n Enabling this option makes names print in a single line.\n  Objects: A list of hierarchical objects to query.\n Ignores the rest of the objects in the design, and displays an error for each non-hierarchical cell passed with CGT-2500 and CGT-2501 warning messages.\n       The output of the report_clock_gating_options command is shown in  Output of the report_clock_gating_options Command When no gating options have been set, or when the design has been set only with specific options, the table displays the assigned options in a single row, alongside any other objects that might have been passed through the -objects parameter.\n  shows the report_clock_gating_options command output when no options are set to specific objects.\n  Command Output With no Options set to Specific Objects  shows the report_clock_gating_options command output when no options are set to specific objects, but both the design and b0 have been passed as parameters for objects.\n        Command Output With no Specific Object Options but Passed as Parameters When multiple objects are set using the same options, they appear in a single row, with the name column being divided into multiple lines if the -nosplit parameter is not used.\n Otherwise, everything occupies a single line.\n report_clock_gating_options -objects {design, obj1, obj2, obj3, obj4}  Command Output With the Same Option set to Multiple Objects  shows the same scenario as, but with the -nosplit parameter: report_clock_gating_options -objects {design, obj1, obj2, obj3, obj4} -nosplit"}
{"instruction": "How do I Considering Legalization Effects During Placement", "input": "", "output": "A sequential array, which is commonly used in storage devices, is a group of registers that are arranged as a two-dimensional array.\n The clock pins of the registers along a row of a sequential array are usually driven by a single clock-gating cell.\n To consider the clock-gating cells of a sequential array during placement, set the place.coarse.seq_array_icg_aware application option to true.\n Doing so can reduce the congestion caused by the clock nets."}
{"instruction": "How do I Performing Incremental Shielding", "input": "", "output": "Example 22 provides an example of shielding clock nets using preroute shielding and shielding critical nets using postroute shielding.\n Example 22 Shielding Flow Example # Define shielding rule create_routing_rule shield_rule \\ -shield_widths {M1 0.1 M2 0.1 M3 0.1 M4 0.1 M5 0.1 M6 0.3} \\ -shield_spacings {M1 0.1 M2 0.1 M3 0.1 M4 0.1 M5 0.1 M6 0.3}  # Assign shielding rule to clock nets set_clock_routing_rules -clocks CLK \\ -rules shield_rule  # Perform clock tree synthesis synthesize_clock_trees  # Route the clock nets, reusing the global routing result route_group -all_clock_nets -reuse_existing_global_route true  # Perform preroute shielding for the clock nets create_shields -nets $clock_nets -with_ground VSS        # Assign shielding rule to critical nets set_routing_rule -rule shield_rule $critical_nets  # Route signal nets using shielding soft rules route_auto  # Perform postroute shielding for critical nets create_shields -nets $critical_nets -with_ground VSS"}
{"instruction": "How do I Running the Hierarchical Synthesis Flow Using ETMs", "input": "", "output": "To minimize the number of iterations and obtain optimal results, you can perform feasibility analysis before running hierarchical synthesis, as shown in The hierarchical synthesis feasibility flow uses the following block-level information generated post compile_fusion  Floorplanning information to generate the ETM in step 2  Power intent and test model (CTL) information for top-level synthesis in step 4        Hierarchical Synthesis Feasibility Analysis 1.\n Synthesize the block in the front- end tool compile_fusion RTL Block UPF (front end) Block CTL (front end) UPF SDC, floorplan SDC Design (ndm) Netlist UPF CTL SPEF Design (frame) 2.\n Create an ETM in the front-end tool extract_model Block_ETM (ndm) 3.\n Synthesize the top-level design in the front-end tool compile_fusion RTL Top-level UPF SDC, floorplan"}
{"instruction": "How do I Specifying Library Subset Restrictions", "input": "", "output": "By default, the tool can select any library cell from the target library during optimization.\n In some cases, you might want to restrict the library cells used for clock tree synthesis and optimization.\n For example, you might want to exclude specific double-height cells from the target library for some design blocks during optimization.\n To restrict the library cells used for clock tree synthesis and optimization, 1.\n Specify the library subset by using the set_target_library_subset command.\n By default, the library subset restriction applies to        The top block and all its subblocks To set it for a specific subblock, use the -objects option.\n  Both clock and data paths.\n To set it for only the clock or data paths, use the -clock or -data option.\n You can further restrict the target library subset setting as follows:  Specify a list of cells from the libraries that should not be used by using the -dont_use option.\n  Specify that these libraries cannot be used for any other objects, other than the specified objects, by using the -only_here option.\n 2.\n Enable the use of the target library subset by setting the opt.common.enable_target_library_subset_opt application option to 1.\n When you set target library subsets, remember the following points:  The subset restriction applies to hierarchical cells but not to leaf cells.\n  The command enforces the subset restriction on the specified blocks and their subdesigns in the hierarchy, except those subdesigns where a different subset restriction is set.\n  A subset specified at a lower level supersedes any subset specified at a higher level.\n  Logic Hierarchy of Design       For example, assume your design has a logic hierarchy as shown in and you want to implement the following library restrictions during optimization and clock tree synthesis:  Use only the cells from the library named LVT_lib for the Sub1 block and its subblocks, SubA and SubB.\n  Do not use the cells from this library anywhere else in the design.\n To do so, use the following settings: fc_shell> set_target_library_subset -objects {top/Sub1} \\ -only_here [get_lib_cells LVT_lib/*] [get_libs LVT_lib] fc_shell> set_app_options \\ -name opt.common.enable_target_library_subset_opt -value 1 In addition to these settings, assume you specify the following setting: fc_shell> set_lib_cell_purpose -include cts \\ {HVT_lib/buf1 HVT_lib/buf2 LVT_lib/buf1 LVT_lib/buf2} Then, when adding buffers on the clock network during clock tree synthesis, the tool uses  The buf1 and buf2 cells from the LVT_lib library for the block named Sub1 and its subblocks  The buf1 and buf2 cells from the HVT_lib library for the rest of the design Reporting Target Library Subsets To find out which target library subsets have been defined for a top block or hierarchical cell, use the report_target_library_subset command.\n Reports that are generated by reporting commands, such as report_cells and report_timing, show the td attribute attached to the cells that are specified by the -dont_use or -only_here option.\n Removing Target Library Subsets To remove a target library subset restriction from a top block or hierarchical cell, use the remove_target_library_subset command."}
{"instruction": "How do I Mapping Combinational Multibit Cells", "input": "", "output": "Fusion Compiler uses the WordView infrastructure to map multibit components.\n To view multibit components in the GUI, you must complete the following requirements:  Ensure the library contains multibit components in.ndm format.\n  Enable multibit component mapping by setting the compile.flow.enable_multibit application option as follows: fc_shell> set_app_options -name compile.flow.enable_multibit \\ -value true Follow these steps to view multibit components in WordView: 1.\n Read the RTL by using the analyze and elaborate commands.\n 2.\n Run the compile_fusion -to initial_opto command.\n The tool maps the bused registers to multibit components during compile.\n 3.\n (Optional) Check the multibit component mapping information by using the report_cells command.\n For example, The RTL contains the reg_state bused register.\n       module top (...); output [2:0] reg_state; always @(posedge clk) reg_state <= next_reg_state; endmodule The report shows that the reg_state bus register is mapped to the MB_DFF multibit component.\n fc_shell> report_cells [get_cells *reg_state_reg* -hierarchical]...\n Cell Reference Library Attributes  reg_state_reg[2:0] MB_DFF my_mb_lib n"}
{"instruction": "How do I Reporting the Constraints Assigned to a Group of a Repeaters", "input": "", "output": "Voltage area aware group repeaters choose legal library cells for repeaters based on the corresponding voltage area and updates supply nets for repeaters to meet multivoltage constraints.\n You can use the add_group_repeaters command with specific options to add voltage area aware group repeaters in three different ways, depending on the design methodology:  When the design is single-rail buffer only, use the -va_aware option fc_shell> add_group_repeaters -lib_cell <lib_cell> -va_aware  When the design is single-rail buffer based on voltage area, use the -voltage_area_specific_lib_cell option       fc_shell> add_group_repeaters -lib_cell <default_lib_cell> \\ -voltage_area_specific_lb_cell {{VA1 libcell1}} {{VA2 libcell2}}  When the design is single- and dual-rail buffers, use the -select_mv_buffer option fc_shell> add_group_repeaters -lib_cell <default_lib_cell> \\ -select_mv_buffer {{VA1 SingleRail1 DualRail1}} \\ {{VA1 SingleRail2 DualRail2}} Voltage area aware group repeaters help resolve multivoltage violations by choosing the library cell based on voltage area and repeater location."}
{"instruction": "How do I Specifying Options for Relative Placement Groups", "input": "", "output": "To add cells anywhere within a predefined area of a relative placement group, use the -free_placement option with the add_to_rp_group command.\n With this option, you must specify  The origin of the placement area by using the -column and -row options  The height and width of the placement area by using the -height and -width options  The number of columns and rows the cells occupy by using the -num_columns and -num_rows options For example, to add a leaf cell that occupies two columns and two rows to a placement area with a height of 4 and a width of 5 and is at position (0,0), use fc_shell> add_to_rp_group rp2 -cells U41 -free_placement \\ -column 0 -row 0 -height 4 -width 5 -num_columns 2 -num_rows 2"}
{"instruction": "How do I Library Requirements", "input": "", "output": "The Fusion Compiler tool infers SELECT_OP cells for logic that selects data signals based on control signals.\n SELECT_OP cells are mapped to combinational logic.\n The following topics describe SELECT_OP inferences:  Inferring SELECT_OPs Inferring SELECT_OPs HDL Compiler uses SELECT_OP components to implement conditional operations implied by if and case statements.\n An example of a SELECT_OP cell implementation for an 8-bit data signal is shown in        SELECT_OP Implementation for an 8-bit Data Signal  Verilog Output\u2014SELECT_OP and Selection Logic       Depending on the design constraints, Fusion Compiler implements the SELECT_OP with either combinational logic or multiplexer cells from the logic library.\n For more information on SELECT_OP inference, see the HDL Compiler documentation."}
{"instruction": "How do I Setting Clock-Gating Options", "input": "", "output": "Use the set_clock_gate_style command to select the type of integrated clock-gating cell you want to use.\n The following options are available: Option Description -test_point  none before after       Option Description -observation_output  true  false -target  pos_edge_flip_flop  neg_edge_flip_flop -objects"}
{"instruction": "How do I Calculating the Maximum Antenna Ratio", "input": "", "output": "Antenna rules define how to calculate the maximum antenna ratio for the nets in a block, as well as the antenna ratio for a pin.\n You must define a global antenna rule by using the define_antenna_rule command.\n You can also specify layer-specific antenna rules by using the define_antenna_layer_rule command.\n The tool uses the global antenna rule whenever a layer-specific antenna rule does not exist.\n The following topics describe how to define the antenna rules:  Calculating the Maximum Antenna Ratio  Calculating the Antenna Ratio for a Pin"}
{"instruction": "How do I Routing Clock Trees", "input": "", "output": "For designs with poor timing correlation between the preroute and postroute stages, you can improve the correlation by performing optimization after clock tree synthesis using machine learning (ML) data derived after detail routing.\n The machine learning concept uses the following terminology:  Labels, which are the outputs you are trying to predict  Features, which are the inputs that affect the outputs  Model, which is the relationship between the features (inputs) and the labels (outputs)  Training, which is the process of learning the relationship between the features (inputs) and the labels (outputs), based on the data collected To perform global-route-based optimization using machine learning (ML) data derived from the postroute stage, 1.\n Collect machine learning features and labels, and train the model by performing the following steps in a specific iteration, iteration N: a.\n Perform clock tree synthesis and clock routing by completing the build_clock and route_clock stages of the clock_opt command.\n b.\n Specify the output directory for the machine learning data by using the new est_delay.ml_delay_opto_dir application option.\n fc_shell> set_application_option \\ -name est_delay.ml_delay_opto_dir -value./ML c.\n Enable integrated feature extraction by setting the new est_delay.ml_delay_gre_mode application option to feature fc_shell> set_application_option \\ -name est_delay.ml_delay_gre_mode -value feature d.\n Perform global-route-based optimization by using the clock_opt -from final_opt command.\n e.\n Perform detail routing.\n f.\n Collect labels associated with the features that were previously collected during the clock_opt -from final_opt command by using the estimate_delay -training_labels command, as shown in the following example: fc_shell> estimate_delay -training_labels \"label_1\" \\ -output_dir./ML       The labels must be collected after detail routing, but before postroute optimization.\n g.\n Train the machine learning model for the collected features and label by using the estimate_delay -train_model command as shown in the following example: fc_shell> estimate_delay -train_model \"features_1 label_1\" \\ -output_dir./ML h.\n Continue with postroute optimization and the rest of the steps in the implementation flow.\n 2.\n Use the machine learning data to improve correlation by performing the following steps in the subsequent iteration, iteration N+1: a.\n Perform clock tree synthesis and clock routing by completing the build_clock and route_clock stages of the clock_opt command.\n b.\n Specify the output directory for the machine learning data by using the new est_delay.ml_delay_opto_dir application option.\n fc_shell> set_application_option \\ -name est_delay.ml_delay_opto_dir -value./ML c.\n Enable the machine learning model, which was derived in the previous iteration, by setting the new est_delay.ml_delay_gre_mode application option to enable fc_shell> set_application_option \\ -name est_delay.ml_delay_gre_mode -value enable d.\n Perform global-route-based optimization with the machine learning data by using the clock_opt -from final_opto command.\n e.\n Perform detail routing.\n f.\n Disable the machine learning model after you complete all the detail routing steps by using the estimate_delay -disable_model command, as shown in the following example: fc_shell> estimate_delay -disable_model -output_dir./ML g.\n Continue with postroute optimization and the rest of the steps in the implementation flow.\n When using this feature, ensure that  The same scenarios are active at each step and each iteration of the flow  The machine learning model is re-created if there are changes to the design, its environment, or the flow"}
{"instruction": "How do I Considering Wide Cell Density During Placement", "input": "", "output": "To reduce the congestion of your block, use the following settings:  Consider the congestion of each layer separately and improve the accuracy of congestion reduction in coarse placement by setting the place.coarse.congestion_layer_aware application option to true By default, the tool combines the congestion information of all layers during placement.\n By considering the congestion of each layer separately, the tool can identify and fix areas with high pin densities that have insufficient resources on the lower layers for making pin connections.\n  Expand the virtual area of cells during placement based on the local routing resource needs by setting the place.coarse.increased_cell_expansion application option to true By increasing the virtual area of cells in areas where there is a shortage of routing resources, the tool can minimize the cell density and congestion.\n By default, the tool expands the cells in the horizontal direction, which helps reduce congestion in the vertical routing layers.\n For designs with congestion mainly in the horizontal routing layers, set the place.coarse.congestion_expansion_direction application option to both.\n The default is horizontal       By default, the tool uses the uses global route congestion map to identify highly congested areas that need cell expansion.\n However, the tool does not consider the congestion due to soft routing rules.\n To consider the congestion effects of soft routing rules, set the route.global.export_soft_congestion_maps application option to true"}
{"instruction": "How do I Checking and Fixing Isolated Vias", "input": "", "output": "To check for isolated vias without fixing them, run the signoff_fix_isolated_via command with the -check_only true option.\n fc_shell> signoff_fix_isolated_via -check_only true       When run you run the signoff_fix_isolated_via command in check-only mode, it generates a summary report that specifies the number of isolated vias detected on each via layer."}
{"instruction": "How do I Implementing Multisource Clock Trees", "input": "", "output": "To remove the buffers and inverters on a clock tree, use the remove_clock_tre comandcomandsremove_clock_tre remove_clock_trees command.\n The remove_clock_trees commands traverses the clock tree from its root to its sinks and removes all buffers and inverters, except those with dont_touch or size_only attributes.\n Cells beyond clock tree exceptions are considered part of the clock tree, but cells beyond a clock-to-data pin are considered part of the data path and are       not removed.\n In addition to removing the buffers and inverters, the remove_clock_trees command resets the attributes related to clock tree synthesis.\n Note: The remove_clock_trees command does not support clock mesh nets.\n By default, this command removes the buffers and inverters from all clock trees in all modes.\n To remove the buffers and inverters from specific clock trees, use the -clocks option to specify the clock trees.\n By default, when you use the -clocks option, the clocks are selected from the current mode.\n To select clocks from a specific mode, use the get_clocks -mode command to select the clocks.\n For example, to remove only the clock tree in the current mode named my_clk, use the following command: fc_shell> remove_clock_trees -clocks [get_clocks my_clk] Table 28 shows how clock tree removal is affected by the structure of the clock tree.\n Table 28 Clock Tree Removal Behavior Object Impact on clock tree removal                                 Table 28 Clock Tree Removal Behavior (Continued) Object Impact on clock tree removal       By default, the remove_clock_trees command removes the detail route shapes of the clock nets it removes.\n However, you can preserve the route shapes of the clock nets by setting the shape_use attribute of the clock nets to user_route"}
{"instruction": "How do I Improving the Repeater Levels in Clock Tree", "input": "", "output": "The latency-driven clock-gate (CG) splitting now automatically identifies and evaluates different CG cloning moves.\n This improves clock insertion delay to meet the maximum level constraint.\n To enable level driven CG splitting, set the cts.icg.latency_driven_cloning application option to true  Latency Driven Clock-Gate Splitting"}
{"instruction": "How do I Considering the Effects of Cell Pins During Placement", "input": "", "output": "If a cell cannot straddle the vertical power straps and the width of the cell is more than half the pitch of the power straps, only one such cell can be placed between the power straps.\n Therefore, during placement, the tool must minimize the density of such wide cells to ensure that they can be placed and legalized without large displacements.\n For advanced technology nodes with wide cells, enable wide-cell modeling during placement by setting the place.coarse.wide_cell_use_model application option to true"}
{"instruction": "How do I Previewing Repeater Groups", "input": "", "output": "The crosstalk management is only supported in the create_group_repeaters_guidance and add_group_repeaters commands flow.\n       The flow to avoid crosstalk during group repeater insertion is as follows: 1.\n Use the following command to query interleaving groups: create_group_repeaters_guidance -net -number_of_interleaving_groups number By default, the number argument is set to 1.\n For example, if the value of the number is 2, one net group is separated into two interleaving net groups.\n 2.\n Use the following command to manage crosstalk during group repeater insertion: add_group_releaters -nets | -bundles -first_distance_of_net_groups { {group_id1 $first_dist1} {group_id2 $first_dist2} \u2026} -repeater_distance $dist or add_group_releaters -nets | -bundles -cutlines_of_net_groups { {group_id1 $cutline_list1} {group_id2 $cutline_list2} \u2026}  If the option -first_distance_of_net_groups is specified, the command groups the nets based on the existing group IDs on nets and inserts repeaters based on the specified first distance for the various net groups.\n The repeater distance is same for all net groups.\n  If the option -cutlines_of_net_groups is specified, the command groups the nets based on the existing group IDs of the nets and inserts repeaters based on the cutlines for the various net groups."}
{"instruction": "How do I Voltage Hotspot Analysis Examples", "input": "", "output": "The tool saves the generated hotspot data in memory.\n If you want to generate hotspots for other nets in another run, you need to remove the previously generated hotspot data from memory with the remove_hot_spots command before proceeding to another hotspot analysis run.\n For example, if you have run the generate_hot_spots command on the VDD net, you must first remove the hotspot data for the VDD net before generating hotspots for other nets.\n       See Also  Generating Hotspots  Reporting Hotspots  Voltage Hotspot Analysis Examples"}
{"instruction": "How do I Timing-Driven Global Routing", "input": "", "output": "During design planning you can perform exploration-mode global routing by using the -floorplan true option with the route_global command.\n fc_shell> route_global -floorplan true If you are using the hierarchical flow, enable virtual-flat global routing by using the -virtual_flat option with the route_global command.\n When you set the -virtual_flat option to all_routing, Zroute routes all the nets in the block and preserves the hierarchy and pin constraints.\n You can increase the virtual-flat global routing speed by routing only the top-level nets.\n To do this, set the -virtual_flat option to top_and_interface_routing_only.\n When the -virtual_flat option is set to off which is the default, Zroute ignores the physical hierarchy and routes the block as flat.\n fc_shell> route_global -floorplan true -virtual_flat all_routing"}
{"instruction": "How do I Preserving Networks During Optimization", "input": "", "output": "To allow only sizing on cells or instances during optimization, use the set_size_only command.\n The command places the size_only attribute on specified cells or instances to prevent them from being modified or replaced except cell sizing.\n To query the size_only attribute, use the report_attributes get_attribute, or report_cells command.\n To remove the attribute, use the remove_attributes command or the set_size_only command set to false This example sets the size_only attribute on a specific cell.\n fc_shell> set_size_only [get_cells cell_name] true This example queries the size_only attribute on the cell_name cell.\n fc_shell> get_attribute [get_cells cell_name] size_only       This example reports all the size_only information on design objects in the current design.\n fc_shell> report_size_only -all"}
{"instruction": "How do I Specifying the Primary Corner for Clock Tree Synthesis", "input": "", "output": "During clock tree synthesis, by default, the tool tries to minimize the global skew, which is the difference between the longest and shortest clock paths.\n If there is no timing path between the registers with the longest and shortest clock paths, optimizing global skew does not improve the timing QoR of the design.\n However, by optimizing the local skew, which is the worst skew between launch and capture registers of timing paths, you can improve the timing QoR of the design.\n During local skew optimization, the tool works on improving local skew of all violating setup and hold timing paths.\n To enable local skew optimization during the clock tree synthesis and clock tree optimization stages of the synthesize_clock_trees and clock_opt commands, set the cts.compile.enable_local_skew and cts.optimize.enable_local_skew application options to true fc_shell> set_app_options -list {cts.compile.enable_local_skew true} fc_shell> set_app_options -list {cts.optimize.enable_local_skew true} When you enable local skew optimization using the previous settings, by default, the tool derives skew targets that help improve the timing QoR, and ignores the target skew you       specify with the set_clock_tree_options -target_skew command.\n To prevent the tool from deriving skew targets, use the following application option setting: fc_shell> set_app_options \\ -name cts.common.enable_auto_skew_target_for_local_skew -value false"}
{"instruction": "How do I Performing Hierarchical Synthesis Using ETMs", "input": "", "output": "To synthesize the top-level, perform the following steps: 1.\n Read in the top-level design generated after the rebuild step as described in Partitioning and Planning the Full Chip Design 2.\n Set appropriate settings and link the top-level design as shown in the following example: fc_shell> set_label_switch_list \\ -reference {BLOCK1 BLOCK2} PostCompile fc_shell> set_attribute -objects {BLOCK1.nlib BLOCK2.nlib} \\ -name use_hier_ref_libs -value true fc_shell> set_top_module TOP When a subblock has been saved in the design library using labels, use the set_label_switch_list command to specify which label to link to.\n The set_top_module command links the specified top-level module.\n 3.\n Apply any block-specific constraints and settings required for synthesizing the top block and read in the test models for the subblocks as shown in the following example: fc_shell> read_test_model BLOCK1.ctl fc_shell> read_test_model BLOCK2.ctl fc_shell> create_test_protocol 4.\n Insert DFT logic and synthesize the top-level design by using the following commands: fc_shell> compile_fusion -to initial_opto fc_shell> insert_dft fc_shell> compile_fusion -from initial_place"}
{"instruction": "How do I Resolving Multiple References With the Uniquify Process", "input": "", "output": "The following topics describe how you can control the optimization of cells, nets, pins, and ports:  Resolving Multiple References With the Uniquify Process  Preserving Cells and Nets During Optimization  Restricting Optimization to Cell Sizing Only  Preserving Networks During Optimization  Marking the Clock Networks  Disabling Design Rule Checking (DRC)  Preserving Pin Names During Sizing  Preserving Ports of Existing Hierarchies  Isolating Input and Output Ports  Fixing Multiple-Port Nets  Controlling the Addition of New Cells to Modules, Hierarchical Cells, and Voltage Areas  Specifying a Cell Name Prefix for Optimization"}
{"instruction": "How do I Concurrent Soft-Rule-Based Redundant Via Insertion", "input": "", "output": "To perform postroute redundant via insertion, use the add_redundant_vias command.\n This command can replace single-cut vias with multiple-cut via arrays, single-cut vias with other single-cut vias that have a different contact code, and multiple-cut via arrays       with different multiple-cut via arrays.\n During redundant via insertion, the detail router also checks the design rules within the neighboring partition to minimize DRC violations.\n By default, the add_redundant_vias command inserts redundant vias on all nets.\n To insert redundant vias only on specific nets, use the -nets option to specify the nets.\n Using net-specific redundant via insertion allows you to further improve the optimized via rate without causing large-scale routing and timing changes.\n After the vias are checked and replaced, the detail router rechecks for DRC violations and corrects any violations.\n If the percentage of redundant vias is not high enough, you can increase the effort level by using the -effort option to get a better redundant via rate.\n Increasing the effort level to high can increase the redundant via rate by about 3 to 5 percent by shifting the vias to make room for additional vias.\n However, because high-effort redundant via insertion moves the vias more, it can result in a less lithography-friendly pattern at the 45-nm technology node and below.\n In this case, you should use concurrent soft-rule-based redundant via insertion to improve the redundant via rate.\n You can also try to increase the postroute redundant via rate by setting the route.detail.optimize_wire_via_effort_level application option to high, which reduces the number of single vias and makes more room for redundant vias by reducing wire length.\n After you perform the initial postroute redundant via insertion, set the route.common.post_detail_route_redundant_via_insertion application option to enable automatic insertion of redundant vias after subsequent detail routing or ECO routing.\n This helps to maintain the redundant via rate in your block."}
{"instruction": "How do I Embedding Tcl Commands in RTL Code", "input": "", "output": "You can read a combination of Verilog netlists and RTL files.\n Use the read_verilog command to read the Verilog netlists.\n Use the analyze and elaborate commands to read the RTL files.\n After all the netlist files and RTL files have been read into the tool, use the set_top_module command to link the designs to create the block for synthesis.\n The following example shows how you can read a mix of netlist and RTL files.\n As shown in this example, when you elaborate the top-level module with parameters, the top- level module name also contains the parameterized module name.\n You can control the naming style for parameterized modules using the following application options: hdlin.naming.template_naming_style hdlin.naming.template_parameter_style and hdlin.naming.template_separator_style # Read netlist files read_verilog NETLIST/bot.v # Analyze design files analyze -format sverilog -hdl_library MID RTL/mid.sv analyze -format sverilog RTL/top.svd # Elaborate top design module elaborate -parameters \"N=8,M=3\" top # Set the top level module to resolve references set_top_module top_N8_M3"}
{"instruction": "How do I Controlling Threshold-Voltage-Based Filler Cell Insertion", "input": "", "output": "In the threshold-voltage-based flow, the tool selects the filler cells by using user-defined insertion rules, which are based the threshold-voltage types of the cells that border the gap.\n This flow is typically used only for established foundry nodes.\n To perform threshold-voltage-based filler cell insertion, 1.\n Ensure that the block is legalized by using the check_legality command.\n 2.\n Annotate the threshold-voltage types on the reference cells by using the set_cell_vt_type command.\n For example, to specify that all cells whose names end with _vtA have a threshold voltage type of vtA and the invx1 cell has a threshold voltage type of default, use the following commands: fc_shell> set_cell_vt_type -lib_cells \"*/*_vtA\" -vt_type vtA fc_shell> set_cell_vt_type -lib_cells \"mylib/invx1\" -vt_type default 3.\n Define the rules for inserting the filler cells by using the set_vt_filler_rule command.\n You define the rules by specifying the filler cells to insert based on the threshold- voltage types of the cells on the left and right of the gap being filled.\n For example, to specify that filler2x or filler1x cells can be inserted in a gap that has default threshold-voltage cells on the left and right sides and fillerA2x and fillerA1x cells       can be inserted in a gap that has vtA threshold-voltage cells on the left and right sides, use the following commands: fc_shell> set_vt_filler_rule -vt_type {default default} \\ -filler_cells {myLib/filler2x myLib/filler1x} fc_shell> set_vt_filler_rule -vt_type {vtA vtA} \\ -filler_cells {myLib/fillerA2x myLib/fillerA1x} 4.\n Insert the filler cells by using the create_vtcell_fillers command."}
{"instruction": "How do I Calculating the Antenna Ratio for a Pin", "input": "", "output": "To control the method used to calculate the maximum allowable antenna ratio, you must specify the following information:  How much protection the diodes provide (the diode protection mode) To specify the diode protection mode, use the -diode_mode option with the define_antenna_rule command, as described in Setting the Diode Protection Mode This is a required option of the define_antenna_rule command.\n  How to perform antenna ratio calculation with diode protection (the diode ratio vector) To specify the diode ratio vector, use the -diode_ratio option with the define_antenna_layer_rule command, as described in Specifying the Diode Ratio Vector.\n This is a required option of the define_antenna_layer_rule command.\n       Setting the Diode Protection Mode The diode protection mode specifies how much protection the diodes provide.\n Advanced technology designs often use cells and macros with different gate oxide thicknesses, which affect the antenna rule calculations.\n The Fusion Compiler tool uses gate classes to represent the various gate oxide thicknesses.\n The tool supports up to four gate oxide thicknesses: gate class 0, gate class 1, gate class 2, and gate class 3.\n If a cell pin does not have gate class data, it is treated as gate class 0.\n The Fusion Compiler tool supports 18 diode protection modes, most of which support only a single gate oxide thickness.\n For advanced technology designs that use cells and macros with different gate oxide thicknesses, you must use one of the diode protection modes that support multiple gate oxide thicknesses: 14, 16, and 18.\n To specify the diode protection mode, use the -diode_mode option with the define_antenna_rule command.\n This is a required option of the define_antenna_rule command.\n Table 35 defines how each of the diode protection mode settings affect the maximum antenna ratio for a net.\n The diode protection mode also affects the computation of the maximum antenna ratio for individual diodes.\n The antenna ratio calculation formulas for diodes are shown in Table 37 ; these formulas use the values in the diode ratio vector specified in the -diode_ratio option of the define_antenna_layer_rule command, as described in Specifying the Diode Ratio Vector Table 35 Diode Protection Mode Settings Diode protection mode Definition                           Table 35 Diode Protection Mode Settings (Continued) Diode protection mode Definition                                        Specifying the Diode Ratio Vector The diode ratio vector specifies the values used to calculate the maximum antenna ratio.\n To specify the diode ratio vector, use the -diode_ratio option with the define_antenna_layer_rule command.\n If you do not specify a diode ratio, the tool uses {0 0 1 0 0}.\n The actual usage of the diode ratio vector depends on the diode mode that was specified in the define_antenna_rule -diode_mode option.\n       Table 36 shows the format of the diode ratio vector for the various diode modes.\n Table 37 shows how the vector values are used to calculate the maximum antenna ratio for each diode mode.\n In this table,  dp represents the diode protection value specified for an output pin.\n For information about specifying this value, see Specifying Antenna Properties  layerMaxRatio represents the maximum antenna ratio for the layer, as specified by the -ratio option of the define_antenna_layer_rule command or one the -metal_ratio or -cut_ratio option of the define_antenna_rule command if the layer-specific value is not defined.\n For examples of the maximum antenna ratio calculations, see  Example for Diode Modes 2, 3, and 4  Example for Diode Modes 5 and 6  Example for Diode Modes 7 and 8  Example for Diode Mode 14 With Multiple Gate Oxide Thicknesses Table 36 Diode Ratio Vector Formats Diode modes Format  v0 v1 v2 v3 v4 ]} v4  v4    {{ v0 v1 v2 v3 v4 ]}{ s0 s1 s2 s2 s4 s5 }} s0 s1 s2 s3 s4 s5}  v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 v0 v1 v2 v3 v4 v5 v6 v7 v8 v9   v0 v1 v2 v3 v4 v5 v6 v7 v0 v1 v2 v3 v4 v5 v6 v7 v0 v1 v2 v3 v4 v5 v6 v7 v0 v1 v2 v3 v4 v5 v6 v7   v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11        Table 37 Antenna Ratio Calculation Based on Diode Mode Diode mode Calculation    dp v0 v4 dp v1 v2 v3 v4 dp v0 v4 dp v1 v2 v3 dp v0 layerMaxRatio   v0 v4 v1 v2 v3 v4 v0 v4 v1 v2 v3 v0    v0 v4 v1 v2 v3 v4 v0 v4 v1 v2 v3 v0    v0 v4 v1 v2 v3 v4 v0 v4 v1 v2 v3 v0      v0 v4 v1 v2 v3 v4 v0 v4 v1 v2 v3 v0          Table 37 Antenna Ratio Calculation Based on Diode Mode (Continued) Diode mode Calculation   v0 v1 v2 v3 v4 v0    v0 v1 v2 v3 v4 v0    v0 s0 v1 s1 v2 s2 v3  v4 s4 v4 s5   v0 s0 v1 s1 v2 s2 v3 s3 v4 s4 v4 s5         Table 37 Antenna Ratio Calculation Based on Diode Mode (Continued) Diode mode Calculation   dp v1 v2  v5 v6 dp v7  v3  v8 v9    dp v0 v4 dp v1 v2 v3 v4 dp v0 v4 dp v1 v2 v3 dp v0 layerMaxRatio v5 v6 dp  v6 dp v7   v0 v4 v1 v2 v3 v4 v0 v4 v1 v2 v3 v0    dp v0 v4 dp v1 v2 v3 v4 dp v0 v4 dp v1 v2 v3 dp v0 layerMaxRatio v5 v6 dp  v6 dp v7  v8 v9  v10 v11        Example for Diode Modes 2, 3, and 4 Assume that the antenna area is calculated using surface area in single-layer mode (antenna mode 1), the diode ratio vector for the M1 layer is {0.7 0.0 200 2000}, the layerMaxRatio value for the M1 layer is 400, and the following diodes are connected to a single M1 net: diode A with a diode-protection value of 0.5, diode B with a diode-protection value of 1.0, and diode C with a diode-protection value of 1.5.\n In this example, the v4 value is not specified so a value of 0 is used.\n Use the following commands to define the antenna rules for this example: fc_shell> define_antenna_rule -mode 1 -diode_mode diode_mode -metal_ratio 400 -cut_ratio 20 fc_shell> define_antenna_layer_rule -mode 1 -layer \"M1\" \\ -ratio 400 -diode_ratio {0.7 0.0 200 2000} The maximum antenna ratio for each diode is computed by using the formula for the diode mode from Table 37.\n In this case,  If dp v0 and v4 <>0, allowable max-antenna-ratio = min ((( dp v1 ) * v2 v3 ), v4  If dp v0 and v4 =0, allowable max-antenna-ratio = ( dp v1 ) * v2 v3  If dp <= v"}
{"instruction": "How do I Preventing Off-Grid Routing", "input": "", "output": "You can prevent off-grid routing of wires or vias by requiring them to be aligned to the wire track or discourage off-grid routing of vias by increasing the cost associated with off-grid routing."}
{"instruction": "How do I Setting Up for Variant-Aware Legalization", "input": "", "output": "To enable advanced legalization algorithms for 2D rule checking and cell interaction, which can reduce legalization runtime, set the place.legalize.enable_advanced_legalizer application option to true To specify additional advanced legalization rules that are not automatically detected, set the place.legalize.enable_advanced_legalizer_rules application option.\n When you enable the advanced legalization algorithms, by default, the tool aligns vertical pin shapes during pin access optimization to improve routability.\n However, you can specify additional strategies for pin access optimization by setting the place.legalize.optimize_pin_access_strategies application option as shown in the following table.\n Table 12 Settings for the place.legalize.optimize_pin_access_strategies Application Option To do this Use this setting  horizontal_align   avoid_high_pin_density   horizontal_align avoid_high_pin_density During advanced legalization and legality checking, the tool can simultaneously check for DRC violations between multiple cells and PG net shapes.\n To enable this feature, set the place.legalize.enable_multi_cell_pnet_checks application option to true To enable multi cell checks at the last run of route_opt command or after the route_auto command, set the following application options to true.\n This helps to improve the runtime for QoR considerations.\n To enable the following application options, set the place.legalize.enable_multi_cell_pnet_checks application option to true  place.legalize.enable_multi_cell_access_check : considers impact on a cell's pin access due to presence of abutting neighbor cells and any prerouted net PG net shapes in the vicinity.\n Default is false  place.legalize.enable_multi_cell_track_capacity_check : analyzes the track capacity of the cell's pins and abutting neighbor cells.\n Track capacity estimates the sufficiency of routing tracks in the area to access all the specified pins.\n To enable this application option, set the place.legalize.enable_multi_cell_access_check application option to true.\n Default is false       To enable multithreaded advanced legalization, perform the following steps: 1.\n Enable multithreaded advanced legalization algorithms by setting the place.legalize.enable_threaded_advanced_legalizer application option to true 2.\n Configure for multithreading as described in Configuring Multithreading To perform multithreaded advanced legality checking, 1.\n Enable advanced legalization algorithms by setting the place.legalize.enable_advanced_legalizer application option to true 2.\n Configure for multithreading as described in Configuring Multithreading 3.\n Perform legality checking using the check_legality command."}
{"instruction": "How do I Reading the Design", "input": "", "output": "To compare libraries, perform the following steps: 1.\n Create a collection of the libraries you want to analyze, which is known as a libset, by using the create_libset command, as shown in the following example: fc_shell> create_libset -name myLibset -libs {abcd1.db abcd2.db} When you perform library analysis, you can specify the libset name, instead of specifying a list of libraries.\n In addition, the tool uses the libset name, instead of using a list of the library names, when it generates reports, plots, and so on during analysis.\n You can include a cell library in more then one libset.\n 2.\n Set up the library analysis environment by using the create_environment command.\n The create_environment command associates a predefined libset with a process label, process number, voltage, and temperature, which is then used for analyzing the libraries.\n You can specify the process label, process number, voltage, and temperature of the analysis environment by using one of the following methods:  Specify the process label, process number, voltage, and temperature values by using the -ppvt option, as shown in the following example: fc_shell> create_environment -name env1 -libset myLibset \\ -ppvt {{SS1p, 1.0, 0.6, 125}}        Specify a predefined operating condition from the reference library by using the -op_conds option, as shown in the following example: fc_shell> create_environment -name env2 -libset myLibset \\ -op_conds SS1p6125  Specify corners from which to infer the process label, process number, voltage, and temperature values by using the -corners option, as shown in the following example: fc_shell> create_environment -name env3 -libset myLibset \\ -corners nworst  Specify that the tool infers the process label, process number, voltage, and temperature values from the scenario and UPF associated with the current design by using the -auto_infer_design option, as shown in the following example: fc_shell> create_environment -name env4 -libset myLibset \\ -auto_infer_design 3.\n (Optional) Specify a naming convention for library cells and perform lexical classification by using lexical attributes.\n A library cell naming convention can help the tool identify cell families in the subsequent steps of the flow.\n For example, assume a cell in your library is named AND2D4WSULVT.\n This name is the concatenation of the AND2, D4, WS, and ULVT strings, which represent cell characteristics that are associated to a lexical attribute as shown in the following table: Table 3 Lexical Attributes for the Naming Convention of an Example Library Column Lexical Attribute Allowed Pattern Values for Example Cell  lexical_cell_prefix_name    lexical_drive1_name    lexical_well_substrate_bias_arch itecure    lexical_device_threshold_voltage         You can specify the naming convention for this library by using the set_lib_cell_naming_convention command, as shown in the following example: fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 1 -pattern {[a-zA-Z0-9]*} -attribute \"lexical_cell_prefix_name\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 2 -pattern {D[0-9]*} -attribute \"lexical_drive1_name\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 3 -pattern {WS} -attribute \"lexical_well_substrate_bias_architecure\" fc_shell> set_lib_cell_naming_convention -library $lib \\ -column 4 -pattern {ULVT|SVT|LVT|MVT} -attribute \"lexical_device_threshold_voltage\" Note: For a list of all the predefined lexical attributes, see the man page for the set_lib_cell_naming_convention command.\n After you specify the naming convention, perform lexical classification, by using the classify_lib_cell_attributes command as shown in the following example: fc_shell> classify_lib_cell_attributes -libset myLibset To report the lexical classification results, use the report_lib_cell_classification_attributes command.\n To report the library cells which are not lexical classified under the given naming convention, use the report_lexical_unclassified_lib_cells command.\n If there are lexically unclassified cells, specifies addition naming conventions and rerun lexical classification.\n 4.\n (Optional) Exclude specific cells from library analysis by using the set_use_for_library_analysis command.\n By default, the tool excludes physical-only cells during library analysis.\n However, you can exclude additional cells, such as library cells with a dont_use setting, as shown in the following example: fc_shell> set exclude_cells [filter_collection [get_lib_cells $libs/*]  \"dont_use == true\"] fc_shell> set_use_for_library_analysis $exclude_cells false To report the cells that are excluded, use the report_lexical_ignored_lib_cells command.\n 5.\n Identify all library cell families by using the identify_lib_cell_families command.\n       To report the tool-identified library cell families, use the report_lib_cell_families -all_initial command.\n To manually define library cell families, use the define_lib_cell_family command, and to report these user-defined cell families, use the report_lib_cell_families -all_userdef command.\n 6.\n Analyze the libraries by using the run_library_analysis command, as shown in the following example: fc_shell> run_library_analysis -name myLibset_run1 -environment env1 The name of the environment you specify with the -environment option must correspond to an environment you previously created by using the create_environment command.\n 7.\n Compare libraries by using the compare_libraries command.\n When doing so, you must specify the two libraries you want to compare by using the -ref_lib and -target_lib options.\n In addition, you must specify the name of the library analysis runs, performed by using the run_library_analysis command in the previous step, for each of these libraries.\n To do so use the -ref_lib_analysis and -target_lib_analysis options, as shown in the following example: fc_shell> compare_libraries -ref_lib $rvt_lib -ref_lib_analysis sa \\ -target_lib $lvt_lib -target_lib_analysis sa -verbose -plot When comparing libraries, you might encounter the following situations related to name matching between the cells of the reference and target libraries:  The cell names in the reference and target libraries can be an exact match, as in the case when it is two different versions of the same library.\n If so, additional lexical classification is not required.\n  The reference and target libraries can have the same naming convention, except less lexical attributes in one library.\n For example, assume a cell named ND2D1SVT in the reference library matches a cell named ND2D1 in the target library.\n In this case, the target library cell names do not contain a lexical_device_threshold_voltage attribute, as shown in the following table.\n Table 4 Reference and Target Library Lexical Attribute Differences Lexical Attribute Reference Library Target Library lexical_cell_prefix_name   lexical_drive1_name         Lexical Attribute Reference Library Target Library lexical_device_threshold_voltage   In this situation, you should ignore the lexical_device_threshold_voltage attribute when comparing the two libraries.\n To do so, use the following application option settings before you run the compare_libraries command: fc_shell> set_app_options -name libra.complib.match_full_cell_name -value false fc_shell> set_app_options -name libra.complib.match_lexical_device_threshold_voltage -value false fc_shell> compare_libraries -ref_lib $rvt_lib -ref_lib_analysis sa \\ -target_lib $lvt_lib -target_lib_analysis sa -verbose -plot Note: For a complete list of application options you can use to control name matching during library comparison, see the man page for the compare_libraries command.\n  The reference and target libraries can have a similar naming convention, but the lexical attributes values might not identical.\n For example, assume a cell named ND2D1SVT in the reference library matches a cell named nand2_svth_x1 in the target library.\n In this case, the reference and target libraries have the same lexical attributes, which have similar but not the same values, as shown"}
{"instruction": "How do I Scan Synthesis Flow", "input": "", "output": "Fusion Compiler test synthesis solution enables transparent implementation of DFT capabilities into the Synopsys synthesis flow without interfering with functional, timing, signal integrity, or power requirements.\n The test solution for Fusion Compiler consists of two stages:  Scan Synthesis Flow  Minimizing Glitches on Asynchronous Set and Reset Lines of Scan Registers       Benefits of two-stage test insertion include  Better timing, area, power, and congestion QoR for logic and DFT synthesis at the same time  Better integration of power intent and DFT logic  Improved congestion optimization that considers the placement for compressor and decompressor (codec) test logic  Eliminating the incremental compile step to synthesize and place the DFT logic"}
{"instruction": "How do I Specifying Net-Specific Layer Constraints", "input": "", "output": "To remove the global layer constraints, use the remove_ignored_layers command.\n You must specify one or more of the following options:  -min_routing_layer This option removes the minimum routing layer setting.\n When you remove the minimum routing layer setting, it also removes the ignored layers for RC estimation and congestion analysis that are below the minimum routing layer.\n  -max_routing_layer This option removes the maximum routing layer setting.\n When you remove the maximum routing layer setting, it also removes the ignored layers for RC estimation and congestion analysis that are above the maximum routing layer.\n  -all This option removes the ignored layers for RC estimation and congestion analysis that are between the minimum and maximum routing layers.\n  -rc_congestion_ignored_layers layer_list This option removes the specified ignored layers for RC estimation and congestion analysis.\n The specified layers must be between the minimum and maximum routing layers."}
{"instruction": "How do I Routing Using Custom Router", "input": "", "output": "By default, the signoff_fix_isolated_via command both checks for and fixes isolated vias.\n The command performs the following tasks: 1.\n Checks for isolated vias by using the ranges defined in the signoff.fix_isolated_via.isolated_via_max_range application option 2.\n (Optional) Performs track-based metal fill insertion to insert dummy fill shapes within the specified range around each detected isolated via  By default, the signoff_fix_isolated_via command ignores the existing fill data and performs track-based metal fill insertion to insert the fill shapes to use for fixing the isolated vias.\n The fill shapes are inserted on-track following the design rules defined in the technology file.\n Some foundries have additional design rules, which you enable by setting the appropriate TRACK_FILL_FOUNDARY_ name IC Validator variable.\n To set this variable, set the signoff.create_metal_fill.user_defined_options application option, as shown in the following example: fc_shell> set_app_options \\ -name signoff.create_metal_fill.user_defined_options \\ -value {-D TRACK_FILL_FOUNDARY_ name  If you have already performed track-based metal fill insertion, you can use the existing fill shapes to fix isolated vias by using the -update_track_fill true option.\n When you use this option, you must use the -track_fill_runset_include_file option to specify the parameter file used for the initial track-based metal fill insertion.\n This parameter file is named track_fill_params.rh and is located in the run directory for the initial run.\n For example, if the run directory for the initial track-based metal fill insertion run is named init_fill, use the following command to fix isolated vias using the existing fill shapes: fc_shell> signoff_fix_isolated_via -update_track_fill true \\ -track_fill_runset_include_file init_fill/track_fill_params.rh For information about performing track-based metal fill insertion, see Track-Based Metal Fill Insertion       3.\n Inserts fixing vias within the specified range by using one of the following methods, in order of priority: a.\n Inserting a via between an existing non-wide net shape and a fill shape b.\n Extending the line end of an existing non-wide net shape and inserting a via between the extension and a fill shape c.\n Inserting a via between a wide metal shape and a fill shape When inserting the fixing vias, the tool considers the routing rules, including double- patterning rules and nondefault spacing rules, to prevent the introduction of DRC violations.\n Note: If the block is either very congested or very sparse, the signoff_fix_isolated_via command might not be able to fix all isolated vias.\n 4.\n Removes the unused dummy fill shapes 5.\n (Optional) Saves the updated block to disk  By default, the signoff_fix_isolated_via command does not save the updated block to disk.\n After running the command, you must use the save_block command to save the updated block to disk.\n  To save the block at the end of the signoff_fix_isolated_via run, use the -save_design true option with the signoff_fix_isolated_via command.\n 6.\n Saves the results to disk The signoff_fix_isolated_via command generates the following results files:  A summary report This report specifies the number of isolated vias for each via layer before and after fixing.\n  An error data file By default, the error data generated by the signoff_fix_isolated_via command is saved in a file named signoff_fix_isolated_via.err.\n To specify the name for the error data file, use the -error_data option.\n To report or display the information in the error data file, use the error browser, as described in the Fusion Compiler Graphical User Interface User Guide"}
{"instruction": "How do I Aligning Leaf Cells Within a Column", "input": "", "output": "The following example specifies that relative placement cell at column 1, row 2 of the RP1 relative placement group should be anchored at location (100, 100).\n fc_shell> set_rp_group_options RP1 \\ -anchor_corner rp_location -anchor_column 1 -anchor_row 2 \\ -x_offset 100 -y_offset 100  Using an Object Within the Relative Placement Group for Anchoring The relative placement group is anchored by placing the cell at column 1 row 2 at location (100, 100)"}
{"instruction": "How do I Defining Sink Pins", "input": "", "output": "Clock tree exceptions are user-defined changes to the default endpoints (balancing and ignore pins) derived by the tool for a specific clock.\n The Fusion Compiler tool supports the following types of clock tree exceptions:  User-defined sink pins These exceptions define sink pins in addition to those derived by the tool for a clock tree.\n For example, you might define a tool-derived ignore pin as a clock sink.\n For information about defining sink pins, see Defining Sink Pins  User-defined insertion delay requirements These exceptions specify special insertion delay requirements for a sink pin (either derived or user-specified).\n       For information about defining insertion delay requirements, see Defining Insertion Delay Requirements  User-defined ignore pins These exceptions exclude clock endpoints that were derived as sink pins by the tool.\n For example, you might define an ignore pin to exclude all branches of the clock tree that fan out from some combinational logic or to exclude a tool-derived sink pin.\n For information about defining ignore pins, see Defining Ignore Pins"}
{"instruction": "How do I Performing ECO Routing", "input": "", "output": "Before fixing signal electromigration violations, make sure that the block is detail routed, the signal electromigration constraints are defined in the design library, the switching activity is defined for all boundary nets, crosstalk analysis is enabled, and there are no timing or DRC violations.\n You can optionally use the report_signal_em command first to report electromigration violations before attempting to fix them.\n       To fix signal electromigration violations, use the fix_signal_em command, which performs the following tasks:  Runs signal electromigration analysis for nets by calculating the current on every edge of a net and comparing it with the constraints set by the read_signal_em_constraints command.\n  Fixes violations by applying nondefault routing rules to widen the metal shapes and then performing ECO routing.\n The fix_signal_em command runs a single fixing iteration; if violations remain after running this command, you must rerun it to address the remaining violations.\n By default, the command fixes signal electromigration violations on all nets.\n To fix the violations only on specific nets, use the -nets option.\n See Also  Analyzing Signal Electromigration  Performing ECO Routing"}
{"instruction": "How do I Reporting the Metal Density", "input": "", "output": "To view the metal fill shapes in the layout view of the GUI, 1.\n Display the View Settings panel by clicking the View Settings icon ( ) in the panel area.\n  Set the Level to 1.\n  Select the Objects tab and enable visibility for Fill Cell objects and Area Fill metal shapes (if you do not see Area Fill in the object list, expand the Route object and Shape Use attribute).\n 2.\n Display the Hierarchy Settings panel by clicking the Hierarchy Settings icon ( ) in the View Settings panel or the menu bar.\n  Set the \"View level\" to 1.\n  Select \"Fill Cell\" in the \"Expanding cell types\" section.\n  shows the View Settings and Hierarchy Settings panels with the required settings for displaying fill shapes.\n        Settings Used to Display Fill Shapes in the GUI"}
{"instruction": "How do I Crosstalk-Driven Global Routing", "input": "", "output": "By default, the route_global command is not timing-driven.\n To enable timing-driven global routing, set the route.global.timing_driven application option before you run the route_global command.\n During timing-driven global routing, the global router considers the impact of layer resistance by minimizing the usage of pin access layers, even if this increases wire length.\n By default, when you enable timing-driven global routing, the tool 1.\n Calculates the net delays of the block If the design library contains global route information, the tool uses the global route information to calculate the net delays; otherwise, it uses virtual routing to calculate the net delays.\n The global routing results can vary depending on whether the initial net delays were calculated by using global route information or virtual routing.\n To remove existing global route information from the signal nets in the block, use the -global_route and -net_types signal options with the remove_routes command, as shown in the following example: fc_shell> remove_routes -global_route -net_types signal 2.\n Performs an initial routing phase (phase 0), in which the tool routes the unconnected nets and calculates the overflow for each global routing cell       3.\n Performs two rerouting phases, in which the tool tries to reduce congestion by ripping up and rerouting nets around global routing cells with overflows 4.\n Updates the design timing based on the first three global routing phases 5.\n Identifies newly critical nets based on the timing update 6.\n Performs three rerouting phases, in which the tool rips up and reroutes the newly critical nets To increase the number of rerouting phases after the timing update, set the global routing effort level to high or ultra by using the -effort_level option with the route_global command.\n To reduce runtime, at a possible cost to the timing QoR, set the route.global.advance_node_timing_driven_effort application option.\n  When you set this application option to medium, the tool rips up and reroutes the newly critical nets only if doing so does not increase congestion.\n In addition, the number of rerouting phases is reduced for medium-, high-, and ultra-effort global routing.\n  When you set this application option to low, the tool does not perform the timing update and the number of rerouting phases is reduced for medium, high-, and ultra- effort global routing.\n The following table summarizes the timing-driven global routing behavior based on the effort level settings.\n The shaded cell shows the default behavior.\n Table 32 Number of Rerouting Phases Based on the Effort Level Settings route.global.advance_node_timing_driven_effort setting route_global -effort_level setting low medium high (default) low    medium   3,4 3,5 high  3,4 3,4 ultra  3,4 3,4 To control the tradeoff between timing QoR and DRC convergence, set the route.global.timing_driven_effort_level application option.\n By default, this option has a setting of high, which favors timing QoR over DRC convergence.\n 3.\n Timing update performed after the second rerouting phase.\n 4.\n Newly critical nets are ripped up and rerouted only if it does not increase congestion.\n 5.\n Newly critical nets are always ripped up and rerouted."}
{"instruction": "How do I Implementing a Structural Multisource Clock Tree", "input": "", "output": "The tool derives tap driver configurations by adding power awareness in the flexible H-Tree synthesis flow, which is optimal in terms of total clock power and overall clock latency.\n       This requires at least one of the dynamic power scenarios to be defined and active during H-tree Synthesis.\n In case of multiple scenarios, the worst active scenario is considered for power costing.\n The initial tap configurations are generated based on floorplan shape, size, sink distribution, placement, routing constraints, and or each tap configuration.\n This helps to estimate the latency and power for each subtree, and then tabulates and compares the results of different tap configurations to determine the best solution for power aware flexible H-Tree synthesis.\n To enable power aware flexible H-tree synthesis, set the cts.multisource.power_aware_flexible_htree_synthesis application option to true"}
{"instruction": "How do I Removing the Threshold-Voltage Filler Cell Information", "input": "", "output": "By default, the create_vtcell_fillers command fills all empty space in the horizontal standard-cell rows of the entire block.\n You can control the following aspects of the filler cell insertion:  The region in which to insert the filler cells  Use the -region option to restrict filler cell insertion to the specified regions.\n  Use the -voltage_area option to restrict filler cell insertion to the specified voltage areas.\n  The naming convention used to identify the inserted filler cell instances By default, the command uses the following naming convention for inserted filler cells: xofiller!\n filler_library_cell_name number To identify the filler cells inserted in a specific run, use the -prefix option to specify a prefix string.\n When you use this option, the command uses the following naming convention: xofiller!\n prefix filler_library_cell_name number To change the separator character from the default exclamation mark (!), use the -separator option."}
{"instruction": "How do I Running Commands in the Background", "input": "", "output": "You can improve runtime by running checking and reporting commands in parallel on your local host.\n You can run the commands either in the background (with the redirect -bg command) or in the foreground (with the parallel_execute command).\n These techniques do not require the use of additional licenses beyond the licenses required for the parent run.\n They also do not require you to use or configure distributed processing.\n When you use these techniques, consider the following guidelines:  To reduce runtime and memory usage, run the update_timing command before running the commands in parallel; otherwise, each command that requires updated timing runs the update_timing command independently.\n  To pass variables from a child process to the parent process, you must write the contents of the variables to a file during the child process, and then read that file in the parent process.\n See Also  Running Commands in the Background  Running Commands in Parallel"}
{"instruction": "How do I Transparent Hierarchy Optimization", "input": "", "output": "Before you can use the ETMs at the top level, you must perform the following tasks:  Apply the top-level-only timing constraints and settings.\n If the cell instances represented by ETMs have internal clocks, you must reapply these clock definition from the top level.\n Avoid cross-boundary timing exceptions that refer to objects inside the blocks.\n  To apply the UPF constraints and settings, you must: 1.\n Generate the block-level UPF from each of the block-level designs using the save_upf command.\n 2.\n At the top level, load the block UPF for each of the block instances that are represented by an ETM and the top only UPF: load_upf -scope <instance_name> block.upf load_upf top.upf  Apply top-level placement constraints and settings such as hard keepout margins.\n When using ETMs at the top-level, you can perform synthesis, top-level placement, optimization, clock tree synthesis, routing, and postroute optimization using the commands supported at the block level."}
{"instruction": "How do I Viewing Minimum Path Resistance Analysis Results", "input": "", "output": "Note: Minimum path resistance analysis is supported in both RedHawk Fusion and RedHawk-SC Fusion analysis flows.\n Perform minimum path resistance analysis to quickly detect power and ground network weaknesses in the design.\n The minimum path resistance value of a node is the resistance value on the smallest resistive path to the ideal voltage source locations (taps).\n The ideal voltage sources can be power or ground pins, user-defined taps, or packages.\n Because this value represents only the resistance on a single path, it is not necessarily same as the effective resistance value from the node to the supply or ground; instead, it represents an upper bound on the effective resistance of a node to the supply or ground.\n       To perform minimum path resistance analysis, run the analyze_rail -min_path_resistance command.\n You must specify the power and ground nets to analyze by using the -nets option.\n Note: In RedHawk-SC rail analysis flow, you must specify both the -voltage_drop and -min_path_resistance options for minimum path resistance calculation.\n You can perform minimum path resistance analysis before or during voltage drop analysis.\n When the analysis is completed, the tool writes the minimum path resistance values and the voltage drop results into the same rail result.\n The following example performs voltage drop analysis and minimum path resistance calculation at the same time.\n fc_shell> analyze_rail -nets {VDD VSS} \\ -min_path_resistance -voltage_drop static The following example performs only minimum path resistance analysis.\n fc_shell> analyze_rail -nets {VDD VSS} -min_path_resistance"}
{"instruction": "How do I Handling Tap Columns and Relative Placement Group Overlaps", "input": "", "output": "You can control the movement of a relative placement group during legalization by using the -move_effort option of the set_rp_group_options command.\n During coarse placement, the tool estimates an initial location for every top-level relative placement group.\n The -move_effort option controls the extent to which a relative placement group can be moved from its initial location to preserve the relative placement without violating relative placement constraints.\n When you change the option setting from a higher effort level to a lower effort level, you reduce the size of the region searched for placement of a relative placement group."}
{"instruction": "How do I Performing Integrated Multibit-Register Optimization", "input": "", "output": "The Fusion Compiler tool can combine (bank) single-bit registers or smaller multibit registers and replace them with equivalent larger multibit registers.\n For example, the tool can combine eight 1-bit registers or four 2-bit register banks and replace them with one 8-bit register bank or two 4-bit register banks.\n The tool merges single-bit registers only if they have the same timing constraints, and it copies the timing constraints to the resulting multibit register.\n        Replacing Multiple Single-Bit Register Cells With a Multibit Register Cell Two 1-bit register cells One 2-bit register cell Replace with         SI SE SI SE SI SE Replacing single-bit registers with multibit registers reduces  Area due to shared transistors and optimized transistor-level layout  The total clock tree net length  The number of clock tree buffers and clock tree power The tool can also split (debank) large multibit registers into smaller multibit registers or single-bit registers, if it improves the total negative slack.\n Some logic libraries have mixed-drive-strength multibit registers where some bits have a higher drive strength than others.\n For designs that use such cells, if a violating path goes through a lower-drive-strength bit, the tool can rewire the mixed-drive-strength multibit cell such that the violating path goes through a higher-drive-strength bit."}
{"instruction": "How do I Specifying Locations for Unmapped Cells", "input": "", "output": "A placement attraction is a constraint that you can use to specify that a large group of cells be placed together in the same vicinity of the placement area.\n It is a soft constraint that the tool considers during initial placement.\n However, it has less effect during subsequent incremental placement stages.\n To specify a more restrictive placement constraint for a smaller group of cells, create a move bound or group bound by using the create_bound command.\n To define a placement attraction, use the create_placement_attraction command.\n For example, to specify that the cells from the subblocks named add1 and mult1 be placed in the same vicinity, use the following command: fc_shell> set add1_cells [get_flat_cells add1/*] fc_shell> set mult1_cells [get_flat_cells mult1/*] fc_shell> create_placement_attraction -name add1_mult1 \\ \"$add1_cells $mult1_cells\" You can use the -region option to specify  A region in which to place the cells To do so, specify the lower-left and upper-right coordinates of the region, as shown in the following example: fc_shell> set U1_cells [get_flat_cells U1/*] fc_shell> create_placement_attraction -name U1 \\ -region {{0 0} {2000 1500}} $U1_cells  A straight line along which to place the cells, such as an edge of a macro cell To do so, specify the coordinates of the two ends of the line, as shown in the following example: fc_shell> set U2_cells [get_flat_cells U2/*] fc_shell> create_placement_attraction -name U2 \\ -region {{1000 200} {1000 800}} $U2_cells        A location around which to place the cells, such as a port location To do so, specify the coordinates of the location, as shown in the following example: fc_shell> set U3_cells [get_flat_cells U3/*] fc_shell> create_placement_attraction -name U3 \\ -region {{0 700}} $U3_cells The following table shows additional commands available for changing, reporting, and removing placement attractions.\n For more information, see the command man pages.\n Table 9 Commands Related to Placement Attractions To do this Use this command  create_placement_attraction  add_to_placement_attraction  remove_from_placement_attraction  report_placement_attractions  get_placement_attractions  remove_placement_attractions See Also  Defining Move Bounds"}
{"instruction": "How do I Verifying Via Ladders", "input": "", "output": "To prevent via ladders from being edited, set the design.enable_via_ladder_protection application option to true fc_shell> set_app_options -name design.enable_via_ladder_protection \\ -value true When the design.enable_via_ladder_protection application option is true, the following commands process only objects that do not belong to via ladders:  remove_shapes  remove_vias        remove_objects  set_attribute  set_via_def  add_to_edit_group If an object belongs to a via ladder, these commands issue a warning message and do not process the object.\n The following example sets the design.enable_via_ladder_protection application option to true and attempts to remove a collection of shapes: fc_shell> set_app_options -name design.enable_via_ladder_protection \\ -value true...\n fc_shell> sizeof_collection [get_shapes -of net1] 77  fc_shell> remove_shapes [get_shapes -of net1] -verbose Warning: Cannot PATH_32_7403 because it is part of via_ladder.\n (NDM-160) 76"}
{"instruction": "How do I Prerequisites for Clock Tree Synthesis", "input": "", "output": "To learn how to perform clock tree synthesis in the Fusion Compiler tool, see the following topics:  Prerequisites for Clock Tree Synthesis  Defining the Clock Trees  Verifying the Clock Trees  Setting Clock Tree Design Rule Constraints  Specifying the Clock Tree Synthesis Settings  Implementing Clock Trees and Performing Post-CTS Optimization  Implementing Multisource Clock Trees  Analyzing the Clock Tree Results"}
{"instruction": "How do I Generating Instance-Based Analysis Reports", "input": "", "output": "After you perform rail analysis on the top-level design, the tool saves the analysis results in the top-level rail database.\n To locate issues that are reported in a block-level design, run the open_rail_result command with the -top_design and -block_instance options to excerpt block-level instance data from the top-level rail database.\n The tool then displays maps for the specified block instances based on the extracted block-level rail analysis data.\n Note: The excerpt of the block-level rail results is saved in memory and is deleted when you exit the current session.\n The following example opens the rail result named REDHAWK_RESULT in the./ RAIL_DATABASE directory; the result is the rail analysis result for the top design named bit_coin.\n The example then creates an excerpt of rail analysis data for the block instance slice_5.\n Note that in this example, the block design of instance slice_5 has to be opened, not the top-level design.\n fc_shell> open_lib block.nlib fc_shell> open_block block fc_shell> set_app_options -name rail.database -value RAIL_DATABASE fc_shell> open_rail_result REDHAWK_RESULT \\ -top_design bit_coin \\ -block_instance slice_5 fc_shell> report_rail_result"}
{"instruction": "How do I Setting Options for Signoff Design Rule Checking", "input": "", "output": "To perform signoff design rule checking by running the signoff_check_drc command, 1.\n Set up the IC Validator environment as described in Setting Up the IC Validator Environment 2.\n (Optional) Enable distributed processing by using the set_host_options command, as described in Enabling IC Validator Multicore Processing       3.\n Set the application options for signoff design rule checking.\n At a minimum, you must specify the foundry runset to use for design rule checking by setting the signoff.check_drc.runset application option.\n For information about the options for command-based signoff design rule checking, see Setting Options for Signoff Design Rule Checking 4.\n Save the block to disk.\n When you run signoff design rule checking, the IC Validator tool uses the on- disk information for the block, not the information in memory.\n To ensure accurate information, use the save_block command to save the current state of the block before running signoff design rule checking.\n 5.\n Run signoff design rule checking by using the signoff_check_drc command.\n By default, the signoff_check_drc command performs the following tasks: 1.\n Loads the block into the IC Validator tool, as described in Reading Blocks for Signoff Design Rule Checking 2.\n Performs signoff design rule checking, as described in Signoff Design Rule Checking 3.\n Generates an error data file and IC Validator results files, as described in Signoff DRC Results Files You can use the results files to view the violations in the error browser, as described in Using the Error Browser, or to perform automatic design rule fixing, as described in Automatically Fixing Signoff DRC Violations You can use the following methods to analyze the DRC violations reported by the signoff_check_drc command:  View the violations in the error browser, as described in Using the Error Browser.\n  View the density of violations in a heat map, as described in Viewing the Violations in an ICV Heat Map See Also  Checking Signoff Design Rules Interactively in the GUI"}
{"instruction": "How do I Automatically Fixing Double-Patterning Odd-Cycle Violations", "input": "", "output": "By default, the signoff_fix_drc command performs the following tasks: 1.\n Runs the signoff_check_drc command to generate the initial signoff DRC results, as described in Checking for DRC Violations If you have already run the signoff_check_drc command, you can use the existing signoff DRC results instead of rerunning signoff design rule checking.\n To reuse the existing results, specify the directory that contains the results by setting the signoff.fix_drc.init_drc_error_db application option.\n You can specify either a relative path or an absolute path.\n If you specify a relative path, it is relative to the current working directory.\n For example, to use the results from the default signoff_check_drc run directory, use the following commands: fc_shell> set_app_options \\ -name signoff.fix_drc.init_drc_error_db \\ -value signoff_check_drc_run fc_shell> signoff_fix_drc Note: To use existing signoff DRC results as input to the signoff_fix_drc command, you must use the signoff DRC application options described in Generating Input for the Automatic Fixing Flow 2.\n Runs two repair loops to fix the DRC violations In each repair loop, the signoff_fix_drc command performs the following tasks; a.\n Tries to fix the DRC violations detected in the previous IC Validator signoff DRC run, as described in Fixing DRC Violations b.\n Runs IC Validator signoff design rule checking on the modified block, as described in Checking for DRC Violations To change the number of repair loops, use the -max_number_repair_loop option; you can specify an integer between 1 and 10.\n If no signoff DRC violations remain after a loop, the tool does not perform additional repair loops.\n As the repair loop number increases, so does the physical scope of the rerouting performed by that repair loop.\n To change the scope, increase the number of the initial repair loop by using the -start_repair_loop option.\n Note that increasing the initial loop number might cause a greater disturbance to the block, which would require additional design rule checking and fixing by Zroute.\n       3.\n Writes a summary report of the results For information about the summary report, see Summary Report for Automatic Design Rule Fixing Fixing DRC Violations In each repair loop, the signoff_fix_drc command tries to fix the DRC violations detected in the previous signoff DRC run.\n  For the first loop, the tool uses the IC Validator data in the directory specified by the signoff.fix_drc.init_drc_error_db application option (or the signoff_drc_run_init directory if you do not use this option).\n  For successive loops, the tool uses the IC Validator DRC data from the previous loop, which is stored in the directory specified by the signoff.fix_drc.run_dir application option (or the signoff_fix_drc_run directory if you do not use this option).\n When fixing DRC violations, the signoff_fix_drc command has the following default behavior:  Tries to fix all design rule violations detected in the previous signoff DRC run except those design rules that have more than 1000 violations  Performs DRC fixing on data nets for the whole block To modify the regions for design rule fixing, use one or both of the following options:  -coordinates This option restricts design rule fixing to the specified regions.\n  -excluded_coordinates This option prevents design rule fixing in the specified regions.\n If you specify this option with the -coordinates option, the command does not fix design rules in the overlapping regions.\n To specify the coordinates, use the following format: {{x } {x }...\n {x }} Note that there must be a space between each set of coordinates.\n       To prevent fixes that might impact timing-critical nets, use one or both of the following options:  -nets This option explicitly specifies the critical nets.\n  -timing_preserve_setup_slack_threshold This option enables the tool to automatically determine the critical nets based on a setup slack threshold.\n The unit for the slack threshold setting is the library time unit.\n  Performs five iterations of detail routing after DRC fixing to fix any DRC violations introduced during DRC fixing  Saves the modified block in a design view named block _ADR_# To change the default behavior, use the following options:  signoff.fix_drc.max_errors_per_rule This application option specifies the error threshold for ignoring a design rule.\n  signoff.fix_drc.fix_detail_route_drc This application option controls the scope of DRC fixing.\n To reduce runtime by performing DRC fixing only in areas near the signoff DRC violations, set this application option to local  signoff.fix_drc.target_clock_nets This application option controls whether DRC fixing targets data nets or clock nets.\n To target clock nets, set this application option to true  signoff.fix_drc.max_detail_route_iterations This application option specifies the number of detail routing iterations; you can specify an integer between 0 and 1000.\n Checking for DRC Violations When performing signoff design rule checking, the signoff_fix_drc command has the following default behavior:  Reads the design view for the top-level block and library cell instances, and the pin information from the frame view for the macro cell and I/O pad cell instances For information about changing the view used for specific cells, see Reading Blocks for Signoff Design Rule Checking  After the initial run, performs design rule checking only on those portions of the design affected by design rule fixing       To perform signoff design rule checking on the entire block for the last repair loop, set the signoff.fix_drc.last_run_full_chip application option to true.\n Setting this option to true increases accuracy but also increases runtime.\n To perform signoff design rule checking on the entire block for all repair loops, set the signoff.fix_drc.check_drc application option to global.\n Setting this option to global increases accuracy but can greatly increase runtime.\n  Does not check for violations in child blocks  Excludes rules that are not suitable for automatic design rule fixing To explicitly specify the design rules checked by the IC Validator tool, set the -select_rules rules and -unselect_rules rules application options.\n  Stores the IC Validator results in the signoff_fix_drc_run directory To use a different run directory, set the signoff.fix_drc.run_dir application option.\n Note: The IC Validator results for the initial signoff DRC run are stored in the signoff_drc_run_init directory.\n In addition, you can specify options to add to the IC Validator command line by setting the signoff.fix_drc.user_defined_options application option.\n See Also  Performing Signoff Design Rule Checking"}
{"instruction": "How do I Routing Application Options", "input": "", "output": "By default, Zroute can reroute nets as needed.\n You can prevent rerouting or limit rerouting to minor changes by setting the physical_status attribute on the nets.\n  To freeze the net and prevent rerouting, set the attribute to locked  To limit rerouting to minor changes, set the attribute to minor_change  To allow Zroute to reroute the nets as needed, set the attribute to unrestricted For example, to prevent rerouting of the net1 net, which uses the default routing rule, use the following command: fc_shell> set_attribute -objects [get_nets net1] \\ -name physical_status -value locked"}
{"instruction": "How do I Generating Macro Models", "input": "", "output": "Memory and macro cells are used in designs with advanced process technologies to reduce memory usage and improve the rail analysis performance.\n Macro models are supported in both RedHawk Fusion and RedHawk-SC Fusion analysis flows.\n To learn about working with macro models, see the following topics:  Generating Macro Models  Creating Block Contexts"}
{"instruction": "How do I Querying Group Repeater", "input": "", "output": "To remove a group of repeaters, use the remove_eco_repeater command.\n The nets are restored to their original names and routes shapes."}
{"instruction": "How do I Analyzing Relative Placement in the GUI", "input": "", "output": "To query relative placement groups that contain specific objects or attribute values, use the get_rp_groups command.\n For example, the following command returns a collection consisting of all the relative placement groups: fc_shell> get_rp_groups {RP_TA RP_TCO RP_HO_1 RP_HO_2 RP_HO_3 RP_HO_4 RP_THO} The following command returns only the top-level relative placement groups: fc_shell> get_rp_groups -top {RP_TA RP_TCO RP_THO} The following command returns the relative placement group that contains the leaf cell named U129: fc_shell> get_rp_groups -of_objects U129 {RP_HO_4}"}
{"instruction": "How do I Defining Blockages That Allow Relative Placement Cells Only", "input": "", "output": "To define partial blockages that exclude relative placement groups, specify the boundary, type ( -type rp_group option), blockage percentage ( -blocked_percentage option), and optionally the name for the placement blockage.\n For example, to define a partial blockage that excludes relative placement groups, but allows a cell density of 100 percent for all other cells, use the following commands: create_placement_blockage -boundary {{10 20} {100 200}} \\ -type rp_group -blocked_percentage 0 Note: Relative placement group blockages are honored during placement, but not during legalization, optimization, or clock tree synthesis."}
{"instruction": "How do I Specifying the Orientation of Relative Placement Groups", "input": "", "output": "You can apply compression to a relative placement group with cells that straddle multiple rows or columns, as shown in the following example: fc_shell> add_to_rp_group rp -cells U5 \\ -column 0 -row 0 -num_columns 1 -num_rows 2 fc_shell> set_rp_group_options rp -tiling_type horizontal_compression  shows the placement of the relative placement group in the previous example.\n  Compression of a Relative Placement Group With a Cell That Straddles Multiple Rows           Cell U5 straddles two rows For information about adding cells that straddle multiple rows or columns to a relative placement group, see Adding Leaf Cells"}
{"instruction": "How do I Setting Constraints of Variant Cell", "input": "", "output": "The add_group_repeater and place_group_repeater commands support variant cell in group repeater insertion and placement.\n       The following topics describe the steps in this flow  Setting Constraints of Variant Cell  Setting Application Option  Grouping Variant Cell  Running and Placing Group Repeaters  Troubleshooting"}
{"instruction": "How do I Preserving Preexisting Clock Trees", "input": "", "output": "By default, the tool synthesizes all clocks defined with the create_clock and create_generated_clock commands.\n To prevent the tool from synthesizing a specific clock, specify it by using the cts.common.skip_cts_clocks application option.\n fc_shell> set_app_options \\ -name cts.common.skip_cts_clocks -value CK_B"}
{"instruction": "How do I Specifying Locations for Repeater Groups", "input": "", "output": "To place group repeaters for multibit registers, use the create_repeater_groups command with the -multi_bit option with the following command options: create_repeater_groups -cells | -supernet_bundles | -supernets -multi_bit -pin_mapping {{lib_cell1 {inp1 outp1} {inp2 outp2}\u2026} The -multi_bit option determines the order of a multibit repeater based on the path driver and creates the repeater groups for the multibit repeaters.\n In a design, when a net has no route defined, this option places the group repeaters based on their location.\n To verify the support for multibit register placement, use the following command: fc_shell> preplace_group_repeater -repeater_group_locations repeater_group_location_list The format for the repeater_group_location_list argument is       { {cell_collection, layer, location, potential routing direction, placing order}...} or { {group_id, layer, location, potential routing direction, placing order}...}."}
{"instruction": "How do I Postroute Redundant Via Insertion", "input": "", "output": "To define a customized via mapping, use the add_via_mapping command.\n At a minimum, you must specify the source via and its replacement vias by using the -from and -to options.\n The vias listed in these options must be either vias defined in the technology file or design-specific vias created by the create_via_def command.\n The vias listed in the -from option must be simple vias or via arrays.\n The vias listed in the -to option can be simple vias, simple via arrays, or custom vias.\n For information about creating design- specific vias, see Defining Vias Use the following options to refine the via mapping:  -weight By default, all mappings have the same priority, and Zroute selects the redundant vias to use based on routability.\n To set the priority for a mapping, use the -weight option to assign an integer weight value between 1 and 10 to the mapping.\n If you do not assign a weight, the tool assigns a weight of 1.\n During redundant via insertion, Zroute uses the higher weighted redundant vias first.\n  -transform By default, Zroute can rotate or flip the via arrays during redundant via insertion (the -transform all option).\n  To allow only rotation of the via arrays during redundant via insertion, use the -transform rotate option.\n  To allow only flipping of the via arrays during redundant via insertion, use the -transform flip option.\n  To allow only the specified orientation of the via array during redundant via insertion, use the -transform none option.\n The tool saves the mappings defined by the add_via_mapping command in a via mapping table in the design library.\n By default, if you try to add a via mapping that already exists in the table, the command fails.\n To overwrite an existing mapping definition, use the -force option.\n Example 28 shows an example of using the add_via_mapping command to define a customized via mapping table.\n Example 28 Customized Via Mapping Table fc_shell> add_via_mapping \\ -from {VIA12 1x1} -to {VIA12T 2x1} -weight 5 fc_shell> add_via_mapping \\ -from {VIA12 1x1} -to {VIA12 2x1} -weight 1       To see the customized via mappings associated with a block, use the report_via_mapping command.\n  By default, this command shows all user-defined via mappings.\n  To show the via mappings for specific source vias, use the -from option.\n  To show the via mappings for specific replacement vias, use the -to option.\n To remove mappings from the via mapping table, use the remove_via_mappings command.\n  To remove all via mappings, use the -all option.\n  To remove the via mappings for specific source vias, use the -from option.\n  To remove the via mappings for specific replacement vias, use the -to option.\n Using a Subset of the Via Mapping Table for Redundant Via Insertion To use a subset of the via mapping table for redundant via insertion, specify which weight groups from the via mapping table to use by setting one or both of the route.common.redundant_via_include_weight_group_by_layer_name and route.common.redundant_via_exclude_weight_group_by_layer_name application options.\n  If you set only the route.common.redundant_via_include_weight_group_by_layer_name application option, Zroute selects redundant vias only from the specified weight groups.\n If you do not specify an entry for a layer, redundant via insertion is not performed on that layer.\n  If you set only the route.common.redundant_via_exclude_weight_group_by_layer_name application option, Zroute selects redundant vias from all weight groups in the original via mapping table, except the specified weight groups.\n If you do not specify an entry for a layer, all weight groups are used for that layer.\n  If you set both application options, Zroute selects redundant vias from the weight groups specified in the route.common.redundant_via_include_weight_group_by_layer_name application, excluding the weight groups specified in the route.common.redundant_via_exclude_weight_group_by_layer_name application option."}
{"instruction": "How do I Reading Mixed Gate-Level Netlists and RTL Files", "input": "", "output": "Use the read_verilog command to read Verilog netlist files using a specialized Verilog netlist reader.\n This command cannot be used to read Verilog RTL files.\n Only the analyze and elaborate commands should be used to read RTL files.\n Use the set_top_module command after reading the netlist to specify the top-level module and link the design to create the block.\n read_verilog top.netlist.v set_top_module top When the file has multiple independent modules, use the -top option to identify the top- level module.\n For example, read_verilog -top top2 netlists.v set_top_module top2       To specify a new block name other than top_module_name.design, use the -design option.\n For example, read_verilog -top my_top -design desA../my_data/my_des.v set_top_module my_top"}
{"instruction": "How do I Placing Group Repeaters Before Routing", "input": "", "output": "To remove the constraints that were assigned to a group of repeaters, use remove_repeater_group_constraints command.\n For example: fc_shell> remove_repeater_group_constraints -type type_list"}
{"instruction": "How do I Setting Constraints for a Group of Repeaters", "input": "", "output": "To automatically group a list of repeaters or repeaters of supernets, use the create_repeater_groups command.\n This command takes a list of repeaters, a list of supernets, or bundles of supernets and returns a list of repeater groups created by the set_repeater_group command.\n The command supports both single load and multiple loads supernets.\n create_repeater_groups -supernets supernet_list | \\ -supernet_bundles supernet_bundle_list | \\ -cells cell_list -lib_cells lib_cell_list [-lib_cell_input lib_pin ] \\ [-lib_cell_output lib_pin ] \\ [-group_pin_spacing spacing ] \\ [-ignore_pin_layers]"}
{"instruction": "How do I Defining a Hard Placement Blockage", "input": "", "output": "An area-based placement blockage is a rectangular region in which cells cannot be placed or in which the types or number of cells is limited.\n The Fusion Compiler tool supports the following types of area-based placement blockages:  Hard A hard blockage prevents the placement of standard cells and hard macros within the specified area during coarse placement, optimization, and legalization.\n        Hard macro A hard macro blockage prevents the placement of hard macros within the specified area during coarse placement, optimization, and legalization.\n  Soft A soft blockage prevents the placement of standard cells and hard macros within the specified area during coarse placement, but allows optimization and legalization to place cells within the specified area.\n  Partial A partial blockage limits the cell density in the specified area during coarse placement, but has no effect during optimization and legalization.\n get_placement_blockages comandcomandsget_placement_blockages To define placement blockages, use the create_placement_blockage command.\n At a minimum, you must specify the coordinates of the placement blockage.\n To create a rectangular placement blockage, use the -boundary option to specify the lower-left and upper-right coordinates of the rectangle.\n To create a rectilinear placement blockage, use the -boundary option to specify the coordinates of the polygon.\n You can also create multiple placement blockages by specifying the boundary polygon as a geo_mask or a collection of physical objects.\n If the resulting area resolves to multiple, noncontiguous polygons, the command creates multiple placement blockages, one corresponding to each polygon.\n By default, the create_placement_blockage command creates a hard placement blockage.\n To create another type of placement blockage, use the -type option to specify the blockage type.\n A single create_placement_blockage command can create just one type of placement blockage.\n You can optionally assign a name to a placement blockage by using the -name option.\n You can then reference that blockage by name to query or remove the placement blockage."}
{"instruction": "How do I Balance Mode Option", "input": "", "output": "To create single-loops to extend wires for length matching, use the Custom Router single- loop matching application options.\n Single-loop matching is typically used in DDR routing flows  Adding Single Loops to Extend Wires Application Option Description custom.route.single_loop_match        Application Option Description custom.route.single_loop_match_max_spacing   custom.route.single_loop_match_min_spacing   custom.route.single_loop_match_offset_layer"}
{"instruction": "How do I Removing Repeater Groups", "input": "", "output": "To unplace the previous placement of repeaters, use the unplace_group_repeaters command.\n unplace_group_repeaters \\ -cells cell_list | -repeater_groups group_id_list [-lib_cell_input pin_name ] \\ [-lib_cell_output pin_name"}
{"instruction": "How do I Creating ETMs and ETM Cell Libraries in the Fusion Compiler", "input": "", "output": "To create ETMs and ETM cell libraries in the Fusion Compiler tool, see the Creating ETMs and ETM Cell Libraries in the Fusion Compiler Tool topic.\n To create ETMs and ETM cell libraries in the PrimeTime and the Library Manager tools respectively, see the following topics:  Creating ETMs in the PrimeTime Tool  Creating ETM Cell Libraries in the Library Manager Tool"}
{"instruction": "How do I Setting Routing Rules for Clock Gates", "input": "", "output": "By default, clock gating optimization occurs automatically when you synthesize fhs design.\n You can, however, define settings to allow or prevent clock gating optimization on specific objects.\n The tool makes no distinction among clock gates.\n Every clock gate is subject to optimization, whether it is a preexisting or tool-inserted clock gate.\n The simplest way to restrict clock gating optimization is by using the set_dont_touch and set_size_only commands.\n These commands restrict all clock gating optimization by preventing removal, merging, and splitting of all clock gates.\n For a more refined approach, use the set_clock_gate_transformations command.\n With this command, you can set specific constraints on each integrated clock gate and control which operations can be performed on the clock gate.\n The set_clock_gate_transformations command provides the following options:  -split_fanout Setting this option to false prevents fanout splitting even if the fanout is larger than the maximum allowed.\n This setting also prevents any splitting performed by placement- aware clock gating.\n  -merge_equivalents Setting this option to true allows a clock gate to be merged with an equivalent clock gate.\n  -collapse_levels Setting this option to false prevents redundant clock gates from being collapsed  -ungate_fanout Setting this option to true specifies that the clock gate fanout remain ungated  -reset Resets all clock gate settings for the whole design If the size_only or dont_touch attributes are set on a clock-gating instance or clock- gating library cell, these attributes take precedence over any of the constraints specified by the set_clock_gate_transformations command.\n       Example Using Clock Gating Transformations For the example in, suppose you set the following restrictions: set_clock_gating_options -minimum_bitwidth 6 set_clock_gate_transformations [get_cells uicg_b*] true \\ -ungate_fanout false set_clock_gate_transformations [get_cells uicg_c*] false  Example of Clock Gating Restrictions During optimization, the tool removes the uicg_a clock gate because of a minimum bitwidth violation (minimum bitwidth is 6).\n The uicg_b1 and uicg_b2 clock gates are merged (assuming they have an equivalent enable signal).\n Even though the minimum bitwidth is not met, ungating is not allowed, so the registers remain gated.\n The uicg_c1 and uicg_c2 clock gates do not allow any transformations, therefore no optimizations are performed for these clock gates."}
{"instruction": "How do I Special Naming and Querying for DFT Wrapper Clock Gates", "input": "", "output": "By default, the report_clock_gating command automatically detects the presence of self-gates in the design and displays the corresponding information at the bottom of the report, as shown in the following example report.\n       Clock Gating Summary  | Number of Clock gating elements | 5 | | | | | Number of Tool-Inserted Clock gates | 5 (100.00%) | | | | | Number of Pre-Existing Clock gates | 0 (0.00%) | | | | | Number of Gated registers | 48 (100.00%) | | | | | Number of Tool-Inserted Gated registers | 48 (100.00%) | | | | | Number of Pre-Existing Gated registers | 0 (0.00%) | | | | | Number of Ungated registers | 0 (0.00%) | | | | | Total number of registers | 48 | | | | | Max number of Clock Gate Levels | 2 | | | | | Number of Multi Level Clock Gates | 4 |  Self Gating Summary  | Number of self-gating cells | 3 | | | | | Number of self gated registers | 20 (41.67%) | | | | | Number of registers not self-gated | 28 (58.33%) | | | | | Total number of registers | 48 |  Self-gating cells are also counted as clock gates in the clock gating summary table.\n In this example, there are two clock gates and three self-gates, for a total of five clock-gating elements.\n You can find all the self-gates in a design by using the get_clock_gates command.\n This command returns a collection of clock-gating or self-gating cells.\n For example, the following command returns the self-gating cells that gate registers clock by CLK: fc_shell> get_clock_gates -clock CLK -type self-gate"}
{"instruction": "How do I Basic Zroute Flow", "input": "", "output": "Zroute has five routing engines: global routing, track assignment, detail routing, ECO routing, and routing verification.\n You can invoke global routing, track assignment, and detail routing by using task-specific commands or by using an automatic routing command.\n You invoke ECO routing and route verification by using task-specific commands.\n Zroute includes the following main features:  Multithreading on multicore hardware for all routing steps, including global routing, track assignment, and detail routing  A realistic connectivity model where Zroute recognizes electrical connectivity if the rectangles touch; it does not require the center lines of wires to connect  A dynamic maze grid that permits Zroute to go off-grid to connect pins, while retaining the speed advantages of gridded routers  A polygon manager, which allows Zroute to recognize polygons and to understand that design rule checks (DRCs) are aimed at polygons  Concurrent optimization of design rules, antenna rules, wire optimization, and via optimization during detail routing  Concurrent redundant via insertion during detail routing  Support for soft rules built into global routing, track assignment, and detail routing  Timing- and crosstalk-driven global routing, track assignment, detail routing, and ECO routing  Intelligent design rule handling, including merging of redundant design rule violations and intelligent convergence  Net group routing with layer constraints and nondefault routing rules  Clock routing  Route verification        Optimization for DFM and design-for-yield (DFY) using a soft rule approach  Support for advanced design rules, such as multiple patterning"}
{"instruction": "How do I Analyzing Signal Electromigration", "input": "", "output": "The Fusion Compiler tool supports the following formats for specifying electromigration constraints:  Interconnect Technology File (ITF) An ITF file can contain the following types of electromigration constraints: delta temperature, duration and duty ratio, via size, via direction, metal length, and via-to-via spacing relaxation.\n The Fusion Compiler tool supports both unencrypted and encrypted ITF files.\n  Advanced Library Format (ALF) An ALF file can contain the following types of electromigration constraints: temperature, metal width, and contact area.\n The electromigration constaints are stored in the design library as a lookup table.\n The tool determines the limit values by looking up the values in the lookup table and then using linear interpolation or extrapolation.\n To load the signal electromigration constaints into the design library, use the read_signal_em_constraints command.\n By default, the command reads an unencrypted ITF file.\n To read an encrypted ITF file, use the -encrypted option.\n To read an ALF file, use the -format ALF option.\n The constraints loaded by the read_signal_em_constraints command overwrite any existing signal electromigration constraints in the design library.\n For example, to read an unencrypted ITF file, use the following command: fc_shell> read_signal_em_constraints em.itf For example, to read an encrypted ITF file, use the following command: fc_shell> read_signal_em_constraints -encrypted em.itf.enc To read an ALF file, use the following command: fc_shell> read_signal_em_constraints -format ALF em.alf"}
{"instruction": "How do I Generic ECO Flow for Timing or Functional Changes", "input": "", "output": "An engineering change order (ECO) is an incremental change made to a complete or nearly complete design.\n You can use ECOs to fix functional, timing, noise, and crosstalk violations without synthesizing, placing and routing the entire design.\n You can also use ECOs to implement late-arriving design changes while maintaining design performance.\n The following topics describe the various ECO flows supported in the Fusion Compiler tool, and tasks you perform in these flows:  Generic ECO Flow for Timing or Functional Changes  Freeze Silicon ECO Flow  Signoff ECO Flow  Incremental Signoff ECO Flow  ECO Fusion Flow  ECO Fusion Power Integrity Flow  Manually Instantiating Spare Cells  Automatically Adding Spare Cells  Adding Programmable Spare Cells  Making ECO Changes Using the eco_netlist Command  Making ECO Changes Using Netlist Editing Commands  Resizing Cells  Adding Buffers on Nets  Adding Buffers on Routed Nets  Optimizing the Fanout of a Net  Reporting Available Sites for Placing ECO Cells  Identifying and Reverting Nonoptimal ECO Changes  Placing ECO Cells        Placing and Mapping ECO Cells to Spare Cells  Updating Supply Nets for ECO Cells  Recording the Changes Made to a Layout  Performing Prerequisite Check for Group Repeater Insertion and Placement  Adding a Group of Repeaters  Querying Group Repeater  Swapping Variant Cell  Fixing Multivoltage Violations"}
{"instruction": "How do I Inserting Redundant Vias on Signal Nets", "input": "", "output": "Zroute can insert redundant vias on clock nets either during or after routing.\n This topic describes how to insert redundant vias during clock routing.\n For information about postroute redundant via insertion, see Postroute Redundant Via Insertion To insert redundant vias on clock nets during clock routing, 1.\n Specify the redundant vias in a nondefault routing rule by using the create_routing_rule command, as described in Specifying Nondefault Vias Be sure to consider both DFM and routing when you select the redundant vias; otherwise, if you select the redundant vias based only on DFM considerations, you could negatively impact the routability.\n In addition to using nondefault routing rules to define the redundant vias for clock nets, you can also use them to define stricter wire width and spacing rules and to define the tapering distance.\n For more information about nondefault routing rules, see Using Nondefault Routing Rules 2.\n Assign the nondefault routing rule to the clock nets by using the set_clock_routing_rules command.\n fc_shell> set_clock_routing_rules -rules clock_via_rule 3.\n Run clock tree synthesis.\n fc_shell> synthesize_clock_trees 4.\n Route the clock nets.\n fc_shell> route_group -all_clock_nets \\ -reuse_existing_global_route true Zroute reserves space for the redundant vias during global routing and inserts the redundant vias during detail routing."}
{"instruction": "How do I Clock Tree Synthesis", "input": "", "output": "DFT wrapper clock gates refer to those clock gates that drive only DFT wrapper cells.\n The clock gating engine prefixes dft_wrapper_ to the names of all DFT wrapper clock gates       clock gates irrespective of their origin.\n This helps to identify specific clock gates driving wrapper cells during automatic test pattern generation (ATPG).\n Table 24 Commands and their expected return values Command Expected Return Value get_clock_gates -gating_wrapper_cells_only  dft_wrapper_ICG  dft_wrapper_clock_gate_reg get_clock_gates -gating_wrapper_cells_only -origin tool_inserted   dft_wrapper_clock_gate_reg  get_clock_gates -gating_wrapper_cells_only -origin pre_existing   dft_wrapper_ICG  get_clock_gates {ICG clock_gate_reg} -gating_wrapper_cells_only   get_clock_gates {ICG dft_wrapper_clock_gate_reg} -gating_wrapper_cells_only    dft_wrapper_clock_gate_reg get_clock_gates {*clock_gate_reg} -gating_wrapper_cells_only    dft_wrapper_clock_gate_reg This feature works only when you enable clock gate support for DFT wrapper cells.\n To enable clock gate support for DFT wrapper cells, set the following variable to true compile.clockgate.enable_dft_wrapper_cell_support       The following figures show how a clock gate can be split so that different clock gates can gate internal cells and wrapper cells separately:  Splitting a Clock Gate to Internal Cells and Wrapper Cells  Gating Internal Cells and Wrapper Cells Separately       Note: The clock gates cloned to gate wrapper cells alone, inherit their names from parent clock gates and have dft_wrapper_ appended to their names.\n This is not applicable to the names of self-gates that drive wrapper cells only.\n Table 25 shows how different types of clock gates are renamed.\n Table 25 How different clock gate types are renamed Clock Gate Type Name New Name  clock_gate_reg dft_wrapper_clock_gate_reg  icg_0 dft_wrapper_icg_0 Use the -gating_wrapper_cells_only option with the get_clock_gates command to return all clock gates (except self-gates) that drive wrapper cells only.\n This option can be used in conjunction with the already existing options.\n Use the compile.clockgate.substring_list_for_dft_wrapper_dont_rename_icgs application option to restrict the renaming of pre-existing clock gates (driving wrapper cells only).\n This option accepts a list of strings as values.\n The clock gates matching any of the substrings provided in the application option are not renamed.\n Note: The compile.clockgate.substring_list_for_dft_wrapper_dont_rename_icgs application option does not have any effect on the -gating_wrapper_cells_only querying option.\n Consider the following example: set_app_options -name compile.clockgate.substring_list_for_dft_wrapper_dont_rename_icgs -value {clk_gate_ctl clk_gate_snps} This prevents the renaming of any pre-existing clock gates that have the clk_gate_ctl or clk_gate_snps pattern in their name but can still be queried using the get_clock_gates -gating_wrapper_cells_only command."}
{"instruction": "How do I Excluding Boundary Paths", "input": "", "output": "You can limit the latency adjustment values for concurrent clock and data optimization performed during the place_opt clock_opt, and route_opt commands as follows:  Limit the amount that the clock latencies are advanced by using the ccd.max_prepone application option.\n  Limit the amount that the clock latencies are delayed by using the ccd.max_postpone application option.\n There is no default for these application options.\n Specify these values in the library timing units.\n The following example sets a limit of 0.2 for advancing and 0.1 for delaying clock latencies: fc_shell> set_app_options -list {ccd.max_prepone 0.2} fc_shell> set_app_options -list {ccd.max_postpone 0.1}"}
{"instruction": "How do I Controlling Pin Tapering", "input": "", "output": "By default, Zroute connects a signal route to a pin by using wires or vias anywhere on the pin.\n To restrict the allowed types of pin connections on a per-layer basis, use the following       syntax to set the route.common.connect_within_pins_by_layer_name application option: set_app_options -name route.common.connect_within_pins_by_layer_name -value { { layer mode }...\n } Valid values for the mode argument are  off (the default) There are no restrictions on pin connections.\n  via_standard_cell_pins Only the connections to standard cell pins by using a via are restricted.\n When using a via connection, the via's metal enclosure must be contained within the pin shape.\n There are no restrictions on signal routes connected to macro cell and pad cell pins by using a via or to any pins by using wires.\n  via_wire_standard_cell_pins The connections to standard cell pins by using a via or a wire are restricted.\n When using a via connection, the via's metal enclosure must be contained within the pin shape.\n When using a wire, the wire must be contained within the pin shape.\n  via_all_pins The connections to any pins (standard cell, macro cell, or pad cell) by using a via are restricted.\n When using a via connection, the via's metal enclosure must be contained within the pin shape.\n There are no restrictions on signal routes connected to any pins by using wires.\n  via_wire_all_pins The connections to any pins by using a via or a wire are restricted.\n When using a via connection, the via's metal enclosure must be contained within the pin shape.\n When using a wire, the wire must be contained within the pin shape.\n For example, if you use the following command (or use the default settings), all of the connections shown in are valid and no DRC violations are reported: fc_shell> set_app_options \\ -name route.common.connect_within_pins_by_layer_name \\ -value {{M1 off}}        Unrestricted Pin Connections If you set the mode for M1 to via_all_pins, as shown in the following example, the via enclosures must be inside the pin shape.\n The connections shown on the left side of  are valid; however, the connections on the right side of the figure cause DRC violations.\n fc_shell> set_app_options \\ -name route.common.connect_within_pins_by_layer_name \\ -value {{M1 via_all_pins}}  Restricted Via-to-Pin Connections No DRC Violations DRC Violations If you set the mode for M1 to via_wire_standard_cell_pins, as shown in the following example, both the via enclosures and wires must be inside the pin shape.\n The connections       shown on the left side of are valid; however the connections on the right side of the figure cause DRC violations.\n fc_shell> set_app_options \\ -name route.common.connect_within_pins_by_layer_name \\ -value {{M1 via_wire_standard_cell_pins}}  Restricted Via-to-Pin and Wire-to-Pin Connections DRC Violations No DRC Violations"}
{"instruction": "How do I Scaling the Switching Activity", "input": "", "output": "To use an RTL SAIF file in the Fusion Compiler tool, use the following steps: 1.\n After reading and elaborating the design, use the saif_map -start command, which creates a name-mapping database during synthesis that the tool then uses for power analysis and optimizaiton.\n 2.\n (Optional) Manually change the name mapping as described in Controlling the Name Mapping 3.\n Read in the same RTL SAIF file by using the read_saif command.\n Reading an RTL SAIF file does not affect the name-mapping database.\n 4.\n Perform optimization and other design changes.\n 5.\n Generate a binary name-mapping file that maps RTL object names to gate-level objects by using the saif_map -write_map command, which is needed if you want to continue the flow using ASCII files in a later Fusion Compiler session.\n In the subsequent session, use the saif_map -read_map command to read the saved file and begin tracking further changes to object names.\n 6.\n (Optional) Generate an ASCII name-mapping file for use with the PrimePower tool by using the saif_map -write_map -type primepower -essential command.\n Controlling the Name Mapping In the SAIF-map flow, the name-mapping database tracks the changes to pin, port, cell, and net names that occur during optimization.\n It contains the original name for each object, so that the original SAIF can be applied even after optimization.\n You can manually control the name mapping in the database by using the saif_map command as shown in the following table.\n Table 16 Options for Manually Controlling the Name Mapping To do this Use this option of the saif_map command   -set_name       Table 16 Options for Manually Controlling the Name Mapping (Continued) To do this Use this option of the saif_map command   -add_name -set_name  -add_name  -inverted -set_name  -add_name   -change_name  -report  -remove_name   -reset  -get_saif_names -get_object_names"}
{"instruction": "How do I DRC Toolbar", "input": "", "output": "When you run interactive design rule checking, the IC Validator tool performs design rule checking on what is displayed in the layout window, plus an extension of 1 micron.\n You can modify the size of the extension by setting the signoff.check_drc_live.halo application option.\n Use the View Settings and Hierarchy Settings panels to control the layers, levels, and cell types displayed in the layout window.\n  Display the View Settings panel by clicking the View Settings icon ( ) in the panel area.\n  Specify the number of levels to display in the Level field.\n  Select the Layers tab and enable visibility for the desired layers.\n  Display the Hierarchy Settings panel by clicking the Hierarchy Settings icon ( ) in the View Settings panel or the menu bar.\n        Specify the number of view levels in the \"View level\" field.\n  Select Standard, \"Fill Cell,\" and Block in the \"Expanding cell types\" section.\n shows the View Settings and Hierarchy Settings panels used to control the layout window display for design rule checking.\n  Settings Used to Display Objects for Design Rule Checking"}
{"instruction": "How do I Identifying Redundancies, Outliers, Equivalences, and Gaps in", "input": "", "output": "When a family of cells with the same function is ordered in terms of increasing drive strength, specific cell property values should be monotonically increasing or decreasing.\n For example, when a family is ordered in terms of increasing drive strength, the area of cells should be monotonically increasing, power should be increasing, and delay (for the same output load and input slew) should be decreasing.\n       The following figure shows a monotonic distribution of cell area versus timing for a family of cells with the same function.\n  Monotonic Distribution in Library Cell Family       A cell is considered redundant if all its metrics, such as area, delay, power, and so on, are equal or worse than another cell, as shown in the following plot of cell area versus timing for a family of cells with the same function.\n  Redundant Cell in Library Cell Family       A cell is considered an outlier if its metrics has a very large deviation from the expected values, as shown in the following plot of cell area versus timing for a family of cells with the same function.\n  Outlier Cell in Library Cell Family       Cells are considered equivalent if all their properties are the same for all process, voltage, and temperature (PVT) values, as shown in the following plot of cell area versus timing for a family of cells with the same function.\n  Equivalent Cells in Library Cell Family       A gap is when there is a large difference between two successive drive strengths, as shown in the following plot of cell area versus timing for a family of cells with the same function.\n  Gap in Library Cell Family"}
{"instruction": "How do I Preventing Connections to Register QN Pins", "input": "", "output": "By default, the compile_fusion command can map a register to a sequential library cell with an output that is opposite to that of the register, if it improves QoR.\n This is known as sequential output inversion.\n Sequential output inversion is also useful when mapping to a target library with sequential cells that have only one type of asynchronous inputs (either set or reset).\n In this case, the only way to map a register that uses the missing asynchronous inputs is to use a library cell with the opposite type of asynchronous input and invert the output of that cell, as shown in the following figure.\n This figure shows an example of sequential output inversion during mapping.\n  ___ SET    ___ RST QN  ___ RST       Note: Information about inverted registers is written to the verification guidance (.svf) file.\n You must include the.svf file in the Formality tool when verifying blocks with sequential output inversion.\n To control sequential output inversion for  The entire block, use the compile.seqmap.enable_output_inversion application option.\n The default is true  Specific hierarchical or leaf cells, use the set_register_output_inversion command.\n An instance-specific sequential output inversion setting specified with the set_register_output_inversion command overrides a block-specific setting specified with the compile.seqmap.enable_output_inversion application option.\n The compile_fusion command allows the mapping of sequential elements in the design to sequential library cells whose output phase is inverted.\n In certain cases, the tool might infer a register that has one type of asynchronous control, but your library has the opposite type of pin.\n In such cases, the tool maps to the opposite type of register and inverts all the data inputs and outputs."}
{"instruction": "How do I Inserting Metal Shapes in the Preferred Direction", "input": "", "output": "To control routing around the block boundary, use the derive_perimeter_constraint_objects command to place rectangular metal shapes,       routing guides, and routing blockages along the boundary edges, as described in the following topics:  Inserting Metal Shapes in the Preferred Direction  Inserting Routing Guides Along the Nonpreferred-Direction Edges  Inserting Routing Blockages Along the Boundary Edges  Removing Perimeter Constraint Objects If you change the block boundary after creating the constraint objects, you must regenerate the objects.\n You cannot manually modify the constraint objects.\n To check the perimeter constraint objects, use the -check_only option with the derive_perimeter_constraint_objects command.\n By default, the checking results are stored in an error data file named block_name.err.\n To specify a different name for the error data file, use the -error_view option See Also  Removing Perimeter Constraint Objects"}
{"instruction": "How do I Specifying Parasitic Estimation Settings for the Preroute", "input": "", "output": "Optimizing the design for performance, power, and area (PPA) is one of the primary goals of the tool.\n The following topics describe how to specify settings for performance, power, and area optimization at the preroute stage:  Specifying Parasitic Estimation Settings for the Preroute Optimization  Specifying Automatic Via Ladder Insertion Settings for Preroute Optimization  Assigning Nondefault Routing Rules to Critical Nets        Enabling Area Recovery in Regions of High Utilization  Enabling Advanced Logic Restructuring"}
{"instruction": "How do I Entering fc_shell", "input": "", "output": "You can end the session and exit the Fusion Compiler tool at any time.\n To exit the tool, use the exit or quit command.\n Note: When you exit the tool from the command line, the tool exits without saving the open blocks."}
{"instruction": "How do I Displaying Command Help", "input": "", "output": "The following online information resources are available while you are using the Fusion Compiler tool:  Command help, which is information about an Fusion Compiler command  Man pages See Also  Viewing Man Pages"}
{"instruction": "How do I Performing Postroute Logic Optimization", "input": "", "output": "The Fusion Compiler tool can perform two types of postroute optimization:  Logic optimization This optimization improves the timing, area, and power QoR and fixes logical DRC violations and performs legalization and ECO routing.\n To perform these optimizations, use the route_opt command, as described in Performing Postroute Logic Optimization  Routability optimization This optimization increases the spacing between cells to fix routing DRC violations caused by pin access issues.\n To perform this optimization, use the optimize_routability command, as described in Fixing DRC Violations Caused by Pin Access Issues If you run both logic optimization and routability optimization, you should first perform the logic optimization and then the routability optimization."}
{"instruction": "How do I 11", "input": "", "output": "Table 63 shows some of the key commands used to perform relative placement.\n Table 63 Relative Placement Commands Command Described in section create_rp_group  remove_rp_groups  add_to_rp_group  remove_from_rp_group        Table 63 Relative Placement Commands (Continued) Command Described in section set_rp_group_options  remove_rp_group_options  modify_rp_groups  legalize_rp_groups  check_rp_constraints  report_rp_groups  get_rp_groups  write_rp_groups"}
{"instruction": "How do I Querying Placement Bounds", "input": "", "output": "move boundcreating A group bound is a floating region within which to place a set of cells.\n Group bounds can be hard, soft, or dimensionless.\n  To define a soft group bound, use the following syntax: create_bound -name name [-type soft] [-effort effort_level ] \\ [-dimensions { width height bound_objects The default effort is ultra ; you can also specify low medium, or high For example, to define a soft group bound for the INST_1 and INST_2 cell instances with a width of 100 and a height of 100, use the following command: fc_shell> create_bound -name b4 -dimensions {100 100} \\ {INST_1 INST_2}  To define a hard group bound, use the following syntax: create_bound -name name -type hard -dimensions { width height } \\ bound_objects For example, to define a hard group bound for the INST_1 and INST_2 cell instances with a width of 100 and a height of 100, use the following command: fc_shell> create_bound -name b5 -type hard -dimensions {100 100} \\ {INST_1 INST_2}  To define a dimensionless group bound, use the following syntax: create_bound -name name [-effort effort_level ] \\ bound_objects The default effort is medium ; you can also specify low high, or ultra For example, to define a dimensionless group bound for the INST_1 and INST_2 cell instances in which the tool uses a high level of effort to place the cells closer within the group bound, use the following command: fc_shell> create_bound -name b6 -effort high {INST_1 INST_2}"}
{"instruction": "How do I Reducing Congestion", "input": "", "output": "By default, the tool restructures nets to reduce congestion during  The create_placement command, except when you use the -timing_driven option  The initial_place stage of the compile_fusion command You can control the congestion-driven restructuring as follows:  Specify an effort level by setting the place.coarse.cong_restruct_effort application option to low medium (default), high, or ultra  Prevent the tool from increasing the path depth by more than three levels of logic by setting the place.coarse.cong_restruct_depth_aware application option to true"}
{"instruction": "How do I Identifying and Reverting Nonoptimal ECO Changes", "input": "", "output": "You can report sites available for placing ECO cells by using the report_cell_feasible_space command.\n By default, the tool reports the available sites in the entire block.\n You can restrict the report to specific voltage areas by using the -voltage_areas option or to a specific rectangle or rectilinear area by using the -boundary option."}
{"instruction": "How do I Creating and Specifying Rail Scenarios for Rail Analysis", "input": "", "output": "By default, RedHawk or RedHawk-SC Fusion analyzes only the current design scenario for a multicorner-multimode design.\n To specify a different design scenario for rail analysis, use the rail.scenario_name application option.\n The tool honors the specified design scenario for the IR-driven features in power integrity flow, such as IR-driven placement, IR- driven concurrent clock and data optimization, and IR-driven optimization.\u200b Here is a script example: open_lib open_block  set_app_option -name rail.technology -value 7 set_app_options -name rail.tech_file -value./Apache.tech set_app_options -name rail.lib_files -value {\\ A.lib \\ B.lib } set_app_options -name rail.switch_model_files -value {./switch_cells.txt } source./test_taps.tcl  current_scenario func_max set_app_options -name rail.scenario_name -value func_typ  analyze_rail -voltage_drop"}
{"instruction": "How do I Performing IR-Drop-Aware Placement", "input": "", "output": "To consider the dynamic power QoR during placement, perform the following steps: 1.\n Annotate switching activity on the design, as described in Annotating the Switching Activity 2.\n Ensure that at least one scenario is enabled for dynamic-power optimization by using the set_scenario_status -dynamic_power true command.\n 3.\n Enable power-driven placement for the create_placement refine_placement compile_fusion, or clock_opt command by using one of the following methods:  Enable dynamic-power-driven placement by setting the place.coarse.enhanced_low_power_effort application option to none low medium, or high.\n The default is low During dynamic-power-driven placement, the tool tries to improve both the timing and power of timing-critical nets and the power of the other nets.\n This improves the power QoR without affecting the timing QoR."}
{"instruction": "How do I Generating a Congestion Map", "input": "", "output": "To generate a congestion report, run the report_congestion command.\n fc_shell> report_congestion By default, the report_congestion command uses the congestion map stored with the block to report an overflow summary for the entire block.\n If a congestion map is not stored with the design, the command generates a congestion map by running global routing in congestion-map-only mode.\n Note: By default, only the hard congestion data is saved in the design library.\n To also save the soft congestion data, set the route.global.export_soft_congestion_maps application option to true before performing global routing.\n The command calculates the overflow as the sum of the overflow for each layer, ignoring any underflow.\n Example 26 shows the default report, which includes only the hard congestion data.\n Example 26 Default Global Route Congestion Report **************************************** Report : congestion Design :       Version: Date : ****************************************  Layer | overflow | # GRCs has Name | total | max | overflow (%) | max overflow  Both Dirs | 39 | 8 | 14 ( 0.26%) | 1 H routing | 6 | 2 | 4 ( 0.07%) | 2 V routing | 33 | 8 | 10 ( 0.18%) | 1 In the default congestion report,  \"H routing\" refers to results for horizontal routes only and \"V routing\" refers to results for vertical routes only.\n  The total overflow value is the total number of wires in the block that do not have a corresponding track available.\n The max overflow value is the highest number of overutilized wires in a single global routing cell.\n  The GRCs overflow value is the total number of overcongested global routing cells in the design.\n The GRCs max overflow value is the number of global routing cells that have the maximum overflow.\n Note: The overflow and global routing cell numbers reported by the report_congestion command might look slightly more optimistic than those reported by the route_global command because the tool rounds down the congestion information before saving it with the design.\n Use the following options to modify the default behavior:  -rerun_global_router Use this option to rerun the global routing even if the block already has a congestion map.\n  -boundary coordinates Use this option to restrict the reporting to a specific region of the block.\n  -layers layers Use this option to restrict the reporting to specific layers.\n  -mode global_route_cell_edge_based Use this option to report overflow information for each global routing cell.\n  -include_soft_congestion_maps       Use this option to output soft congestion reports, if they exist.\n A soft congestion report includes the demand from the soft nondefault spacing rules, as well as tool-generated soft rules.\n The command outputs a soft congestion report for each spacing weight level of the soft nondefault spacing rules used in the block.\n Each soft congestion report contains the sum of the hard and soft congestion with a weight level of at least the current soft level.\n See Also  Global Routing  Defining Minimum Wire Spacing Rules  Generating a Congestion Map"}
{"instruction": "How do I Pin-Trunk Offset", "input": "", "output": "The custom.route.bus_intra_shield_placement application option specifies how the bus trunk will be shielded.\n Valid values are: Valid Value Description double_interleave  half_interleave  interleave  outside"}
{"instruction": "How do I Assigning Nondefault Routing Rules to Nets", "input": "", "output": "following command, fc_shell> create_routing_rule cut_rule -cuts {VIA1 {Vrect 1}} The tool selects the following vias: {VIA12_LH 1x1 R}, {VIA12_LH 1x1 NR}, {VIA12_LV 1x1 R}, {VIA12_LV 1x1 NR}, {VIA12_LH 1x2 R}, {VIA12_LH 1x2 NR}, {VIA12_LH 2x1       R}, {VIA12_LH 2x1 NR}, {VIA12_LV 1x2 R}, {VIA12_LV 1x2 NR}, {VIA12_LV 2x1 R}, and {VIA12_LV 2x1 NR}.\n Specifying Nondefault Vias Using the -vias Option The syntax for specifying nondefault vias using the -vias option is create_routing_rule rule_name -vias { { via_type cut_number orientation via_type cut_number orientation...\n via_type cut_number orientation  You can specify multiple via types per layer; each via type must be a via definition defined in the technology file or a via definition created by the create_via_def command.\n For each via type, you must explicitly specify the allowed cut numbers and orientation.\n To specify the orientation, use NR to indicate that the via is not rotated or to indicate that the via is rotated.\n The order of via specification is not important; during routing, Zroute selects the lowest cost nondefault via.\n For example, to specify the vias selected by the -cuts option in the previous example, use the following command: fc_shell> create_routing_rule via_rule \\ -vias { {VIA12_LH 1x1 R} {VIA12_LH 1x1 NR} {VIA12_LV 1x1 R} {VIA12_LV 1x1 NR} {VIA12_LH 1x2 R} {VIA12_LH 1x2 NR} {VIA12_LH 2x1 R} {VIA12_LH 2x1 NR} {VIA12_LV 1x2 R} {VIA12_LV 1x2 NR} {VIA12_LV 2x1 R} {VIA12_LV 2x1 NR} } Specifying Mask Constraints If you are using the precolored design flow for a design that uses multiple-patterning technology, you can set mask constraints on timing-critical nets, such as clock nets, by defining a precoloring rule and applying it to the nets.\n For details about the mask constraints, see Mask Constraints To define a precoloring rule, use the create_routing_rule command using the following syntax: create_routing_rule rule_name -mask_constraints layer constraint layer constraint...\n layer constraint where constraint is one of same_mask mask1_soft, or mask2_soft For example, to define a precoloring routing rule that sets mask_one constraints on the M4 and M5 layers, use the following command: fc_shell> create_routing_rule clock_mask1 \\ -mask_constraints {M4 mask_one M5 mask_one}       Note: To ensure DRC convergence, you should set double-patterning mask constraints only on a very few timing-critical nets.\n Defining Shielding Rules To define shielding rules, use the create_routing_rule command using the following syntax: create_routing_rule rule_name -shield_widths { layer width layer width layer width -shield_spacings { layer spacing layer spacing layer spacing [-snap_to_track] Specify the routing layers by using the layer names from the technology file.\n You can define a single width and spacing value per layer.\n Zroute uses the default wire width for any layers not specified in the -shield_widths option and the default spacing for any layers not specified in the -shield_spacings option.\n By default, shielding wires are not snapped to the routing tracks.\n To snap shielding wires to the routing tracks, use the -snap_to_track option when you define the nondefault routing rule.\n For example, to specify a shielding rule that uses spacing of 0.1 microns and a width of 0.1 microns for M1 through M5 and spacing of 0.3 microns and a width of 0.3 microns for M6, use the following command: fc_shell> create_routing_rule shield_rule \\ -shield_widths {M1 0.1 M2 0.1 M3 0.1 M4 0.1 M5 0.1 M6 0.3} \\ -shield_spacings {M1 0.1 M2 0.1 M3 0.1 M4 0.1 M5 0.1 M6 0.3} Reporting Nondefault Routing Rule Definitions To report the nondefault routing rules defined by the create_routing_rule command, use the report_routing_rules -verbose command.\n By default, this command reports all of the nondefault routing rules for the current block.\n To limit the report to specific nondefault routing rules, specify the rule names as an argument to the command.\n fc_shell> report_routing_rules { rule_names To output a Tcl script that contains the create_routing_rule commands used to define the specified nondefault routing rules, use the -output option when you run the report_routing_rules command.\n Removing Nondefault Routing Rules To remove nondefault routing rules from the current block, use the remove_routing_rules command.\n When you remove a nondefault routing rule, the rule       is removed from all nets to which it is applied and the rule definition is removed from the design library.\n  To remove specific routing rules, specify the routing rules.\n  To remove all routing rules, specify the -all option.\n For example, to remove the new_width_rule routing rule, use the following command: fc_shell> remove_routing_rules new_width_rule Modifying Nondefault Routing Rules To change the definition for an existing rule, you must use the remove_routing_rules command to remove the rule and then use the create_routing_rule command to redefine the rule.\n The tool issues an error message if you try to redefine an existing routing rule."}
{"instruction": "How do I Controlling Hold Time Optimization Effort", "input": "", "output": "By default, the tool focuses on improving the timing and power QoR during concurrent clock and data optimization.\n You can change the effort level for timing optimization by setting the ccd.timing_effort application option to low or high.\n The default is medium This setting affects concurrent clock and data optimization performed during the final_opt stage of the clock_opt command and the route_opt command."}
{"instruction": "How do I Isolating Input and Output Ports", "input": "", "output": "During optimization and clock tree synthesis, the tool can create new ports on existing hierarchical blocks.\n To prevent the tool from doing so, use the set_freeze_ports command and set the freeze_clock_port attribute to true on the corresponding cell instance.\n You can prevent the tool from adding clock ports, data ports, or both by using the -clock -data, or -all option.\n For example, to prevent the tool from creating additional clock ports on the MBX22 cell instance, use the following command: fc_shell> set_freeze_ports -clock [get_cells MBX22] true To report the freeze-port settings, use the report_freeze_ports command."}
{"instruction": "How do I Using RTL Switching Activity With a Name-Mapping File", "input": "", "output": "When performing low-power placement and dynamic-power optimization, you obtain better power savings if you annotate switching activity on the design.\n You can annotate the switching activity in the following ways:  Read a switching activity file (SAIF) by using the read_saif command.\n The switching activity is scenario specific.\n So, when you use this command, ensure that the current scenario is enabled for dynamic power optimization.\n  Set the switching activity by using the set_switching_activity command.\n When you use this command, you can set the switching activity for a specific mode, corner, or scenario by using the -mode -corner, or -scenario options.\n When doing so, ensure that the scenarios you specify or the scenarios corresponding to the modes and corners you specify are enabled for dynamic power optimization.\n       If you do not specify the switching activity, the tool applies the default toggle rate to the primary inputs and black box outputs and then propagates it throughout the design.\n To report the switching activity of a block, use the report_activity command.\n To remove switching activity of specific nets, pins, ports, or the entire block, use the reset_switching_activity command."}
{"instruction": "How do I Running IC Validator Using Hybrid Multicore Processing", "input": "", "output": "To enable IC Validator multicore processing using a job scheduler, use the set_host_options command to specify the following information:  The submit protocol  To use LSF, use the -submit_protocol lsf option.\n The LSF bsub command must be in your Linux path.\n  To use the Univa Grid Engine (UGE) job scheduler, which was previously known as SGE (Sun Grid Engine) or GRD (Global Resource Directory), use the -submit_protocol grid option.\n The Grid qsub command must be in your Linux path.\n If you do not specify a host name or submit protocol, the IC Validator tool runs on the host on which the Fusion Compiler tool is currently running.\n  The number of hosts to use Use the -num_processes option to specify the number of hosts.\n  The number of cores to use on each host Use the -max_cores option to specify the number of cores per host.\n  (Optional) Extra arguments needed for the submit command Use the -submit_command option to specify the submit command with the extra arguments.\n For example, to enable 4 hosts to be acquired by LSF, each of which can use 8 cores (for a total of 32 cores), use the following command: fc_shell> set_host_options -submit_protocol lsf -target ICV \\ -num_processes 4 -max_cores 8 To set the memory requirement of the jobs in the previous example to 1000M, use the following command: fc_shell> set_host_options -submit_protocol lsf -target ICV \\ -num_processes 4 -max_cores 8 \\ -submit_command {bsub -R rusage[mem=1000]} To enable 4 hosts to be acquired by the Grid Engine, each of which can use 8 cores (for a total of 32 cores), use the following command: fc_shell> set_host_options -submit_protocol grid -target ICV \\ -num_processes 4 -max_cores 8       To set the process name of the jobs in the previous example to bnormal, use the following command: fc_shell> set_host_options -submit_protocol grid -target ICV \\ -num_processes 4 -max_cores 8 -submit_command \"qsub -P bnormal\""}
{"instruction": "How do I Applying Required Settings for Advanced Technology Nodes", "input": "", "output": "You can use mega-switch commands to apply setting for different stages of the tool flow that are applicable for specific situations with the use of a single command.\n  Applying Required Settings for Advanced Technology Nodes  Applying Required Settings for High Performance Cores  Applying Required Settings for Improving Specific QoR Metrics"}
{"instruction": "How do I Inserting Metal Fill With IC Validator In-Design", "input": "", "output": "To automatically remove PG augmentation shapes that are causing DRC violations after ECO, use the -mode remove_by_drc_auto option with the signoff_create_pg_augmentation command.\n This action removes DRC violating       PG augmentation metal shapes or vias from design, including any floating connections created due to removal of DRC violating shape.\n For example, fc_shell> signoff_create_pg_augmentation -node <nodename> -mode remove_by_drc_auto"}
{"instruction": "How do I Exploring the Detailed Comparison Data", "input": "", "output": "You can sort and filter the run data to reveal patterns in the results and determine the parameters you want to explore further.\n To sort and filter the data, see the following topics:  Sorting the Data  Filtering Metrics  Filtering Runs  Example Analysis See Also  Exploring the Detailed Comparison Data       Sorting the Data Click any column header to sort the data using that metric.\n The first click performs an ascending sort, while the second click performs a descending sort.\n For example, to show the worst TNS numbers at the top of the table and the best TNS numbers at the bottom, click the TNS column header: Click the TNS column header again to show the best TNS numbers at the top and the worst at the bottom:       Filtering Metrics To control which metrics are displayed in the table, click the Metrics button and select or deselect the metrics from the Metrics dialog box accordingly.\n For example, to show only the setup timing, netlist area, and cell counts, select the metrics as shown in  Metrics Dialog Box Filtering Runs To control which runs are displayed in the table, click the Runs button and select or deselect the exploration runs accordingly from the Visible Runs (Summary) column.\n For example, to display only the first four runs in a table, select the runs as shown in   Choose Baseline and Visible Runs Dialog Box       Example Analysis The following example demonstrates how you might sort and filter your data to narrow down the runs you would like to explore further.\n Suppose you open the comparison report shown in and set the util60_aspect1%1_layerM9 run as the base run.\n  Comparison Report You could look at your TNS numbers first and sort the data from best TNS to worst TNS, as shown in the following figure: Notice that the best TNS runs have the M9 top layer, and the worst have the M7 top layer.\n This suggests that restricting the metal layers significantly impacts timing.\n       You could then restrict your analysis to your M9 runs by turning off the visibility of your M7 runs, as shown in the following figure: Now that you have your best TNS runs, you could compare their congestion by sorting the GRCOverflow column to show the worst overflow at the top, as shown in the following figure: Notice that your higher-utilization runs have more congestion than your lower-utilization runs.\n You could restrict your analysis to your lower-utilization runs by turning off the       visibility of your higher-utilization runs as shown in, leaving you with a manageable subset of exploration runs that better meet your timing and congestion goals.\n  Displaying Lower-Utilization Runs"}
{"instruction": "How do I Avoiding Overlapping Repeaters With Existing Tap Cells", "input": "", "output": "Use the -lib_cell option to specify a library cell as a repeater.\n This option supports the following additional options:  -lib_cell_input specifies the input pin of the library cell to connect, if it has multiple inputs.\n  -lib_cell_output specifies the output pin of the library cell to connect, if it has multiple outputs."}
{"instruction": "How do I Performing Top-Level Closure With ETMs", "input": "", "output": "To link to ETMs at the top level, add the ETM libraries to the reference library list before you create the top-level design library, as shown in the following example: fc_shell> lappend nt_ref_lib \"AMS_BLK_ETM.ndm\" fc_shell> create_lib -technology tech.tf -ref_libs $nt_ref_lib TOP An ETM that contains multiple modes is called a moded ETM.\n For a cell instance that links to a moded ETM, you must specify the required mode by using the set_cell_mode command, as shown in the following example: fc_shell> current_mode m1 fc_shell> set_cell_mode m1 U1 For a cell instance that links to a moded ETM, if you do not specify a mode, the tool does not activate the timing arcs, timing checks, generated clocks, and case values.\n To report the cell modes for specific cell instances, use the report_cell_modes command.\n To switch a cell instance from its abstract view to its ETM or vice versa, use the set_reference command.\n The following example switches the UI cell instance from its abstract view to its ETM: fc_shell> set_reference -block AMS_BLK_ETM.ndm:AMS_BLK.timing U1 The following example switches the UI cell instance from its ETM to its abstract view: fc_shell> set_reference -block AMS_BLK.ndm:AMS_BLK.abstract U1 When you switch between ETMs and abstract views, reapply the top-level timing constraints."}
{"instruction": "How do I Adding Hard Macro Cells", "input": "", "output": "You can specify orientations for leaf cells when you add them to a relative placement group.\n If you do not specify a leaf cell orientation, the tool automatically assigns a legal orientation for the leaf cells.\n To specify the orientation for leaf cells, use one of the following two methods:  Use the -orientation option with a list of possible orientations when you add the cells to the group with the add_to_rp_group command.\n  Set the rp_orientation attribute on leaf cells by using the set_attribute command.\n physopt_rp_enable_orient_opt variablevariablesphysopt_rp_enable_orient_opt The tool chooses one legal orientation from the list of orientations that you provide."}
{"instruction": "How do I Enabling Level-Based Clock Nondefault Routing Rule", "input": "", "output": "To specify clock-tree layer constraints, use the set_clock_routing_rules command.\n To specify the minimum and maximum routing layers, use the -min_routing_layer and       -max_routing_layer options.\n Specify the routing layers by using the layer names from the technology file.\n By default, the set_clock_routing_rules command assigns the specified layer constraints to all clock trees.\n Table 17 shows the options used to restrict the layer constraints.\n Table 17 Restricting Clock-Tree Layer Constraints To assign a layer constraint to Use this option  -clocks clocks  -net_type root  -net_type sink   -net_type internal  -nets nets  shows the root, internal, and sink nets of a clock tree after clock tree synthesis.\n By default, the root-net layer constraints are applied to all the single-fanout clock nets starting from the clock root up to the point where the clock tree branches out to a fanout of more than one.\n Internal-net layer constraints are applied to the nets from this point until the sink nets.\n  Root, Internal, and Sink Clock Net Types 1.\n You can use this option with the -clocks option to further restrict the assignment.\n This option is not valid with the -nets option.\n       For example, to use layers M4 through M7 when routing the root nets of the CLK1 clock, use the following command: fc_shell> set_clock_routing_rules -clocks CLK1 -net_type root \\ -min_routing_layer M4 -max_routing_layer M7 To specify a transitive fanout limit to use when identifying root nets, use the set_clock_tree_options -root_ndr_fanout_limit command.\n For example, to specify that any clock net with a transitive fanout of more than 300 sinks be considered as a root net, use the following command: fc_shell> set_clock_tree_options -root_ndr_fanout_limit 300  shows the root, internal, and sink nets of the same clock tree when a transitive fanout limit of 300 is specified for identifying the clock root nets.\n  Using a Fanout Limit for Selecting Root Nets When calculating the transitive fanout of clock nets for the purpose of identifying root nets, the tool includes only the valid clock sinks; It does not include the ignore pins.\n If a net identified as a root net is less than 10 microns, the tool uses internal-net layer constraints for that net.\n Note: Specifying a smaller value with the set_clock_tree_options -root_ndr_fanout_limit command increases the number of clock nets that are assigned the root-net layer constraints, which can increase routing congestion.\n To remove the transitive fanout limit specified with the set_clock_tree_options -root_ndr_fanout_limit command, use the remove_clock_tree_options -root_ndr_fanout_limit command."}
{"instruction": "How do I Reporting Commands and Examples", "input": "", "output": "Use the reports generated by Fusion Compiler to analyze and debug your design.\n You can generate reports before and after you compile your design.\n Generate reports before compile to check that you have set attributes, constraints, and design rules properly.\n Generate reports after compile to analyze the results and debug your design.\n This section includes the following topics:  Reporting Commands and Examples  Measuring Quality of Results  Comparing QoR Data  Reporting Logic Levels in Batch Mode  Querying Specific Message IDs"}
{"instruction": "How do I Adding Voltage Area Aware Group Repeaters", "input": "", "output": "To define the constraints for a group of repeaters, use the set_repeater_group_constraints command.\n The set_repeater_group_constraints command has a lower priority than the corresponding command options in the add_group_repeaters and place_group_repeaters commands.\n set_repeater_group_constraints -type type_list [-horizontal_repeater_spacing spacing ] \\ [-vertical_repeater_spacing spacing ] \\ [-layer_cutting_distance layer_scale_list"}
{"instruction": "How do I Preserving Cells and Nets During Optimization", "input": "", "output": "The uniquify command resolves multiple references, except those with the dont_touch attribute, throughout the hierarchy in the current design.\n The uniquify process copies and renames any multiply referenced design so that each instance references a unique design.\n The process removes the original design from memory after it creates the new and unique designs.\n The original design and any collections that contain it or its objects are no longer accessible.\n After this process finishes, the tool optimizes each design copy based on the unique context of its cell instance.\n To control the names of the reference copies, use the design.uniquify_naming_style application option.\n The default is %s_%d, where %s denotes the name of the existing reference and %d denotes the smallest integer value that forms a unique design name.\n This example creates unique reference copies of the specified cells A1, A2, and A3.\n fc_shell> uniquify {A1 A2 A3}        This example creates unique reference copies for the filtered cells A and B.\n fc_shell> uniquify [get_cells {A B}]"}
{"instruction": "How do I Viewing Man Pages", "input": "", "output": "In general, the Fusion Compiler tool modifies default behavior by using application options rather than application variables; however it does support user-defined Tcl variables, as well as a minimal number of application variables, such as the search_path variable.\n       To list the variables and their values, use the printvar command.\n For example, to list all variables defined in the current session, use the following command: fc_shell> printvar * To print the value of the search_path variable, use the following command: fc_shell> printvar search_path See Also  Defining the Search Path"}
{"instruction": "How do I Setting Up Clock Gating", "input": "", "output": "To perform clock gating, the tool requires that the enable signal of the register bank is synchronous with its clock.\n This is the setup condition.\n For latch-based or integrated clock gating, the tool can insert clock gating irrespective of the enable signal's and the clock's clock domains.\n If the enable signal and the register bank reside in different clock domains, you must ensure that the two clock domains are synchronous and that the setup and hold times for the clock-gating cell meet the timing requirements.\n An exception exists for primary input ports that use a clock specified with the set_input_delay command.\n In this case, the input port is synchronous with the clock and the setup condition is true."}
{"instruction": "How do I Controlling Physical-Feedthrough Nets in Voltage Areas", "input": "", "output": "After you have created voltage areas, you can make the following modifications to a voltage area:  Change the power domains associated with the voltage area To change the power domains associated with a voltage area, use the -add_power_domains and -remove_power_domains options with the set_voltage_area command.\n  Change the voltage area name To change the name of the voltage area, use the -name option with the set_voltage_area command.\n  Change the voltage area region To add shapes to a voltage area, use the create_voltage_area_shape command.\n To remove shapes from a voltage area, use the remove_voltage_area_shapes command."}
{"instruction": "How do I 12", "input": "", "output": "By performing THO at the end of the implementation flow, you have the best optimization potential because you optimize the complete design.\n The route_opt and hyper_route_opt commands perform postroute optimization on the top and blocks together.\n ECO routing after optimization is performed on all subblocks that are open for editing.\n       THO optimization is completely integrated with the route_opt and hyper_route_opt commands.\n Follow this example to use the hyper_route_opt or route_opt command on a single host: fc_shell> init_hier_optimization -flow post_route fc_shell> route_opt or hyper_route_opt fc_shell> commit_hier_optimization When using the route_opt command, it is possible to perform the ECO routing operations, which have to occur after each route_opt command in parallel, reducing overall runtime.\n You must set up the host options as shown in the Setting up Transparent Hierarchy Optimization section.\n The hier_route_eco command performs the ECO routing in the blocks as shown in the following example.\n fc_shell> init_hier_optimization -flow post_route -detached_hier_route_eco fc_shell> route_opt fc_shell> hier_route_eco fc_shell> commit_hier_optimization Note: When the hyper_route_opt command is used in this way, the tool ignores the setting and does not distribute the ECO routing."}
{"instruction": "How do I Specifying Mapping Rules for Programmable Spare Cells", "input": "", "output": "For the freeze silicon ECO flow, place and map the ECO cells to spare cells by using the place_freeze_silicon command.\n By default, this command places and maps all ECO cells.\n To place and map specific ECO cells, use the -cells option.\n The following example places and maps the ECO cells named ECO1, ECO2, and ECO3: fc_shell> place_freeze_silicon -cells {ECO1 ECO2 ECO3} This command places each ECO cell and maps it to the nearest matching spare cell.\n Cells that are deleted as a result of the ECO changes are not removed from the design.\n They are converted to spare cells and remain in the design.\n When you use the place_freeze_silicon command, you can  Place the ECO cells by the target spare cells, but not map them to the spare cells, by using the -no_spare_cell_swapping option This feature allows you to place the ECO cells and analyze the QoR before you map them.\n        Map the spare cells that are already placed by the target spare cells by using the -map_spare_cells_only option.\n  Generate an ECO cell to spare cell mapping file by using the -write_map_file command.\n You can the mapping file, if necessary, and map the ECO cells by using the map_freeze_silicon command.\n  Specify a minimum distance between ECO cells that are mapped to programmable filler cells by using the -min_filler_distance option."}
{"instruction": "How do I Cleaning Up Routed Nets", "input": "", "output": "You can modify routed nets in the GUI by using the following tools:  Area Push tool To activate the Area Push tool, click the button on the Edit toolbar or choose Edit > Area Push.\n You can use the Area Push tool to move unfixed objects away from a rectangular area on a layer while maintaining their physical connections.\n You select the layer and control whether the tool complies with nondefault routing rules.\n The tool supports both interactive and batch push operations.\n  Spread Wire tool To activate the Spread Wires tool, click the button on the Edit toolbar or choose Edit > Spread Wires.\n You can use the Spread Wires tool to move selected, unfixed wires evenly between two points on a layer while maintaining their physical connections.\n You control whether the tool spreads the wires by layer and whether the tool complies with nondefault routing rules.\n  Stretch Connected tool To activate the Stretch Connected tool, click the button or choose Edit > Stretch Connected.\n You can use the Stretch Connected tool to move and stretch unfixed wire shapes while optionally maintaining their physical connections.\n  Quick Connect tool To activate the Quick Connect tool, click the button or choose Edit > Route Utilities > Quick Connect.\n You can use the Quick Connect tool to quickly connect wires to pin shapes, port shapes, terminals, or other wires.\n       If you need assistance while using these tools, click to open a Help page in the man page viewer."}
{"instruction": "How do I Running Rail Analysis Using RedHawk-SC Fusion", "input": "", "output": "The RedHawk and RedHawk-SC power integrity solution is integrated with the implementation flow through the RedHawk Fusion and RedHawk-SC Fusion interface.\n You can use the RedHawk Fusion and RedHawk-SC Fusion feature for rail analysis at different points in the physical implementation flow after power planning and initial placement are completed.\n This enables you to detect potential power network issues before you perform detail routing and thus significantly reduce the turnaround time of the design cycle.\n When placement is complete and the PG mesh is available, use RedHawk Fusion or RedHawk-SC Fusion to perform voltage drop analysis on the power and ground network to calculate power consumption and to check for voltage drop violations.\n You can perform voltage drop analysis at other stages in the design flow, such as after detail routing.\n When chip finishing is complete, use RedHawk Fusion or RedHawk-SC to perform PG electromigration analysis to check for current density violations.\n This chapter describes how to use the RedHawk Fusion or RedHawk-SC Fusion feature to perform rail analysis in the environment.\n For a detailed description about the RedHawk or RedHawk-SC analysis flows and commands, see the RedHawk User Manual or RedHawk-SC User Manual This chapter includes the following topics:  Running Rail Analysis Using RedHawk-SC Fusion  An Overview for RedHawk Fusion and RedHawk-SC Fusion  Setting Up the Executables  Specifying RedHawk and RedHawk-SC Working Directories  Preparing Design and Input Data for Rail Analysis  Specifying Ideal Voltage Sources as Taps  Missing Via and Unconnected Pin Checking  Running Rail Analysis with Multiple Rail Scenarios  Performing Voltage Drop Analysis  Performing PG Electromigration Analysis        Performing Minimum Path Resistance Analysis  Performing Effective Resistance Analysis  Performing Distributed RedHawk Fusion Rail Analysis  Voltage Driven Power Switch Cell Sizing  Working With Macro Models  Performing Signoff Analysis  Writing Analysis and Checking Reports  Displaying Maps in the GUI  Displaying ECO Shapes in the GUI  Voltage Hotspot Analysis  Querying Attributes"}
{"instruction": "How do I Using the Global Router During Initial Clock Tree Synthesis", "input": "", "output": "You can specify a name prefix for the cells added on the clock network during clock tree synthesis by using the cts.common.user_instance_name_prefix application option.\n The following example specifies CTS_ as the name prefix for the cells added on the clock network during clock tree synthesis: fc_shell> set_app_options \\ -name cts.common.user_instance_name_prefix -value \"CTS_\" To specify a name prefix for the cells added on the data nets during optimization, including during the clock_opt command, use the opt.common.user_instance_name_prefix application option, as described in Specifying a Cell Name Prefix for Optimization"}
{"instruction": "How do I Setting Your Baseline Run", "input": "", "output": "You can generate a web-based report to view and compare your QoR data with a QORsum report by performing the following steps: 1.\n (Optional) Configure the QoR data, which is captured and displayed in the subsequent steps, by using the set_qor_data_options command.\n  To specify the most critical power scenarios in your design, use the -leakage_scenario and -dynamic_scenario options.\n The tool uses the power scenarios you specify to generate the high-level summary of the power QoR in the QORsum report.\n If you do not specify these options, it uses the active power scenario with the highest total power for both the leakage and dynamic scenario for the power QoR summary.\n These settings are only used for the power QoR summary.\n The tool uses the power information of all active power scenarios to capture and report the detailed power information in the QORsum report.\n  To specify the most critical clock name and clock scenario, use the -clock_name and -clock_scenario options.\n The tool uses the clock name and scenario you specify to generate the high-level summary of the clock QoR in the QORsum report.\n If you do not specify these options, the tool identifies the most critical clock and uses it for the clock QoR summary.\n These settings are only used for the clock QoR summary.\n The tool uses all clocks to generate the detailed clock QoR information in the QORsum report.\n  To specify a name to identify the run in the QORsum report, use the -run_name option.\n By default, the tool names each run with a number, such as Run1, Run2, and so on.\n You can use this option to give a more meaningful name to each run.\n You can also specify the run name by using the -run_names option when you generate the QORsum report by using the compare_qor_data command in step.\n If you do so, the tool ignores the run name specify by the set_qor_data_options -run_name command.\n       The following example specifies the leakage-power scenario, dynamic-power scenario, clock scenario, and the clock to use for the corresponding summary in the QORsum report: fc_shell> set_qor_data_options \\ -leakage_scenario S1 -dynamic_scenario S2 \\ -clock_scenario S3 -clock_name sys_clk 2.\n Collect the QoR data for your report by using the write_qor_data command.\n The write_qor_data command captures QoR data related to timing, power, runtime, and other metrics.\n You can run the write_qor_data command multiple times, at each stage of the design flow at which you want to capture QoR data.\n Use the -label and specify a value to indicate at which stage of the flow you are capturing the data.\n By default, the write_qor_data command captures data relevant to a placed and optimized design.\n You can specify the -report_group option with any of the five supported value ( unmapped mapped placed cts routed ) to tune the set of reports being captured, based on the state of the flow where you are capturing the data.\n For finer-grain control, use the -report_list option to explicitly specify each report you want to generate.\n The write_qor_data command captures a standard set of QoR data to disk from most of the common tool reports like the report_qor and report_power.\n If there is additional custom data to capture that is not part of the command collection, use the capture_qor_data command to capture fully custom QoR data or GUI layout images to include in the web-based QORsum report.\n This can be used to supplement the standard QoR data captured by the write_qor_data command, or to create fully custom QORsum reports.\n Related commands include define_qor_data_panel that lets you create a new panel in the QORsum report for viewing QoR data, and set_qor_data_metric_properties, that lets you customize the properties of any metric column in any QORsum report panel, so you can control things like the style of coloring, or the thresholds for coloring when comparing data.\n 3.\n Generate the QORsum report by using the compare_qor_data command.\n This command takes the data captured by one or more runs of the write_qor_data command and creates a web-based report for viewing and comparing those results.\n You must specify the location of the output of each of the write_qor_data runs from the previous step by using -run_locations option.\n Each path location corresponds to the run result that you want to compare.\n You can assign a specific name to identify the run in the QORsum report by using the -run_names option.\n If you do so, the tool ignores the run name specify by using the set_qor_data_options -run_name command.\n By default, the tool names each run by a number, such as run1, run2, and so on.\n       You can specify the output directory by using the -output option.\n By default, the tool writes the report to a directory named compare_qor_data.\n To overwrite exiting output data, use the -force option.\n By default, the tool does not overwrite existing output data.\n 4.\n View the generated QORsum report by using the view_qor_data command.\n  QORsum Report For more information about exploring the comparison data, see the following topics:  Setting Your Baseline Run  Changing the QoR Display Style  Sorting and Filtering the Data  Exploring the Detailed Comparison Data"}
{"instruction": "How do I Preparing Design and Input Data for Rail Analysis", "input": "", "output": "During rail analysis, RedHawk/RedHawk-SC Fusion creates a working directory to store the generated files, including analysis logs, scripts, and various output data.\n By default,       the RedHawk or RedHakw-SC working directory is named RAIL_DATABASE.\n To use a different name for the working directory, use the rail.database application option.\n The following figure shows the data structures:  RAIL_CHECKING_DIR: The directory where RedHawk Fusion saves report files on the missing information during rail analysis.\n For example, the tool generates the following report files in the RAIL_CHECKING_DIR directory when static rail analysis is run:  libcell.apl_current  libcell.apl_cap  libcell.missing_liberty  RAIL_DATABASE: The directory where the tool saves and retrieves results and log files that are generated during power calculation and rail analysis.\n This directory contains the following sub-directories:  in-design.redhawk or in-design.redhawk_sc: The directory that contains the following sub-directory: design_name.result: The directory where RedHawk or RedHawk-SC saves the analysis result files.\n  design_name : The directory where RedHawk or RedHawk-SC writes analysis results for the Fusion Compiler tool to retrieve for map display and data query.\n For example, running the open_rail_result command loads the data in this design_name directory for displaying maps in the GUI.\n       If you finish the analysis and want to keep the data for later retrieval, you need to save the existing rail data to another working directory by using the rail.database application option before proceeding to another analyze_rail run.\n For example, fc_shell> set_app_options -name rail.database \\ -value RAIL_DATABASE_STATIC_RUN Otherwise, the RedHawk/RedHawk-SC tool overwrites the previously generated rail data with the new data, and issues the following warning message: Warning: Rail result still open!\n Will be overwritten!\n To avoid the warning message, run the close_rail_result command to remove all rail data from memory before proceeding to another analyze_rail command run."}
{"instruction": "How do I Defining Nondefault Routing Rules", "input": "", "output": "Zroute supports the use of nondefault routing rules, both for routing and for shielding.\n  For routing, you can use nondefault routing rules to define stricter wire width and spacing rules, to define the pin tapering distance, to specify the vias used when routing nets with nondefault routing rules, and to specify multiple-patterning mask constraints.\n  For shielding, you can use nondefault routing rules to define the minimum width and spacing rules.\n For information about working with nondefault routing rules, see the following topics:  Defining Nondefault Routing Rules  Reporting Nondefault Routing Rule Definitions  Removing Nondefault Routing Rules  Modifying Nondefault Routing Rules        Assigning Nondefault Routing Rules to Nets  Reporting Nondefault Routing Rule Assignments"}
{"instruction": "How do I Freeze Silicon ECO Flow", "input": "", "output": "Use this flow to incorporate timing or functional ECO changes when you have the flexibility to add new cells and move or delete existing cells.\n This flow is recommended if you have not taped out your design.\n The unconstrained ECO flow consists of the following steps: 1.\n Update the design with the ECO changes by using one of the following methods:  Using the eco_netlist command, as described in Making ECO Changes Using the eco_netlist Command  Using netlist editing Tcl commands, as described in Making ECO Changes Using Netlist Editing Commands 2.\n Update the placement by using the place_eco_cells command, as described in Placing ECO Cells 3.\n Add filler cells to the empty spaces in the site array, as described in Inserting Filler Cells To reduce runtime, use the -post_eco option when you  Insert metal filler cells with the create_stdcell_fillers command, and the tool marks the inserted filler cells as post-ECO cells.\n  Remove filler cells with DRC violations with the remove_stdcell_fillers_with_violation command, and the tool performs DRC checking only for the post-ECO cells.\n 4.\n Update the routing by using the route_eco command, as described in Performing ECO Routing, or by manually rerouting the affected nets."}
{"instruction": "How do I Using the create_cell_array Command", "input": "", "output": "ad_tap_cel_aray comandcomandsad_tap_cel_aray To add a tap cell array based on the site rows, use the create_tap_cells command.\n You must specify the name of the library cell to use for tap cell insertion ( -lib_cell option) and the maximum distance, in microns, between tap cells ( -distance option).\n       For example, fc_shell> create_tap_cells -lib_cell myreflib/mytapcell -distance 30 Note: If the place.legalize.enable_pin_color_alignment_check application option is true (its default is false ), the command ensures that internal cell pins and metal of the tap cells align with a routing track of the appropriate color when inserting the tap cells and shifts the tap cells to avoid violating color alignment.\n To avoid signoff DRC errors due to this shifting, reduce the tap distance slightly when this application option is true By default, the create_tap_cells command inserts tap cells in every row for the entire block.\n The tool starts inserting the tap cells at the left edge of the row and uses the specified tap distance to determine the location of the subsequent tap cells.\n In addition, if a tap cell does not exist within the minimum tap distance (half the specified tap distance) from each row edge adjacent to the block's boundary, a hard macro, or a hard placement blockage, the tool inserts an additional tap cell.\n shows the default tap cell placement for a block that uses the every-row insertion pattern.\n Note that extra tap cells are added to the right of the hard macro to ensure that a tap cell exists within the minimum tap distance from the edge of the hard macro.\n  Default Tap Cell Placement       You can modify the following aspects of the default behavior:  The library cell used for tap cell insertion on rows with a mirrored (MX) orientation Use the -mirrored_row_lib_cell option to specify the library cell to use for mirrored rows.\n  The pattern used to insert the tap cells Use the -pattern option to specify one of the following tap cell insertion patterns:  every_row (the default) This pattern inserts tap cells in every row.\n For this pattern, the tap distance specified with the -distance option should be approximately twice the maximum diffusion-to-tap value specified in the technology design rules.\n  every_other_row This pattern inserts tap cells only in the odd-numbered rows.\n For this pattern, the tap distance specified with the -distance option should be approximately twice the maximum diffusion-to-tap value specified in the technology design rules.\n  stagger This pattern inserts tap cells in every row with the tap cells in even rows offset by half the offset value ( -offset option) relative to the odd rows, which produces a checkerboard-like pattern.\n For this pattern, the tap distance specified with the -distance option should be approximately four times the maximum diffusion-to-tap value specified in the technology design rules.\n  The offset from the left edge of the row Use the -offset option to shift the pattern startpoint to the right by the specified distance in microns.\n  The handling of fixed cells By default, tap cells are inserted at the computed tap locations, regardless of whether a fixed cell occupies that location.\n Use the -skip_fixed_cells option to prevent the insertion of tap cells in locations occupied by fixed cells.\n By default, when you use this option, the command considers the fixed cell as a blockage and breaks the row at the fixed cell, which can result in a tap cell being inserted on each side of the fixed cell.\n To prevent tap cells from overlapping fixed cells without breaking the row, use the -preserve_distance_continuity option with the -skip_fixed_cells option.\n When you use both options, the command shifts the tap cell to avoid overlap with the fixed cell.\n The -preserve_distance_continuity option shifts only those tap cells whose computed tap location is occupied by an instance of one of the specified library cells.\n       For example, the following command shifts tap cells whose computed tap location is occupied by a fixed instance of the mylib/cell1 library cell: fc_shell> create_tap_cells -lib_cell mylib/tap -distance 30 \\ -skip_fixed_cells -preserve_distance_continuity mylib/cell1 When you use the -skip_fixed_cells option, you can also specify the minimum horizontal spacing between the boundary and a corner tap cell that is above or below a blocked area by using the -min_horizontal_periphery_spacing option.\n This option specifies the minimum number of unit tiles between the boundary and an affected corner tap cell.\n  shows how using this option affects the tap cell placement both with and without boundary cells.\n  Tap Cell Boundary Row Spacing -min_horizontal_periphery_spac -min_horizontal_periphery_spac  The addition of extra tap cells You can use one or both of the following methods to prevent the insertion of extra tap cells:  Use the -row_end_tap_bypass option to specify that the boundary cells contain taps, so additional tap cells are not inserted at the boundary.\n Note that the command does not verify that the block has boundary cells or that the boundary cells actually contain taps.\n  Use the -at_distance_only option to allow the insertion of tap cells only at the specified tap distance, half of the tap distance, or one fourth of the tap distance (for the stagger pattern only).\n Note that using this option can cause DRC violations.\n  The regions in which to insert tap cells Use the -voltage area option to restrict the tap cell insertion to the specified voltage areas.\n        The naming convention used to identify the inserted tap cell instances By default, the tool uses the following naming convention for inserted tap cells: tapfiller!\n library_cell_name number Use the -separator option to change the separator character from its default of \"!.\" To identify the tap cells inserted in a specific run, use the -prefix option to specify a prefix string.\n When you use this option, the tool uses the following naming convention: tapfiller!\n prefix library_cell_name number"}
{"instruction": "How do I ...", "input": "", "output": "By default, the Fusion Compiler tool can place a relative placement group anywhere within the core area.\n You can control the placement of a top-level relative placement group by anchoring it.\n To anchor a relative placement group, use the set_rp_group_options command with the -x_offset and -y_offset options.\n The offset values are float values, in microns, relative to the lower-left corner in the core area.\n If you specify both the x- and y-coordinates, the group is anchored at that location.\n If you specify only one coordinate, the Fusion Compiler tool determines the placement by maintaining the specified coordinate and sliding the group along the line passing through the unspecified coordinate.\n To specify a corner of relative placement group to anchor it by, use the -anchor_corner option.\n The tool places the relative placement group such that the corner specified by this option is placed on the anchor point specified by the -x_offset and -y_offset options.\n The settings for the -anchor_corner option are as follows, and are shown in  bottom_left The anchor point of the relative placement group is set to its bottom-left corner.\n The default is the bottom-left corner.\n  bottom_right The anchor point of the relative placement group is set to its bottom-right corner.\n        top_left The anchor point of the relative placement group is set to its top-left corner.\n  top_right The anchor point of the relative placement group is set to its top-right corner.\n  rp_location The anchor point of the relative placement group is the element in the relative placement group at the position specified by the -anchor_row and -anchor_column options.\n When you use the -anchor_corner rp_location setting, the position specified with the -anchor_row and -anchor_column options must contain a cell, keepout, or a relative placement hierarchy.\n  Bottom-Left, Bottom-Right, Top-Left and Top-Right Anchor Corners Bottom-left anchor corner Bottom-right anchor corner Top-left anchor corner Top-right anchor corner For example, to anchor a relative placement by its bottom let corner at location (100, 100), as shown, use the following command: fc_shell> set_rp_group_options misc1 -anchor_corner bottom_left \\ -x_offset 100 -y_offset 100        Anchored Relative Placement Group I33 I32 I34 misc1 I30 I31 I35 I43 I42 I44 I40 I41 I45 I53 I52 I54 I50 I51 I55 (100,100)"}
{"instruction": "How do I Performing THO During compile_fusion", "input": "", "output": "To perform THO, you need to have design views.\n You might be linking to abstract views at the top.\n Use the instructions in the Changing the Block From Abstract to Design View section to switch the blocks you want to optimize to design views.\n Perform the following steps to set up THO: 1.\n To perform THO after block-level compile_fusion, you must propagate the CCD useful skew balancing offsets and latency offsets for integrated clock gating cells to the top-level timing context using the following commands: fc_shell> promote_clock_data -auto_clock all -latency_offset fc_shell> promote_clock_data -auto_clock all -balance_points 2.\n To enable and perform THO on the blocks, ensure that the blocks you want to optimize with THO are enabled for editing: fc_shell> set_editability -blocks {list of blocks} -value true 3.\n To verify block editability, use the following command: fc_shell> report_editability -blocks [get_blocks -hierarchical] 4.\n During THO, operations such as congestion updates and route-driven estimation (RDE) calculations inside subblocks as well as ECO-routing can be run using distributed processing.\n Use the following commands to perform the setup.\n You must specify your own queue parameters such as the qsub parameters as shown in the following example: Note: If host options are not specified, the tool runs the operations for the subblocks serially in the main process.\n fc_shell> set_host_options -name for_tho -num_processes 4 -max_cores 16 -submit_command [list qsub -P queue -pe mt 16 -l mem_free=100G -cwd fc_shell> set_hierarchy_options -host_option for_tho -blocks {list of blocks} 5.\n To enable THO, use the init_hier_optimization command.\n Depending on the stage, you must use different options as shown in the following sections."}
{"instruction": "How do I Routing Must-Join Pins", "input": "", "output": "To discourage off-grid routing for vias, you can increase the cost of routing the via enclosure metal shapes off the wire track grid.\n To specify the extra cost multiplier for the metal layers on which the via enclosures are routed, set the route.common.extra_via_off_grid_cost_multiplier_by_layer_name application option.\n Use the following syntax to set this option: { { layer multiplier }...\n } Specify the layers by using the layer names from the technology file.\n The cost multiplier must be a value between 0.0 and 20.0.\n When you specify this option, the effective cost is the base cost times (1+multiplier).\n For example, assume that the technology file defines the VIA12 layer between the M1 and M2 metal layers and the VIA23 via layer between the M2 and M3 metal layers.\n To set the extra cost multiplier for the via enclosures on the M2 metal layer (and therefore the vias on the VIA12 and VIA23 via layers) to 0.5 (for an effective via cost of 1.5 times the base cost), use the following command: fc_shell> set_app_options \\ -name route.common.extra_via_off_grid_cost_multiplier_by_layer_name \\ -value {{M2 0.5}}"}
{"instruction": "How do I Performing Prerequisite Check for Group Repeater Insertion and", "input": "", "output": "You can record the changes you make to a layout and generate a Tcl file containing the changes by using the record_layout_editing command, as shown in the following example: fc_shell> record_layout_editing -start fc_shell> remove_shapes RECT_32_0 fc_shell> record_layout_editing -stop -output layout_changes1.tcl You can make the following layout changes:  Create or remove layout objects by using Tcl commands  Set attributes by using the set_attribute command  Move or resize objects by using the GUI With this feature, multiple users can make ECO changes to different parts of a layout in parallel.\n Each user can output a Tcl file that contains the layout changes they make by using the record_layout_editing command, and all the Tcl files can be applied to the original layout."}
{"instruction": "How do I Routing Signal Nets by Using Automatic Routing", "input": "", "output": "conversion rate = 98.96% (2902130 / 2932706 vias)  Layer VIA1 = 97.81% (1091973/ 1116367 vias) Layer VIA2 = 99.97% (1071650/ 1071978 vias)   The optimized via conversion rate based on total routed via count = 98.96% (2902130 / 2932706 vias)  Layer VIA1 = 97.81% (1091973/ 1116367 vias) Weight 1 = 97.81% (1091973 vias) Un-optimized = 2.19% (24394 vias)   Total number of nets = 255165 0 open nets, of which 0 are frozen Total number of excluded ports = 0 ports of 0 unplaced cells connected to 0 nets 0 ports without pins of 0 cells connected to 0 nets 0 ports of 0 cover cells connected to 0 non-pg nets Total number of DRCs = 7398 Total number of antenna violations = antenna checking not active Information: Routes in non-preferred voltage areas = 8170 (ZRT-559)  Topology ECO iteration 1 ended with 0 qualifying violations."}
{"instruction": "How do I Preserving Timing During Redundant Via Insertion", "input": "", "output": "You can achieve a redundant via rate near 100 percent by using hard-rule-based redundant via insertion.\n Hard-rule-based redundant via insertion can improve the redundant via rate by treating redundant vias as hard design rules during routing.\n You can use nearly 100 percent redundant via insertion only during initial routing; this method is not supported during ECO routing.\n When you use near 100 percent redundant via insertion during initial routing, you should use soft-rule-based redundant via insertion during ECO routing to preserve the redundant via rate achieved during initial routing.\n Note: This method can result in a very large runtime increase for congested blocks.\n You should use this method only when needed to improve the redundant via rate beyond that provided by concurrent soft-rule-based redundant via insertion and the soft-rule-based approach resulted in a redundant via rate of at least 90 percent.\n To perform concurrent hard-rule-based redundant via insertion, 1.\n Enable nearly 100 percent via insertion by setting the route.common.concurrent_redundant_via_mode application option to insert_at_high_cost.\n (By default, concurrent redundant via insertion is disabled.) fc_shell> set_app_options \\ -name route.common.concurrent_redundant_via_mode \\ -value insert_at_high_cost To control the effort used to reserve space for the redundant vias, set the route.common.concurrent_redundant_via_effort_level application option.\n Note: If you enable the route.common.concurrent_redundant_via_mode option before running the place_opt command, the redundant vias are considered during congestion estimation.\n       2.\n Route the block.\n During routing, Zroute inserts the redundant vias and fixes hard design rule violations.\n In general, redundant via insertion has the same priority as other hard design rules; however, if design rule checking does not converge during detail routing, Zroute automatically relaxes the redundant via constraints to improve DRC convergence."}
{"instruction": "How do I Signoff DRC Results Files", "input": "", "output": "By default, signoff design rule checking has the following default behavior:  Performs checking on all cell types (standard cells, filler cells, macro cells, and I/O pad cells) To exclude certain cell types from design rule checking, set the signoff.check_drc.\n excluded_cell_types application option.\n Specify one or more of the following values: lib_cell (standard cells), filler (filler cells), macro (macro cells), and pad (I/O pad cells).\n  Performs design rule checking on the entire block  To perform design rule checking only on those areas of the block that have been modified since the last run, use the -auto_eco option.\n Note: You can use this option only if you have previously run the signoff_check_drc command and the percentage of change to the block since that run is less than the change threshold.\n The default change threshold is 20 percent; to modify the change threshold, set the signoff.check_drc.auto_eco_threshold_value application option.\n       By default, the tool compares the current block to the version used for the previous signoff_check_drc run.\n To compare the current block to a different block, use the -pre_eco_design option to specify the comparison block.\n The tool gets the information required for incremental change detection from the error data file named block _sdrc.err, where block is either the current block name or the name specified by the -pre_eco_design option.\n By default, the DRC violations reported after performing incremental signoff design rule checking represent only the DRC violations detected in the changed areas of the block.\n To merge the initial DRC violations with the DRC violations detected during the incremental run, set the signoff.check_drc.merge_incremental_error_data application option to true before running the signoff_check_drc -auto_eco command.\n By default, the command reads the initial DRC violations from the signoff_check_drc.err file.\n To read the initial DRC violations from a different error data file, specify the file name with the signoff.check_drc.merge_base_error_name application option.\n  To perform design rule checking only in specific regions, use the -coordinates option, the -excluded_coordinates option, or both.\n You can specify one or more regions for both options.\n If you specify both options, the command does not check design rules in the overlapping regions.\n To specify the coordinates, use the following format: {{x } {x }...\n {x }} Note that there must be a space between each set of coordinates.\n  Performs design rule checking for all rules specified in the foundry runset  To check only specific rules, use the -select_rules option.\n Specify the rules by specifying a matching pattern for the rule names.\n The rule names are specified in the COMMENT section in the runset file.\n  To prevent checking of specific rules, use the -unselect_rules option.\n Specify the rules by specifying a matching pattern for the rule names.\n The rule names are specified in the COMMENT section in the runset file.\n Note: You can use this option with the -select_rules option to customize the set of rules checked by the signoff_check_drc command.\n For example, to restrict the signoff_check_drc command to route validation, select the metal layer rules and exclude the metal density rules.\n        Performs design rule checking for all routing layers  To perform design rule checking only on specific routing layers, use the -select_layers option.\n Specify the routing layers by using the layer names from the technology file.\n  To perform design rule checking on all runset layers, including nonrouting layers, use the -check_all_runset_layers true option.\n You would typically use this setting to perform a quick design rule check on the entire block after you complete design rule checking on the routing layers.\n When you use this option, the block information comes from the design view and the validation tool checks all layers, including the nonrouting layers.\n  Reports a maximum of 1000 errors for each rule, including both top-level and child- level violations  To override the maximum error count, set the signoff.check_drc.max_errors_per_rule application option.\n  To ignore child-level violations, set the signoff.check_drc.ignore_child_cell_errors application option to true The signoff_check_drc command uses the options that you specify to generate the command used to invoke signoff design rule checking in the IC Validator tool.\n You can specify additional options for the IC Validator command line by setting the signoff.check_drc.user_defined_options application option.\n The string that you specify in this option is added to the command line used to invoke the IC Validator tool.\n The Fusion Compiler tool does not perform any checking on the specified string.\n Generating Input for the Automatic Fixing Flow If you plan to use the signoff DRC results as input to the automatic fixing flow, set the signoff.check_drc.ignore_child_cell_errors application option to true before you run the signoff_check_drc command.\n Zroute can fix only top-level violations; using this option ensures that signoff design rule checking reports only fixable violations."}
{"instruction": "How do I Signoff Design Rule Checking", "input": "", "output": "By default, the IC Validator tool reads the design view for the top-level block and library cell instances, and the pin information from the frame view for the macro cell and I/O pad cell instances.\n  To read both the pin information and the routing blockages from the frame view for macro cells and I/O pad cells, set the signoff.check_drc.ignore_blockages_in_cells application option to false  To read the GDSII or OASIS data for specific reference cells, specify the stream files by setting the signoff.physical.merge_stream_files application option.\n When you use this option, the GDSII or OASIS data replaces the cell library view for the cells defined in the specified stream files, except for cells in the cell libraries specified in the signoff.physical.merge_exclude_libraries application option.\n  To read the layout view for specific reference cells, specify the cells by setting the signoff.check_drc.read_layout_views application option.\n The layout view is identical to the GDSII or OASIS data that was used to create it, but reduces the runtime for signoff design rule checking.\n Note: By default, layout views are not included in the cell libraries.\n To save the layout views, you must set the lib.workspace.save_layout_views application option to true during library preparation, as described in the Library Manager User Guide  To read the design view for specific reference cells, specify the cells by setting the signoff.check_drc.read_design_views application option.\n Note: By default, design views are not included in the cell libraries.\n To save the design views, you must set the lib.workspace.save_design_views application option to true during library preparation, as described in the Library Manager User Guide The order of precedence for child cell data is 1.\n GDSII or OASIS data specified with the signoff.physical.merge_stream_files application option 2.\n Layout views specified with the signoff.check_drc.read_layout_views application option If the IC Validator tool cannot find a layout view and the cell is specified in the signoff.check_drc.read_design_views application option, it reads the design view.\n       Otherwise, it reads the frame view.\n If it cannot find the frame view, the cell is ignored during design rule checking.\n 3.\n Design views specified with the signoff.check_drc.read_design_views application option If the IC Validator tool cannot find a design view, it reads the frame view instead.\n If it cannot find the frame view, the cell is ignored during design rule checking.\n 4.\n Frame views For example, to read the design view for all blocks, use the following commands: fc_shell> set_app_options \\ -name signoff.check_drc.read_design_views -value {*} fc_shell> signoff_check_drc To read the design view for the top-level block and all instances of reference cells whose name start with mc, and the frame view for all other child blocks, use the following commands: fc_shell> set_app_options \\ -name signoff.check_drc.read_design_views -value {mc*} fc_shell> signoff_check_drc"}
{"instruction": "How do I Splitting Clock Cells", "input": "", "output": "To skew latches and discrete clock gates together, set the ccd.skew_opt_merge_dcg_cells_by_driver application option to true before compile_fusion or place_opt final_opto.\n By default, the application option is set to false.\n Setting the ccd.skew_opt_merge_dcg_cells_by_driver application option to true enhances the concurrent clock and data optimization to skew latches and gating element by the same offset.\n This ensures the latch and gate are at the same level and improves hold timing of discrete clock gates.\n fc_shell> set_attribute -name ccd.skew_opt_merge_dcg_cells_by_driver -value true"}
{"instruction": "How do I Routing and Postroute Optimization", "input": "", "output": "The Fusion Compiler GUI provides the Clock Tree Visual Mode to help you visualize and analyze the clock trees in your design.\n In this mode, you can overlay the clock tree information to the layout view, or view the clock tree structure in the schematic view.\n For more information about analyzing clock tree information in the GUI, see the Using Map and Visual Modes topic in the Fusion Compiler Graphical User Interface User Guide"}
{"instruction": "How do I Loading the Signal Electromigration Constraints", "input": "", "output": "Signal electromigration problems result from an increase in current density caused by the use of smaller line widths and higher operational speeds in IC designs.\n Electromigration can lead to shorts or opens due to metal ion displacement caused by the flow of electrons.\n The more frequently a net switches, the more susceptible it is to electromigration.\n To analyze and fix signal electromigration violations in a detail routed block, 1.\n Apply the signal electromigration constraints by using the read_signal_em_constraints command.\n For more information, see Loading the Signal Electromigration Constraints 2.\n Apply switching activity by either reading in a SAIF file with the read_saif command or annotating the switching activity information on the nets with the set_switching_activity command.\n For more information, see Annotating the Switching Activity 3.\n Report the signal electromigration information by using the report_signal_em command.\n For more information, see Analyzing Signal Electromigration 4.\n Fix any signal electromigration violations by using the fix_signal_em command.\n For more information, see Fixing Signal Electromigration Violations The following example script shows the signal electromigration flow.\n # Open the block and apply the signal electromigration constraints open_block block1_routed read_signal_em_constraints em.itf  # Load switching information read_saif block1.saif  # Perform signal electromigration analysis report_signal_em -violated -verbose > block1.signal_em.rpt  # Fix signal electromigration violations fix_signal_em"}
{"instruction": "How do I An Overview for RedHawk Fusion and RedHawk-SC Fusion", "input": "", "output": "Similar to the RedHawk Fusion capability, RedHawk-SC Fusion supports gate-level rail analysis and checking capabilities in the Fusion Compiler environment.\n To enable the RedHawk-SC Fusion capability, you need to enable the rail.enable_redhawk_sc application option and disable the rail.enable_redhawk application option at the same time.\n For more information about RedHawk-SC Fusion, see Setting Up the Executables and Specifying RedHawk and RedHawk-SC Working Directories RedHawk Fusion and RedHawk-SC share the same set of analysis and checking commands for rail analysis, and the same GUI for examining the analysis results, except for some limitations.\n Table 65 compares the analyze_rail options that are supported in either or both of RedHawk Fusion and RedHawk-SC Fusion.\n Table 65 Comparing Analysis Features Between RedHawk Fusion and RedHawk-SC Fusion RedHawk Fusion RedHawk-SC Fusion -redhawk_script_file   -voltage_drop   -switching_activity         Table 65 Comparing Analysis Features Between RedHawk Fusion and RedHawk-SC Fusion (Continued) RedHawk Fusion RedHawk-SC Fusion -electromigration   -power_analysis   -result_name   -script_only   -bg   -min_path_resistance   -voltage_drop  -effective_resistance   -voltage_drop  -check_missing_via   -voltage_drop     -extra_gsr_option_file   -multiple_script_files   -submit_to_other_machines   Before performing rail analysis using the RedHawk-SC Fusion capability, you must specify the location of the libraries and the required input files as described in Preparing Design and Input Data for Rail Analysis.\n The following two application options are available only in RedHawk-SC Fusion: rail.toggle_rate Specify toggle rates for different cell types for rail analysis.\n For example: fc_shell> set_app_options -name rail.toggle_rate \\ -value {clock 2.0 data 0.2 combinational 0.15 \\ sequential 0.15}       rail.em_only_tech_file Specify the electromigration rule file for performing PG electromigration analysis.\n Use this option when the electromigration rule information is defined in a separate technology file other than the one specified by the rail.tech_file application option.\n For example, fc_shell> set_app_options -name rail.em_only_tech_file \\ -value EM_ONLY.rule"}
{"instruction": "How do I Using the Command Log File", "input": "", "output": "When you start the Fusion Compiler tool, it automatically executes the commands in the.synopsys_fc.setup file.\n The tool looks for this file both in your home directory and in the project directory (the current working directory in which you start the Fusion Compiler tool).\n The file is read in the following order: 1.\n The.synopsys_fc.setup file in your home directory 2.\n The.synopsys_fc.setup file in the project directory       The setup files can contain commands that perform basic tasks, such as initializing application options and setting GUI options.\n You can add commands and Tcl procedures to the setup files in your home and project directories.\n For example,  To set application options that define your Fusion Compiler working environment, create setup files in your home directory.\n  To set project- or block-specific application options that affect the processing of a block, create a setup file in the design directory.\n See Also  Working With the 3DIC Compiler User Interfaces  Using Application Options  Using Variables  Using Tcl Scripts"}
{"instruction": "How do I Incremental Metal Fill Insertion", "input": "", "output": "Timing-driven metal fill insertion inserts metal and via fill in the specified regions of the block, except around timing-critical nets.\n Timing-driven metal fill insertion is supported       for both pattern-based and track-based metal fill insertion.\n By default, the tool does not perform timing-driven metal fill insertion.\n To perform timing-driven metal fill insertion, use one or both of the following options with the signoff_create_metal_fill command:  -nets This option explicitly specifies the critical nets.\n  -timing_preserve_setup_slack_threshold This option enables the tool to automatically determine the critical nets based on a setup slack threshold.\n The unit for the slack threshold setting is the library time unit.\n Be careful when choosing the threshold value; using a large threshold value can result in too many critical nets, which could reduce the metal density and create large empty areas.\n During timing-driven metal fill insertion, the signoff_create_metal_fill command 1.\n Performs timing analysis, including multicorner-multimode analysis, to minimize timing impact 2.\n Identifies timing-critical nets based on the options you specify 3.\n Removes existing metal and via fill By default, the command removes the metal and via fill from the entire block.\n For information about performing incremental metal fill insertion, see Incremental Metal Fill Insertion 4.\n Inserts metal and via fill in the empty regions for the specified regions, except in the areas around the critical nets To enable fill insertion for shielded timing-critical clock nets, set the signoff.create_metal_fill.fill_shielded_clock application option to true 5.\n Invokes the IC Validator tool to perform metal fill insertion By default, the minimum spacing between the metal fill and the net shapes of the critical nets is two times the minimum spacing specified in the technology file.\n This spacing requirement also applies to the vertical extension of the critical net on the adjacent layers.\n To modify these requirements, set the appropriate application options, as described in Specifying the Spacing Requirements for Timing-Driven Metal Fill Insertion Note: You can restrict the regions in which to perform timing-driven metal fill insertion, as described in Specifying the Regions for Metal Fill Insertion       and Performing Metal Fill Insertion Only in Modified Regions ; however, you cannot specify the layers.\n When performing timing-driven metal fill insertion, the signoff_create_metal_fill command inserts metal fill on all routing layers (or all changed layers, if you perform metal fill insertion only in the changed regions).\n 6.\n (Optional) Fixes density errors In some cases, the spacing requirements for timing-driven metal fill insertion might cause density errors.\n To fix these errors automatically during timing-driven metal fill insertion, set the signoff.create_metal_fill.fix_density_errors application option to true (the default is false ).\n For information about the density design rules, see Defining the Density Design Rules 7.\n Stores the metal fill data as fill cell objects in the design view 8.\n Writes the result files to the run directory For information about the generated result files, see Signoff Metal Fill Result Files See Also  Pattern-Based Metal Fill Insertion  Track-Based Metal Fill Insertion Specifying the Spacing Requirements for Timing-Driven Metal Fill Insertion By default, the minimum spacing between the metal fill and the net shapes of the critical nets is based on the minimum spacing specified in the technology file.\n  For metal fill on the same layer as the net shape, the default minimum spacing is two times the minimum spacing value specified for the layer in the technology file.\n  For metal fill on layers adjacent to the net shape, the default minimum spacing is the minimum spacing value specified for the layer in the technology file.\n For example, assume that the red rectangle in is a critical net shape.\n The yellow regions indicate where fill cannot be inserted and the gray regions indicate the inserted fill.\n  Minimum Spacing From Metal Fill to Critical Net Shape       To override the default spacing requirements for timing-driven metal fill insertion, set the following application options:  signoff.create_metal_fill.space_to_nets This application option defines the same-layer minimum spacing requirements between metal fill and a net shape of a timing-critical net.\n  signoff.create_metal_fill.space_to_clock_nets This application option defines the same-layer minimum spacing requirements between metal fill and a net shape of a user-defined clock net that is specified in the -nets option.\n  signoff.create_metal_fill.space_to_nets_on_adjacent_layer This application option defines the adjacent-layer minimum spacing requirements between metal fill and a net shape of a timing-critical net.\n  signoff.create_metal_fill.space_to_clock_nets_on_adjacent_layer This application option defines the adjacent-layer minimum spacing requirements between metal fill and a net shape of a user-defined clock net that is specified in the -nets option.\n For each of these application options, use the following syntax to specify the spacing to use for each layer: { { layer value }...\n { layer value } } You can specify the spacing value either as a multiple of the minimum spacing ( x) or a distance in microns.\n For example, to set the minimum spacing between metal fill and a timing-critical net on the same layer to four times the minimum spacing for the M2 and M3 layers, use the following command: fc_shell> set_app_options \\ -name signoff.create_metal_fill.space_to_nets \\ -value {{M2 4x} {M3 4x}} To set the minimum spacing between metal fill and a timing-critical net on the same layer to 0.125 microns for the M2 layer and 0.133 microns for the M3 layer, use the following command: fc_shell> set_app_options \\ -name signoff.create_metal_fill.space_to_nets \\ -value {{M2 0.125} {M3 0.133}}       Defining the Density Design Rules The IC Validator tool can check and fix the following density design rules:  Minimum density, which specifies the minimum percentage of metal allowed in the density checking window This rule is checked when you enable density error fixing by setting the signoff.create_metal_fill.fix_density_errors application option to true By default, the IC Validator tool uses the setting defined for the minDensity attribute in the DensityRule sections of the technology file.\n If this attribute is not defined for a layer, the IC Validator tool uses a default of 10 percent.\n To override the default, define the _min_density parameter in the IC Validator parameter file.\n  Density gradient, which specifies the maximum percentage difference between the fill density of adjacent density checking windows This rule is checked if it is defined in the technology file and you enable density error fixing by setting the signoff.create_metal_fill.fix_density_errors application option to true The IC Validator tool uses the setting defined for the maxGradientDensity attribute in the DensityRule sections of the technology file.\n If this attribute is not defined for a layer, the IC Validator tool does not check this rule.\n  Maximum open area, which specifies the maximum size of a square area that contains no polygons and does not interact with any polygons This rule is checked if it is defined.\n The maximum open area rule is defined in an IC Validator parameter file.\n Use the following syntax to define the maximum open area rule for each metal layer: _max_open_area_rule = value where is the metal layer number and value is the side length of the square in microns.\n For more information about the IC Validator parameter file, see Using an IC Validator Parameter File For more information about the technology file, see the Synopsys Technology File and Routing Rules Reference Manual"}
{"instruction": "How do I Clock Gate Efficiency Reporting", "input": "", "output": "A clock gate often has an enable signal that is a Boolean expression comprising of one or more of the following reference (or invariant) points:  Hierarchical ports  Sequential output pins  Black box output pins To report the clock gate enable signal as a Boolean function, use the report_clock_gating_enable_condition command.\n You must specify a collection of clock gates to be reported."}
{"instruction": "How do I Fixing Multiple-Port Nets", "input": "", "output": "You can isolate input and output ports to improve the accuracy of timing models.\n To insert isolation logic at specified input or output ports, use the set_isolate_ports command.\n Isolation logic can be either a buffer or a pair of inverters; by default, the command places buffers to isolate  An input port from its fanout networks  An output port from its driver When you specify the -force option with the set_isolate_ports command, the tool ensures that the library cells specified by the -driver option are not sized.\n Note that the tool does not place isolation logic on the following ports:  Bidirectional ports  Ports defined as clock sources or power pins  Ports connected to nets that are marked with the dont_touch attribute       Example The following commands insert isolation logic to  Input port in3 with a buffer  Output port out1 with a buffer  Output port out3 with the LIBCELL_BUFF library cell  Output port out4 with a pair of inverters fc_shell> set_isolate_ports {in3 out1} fc_shell> set_isolate_ports -driver LIBCELL_BUFF out3 -force fc_shell> set_isolate_ports -type inverter out4 The following figures show the design without and with the isolation logic inserted by the set_isolate_ports commands, including buffers and an inverter pair:       Without port isolation       After running the port isolation commands in1 clk out3 rst in3 in2 out4 out2 out1 out1 out2 out3 out4 in1 clk rst in2 in3 LIBCELL_BUFF"}
{"instruction": "How do I Performing Placement With Inaccurate Constraints at Early", "input": "", "output": "The tool performs coarse placement at various points during the preroute stage of the design flow.\n The following topics describe settings for controlling coarse placement:  Performing Placement With Inaccurate Constraints at Early Stages  Generating Automatic Group Bounds for Clock Gating Cells  Controlling the Placement Density  Controlling Congestion-Driven Restructuring During Placement  Reducing Congestion  Considering Wide Cell Density During Placement  Considering the Effects of Cell Pins During Placement  Considering the Congestion Effects Due to the Nondefault Routing Rules of Clock Nets  Considering the Effects of Clock Gating Cells of Sequential Arrays During Placement  Considering Legalization Effects During Placement  Considering DFT Connections During Placement        Considering the Dynamic Power QoR During Placement  Performing IR-Drop-Aware Placement  Spreading Repeater Cells During Placement"}
{"instruction": "How do I Reducing Electromigration", "input": "", "output": "The Fusion Compiler tool supports improved activity driven clock gate relocation by controlling the amount of wire length degradation and considering the power impact before performing the clock gate relocation.\n To enable activity driven clock gate relocation, set the cts.compile.power_opt_mode application option to all or gate_relocation The clock gates are not relocated or moved if:  Relocating an integrated clock gate (ICG) has a minor power benefit or degrades power while degrading wire length.\n  Moving the clock gate has a significant wire length impact for a less power benefit."}
{"instruction": "How do I Using Routing Guides to Limit Edges in the Nonpreferred", "input": "", "output": "You can use a routing guide to control the routing direction within the routing guide boundary, either by requiring routes to be in the preferred direction within the routing guide boundary or by switching the preferred direction within the routing guide boundary.\n  To force the router to route all nets in the preferred direction within the routing guide boundary, use the -preferred_direction_only option with the create_routing_guide command.\n You can use this type of routing guide to prevent wrong-way jog wires on specific layers.\n By default, this routing guide applies to all layers within the routing guide boundary.\n To require preferred direction routing only for specific layers, use the -layers option to specify the affected layers.\n       For example, to force the router to use only the preferred direction on the M4 layer within the rectangle with its lower-left corner at (0, 0) and its upper-right corner at (100, 100), use the following command: fc_shell> create_routing_guide -boundary {{0.0 0.0} {100.0 100.0}} \\ -layers {M4} -preferred_direction_only  To switch the preferred routing direction within the routing guide boundary, use the -switch_preferred_direction option with the create_routing_guide command.\n You can use this type of routing guide to allow routing over macros, which might reduce congestion for a block that contains much detour routing.\n By default, this routing guide applies to all layers within the routing guide boundary.\n To switch the preferred routing direction only for specific layers, use the -layers option to specify the affected layers.\n For example, to switch the preferred routing direction for the M1 and M2 layers within the rectangle with its lower-left corner at (0, 0) and its upper-right corner at (100, 100), use the following command: fc_shell> create_routing_guide -boundary {{0.0 0.0} {100.0 100.0}} \\ -layers {M1 M2} -switch_preferred_direction Note: If a switch-preferred-direction routing guide overlaps with a preferred- direction-only routing guide, the switch-preferred-direction routing guide takes precedence."}
{"instruction": "How do I Datapath Implementation", "input": "", "output": "Datapath extraction transforms arithmetic operators, such as addition, subtraction, and multiplication, into datapath blocks to be implemented by a datapath generator.\n This transformation improves the quality of results (QoR) by using carry-save arithmetic.\n       Carry-save arithmetic does not fully propagate carries but stores results in an intermediate form.\n The carry-save adders are faster than the conventional carry-propagate adders because the carry-save adder delay is independent of bit-width.\n These adders use significantly less area than carry-propagate adders because they do not use full adders for the carry.\n The following example shows the code for the a * b + c + d = z expression.\n As shown in, the conventional implementation of the expression use three carry-propagate adders, whereas the carry-save arithmetic requires only one carry-propagate adder and two carry-save adders.\n The figure also shows the timing and area numbers for both implementations.\n module dp (a, b, c, d, e); input [15:0] a, b; input [31:0] c, d; output [31:0] Z; assign Z = (a * b) + c + d; endmodule  Conventional Carry-Propagate Adder Versus Carry-Save Adder       The tool supports extraction of the following components:  Arithmetic operators that can be merged into one carry-save arithmetic tree  Operators extracted as part of a datapath: *, +, -, >, <, <=, >=, ==,!=, and MUXes  Variable shift operators (<<, >>, <<<, >>> for Verilog and sll, srl, sla, sra, rol, ror for VHDL)  Operations with bit truncation The datapath flow can extract these components only if they are directly connected to each other, that is, no nonarithmetic logic between components.\n Keep the following points in mind:  Extraction of mixed signed and unsigned operators is allowed only for adder trees  Instantiated DesignWare components cannot be extracted"}
{"instruction": "How do I Setting Options for Interactive Design Rule Checking", "input": "", "output": "The DRC toolbar provides buttons that you can use to configure and run interactive design rule checking with Live DRC, and to view the reported DRC violations in the layout view.\n  shows the DRC toolbar.\n        DRC Toolbar Table 46 DRC Toolbar Buttons Button Description               signoff.physical.merge_stream_files"}
{"instruction": "How do I Adding a Group of Repeaters", "input": "", "output": "Use the -check_prerequisites option of the place_group_repeaters or add_group_repeaters command to perform a prerequisite check for a design.\n This prerequisite check detects the design errors at the beginning of the command flow.\n  For the place_group_repeaters command, use the -check_prerequisites option with the following options to provide necessary information for the check: place_group_repeaters -cells | -repeater_groups [-lib_cell_input] [-lib_cell_output] -check_prerequisites  For the add_group_repeaters command, use the -check_prerequisites option with the following options to provide necessary information for the check: add_group_repeaters -nets | -bundles [-lib_cell_input] [-lib_cell_output] -check_prerequisites        The prerequisite check validates the following in a design and search region:  Site rows are present in the design.\n  Each net has only one driver and can have multiple loads.\n  Locations of pins or ports are assigned.\n  For pins on standard cells, the standard cells are placed.\n  Supernets must be routed.\n  Route ends must not be more than 5 \u03bcm away from the driver, load pins, or ports in the search region.\n  The routes are defined for a driver or load in the search region.\n  Load is connected to a driver.\n  When the end of a route is open, no other route of the same net is present in the vicinity of the pin or port.\n  Routes must be assigned to the net of the supernet driver pin.\n  Vias must be created for routes.\n  Repeaters have a physical library cell.\n The following example shows the usage of -check_prerequisites option using the add_group_repeaters command: fc_shell> add_group_repeaters -bundles bundle_wo_load \\ -lib_cell ref_lib1/libCell1 -lib_cell_input A -lib_cell_output X \\ -repeater_distance 10 -check_prerequisite Error: The net net_wo_load does not have loads.\n (ECO-329) Error: The cell driver1 of the pin driver1/X is not placed.\n (ECO-331) Error: 0 Use error_info for more info.\n (CMD-013)"}
{"instruction": "How do I Routing Constraints", "input": "", "output": "After placement is completed, you can use the check_routability command to check whether your block is ready for detail routing.\n By default, this command checks for  Blocked standard cell ports A standard cell port is considered blocked if none of its physical pins is accessible.\n A standard cell pin is considered accessible if the pin contains a via that extends to a neighboring layer, there is a path on the pin layer that is at least as long as the search range distance, or there is a shorter path on the pin layer that ends at a via to a neighboring layer.\n By default, the search range distance is two times the layer pitch.\n To change this distance, use the -standard_cell_search_range option to specify a different pitch multiplier, up to a maximum of 10.\n If you specify a value larger than 10, the command sets the value to 10.\n By default, the tool does not check whether via connections are fully inside the standard cell pins.\n To enable this check, use the -connect_standard_cells_within_pins true option.\n To disable the checking of blocked standard cell ports, use the -check_standard_cell_blocked_ports false option.\n        Blocked top-level or macro cell ports A top-level or macro cell port is considered blocked if none of its physical pins is accessible.\n A top-level or macro pin is considered accessible if a legal path can be extended from the pin to a certain distance around it.\n This path can be just on the pin layer or can extend to a neighboring layer by a single via.\n By default, the distance is 10 times the layer pitch.\n To change this distance for same-layer paths, use the -blocked_range option to specify a different pitch multiplier, up to a maximum of 40.\n To change this distance for neighboring-layer paths through a via, use the -blocked_range_via_side option to specify a different pitch multiplier, up to a maximum of 40.\n If you specify a value larger than 40 for either of these options, the command sets its value to 40.\n By default, the command checks whether a pin is accessible in either the horizontal or vertical direction.\n To require that the pin is accessible in the preferred direction for its layer, use the -obey_direction_preference true option.\n To disable the checking of blocked top-level or macro cell ports, use the -check_non_standard_cell_blocked_ports false option.\n  Out-of-boundary pins This check verifies that all pins are within the block boundary.\n To disable the checking of out-of-boundary pins, use the -check_out_of_boundary false option.\n  Minimum grid violations This check verifies that all pins, including those within library cells, are on the minimum grid, as defined by the gridResolution attribute in the technology file.\n To disable the checking of minimum grid violations, use the -check_min_grid false option.\n  Incorrect via definitions This check verifies that  Uncolored via arrays do not have more than 20 rows or columns  Custom or asymmetric simple via definitions do not have more than 1000 cuts  The design does not contain more than 65535 via definitions You cannot disable these checks.\n        Invalid real metal via cut blockages This check verifies that all real metal via cut blockages match a legal cut size, as defined by the cutWidthTbl cutHeightTbl, and minWidth attributes in the technology file.\n To disable this check, use the -check_via_cut_blockage false option.\n  Minimum width settings This check verifies that the nondefault minimum width and shield width settings are no larger than the maximum width defined in the technology file.\n You cannot disable these checks.\n The check_routability command also supports the following optional checks:  Blocked power or ground ports To enable this check, use the -check_pg_blocked_ports true option.\n  Redundant power or ground shapes To enable this check, use the -check_redundant_pg_shapes true option.\n  Staggered power and ground vias that block the routing tracks To enable this check, use the -check_routing_track_space true option.\n  Blocked ports on frozen nets To enable this check, use the -check_frozen_net_blocked_ports true option.\n  Blocked unconnected pins To enable this check, use the -check_no_net_pins true option.\n  Real metal blockages that overlap library cell pins To enable this check, use the -check_real_metal_blockage_overlap_pin true option.\n  Invalid real metal via cut blockages in the library cells To enable this check, use the -check_lib_via_cut_blockage true option.\n        Shielding checks These checks detect possible issues with shielding by checking for the following conditions:  Signal net shapes with a shape_use attribute of shield_route  PG net shapes, which might be a PG strap or rail, but have a shape_use attribute of detail_route  Signal, clock, or PG nets that have a shielding nondefault rule but no associated shield shapes, which might be caused by inappropriate shape_use attributes.\n To enable these checks, use the -check_shield true option.\n  Via ladder checks These checks detect possible issues with via ladder insertion by checking for the following conditions:  Library cell pins whose top-layer terminals have multiple pin shapes but do not have pattern_must_join attribute This check applies only to signal and secondary PG pins.\n  Missing performance or electromigration via ladder on a pin that has the is_em_via_ladder_required attribute  Via ladder application options that do not have the required settings To enable these checks, use the -via_ladder true option.\n The check_routability command supports the following additional pin connection controls that apply to all pin access checks:  Pin access edges During frame view extraction, the tool annotates the frame views with information about the pin access edges.\n By default, the check_routability command ignores the pin access edges and allows pin connections at any point.\n You can restrict pin connections to the defined access edges by setting the -obey_access_edges option to true.\n You can further restrict pin connections to the access edge mark, which can be a narrow rectangle, a short line, or even a single point, by setting the -access_edge_whole_side option to true.\n In addition, the command can report unconnected pins that do not have an access edge defined.\n To enable this check, use the -report_no_access_edge true option.\n  Via rotation By default, pin connections can use rotated vias.\n To disallow pin connections that use rotated vias, set the -allow_via_rotation option to false       By default, this command considers the following routing layer constraints when checking for blocked ports:  The global minimum and maximum routing layer constraints set by the -min_routing_layer and -max_routing_layer options of the set_ignored_layers command These constraints can cause blocked ports only if they are defined as hard constraints.\n By default, these constraints are soft constraints.\n They are hard constraints only if the route.common.global_min_layer_mode and route.common.global_max_layer_mode application options are set to hard  The net-specific minimum and maximum routing layer constraints set by the -min_routing_layer and -max_routing_layer options of the set_routing_rule command These constraints can cause blocked ports only if they are defined as hard constraints.\n By default, the net-specific minimum layer constraint is a soft constraint and the net- specific maximum layer constraint is a hard constraint.\n They are hard constraints only if the route.common.net_min_layer_mode and route.common.net_max_layer_mode application options are set to hard  The clock minimum and maximum routing layer constraints set by the -min_routing_layer and -max_routing_layer options of the set_clock_routing_rules command These constraints can cause blocked ports only if they are defined as hard constraints.\n By default, the clock minimum layer constraint is a soft constraint and the clock maximum layer constraint is a hard constraint.\n They are hard constraints only if the route.common.net_min_layer_mode and route.common.net_max_layer_mode application options are set to hard  The freeze layer constraints set by the route.common.freeze_layer_by_layer_name and route.common.freeze_via_to_frozen_layer_by_layer_name application options To ignore the routing layer constraints during the blocked port checks, use the -honor_layer_constraints false option.\n You can use the error browser to examine the errors detected by the check_routability command.\n By default, the error data generated by the check_routability command is named check_routability.err; to specify a different name for the error data, use the -error_data option.\n The error data is saved in the design library when you save the block.\n For information about using the error browser, see the Fusion Compiler Graphical User Interface User Guide"}
{"instruction": "How do I Performing Top-Level Closure With Abstract Views", "input": "", "output": "You can generate a report of check records from a design by using the get_early_data_check_records command.\n Using the -hierarchical option with this command enables to you get check records for all subblocks.\n The report generated is in the following format: check_name object For example, fc_shell> get_early_data_check_records -hierarchical {hier.block.missing_leaf_cell_location@eco_cell} You can use the -filter option to filter the results of this command by various parameters such as object class and check name.\n       For example, fc_shell> get_early_data_check_records \\ -filter \"checked_object_class==port\" {place.port_type_mismatch@clk route.missing_layer_direction@feed_in route.layer_name_mismatch@out}"}
{"instruction": "How do I Performing Distributed RedHawk Fusion Rail Analysis", "input": "", "output": "Note: Effective resistance analysis is supported in both RedHawk Fusion and RedHawk-SC Fusion analysis flows.\n To calculate effective resistance for the specified power and ground nets, use the analyze_rail -effective_resistance command.\n To specify a list of cell instances for effective resistance calculation, describe the target instances in a text file.\n Then specify this text file with the rail.effective_resistance_instance_file application option.\n For example, fc_shell> sh cat cell_list.txt u_GS_PRO_65 u_GS_PRO_64 ChipTop_U_compressor_mode/U3  fc_shell> set_app_options \\ -name rail.effective_resistance_instance_file \\ -value cell_list.txt fc_shell> analyze_rail -nets {VDD VSS} -effective_resistance You can perform effective resistance analysis before or during voltage drop analysis.\n When the analysis is completed, the tool writes the effective resistance values into the same rail result with the voltage drop results.\n Note: In the RedHawk-SC rail analysis flow, you must specify both the -effective_resistance and -voltage_drop options for effective resistance calculation.\n When the calculation is complete, the tool saves the results in the working directory.\n To write the effective resistance values to a text file, use the report_rail_result -type effective_resistance command.\n To query the resistance value, use the get_attribute command.\n For example, fc_shell> get_attribute [get_pins u_GS_PRO_65/VDD] \\ effective_resistance       See Also  Specifying RedHawk and RedHawk-SC Working Directories  Writing Analysis and Checking Reports"}
{"instruction": "How do I Running and Placing Group Repeaters", "input": "", "output": "In the same variant cell group, the library cell variants differ from the master-variant only in mask color (or small geometry), while the timing data is same for these variants.\n The variant cell feature supports the following  track_pattern mode  siteId_based mode track_pattern mode To specify the first metal layer for track alignment checking in track_pattern mode, use the set_app_options -name eco.placement.first_track_alignment_layer -value name command.\n  shows the cell group with two variants.\n        Cell Group With Two Variants Based on the first track offset of library cells and the pitch value, you can define and derive the variant library cell types, which start from 0.\n  shows variant type id definition.\n  Variant Type id Definition       You can detect the variant type id for target cell current location.\n You can also check if the current reference is legal or illegal.\n If it is not legal, select the legal reference from cell group and swap it.\n  shows the check and swap with legal reference for target cell.\n  Check and Swap With Legal Reference for Target Cell.\n siteId_based mode The variant mapping and flipped mapping information are defined in the cell group.\n Following is an example of the report_cell_groups command.\n  shows the cell group definition for mapping id and flipped mapping id.\n        Cell Group Definition for Mapping id and Flipped Mapping id There are two sub flows for siteId_based mode: 2-Variants and N-Variant.\n  shows the 2-Variants and N-Variants sub flows in siteId_based mode        2-Variants and N-Variants Sub Flows in siteId_based Mode Each variant reference has both legal mapping id and flipped mapping id.\n  shows the variant mapping and flipped mapping id.\n        Variant Mapping and Flipped Mapping id For variant siteId, you can get the cycle and offset from site rows first, then use the cell placed location to do the calculation.\n  shows the calculation for variant siteId        Calculation for Variant siteId"}
{"instruction": "How do I Reporting Logic Levels in Batch Mode", "input": "", "output": "When you launch the QORsum report, the application opens the QOR Summary table, which summarizes high-level timing, power, and congestion metrics for each of your runs.\n This and other summary views help you sort and filter the data to pinpoint the runs you want to explore further.\n Detailed table panels provide deeper insight into your data than can be found on the summary tables.\n For example, a summary table might give a high-level overview of design timing (WNS, TNS, NVE) at each major stage in the flow.\n A detailed table might show more in-depth path-group based timing information, showing the timing numbers for each separate scenario and path group in the design.\n This is only one example.\n There are many detailed table panels in the QORsum report, that shows information like statistics about each library cell instantiated in the design (number of instances, type of cell, total, area), or information about how much wirelength is on each routed metal layer.\n A detailed table lets you view this information for up to six different datasets at once.\n A dataset is a set of data captured for a specific run and stage/label in the flow.\n For example, you might view the route_opt command results for three different runs at once.\n Or you might view one run, but looking at the progression of results across different stages in the design flow (place_opt, clock_opt, route_opt), as shown in the following figure.\n        Path Group Details The flows are shown side by side as columns under each metric.\n For example, the WNS column group (and the column group of each metric) shows the same six columns labeled 1, 2, 5, 6, 9, 10.\n These columns represent the results of each flow, and are given a flow ID number, rather than showing the full flow name, to prevent the columns from being too wide.\n The first column under each metric is the baseline, and the other five are the test flows being compared to that baseline.\n A legend is shown above the table with the flow ID number in a gold box (for the baseline) or a blue box (for the test flows), followed by the flow name.\n You can expand or collapse the legend by clicking the Legend button.\n You can also see the flow ID numbers in the \"Choose Baseline and Visible Runs\" dialog box, which is opened by clicking the Runs button.\n The detailed table views can display up to six runs at one time (your baseline run, plus the first five other runs selected in the Choose Baseline and Visible Runs dialog box).\n To change the runs that are displayed, 1.\n Click the Runs button to open the Choose Baseline and Visible Runs dialog box.\n 2.\n From the Visible Runs (Summary) column, select or deselect the runs accordingly.\n This selects and deselects those runs from the Visible flows (Detailed) column.\n Because a detailed table may show hundreds or even thousands of lines of data, it is not possible to view more than six datasets at a time.\n But if you want to see more details about these metrics across all runs and all flow stages/labels loaded into the QORsum report, there is an alternate view that shows this.\n This is called the Focused View.\n It can be accessed either by clicking the Focused View toggle button at the top of the report (see  focused view, you can see the results for all runs and all flow stages for a specific row of data from a detailed table.\n For example, in a path-group based timing detailed table, you might want to focus on the information only for path_group1 in scenarioA.\n To see this, click on the row in the detailed table for that path group and scenario.\n The Focused View for it opens.\n       Using the same path-group timing example, the focused view shows one path group at a time.\n You can change the focus to a different path group by selecting a different scenario or path group name in the drop-down menus at the top of the report.\n See  Focused View Drop-down Menu Filtering the Detailed Data Detailed views offer filters to focus on specific data.\n Some views have default filters that are shown automatically.\n For example, the following detailed views have default scenario and path group filters: Path Type Details, Path Group Details, and Logic Level Details.\n You can modify the default filters by  Removing the filter by clicking the X symbol before the filter name  Changing the filter value  Enable and disable the filter by clicking anywhere on the filter but the X symbol or value field You can also apply custom filters to the detailed data.\n To create a custom filter, 1.\n Click the Filter button to display the Add Filter fields, which are shown in the following figure: 2.\n Define the filter by defining the filter criteria and selecting the datasets to include in the results.\n       To define the filter, select the column and comparison type, and then specify the comparison value.\n  To perform a numeric comparison, select one of the following comparison types:!= <=, or >= and specify a numeric value in the Value field.\n  To perform a string comparison, select the contains comparison type and specify a string value in the Value field.\n  To perform a regular expression comparison, select the regexp comparison type and specify a regular expression in the Value field.\n  To filter based on the available values of the selected column, select the enum comparison type, which populates the Value field with a drop-down menu that contains the available values, and then enable one or more of the displayed values.\n When a value is enabled, a check mark is displayed before the value.\n To enable or disable a value, highlight the value by clicking it or navigating to it by using the Up and Down arrows, and then press Enter, which toggles the value status.\n You can also type a string in the Value field to filter the available values in the drop-down menu.\n To dismiss selected values, click the X symbol.\n 3.\n Apply the filter by clicking the green check mark.\n For example, to filter the Path Group Details view by displaying the path groups in the func@cworst scenario, define the filter as shown in the following figure:"}
{"instruction": "How do I Reporting Available Sites for Placing ECO Cells", "input": "", "output": "During the timing ECO flow, you can add buffers to a net and optimize its fanout by using the split_fanout command.\n With this command, you must specify  The net to optimize by specifying its name by using the -net option or its driver by using the -driver option  The library cell to use by using the -lib_cell option        The method in which to optimize the net by specifying one of the following:  A maximum fanout constraints for the net by using the -max_fanout option  The load pins or ports to buffer by using the -load option When you use this option, you can specify the logical hierarchy to add the buffer by using the -hierarchy option.\n In addition, you can  Add the buffers on the existing route topology of the net by using the -on_route option If you use this option with the -load option, you cannot specify the -hierarchy option.\n  Split the fanout of a routed net that is not fully connected to the terminals of the driver or loads of the net by specifying a maximum distance between terminals of the driver or load pins and the incomplete route using the -max_distance_for_incomplete_route option.\n If the tool is unable to find a routed segment or finds more than one routed segment within the specified distance from the unconnected terminal, it issues an error message.\n  Avoid placement blockages and macro cells when placing buffers by using the -respect_blockages option  Specify a name prefix for the new cells and nets by using -cell_prefix and -net_prefix options By default, the tool uses eco_cell and eco_net as the name prefix for the new cells and nets."}
{"instruction": "How do I Via Ladder Association File", "input": "", "output": "The command generates two via ladder rule files:  An XML file that provides information about the via ladder rules The XML file has the following syntax: <EmRule> <!\n template definition > <Template name=\" rule_name \" for_electro_migration=\"true\" for_high_performance=\"true\"> <Layer name=\" layer_name \" row_number=\" cuts_per_row...\n more layers </Template> \u2026 more templates  </EmRule> Each Template section specifies the template for a via ladder rule.\n Within a template section, the via ladder structure is specified by specifying the number of cuts per row for each metal layer involved in the via ladder.\n By default, the generated XML file is named auto_perf_via_ladder_rule.xml.\n To specify a different name, use the -xml_file option.\n  A script file that defines the via ladder rules The script file contains a create_via_rule command for each template defined in the XML file, as well as set_attribute commands to set the appropriate via rule attributes.\n By default, the generated script file is named auto_perf_via_ladder_rule.tcl.\n To specify a different file name, use the -rule_file option."}
{"instruction": "How do I Relative Placement Flow", "input": "", "output": "Along with being technology-independent and having the ability to improve routability, relative placement provides the following benefits:  Reduces the placement search space in critical areas of the design, which improves the predictability of QoR (wire length, timing, power, area) and congestion.\n  Maintains relative placement during placement, optimization, clock tree synthesis, and routing.\n  Provides a method for maintaining structured placement for legacy or intellectual property (IP) designs.\n        Handles flat and hierarchical designs.\n  Allows sizing of relative placement cells while maintaining relative placement."}
{"instruction": "How do I Resizing Cells", "input": "", "output": "The Fusion Compiler tool supports using ECO scripts to make changes in the netlist during design planning.\n You can use the  write_split_net_eco command to push up the branching of a physical multiple- fanout net to the top level  write_push_down_eco command to push down a tree of standard cells one level  write_spare_ports_eco command to create spare ports and nets on a child block For more information, see the Generating ECO Scripts for Netlist Editing topic in the Fusion Compiler Design Planning User Guide"}
{"instruction": "How do I Implementing Power Aware Flexible H-Tree", "input": "", "output": "To improve clock QoR, latency, and skew, you can build multiple global trees in different sections of the floorplan by using the -htree_sections option with the set_regular_multisource_clock_tree_options command.\n The following figure shows a regular multisource clock where the global clock structure does not have a clock mesh.\n The global clock structure consists only of an H-tree, which directly drives the tap drivers.\n        MSCTS Multiple H-Tree You can define the different sections in the floorplan by using a symmetric configuration of tap drivers or only tap locations by coordinates, as shown in the following example: set_regular_multisource_clock_tree_options - htree_sections [ list \\ [list -section_name \"sectionA_name\" -tap_boundary {{315.8800 1275.8000} {1750.0400 2187.1000}} -prefix \"sectionA\" -tap_boxes {4 4}] \\ [list -section_name \"sectionB_name\" -tap_locations {{700472} {1350 472}} -prefix \"section\"]]  MSCTS Multiple H-Tree"}
{"instruction": "How do I Performing Boundary Cell Insertion", "input": "", "output": "dense tap array is a tap cell array whose tap distance is smaller than the tap distance used by the create_tap_cells command.\n To insert a dense tap array, use the create_dense_tap_cells command.\n You must specify the following information:  The tap cell used in the tap wall (the -lib_cell option) You must specify the same tap cell as when you ran the create_tap_cells command.\n By default, the tool uses the following naming convention for inserted tap cells: tapfiller!\n library_cell_name number Use the -separator option to change the separator character from its default of \"!.\" To identify the tap cells inserted in a specific run, use the -prefix option to specify a prefix string.\n When you use this option, the tool uses the following naming convention: tapfiller!\n prefix library_cell_name number  The tap distance (the -distance option) The specified distance must be smaller than the tap distance used by the create_tap_cells command.\n  The tap insertion region (the -bbox option) You specify the tap insertion region by specifying the lower-left and upper-right corners of its bounding box: -bbox {{ llx lly } { urx ury }} The create_dense_tap_cells command first removes the existing tap cells that are completely within the tap insertion region, and then reinserts the tap cells with the specified tap distance to create a denser array in the specified region.\n To avoid tap rule violations, specify the same tap cell insertion pattern as was used by the create_tap_cells command.\n The create_dense_tap_cells command supports the following options to control the tap cell insertion: -offset -pattern -skip_fixed_cells, and -at_distance_only.\n For details about these options, see the man page.\n After insertion, the command fixes the placement of the inserted tap cells."}
{"instruction": "How do I Mapping of Scan Cells", "input": "", "output": "To specify that the tool maps sequential elements exactly as described in the RTL, set the compile.seqmap.exact_map application option to true.\n When this feature is enabled, the tool disables mapping to sequential library cells with inverted outputs or unused ports."}
{"instruction": "How do I Performing PG Electromigration Analysis", "input": "", "output": "When analysis is complete, the tool saves the analysis results and logs in the working directory.\n For more information about the working directories, see Specifying RedHawk and RedHawk-SC Working Directories The analyze_rail command generates the following files to invoke the RedHawk tool for running rail analysis:  The RedHawk run script (analyze_rail.tcl)  The RedHawk GSR configuration file (*.gsr) The analyze_rail command generates the following file to invoke the RedHawk-SC tool for running rail analysis:  The RedHawk-SC Python script (analyze_rail.py) The RedHawk or RedHawk-SC tool generates the following files in the in- design.redhawk/ design_name.result (or in-design.redhawk_sc/ design_name.result in RedHawk-SC flow) directory when rail analysis is complete:  The RedHawk analysis log (analyze_rail.log.*)  Timing window file (*.sta)  Signal SPEF files (*.spef)  DEF/LEF files (*.def, *.lef) for the full-chip analysis When voltage drop analysis is complete, the tool saves the analysis results (*.result) in the design_name directory under the working directory.\n The rail analysis results are used to display maps and query attributes to determine the problem areas with large voltage drops.\n        To reload the rail results for displaying maps in the GUI, use the open_rail_result command.\n For example, fc_shell> open_rail_result REDHAWK_RESULT For more information about displaying maps, see Displaying Maps in the GUI  To write out the rail analysis results, use the report_rail_result command.\n You must specify the result type with the -type option.\n For example, to write out the voltage drop values for the current block, set the -type option with the voltage_drop_or_rise argument.\n For more information, see Writing Analysis and Checking Reports  To query the results stored in the current rail results cache, use the get_attribute commands.\n For more information, see Querying Attributes Note: RedHawk Fusion does not support results that are generated by the RedHawk tool outside of the Fusion Compiler environment.\n The RedHawk dump icc2_result command works only within the Fusion Compiler session."}
{"instruction": "How do I Defining Placement Attractions", "input": "", "output": "To remove placement bounds from a block, use the remove_placement_blockage comandcomandsremove_placement_blockage remove_bounds command.\n To remove all placement bounds from a block, use the -all option.\n To remove specific placement bounds from a block, specify the placement bound names."}
{"instruction": "How do I Mapping ECO Cells to Specific Spare Cells", "input": "", "output": "You can define mapping rules for programmable spare cells to control the  Overlapping of spare cells and PG nets  Splitting of multiple-height and merging of single-height spare cells  Compatibility of the different types of spare cells To do so, use the set_programmable_spare_cell_mapping_rule command before you run the place_freeze_silicon command.\n To report or remove the mapping rules you specify, use the report_programmable_spare_cell_mapping_rule command."}
{"instruction": "How do I Enabling IC Validator Multicore Processing", "input": "", "output": "To run an IC Validator In-Design command, you must specify the location of the IC Validator executable by setting the ICV_HOME_DIR environment variable.\n You can set this variable in your.cshrc file.\n To specify the location of the IC Validator executable, use commands similar to those shown in the following example: setenv ICV_HOME_DIR /root_dir/icv set path = ($path $ICV_HOME_DIR/bin/LINUX.64) You must ensure that the version of the IC Validator executable that you specify is compatible with the Fusion Compiler version that you are using.\n To report the version compatibility, use the report_versions command.\n For more information about the IC Validator tool, see the IC Validator documentation, which is available on SolvNet."}
{"instruction": "How do I Signoff ECO Flow", "input": "", "output": "Use this flow if your cell placement is fixed, and you can only change the metal and via mask patterns.\n This flow is recommended if you have taped out your design and you want to avoid the expense of generating a whole new mask set.\n To perform the freeze silicon ECO flow, your block must contain spare cells.\n You can add spare cells to a block, anytime during the design flow, by using one of the following methods:  Manually instantiate spare cells, as described in Manually Instantiating Spare Cells  Automatically add spare cells after placement by using the add_spare_cells command, as described in Automatically Adding Spare Cells  Add programmable spare cells during the chip finishing stage by using the create_stdcell_fillers, as described in Adding Programmable Spare Cells The freeze silicon ECO flow consists of the following steps: 1.\n Enable ECO changes in the freeze silicon mode by setting the design.eco_freeze_silicon_mode application option to true 2.\n Update the design with the ECO changes by using one of the following methods:  Using the eco_netlist command, as described in Making ECO Changes Using the eco_netlist Command  Using netlist editing Tcl commands, as described in Making ECO Changes Using Netlist Editing Commands 3.\n Analyze the mapping of ECO cells to spare cells by using the check_freeze_silicon command.\n 4.\n Automatically map all the ECO changes to spare cells by using the place_freeze_silicon command or manually map each ECO cell to a specific spare cell by using the map_freeze_silicon command, as described in Placing and Mapping ECO Cells to Spare Cells 5.\n Update the routing by using the route_eco command, as described in Performing ECO Routing, or by manually rerouting the affected nets."}
{"instruction": "How do I Creating Clock Straps", "input": "", "output": "To insert clock drivers from the top level of a design with multiple levels of physical hierarchy, 1.\n Enable the lower-level physical blocks for editing by using the set_editability command.\n 2.\n Enable clock driver insertion for multiple levels of physical hierarchy by setting the cts.multisource.enable_mlph_flow application option to true 3.\n Insert clock drivers by using the create_clock_drivers command.\n When inserting clock drivers to lower-level blocks, the tool reuses existing clock ports.\n If there are no existing clock ports, the tool creates new ports, if it is allowed.\n You can allow new ports to be added to a block by using use the set_freeze_ports command."}
{"instruction": "How do I Generating Rail Analysis Script Files", "input": "", "output": "Before analyzing voltage drop and current density violations in a design using the RedHawk/RedHawk-SC Fusion capability, use the set_app_options command to specify the location of the libraries and the required input files.\n Table 66 lists the application options for specifying design and input data for running RedHawk/RedHawk-SC rail analysis within the Fusion Compiler environment.\n Table 66 Application Options for Specifying Design and Input Data Application Option Description rail.lib_files   fc_shell> set_app_options \\ -name rail.lib_files \\ -value {test1.lib test2.lib} rail.tech_file    fc_shell> set_app_options \\ -name rail.tech_file \\ -value ChipTop.tech   fc_shell> set_app_options \\ -name rail.3dic_enable_die\\ -value {DIE1 true DIE2 true...}       Table 66 Application Options for Specifying Design and Input Data (Continued) Application Option Description    fc_shell> set_app_options \\ -name rail.instance_power_file\\ -value {E1 FILE1 DIE2 FILE2...} rail.apl_files    fc_shell> set_app_options \\ -name rail.apl_files -value \\ {cell.current current \\ cell.cdev cap} rail.switch_model_files    fc_shell> set_app_options \\ -name rail.switch_model_files \\ -value {switch.model} rail.macro_models   fc_shell> set_app_options \\ -name rail.macro_models \\ -value {gds_cell1 mm_dir1 \\ gds_cell2 mm_dir2} rail.lef_files   rail.def_files   rail.pad_files   create_taps  rail.sta_file     rail.spef_files            Table 66 Application Options for Specifying Design and Input Data (Continued) Application Option Description rail.effective_resistance_ instance_file   rail.generate_file_type    tcl  set_user_input.tcl python input_files.py       fc_shell> set_app_options \\ -name rail.generate_file_type -value python rail.generate_file_variab les               fc_shell> set_app_options \\ -name rail.generate_file_variables -value \\ {PLOC ploc LEF lef_files DEF def_files \\ SPEF spef_files TWF tw_files} rail.enable_new_rail_scena rio  false   rail.enable_parallel_run_r ail_scenario         Table 66 Application Options for Specifying Design and Input Data (Continued) Application Option Description rail.scenario_name        rail.dump_icc2_results_st yle -value 2.0"}
{"instruction": "How do I Limiting the Latency Adjustment Values", "input": "", "output": "Applying useful skew techniques during datapath optimization to improve the timing QoR by taking advantage of the positive slack and adjusting the clock arrival times of registers is referred to as concurrent clock and data (CCD) optimization.\n Concurrent clock and data optimization is enabled by default for the compile_fusion and clock_opt commands.\n To enabled it for the route_opt command, set the route_opt.flow.enable_ccd application option to true You can change the default behavior of concurrent clock and data optimization by  Limiting the Latency Adjustment Values  Excluding Boundary Paths  Excluding Specific Path Groups  Excluding Specific Scenarios  Excluding Specific Sinks  Controlling Timing Optimization Effort  Controlling Hold Time Optimization Effort  Controlling the Adjustment of I/O Clock Latencies  Performing Dynamic-Voltage-Drop-Driven Concurrent Clock and Data Optimization During the route_opt Command  Specifying Optimization Targets at the Preroute Stage  Specifying Optimization Targets at the Postroute Stage  Enabling Buffer Removal at the Postroute Stage  Reporting Concurrent Clock and Data Timing  Scaling CCD Offsets to New Scenarios for Reporting Timing  Skewing Latch and Discrete Clock Gates"}
{"instruction": "How do I Changing the QoR Display Style", "input": "", "output": "By default, the first row of data is your baseline run against which all other runs are compared.\n The name of the baseline run is highlighted in gold to mark it as the \"golden,\" or baseline, result.\n       The shading of non-baseline cells indicates the direction and degree by which the data differs from the baseline:  Red indicates a degradation compared to the baseline; green indicates an improvement compared to the baseline  Lighter shading represents a smaller difference compared to the baseline; darker shading represents a larger difference compared to the baseline To view the delta thresholds corresponding to each shade, hover the cursor over the column headers: To change your baseline run, click the Runs button and select a new run as the baseline from the Baseline Run column, as shown in the following figure:       See Also  Changing the QoR Display Style  Sorting and Filtering the Data  Exploring the Detailed Comparison Data"}
{"instruction": "How do I Placement-Aware Clock Gating", "input": "", "output": "In the Fusion Compiler flow, clock gating works in conjunction with multibit register mapping.\n The multibit packing ratio has a higher precedence than clock gating.\n The following command enables multibit mapping: fc_shell> set_app_options -name compile.flow.enable_multibit -value true Clock-gating cells are not inserted to gate preinstantiated multibit registers.\n For more information on clock gating reports, see Reporting Clock-Gating Results"}
{"instruction": "How do I Physical Constraints Extracted From the DEF File", "input": "", "output": "To analyze the input DEF files before annotating the floorplan information on the block, enable check-only mode by using the -syntax_only option.\n The check-only mode provides diagnostic information about the correctness and integrity of the DEF file.\n The check-only mode does not annotate any floorplan information onto the block.\n fc_shell> read_def -syntax_only block.def  fc_shell> check_duplicates -remove The check_duplicates -remove command ensures that no duplicate shapes and vias are present in the block due to the execution of the read_def command."}
{"instruction": "How do I Verifying the Clock Trees", "input": "", "output": "In some blocks, the clocks have endpoints that are structurally impossible to balance.\n The Fusion Compiler tool can automatically detect endpoints with balancing conflicts and derive an ignore pin to resolve the conflict.\n This capability is enabled by default for the synthesize_clock_trees and clock_opt commands.\n To disable this capability, set the cts.common.enable_auto_exceptions application option to false       With this capability, the tool detects and fixes the following types of balancing conflicts:  Internal sink pins of modules that have a combinational clock path If a module contains both internal sink pins and combinational clock paths, it is impossible to balance the internal sink pins.\n To resolve the balancing conflict, the tool defines the internal sink pins as ignore pins.\n For example, assume that you have a module that contains a combinational path in the clock network, as shown in figure (a) in, which is represented by the extracted timing model (ETM) shown in figure (b).\n  Module With Combinational Clock Path  Clk_out  Clk (a) Netlist (b) ETM with check pin Clk   Clk_out Clk_check_pin_1 Combinational delay In this case, the internal check pin inside the ETM has delays coming from the ETM timing library; these delays are considered by clock tree synthesis as sinks and are balanced with other sinks at the top level.\n If the path to the check pin is the shortest path, top-level clock tree synthesis cannot insert buffers on the shortest path and therefore leaves a large skew at the top level.\n Setting the internal check pin as an ignore pin resolves the skew issue coming from the ETM.\n  Sink pins that cannot be balanced simultaneously If a block contains two clocks that drive common sinks, one of which is a clock pin that is the parent of another sink, due to a generated clock, it is impossible to simultaneously balance the parent and child clock pins with another common sink.\n To resolve the balancing conflict, the tool defines the parent clock pin as an ignore pin.\n For example, in, FF_gen/CLK and FF0_1-10/CLK are sinks of clkb, FF1_1-10/CLK and FF0_1-10/CLK are sinks of clka, and FF_gen/CLK is the parent of       FF1_1-10/CLK.\n In this case, the tool defines FF_gen/CLK as an ignore pin for clkb to resolve the balancing conflict.\n  Clock Pins That Cannot Be Balanced Simultaneously"}
{"instruction": "How do I Analyzing Clock Trees in the GUI", "input": "", "output": "The timing characteristics of the clock network are important in any high-performance design.\n To obtain detailed information about the clock networks in the current block, use the report_clock_timing comandcomandsreport_clock_timing report_clock_timing command.\n You must use the -type option to specify the type of report to generate.\n The report_clock_timing command can generate the following types of reports:  Single-clock local skew To generate a single-clock local skew report, use the -type skew option.\n  Interclock skew To generate an interclock skew report, use the -type interclock_skew option.\n  Latency To generate a latency report, use the -type latency option.\n        Transition To generate a transition time report, use the -type transition option."}
{"instruction": "How do I Enabling Power Optimization for the clock_opt Command", "input": "", "output": "To check your leakage power, use the report_power command.\n By default, state- dependent leakage power is used to calculate leakage power.\n To change the leakage power calculation mode, set the power.leakage_mode application option.\n For example, to set the application option to average do the following: fc_shell> set_app_options -name power.leakage_mode -value average To report threshold voltage usage, use the report_threshold_voltage_groups command.\n Classification is based on the threshold_voltage_group attribute from the reference library cells or the default_threshold_voltage_group attribute on the library.\n You can also set the attributes on the objects by using the set_attribute command.\n You can generate several types of reports with the report_threshold_voltage_groups command.\n For example, the summary report (using the -summary option) lists the number, area, and percentage of cells in each threshold voltage group.\n The default report (without using any options) shows the cell count and area based on the following groups: repeater, combinational, register, sequential, clock network, and physical-only cells.\n For the repeater, register, and clock network cell groups, you can obtain additional information by using the -detailed option.\n The -hierarchy option of the report_threshold_voltage_groups command generates a report based on the design hierarchy.\n The report shows the cell count, area, and area ratio for a top-level block and its subblocks.\n The -hierarchy option is mutually exclusive with the other report types, but can be used with the -datapath_cells_only and -standard_cells_only options.\n You can restrict the hierarchy-based report to specific threshold voltage groups by using the -hier_vt_groups option.\n You can also specify to report only the top-level cells that contain more than a specified number of constituent cells by using the -hier_threshold option.\n These options must be used with the -hierarchy option.\n If you specify a cell list, a separate report is created for each top-level cell in the list."}
{"instruction": "How do I Implementing a Structural Multisource Clock Tree Using", "input": "", "output": "In a structural multisource clock tree, the subtrees are driven directly by the clock mesh or through predefined drivers.\n These drivers are in turn driven by global clock distribution structure such as a clock mesh and H-tree.\n The local subtrees of a structural multisource clock tree are optimized by clock cell merging, splitting, sizing, and relocation, while preserving the levels of the user-defined clock tree structure.\n To implement a structural multisource clock tree, perform the following steps: 1.\n Specify your clock tree constraints and settings.\n 2.\n For multivoltage designs,  Enable multivoltage support by using the following application option setting: fc_shell> set_app_options \\ -name cts.multisource.enable_full_mv_support -value true  Enable the addition of physical feedthrough cells, which belong to a voltage area physically but not logically, by using the following application option setting: fc_shell> set_app_options \\ -name opt.common.allow_physical_feedthrough -value true  Run the check_mv_design command and fix all multivoltage issues 3.\n Enable cell electromigration fixing by setting the cts.multisource.cell_em_aware application option to true 4.\n Create the clock mesh by using the create_clock_straps command, as described in Creating Clock Straps 5.\n Insert the mesh drivers by using the create_clock_drivers command, as described in Inserting Clock Drivers       6.\n Build the global clock tree structure by using the synthesize_multisource_global_clock_trees command, as described in Synthesizing the Global Clock Trees 7.\n Model the delays of the mesh nets using set_annotated_delay and set_annotated_transition commands.\n The loads of the clock mesh are not finalized until the local subtrees are synthesized in step 6 and routed to the mesh in step 7.\n Therefore the clock mesh can only be analyzed after you complete those steps.\n However, to prevent the tool from seeing a very large delay through the clock mesh when synthesizing the local subtrees, annotate a realistic delay and transition value on the clock mesh net.\n 8.\n Build the local subtrees by using the synthesize_multisource_clock_subtrees command, as described in Building the Local Clock Subtree Structures 9.\n Route the connections between the clock mesh and the local subtrees by using the route_clock_straps command, as described in Routing to Clock Straps 10.\n Analyze the clock mesh by using the analyze_subcircuit command, as described in Analyzing the Clock Mesh 11.\n Synthesize the entire clock tree, from the clock root, by using the clock_opt -from build_clock -to route_clock command.\n During this step, the tool fixes DRC violations beyond ignore exceptions of the multisource clock tree, if applicable.\n You can also synthesize other clocks in the design that are not synthesized.\n 12.\n Analyze the clock tree results as described in Analyzing the Clock Tree Results"}
{"instruction": "How do I Reading Designs Using the VCS Command-line Options", "input": "", "output": "To enable the tool to automatically read a list of RTL files, specify the -autoread option with the analyze command, as shown in the following example.\n When this option is specified, the RTL language used to analyze the RTL files is automatically determined by the file name extension.\n To control the file name extensions, use the following application options: hdlin.autoread.verilog_extensions hdlin.autoread.sverilog_extensions hdlin.autoread.vhdl_extensions, and hdlin.autoread.exclude_extensions set DESIGN_NAME top analyze -autoread -top ${DESIGN_NAME} ${RTL_FILE_LIST} elaborate ${DESIGN_NAME} elaborate -hdl_library TOP_HDL_LIB top set_top_module ${DESIGN_NAME}"}
